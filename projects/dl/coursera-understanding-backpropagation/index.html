<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Understanding backpropagation in neural networks | Atma's blog</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#d4567b">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../rss.xml">
<link rel="canonical" href="https://atmamani.github.io/projects/dl/coursera-understanding-backpropagation/">
<link rel="icon" href="../../../scatter-16.png" sizes="16x16">
<link rel="icon" href="../../../scatter-32.png" sizes="32x32">
<link rel="icon" href="../../../scatter-64.png" sizes="64x64">
<link rel="icon" href="../../../scatter-128.png" sizes="128x128">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-113202945-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113202945-1');
</script><meta name="author" content="Atma Mani">
<meta property="og:site_name" content="Atma's blog">
<meta property="og:title" content="Understanding backpropagation in neural networks">
<meta property="og:url" content="https://atmamani.github.io/projects/dl/coursera-understanding-backpropagation/">
<meta property="og:description" content="This page talks about the formula, intuition and the mechanics of the backpropagation optimization function. 
A neural network is composed of several layers of neurons connected to one another. Each n">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-05-02T14:22:08-07:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://atmamani.github.io/">

                <span id="blog-title">Atma's blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Projects <b class="caret"></b></a>
            <ul class="dropdown-menu">
<li>
<a href="../../math/">Learn math with Python</a>
                    </li>
<li>
<a href="../../stats/">Learn stats with Python</a>
                    </li>
<li>
<a href="../../ml/">Machine Learning projects</a>
                    </li>
<li>
<a href="../">Deep Learning projects</a>
                    </li>
<li>
<a href="../../spatial/">Spatial analysis</a>
                    </li>
<li>
<a href="../../thermal/">Thermal remote sensing</a>
                    </li>
<li>
<a href="../../mwrs/">Microwave remote sensing</a>
                    </li>
<li>
<a href="../../fun/">Fun projects</a>
            </li>
</ul>
</li>
<li>
<a href="../../../blog/">Blog</a>
                </li>
<li>
<a href="../../../talks/">Talks</a>
                </li>
<li>
<a href="../../../books/">Books</a>
            </li>
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Cheatsheets <b class="caret"></b></a>
            <ul class="dropdown-menu">
<li>
<a href="../../../cheatsheets/learning-resources/">General learning resources</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_datatypes/">Python datatypes</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_numeric_datatypes/">Python numeric datatypes</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_conditional_execution/">Python conditional execution</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_functions/">Python functions</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_iterations/">Python iterations</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_exception_handling/">Python exception handling</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_classes/">Python classes</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_memory/">Python memory, ref counts, garbage collection</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_mutability/">Python - mutability and immutability</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_language_optimizations/">Python - language optimizations</a>
                    </li>
<li>
<a href="../../../images/regex-mindmap.jpg">Python regex</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/conda_cheat_sheet/">Conda basics</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/numpy/numpy_cheat_sheet_1/">NumPy basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/numpy/numpy_cheat_sheet_2/">NumPy array slicing, dicing, searching</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_1/">Pandas basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_2/">Pandas multilevel index, <br>    missing data, aggregation, merging</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_3/">Productivity with Pandas</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_data_viz_1/">Pandas data visualization</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_1/">Matplotlib basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_2/">Matplotlib log scales, ticks, scientific</a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_geo/">Geographical plotting with Basemap - matplotlib toolkit</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_1/">Seaborn dist, joint, pair, rug plots</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_2/">Seaborn categorical - bar, count, <br>violin, strip, swarm plots</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_3/">Seaborn matrix, regression - heatmap, <br> cluster, regression</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_4/">Seaborn grids &amp; custom - pair, facet grids <br> customization</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_cufflinks_cheat_sheet_1/">Plotly introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_cufflinks_cheat_sheet_2/">Plotly - interactive plotting</a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_geographical_plotting_1/">Plotly - geographic plotting</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/r_cheat_sheet_1/">R basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/octave-1/">Octave / MATLAB - basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/octave-2/">Octave - handling data</a>
                    </li>
<li>
<a href="../../../cheatsheets/js_essentials/">Javascript essentials</a>
                    </li>
<li>
<a href="../../../cheatsheets/latex-1/">Latex introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/docker-1/">Docker introduction</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-1/">GeoPandas - IO, projections, plotting</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-2/">GeoPandas - GP, IO, interactive plotting, geocoding</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-3/">GeoPandas - spatial overlays, topology</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-4/">GeoPandas - PySal, OSM data IO</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/open-geo-raster-1/">Rasterio - IO, plotting, histograms</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/open-geo-raster-2/">Rasterio - hyperspectral, SAM</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/reading-multidim-data-using-opengeotools/">Reading multi-dimensional data using open geo tools</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/postgis-1/">PostGIS - introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/postgis-2/">PostGIS - SQLAlchemy, GeoAlchemy, GeoPandas</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/web-development/">Web Development</a>
                    </li>
<li>
<a href="../../../cheatsheets/html/">HTML basics</a>
            </li>
</ul>
</li>
<li>
<a href="../../../apps/">Apps</a>
                </li>
<li>
<a href="../../../rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- DuckDuckGo custom search --><form method="get" id="search" action="https://duckduckgo.com/" class="navbar-form pull-left">
<input type="hidden" name="sites" value="https://atmamani.github.io/"><input type="hidden" name="k8" value="#444444"><input type="hidden" name="k9" value="#D51920"><input type="hidden" name="kt" value="h"><input type="text" name="q" maxlength="255" placeholder="Search…" class="span2" style="margin-top: 4px;"><input type="submit" value="DuckDuckGo Search" style="visibility: hidden;">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Understanding backpropagation in neural networks</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>This page talks about the formula, intuition and the mechanics of the backpropagation optimization function. </p>
<p>A neural network is composed of several layers of neurons connected to one another. Each neuron is activated when the sum of weights multiplied by the values of the neurons in previous layer is over a threshold when you pass it through a sigmoid / logit function. The threshold is usually <code>0.5</code>. In order for a neural network to get trained, the weights of all the different neurons need to be adjusted to yield the minimal loss. <strong>Backpropagation</strong> is the process by which the weights are adjusted during the training process. To understand how this works, we need to understand how to calculate the <strong>loss/cost function</strong> of a neural network.</p>
<h3 id="cost-function-of-a-neural-network">Cost function of a neural network<a href="#cost-function-of-a-neural-network" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>The cost function of a NN builds on the cost function of a logistic regression. Recall from <a href="../../ml/coursera-logistic-reg/">Logistic regression page</a>, the cost function of a logistic regression is</p>
<p>$$
J(\theta) = -\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}log(h_{\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}
$$</p>
<p>where $m$ is number of training samples and $n$ is number of parameters.</p>
<p>A neural network is like a multi-class logistic regression. Thus, we need to sum over $K$ classes. In reality, $K$ refers to the number of nodes in the output layer. Thus, in binary classification, although the number of classes is <code>2</code>, the number of nodes in output layer is just <code>1</code>. Thus <code>K=1</code>. The cost function is given as</p>
<p>$$
J(\Theta) = \frac{-1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left[ y_{k}^{(i)}log(h_{\theta}(x_{k}^{(i)})) + (1-y_{k}^{(i)})log(1-h_{\theta}(x_{k}^{(i)}))\right]
$$</p>
<p>where
 - $m$ is number of training samples. $i=1\;to\;m$
 - $K$ is number of nodes in output layer and $k=1 \; to \; K$</p>
<p>The above formula is <strong>cost without penalty or regularization</strong>. It is similar to the cost of a <strong>multiclass logistic regression</strong>. In practice, for logistic regression, we compute cost for training data as a whole. In NN, we compute cost for each training sample per by summing the loss over each class, each sample and finally dividing by number of samples.</p>
<h4 id="cost-function-of-neural-network-with-regularization">Cost function of neural network with regularization<a href="#cost-function-of-neural-network-with-regularization" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>We penalize a network for the number of hidden layers and the number of nodes in each layer, simple. The penalty is simply the square of weights for each node, summed up and divided by twice the number of training samples. We finally multiply this result by a factor denoted by $\lambda$. Thus</p>
<p>$$
J(\Theta) = \frac{-1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} 
            \left[ 
                y_{k}^{(i)}log(h_{\theta}(x_{k}^{(i)})) + 
            (1-y_{k}^{(i)})log(1-h_{\theta}(x_{k}^{(i)}))
            \right] + 
            \frac{\lambda}{2m} \left[<br>
                                    \sum_{l=1}^{L-1}\sum_{r=1}^{s_{l}}\sum_{c=1}^{s_{l+1}} (\Theta_{r,c}^{l})^{2}
                                \right]
$$</p>
<p>where </p>
<ul>
<li>$m$ is number of training samples. $i=1\;to\;m$</li>
<li>$K$ is number of nodes in output layer and $k=1 \; to \; K$</li>
<li>$L$ is number of layers in the network and $l=1 \; to \; L-1$ as we exclude the input layer</li>
<li>$s_{l}$ is number of nodes in a given layer $l$. </li>
<li>In the penalty term we sum from for each row and column of the weight matrix. That is, from $r=1 \; to \; s_{l}$ and $c=1 \; to \; s_{l+1}$ <code>r</code> and <code>c</code> represent the rows and columns of the weight matrix for each layer. </li>
</ul>
<p>Intuitively, the number rows in a weight matrix depends on number of nodes in that layer. The number of columns however, depends on number of nodes in the next layer (hence summing up to $s_{l+1}$).</p>
<p>From the course, the cost function is given as below. Notice a slight change in the suffix used for penalty. I changed it for clarity in my formula above.</p>
<p><img src="../../../images/coursera-neural-nets-backprop1.png" width="500"></p>
<p>The $\Theta$ (weight) matrix is 2D for each layer. When you stack all layers together, it becomes a 3D matrix.</p>
<h3 id="backpropagation">Backpropagation<a href="#backpropagation" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>The cost function gives us a single unit of measure on how well the network performs. The value of the cost function itself is not to be interpreted as such (unlike RMSE or $R^{2}$ which give you an interpretable result). However, you can compare the performance of different hyperparameters or weights by comparing the resulting loss reported by the cost function. </p>
<p>Thus, the objective of backpropagation is to minimize the cost function described above using partial derivatives. For each training sample, we compute an error matrix, which reflects the difference between predicted and output values. It is straightforward to compute the error for the last layer, which is the difference between expected and predicted outputs. Progressively, we compute the error for each of the previous layers.</p>
<p>Let us start by reviewing the steps in <strong>forward propagation</strong>. The vectorized implementation of it is given in the slide below:</p>
<p><img src="../../../images/coursera-neural-nets-backprop2.png" width="500"></p>
<p>Next, we start by computing the error in the last layer. </p>
<p>$$
\delta_{j}^{(4)} = a_{j}^{(4)} - y_{j}
$$$$
Where \; j \; stands \; for \; each \; node \; in \; the \; layer.\; In \; vectorized \; terms
$$$$
\delta^{(4)} = a^{(4)} - y
$$</p>
<p>The error vector is now simply the difference between the expected and predicted vectors. We can compute the delta terms for the earlier layers of the network as follows:</p>
<p>$$
\delta^{(3)} = (\Theta^{(3)})^{T}\delta^{(4)}.*g'(z^{(3)})
$$</p>
<p>Where $g'$ for <code>l=3</code> is</p>
<p>$$
g' = a^{(2)}.*(1-a^{(2)})
$$</p>
<p>and so on..</p>
<p>We don't calculate error for layer 1. Thus no $\delta^{(1)}$. The equations above give the formulae to calculate error for one training sample. When calculating the backprop for many training samples, we first perform forward propagation, compute the weights, then immediately, perform backprop to calculate the error terms and update the weights immediately. Next, we proceed to the next training sample.</p>
<p><img src="../../../images/coursera-neural-nets-backprop3.png" width="500"></p>
<p>The figure above shows the vectorized implementations of calculating the error term for all samples as a 2D matrix $\Delta_{ij}^{(l)}$, where $i=1 to m$ represents individual training samples and $j$ and $l$ stand for number of nodes in each layer and layer number respectively. The vectorized implementation is given by</p>
<p>$$
\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^{T}
$$</p>
<p>The matrixes $D^(l)_{ij}$ represent the <strong>gradient matrices</strong>.</p>
<h4 id="backpropagation-intuition">Backpropagation Intuition<a href="#backpropagation-intuition" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>Consider the following example of a simple neural net.</p>
<p><img src="../../../images/coursera-neural-nets-backprop4.png" width="500"></p>
<p>The error term for last layer is calculated as $\delta_{1}^{(4)} = y^{(i)} - a_{1}^{(4)}$. Then, the error term for previous layers is given by</p>
<p>$$
\delta_{1}^{(3)} = \Theta_{1,1}^{(3)}\delta_{1}^{(4)}
$$
$$
\delta_{2}^{(3)} = \Theta_{1,2}^{(3)}\delta_{1}^{(4)}
$$</p>
<p>Similarly, for the previous layer,
$$
\delta_{1}^{(2)} = \Theta_{1,1}^{(2)}\delta_{1}^{(3)} + \Theta_{2,1}^{(2)}\delta_{2}^{(3)}
$$</p>
<p>Thus, to calculate the errors in each of the nodes, we need to errors in the terms to the right. In other words, we calculate the error terms from the right to the left, in reverse of the network direction, thus the name backpropagation (of errors).</p>
<h3 id="backpropagation-implementation">Backpropagation implementation<a href="#backpropagation-implementation" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p><strong>Unrolling</strong>: The input layer of a neural network is usually a 1-D array. Thus, when training on image data (which is at a minimum 2D), the images are flattened to a long 1D array (vector), where each row is spliced one after another. This process is called <strong>unrolling</strong>. The inverse transformation is called <strong>rolling</strong> which returns the matrix back to its n-D form.</p>
<h4 id="gradient-checking">Gradient checking<a href="#gradient-checking" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>Gradient checking is an alternate method of getting gradients through numerical approximation techniques. The idea is, consider the cost function is a curve and is a function of your weights $\theta$. The objective is to compute the gradient of the curve at different locations. This numerical approximation method computes the gradient by treating infinitesimally small segments as a straight line and compute the slope of such segments as an approximation for the gradient.</p>
<p>Thus:</p>
<p>$$
gradApprox \; = \; \frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2\epsilon}
$$</p>
<h4 id="random-initialization">Random initialization<a href="#random-initialization" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>In logistic regression and linear regression, we initialized the weights to 0 and were able to compute the cost and gradients and finally arrive at the best result. However, this is not possible with neural networks. When weights are all <code>0</code> (or a constant), then each neuron is essentially identical to the rest. Thus, we need to break this symmetry by initializing the weights randomly.</p>
<p>In practice, a lower and upper bound (denoted as $[-\epsilon_{init}, \epsilon_{init}]$) is chosen and weights are assigned randomly between these values. A rule of thumb for calculating the limits is to base it on the number of units in the network as:</p>
<p>$$
\epsilon_{init} = \frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}
$$</p>
<p>where
 - $L_{in} = s_{l}$ and $L_{out} = s_{l+1}$ which are the number of units in the current and next layer.</p>
<h3 id="network-architecture-choices">Network architecture choices<a href="#network-architecture-choices" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>Architecture choices refer to number of hidden layers and number of nodes / neurons in each layer etc. In general, the number of nodes in input layer depend on number of parameters (or pixels in an image) and the number nodes in output layer depend on number of classes. </p>
<p>As to the hidden layers, having <code>1</code> hidden layer is pretty common, the next common is if you are having multiple hidden layers, then the number of nodes in each hidden layer is maintained the same. By and large, the more hidden units / nodes in each layer, the better it is.</p>
<p>The number of nodes in hidden layers is comparable or (slightly more than) to number of units in input layer.</p>
    </div>
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2022         <a href="mailto:atma.mani@outlook.com">Atma Mani</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>