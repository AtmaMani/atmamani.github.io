<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>02_stat_learning | Atma’s blog</title>
<link href="../../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#d4567b">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../../rss.xml">
<link rel="canonical" href="https://atmamani.github.io/projects/stats/islr/02_stat_learning/">
<link rel="icon" href="../../../../scatter-16.png" sizes="16x16">
<link rel="icon" href="../../../../scatter-32.png" sizes="32x32">
<link rel="icon" href="../../../../scatter-64.png" sizes="64x64">
<link rel="icon" href="../../../../scatter-128.png" sizes="128x128">
<!--[if lt IE 9]><script src="../../../../assets/js/html5.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-113202945-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113202945-1');
</script><meta name="author" content="Atma Mani">
<meta property="og:site_name" content="Atma's blog">
<meta property="og:title" content="02_stat_learning">
<meta property="og:url" content="https://atmamani.github.io/projects/stats/islr/02_stat_learning/">
<meta property="og:description" content="Chap 2: Start Learning¶ToC

Prediction
Reducible and irreducible errors


Inference
Parametric and unparametric methods
Parametric methods of estimating f
Non parametric methods of estimating f


Gene">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-11-26T14:18:46-08:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://atmamani.github.io/">

                <span id="blog-title">Atma’s blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Projects <b class="caret"></b></a>
            <ul class="dropdown-menu">
<li>
<a href="../../../math/">Learn math with Python</a>
                    </li>
<li>
<a href="../../">Learn stats with Python</a>
                    </li>
<li>
<a href="../../../ml/">Machine Learning projects</a>
                    </li>
<li>
<a href="../../../dl/">Deep Learning projects</a>
                    </li>
<li>
<a href="../../../spatial/">Spatial analysis</a>
                    </li>
<li>
<a href="../../../thermal/">Thermal remote sensing</a>
                    </li>
<li>
<a href="../../../mwrs/">Microwave remote sensing</a>
                    </li>
<li>
<a href="../../../fun/">Fun projects</a>
            </li>
</ul>
</li>
<li>
<a href="../../../../blog/">Blog</a>
                </li>
<li>
<a href="../../../../talks/">Talks</a>
                </li>
<li>
<a href="../../../../books/">Books</a>
            </li>
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Cheatsheets <b class="caret"></b></a>
            <ul class="dropdown-menu">
<li>
<a href="../../../../cheatsheets/learning-resources/">General learning resources</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/python/python_cheat_sheet_1/">Python basics, comprehensions</a>
                    </li>
<li>
<a href="../../../../cheatsheets/python/python_cheat_sheet_2/">Python OO and exception handling</a>
                    </li>
<li>
<a href="../../../../cheatsheets/python/python_cheat_sheet_3/">Python map, reduce, lambdas</a>
                    </li>
<li>
<a href="../../../../images/regex-mindmap.jpg">Python regex</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/conda_cheat_sheet/">Conda basics</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/numpy/numpy_cheat_sheet_1/">NumPy basics</a>
                    </li>
<li>
<a href="../../../../cheatsheets/numpy/numpy_cheat_sheet_2/">NumPy array slicing, dicing, searching</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/pandas/pandas_cheat_sheet_1/">Pandas basics</a>
                    </li>
<li>
<a href="../../../../cheatsheets/pandas/pandas_cheat_sheet_2/">Pandas multilevel index, <br>    missing data, aggregation, merging</a>
                    </li>
<li>
<a href="../../../../cheatsheets/pandas/pandas_cheat_sheet_3/">Productivity with Pandas</a>
                    </li>
<li>
<a href="../../../../cheatsheets/pandas/pandas_data_viz_1/">Pandas data visualization</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/matplotlib/matplotlib_1/">Matplotlib basics</a>
                    </li>
<li>
<a href="../../../../cheatsheets/matplotlib/matplotlib_2/">Matplotlib log scales, ticks, scientific</a>
                    </li>
<li>
<a href="../../../../cheatsheets/matplotlib/matplotlib_geo/">Geographical plotting with Basemap - matplotlib toolkit</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/seaborn/seaborn_cheat_sheet_1/">Seaborn dist, joint, pair, rug plots</a>
                    </li>
<li>
<a href="../../../../cheatsheets/seaborn/seaborn_cheat_sheet_2/">Seaborn categorical - bar, count, <br>violin, strip, swarm plots</a>
                    </li>
<li>
<a href="../../../../cheatsheets/seaborn/seaborn_cheat_sheet_3/">Seaborn matrix, regression - heatmap, <br> cluster, regression</a>
                    </li>
<li>
<a href="../../../../cheatsheets/seaborn/seaborn_cheat_sheet_4/">Seaborn grids <span class="amp">&amp;</span> custom - pair, facet grids <br> customization</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/plotly/plotly_cufflinks_cheat_sheet_1/">Plotly introduction</a>
                    </li>
<li>
<a href="../../../../cheatsheets/plotly/plotly_cufflinks_cheat_sheet_2/">Plotly - interactive plotting</a>
                    </li>
<li>
<a href="../../../../cheatsheets/plotly/plotly_geographical_plotting_1/">Plotly - geographic plotting</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/r_cheat_sheet_1/">R basics</a>
                    </li>
<li>
<a href="../../../../cheatsheets/octave-1/">Octave / MATLAB - basics</a>
                    </li>
<li>
<a href="../../../../cheatsheets/octave-2/">Octave - handling data</a>
                    </li>
<li>
<a href="../../../../cheatsheets/js_essentials/">Javascript essentials</a>
                    </li>
<li>
<a href="../../../../cheatsheets/latex-1/">Latex introduction</a>
                    </li>
<li>
<a href="../../../../cheatsheets/docker-1/">Docker introduction</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/geopandas-1/">GeoPandas - IO, projections, plotting</a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/geopandas-2/">GeoPandas - GP, IO, interactive plotting, geocoding</a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/geopandas-3/">GeoPandas - spatial overlays, topology</a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/geopandas-4/">GeoPandas - PySal, OSM data IO</a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/open-geo-raster-1/">Rasterio - IO, plotting, histograms</a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/open-geo-raster-2/">Rasterio - hyperspectral, SAM</a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/reading-multidim-data-using-opengeotools/">Reading multi-dimensional data using open geo tools</a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/postgis-1/">PostGIS - introduction</a>
                    </li>
<li>
<a href="../../../../cheatsheets/open-geo/postgis-2/">PostGIS - SQLAlchemy, GeoAlchemy, GeoPandas</a>
                    </li>
<li>
<a href="../../../../"><hr></a>
                    </li>
<li>
<a href="../../../../cheatsheets/web-development/">Web Development</a>
                    </li>
<li>
<a href="../../../../cheatsheets/html/">HTML basics</a>
            </li>
</ul>
</li>
<li>
<a href="../../../../apps/">Apps</a>
                </li>
<li>
<a href="../../../../rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- DuckDuckGo custom search --><form method="get" id="search" action="https://duckduckgo.com/" class="navbar-form pull-left">
<input type="hidden" name="sites" value="https://atmamani.github.io/"><input type="hidden" name="k8" value="#444444"><input type="hidden" name="k9" value="#D51920"><input type="hidden" name="kt" value="h"><input type="text" name="q" maxlength="255" placeholder="Search…" class="span2" style="margin-top: 4px;"><input type="submit" value="DuckDuckGo Search" style="visibility: hidden;">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">02_stat_learning</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Chap-2:-Start-Learning">Chap 2: Start Learning<a class="anchor-link" href="#Chap-2:-Start-Learning">¶</a>
<a href="#Chap-2:-Start-Learning" class="headerlink" title="Permalink to this heading">¶</a></h2>
<p><strong>ToC</strong></p>
<ul>
<li>
<a href="#Prediction">Prediction</a><ul>
<li><a href="#Reducible-and-Irreducible-errors">Reducible and irreducible errors</a></li>
</ul>
</li>
<li><a href="#Inference">Inference</a></li>
<li>
<a href="#Parametric-and-Unparametric-methods-for-Estimating-f">Parametric and unparametric methods</a><ul>
<li><a href="#Parametric-methods-for-estimating-f">Parametric methods of estimating f</a></li>
<li><a href="#Non-parametric-methods-for-estimating-f">Non parametric methods of estimating f</a></li>
</ul>
</li>
<li><a href="#General-concepts">General concepts</a></li>
<li>
<a href="#Model-accuracy---Regression-problems">Model accuracy - Regression problems</a><ul>
<li><a href="#Bias-variance-trade-off">Bias variance trade-off</a></li>
</ul>
</li>
<li><a href="#Model-accuracy---Classification-problems">Model accuracy - Classification problems</a></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Prediction">Prediction<a class="anchor-link" href="#Prediction">¶</a>
<a href="#Prediction" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>In a prediction / regression problem, the inputs (denoted by <code>X</code>) are called as <code>predictors</code>, <code>independent variables</code>, <code>features</code> and the predicted variable is called as <code>response</code>, <code>dependent variable</code> and is denoted by <code>Y</code>.</p>
<p>The relationship betwen input and predicted is represented as</p>
$$
Y = f(X) + \epsilon
$$<p>where $f$ is some fixed, unknown function that is to be determined. $\epsilon$ is <strong>random error</strong> term that is independent of <code>X</code> and has <strong>zero mean</strong>.</p>
<p>In reality, $f$ may depend on more than 1 input variable $X$, for instance 2. In this case, $f$ is a <code>2D</code> surface that is fit. In general, the process of <strong>estimating</strong> $f$ is <strong>statistical learning</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Reducible-and-Irreducible-errors">Reducible and Irreducible errors<a class="anchor-link" href="#Reducible-and-Irreducible-errors">¶</a>
<a href="#Reducible-and-Irreducible-errors" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>Since $f$ and $Y$ cannot be <strong>calculated</strong>, the best we can get is to <strong>estimate</strong> them. Thus, the estimates are called $\hat f$ and $\hat Y$</p>
$$
\hat Y = \hat f(X)
$$<p>The accuracy of $\hat Y$ depends on <strong>reducible</strong> and <strong>irreducible</strong> errors. The error in prediction of $\hat f$ is <strong>reduible</strong> and can be improved wth more data and better models. However, $\hat Y$ is also a function of $\epsilon$ which is <strong>irreducible</strong>. Thus, the best our predictions can get is</p>
$$
\hat Y = f(X)
$$<p>Focus of Statistical learning is to estimating $f$ as $\hat f$ with least <strong>reducible</strong> error. However, the accuracy of $\hat Y$ will always be controlled by <strong>irreducible</strong> and <strong>unknown</strong> error $\epsilon$.</p>
<p>In <strong>prediction problems</strong>, $\hat f$ can be treated as <strong>blackbox</strong> as we are only interested in predicting $Y$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Inference">Inference<a class="anchor-link" href="#Inference">¶</a>
<a href="#Inference" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>We are interested in understanding how each of the different $X_{1}… X_{p}$ affect the dependent variable $Y$, hence the name <strong>inference</strong>. Here, $\hat f$ <strong>cannot be treated as blackbox</strong> and we need to know its exact form. Some questions that are sought to be answered through inference:</p>
<ul>
<li>which predictor variables are associated with the response?</li>
<li>what is the relationship b/w response and each predictor?</li>
<li>is the relationship linear or is more complicated?</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Parametric-and-Unparametric-methods-for-Estimating-f">Parametric and Unparametric methods for Estimating f<a class="anchor-link" href="#Parametric-and-Unparametric-methods-for-Estimating-f">¶</a>
<a href="#Parametric-and-Unparametric-methods-for-Estimating-f" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>The observations for X and Y can be written as ${(x_{1}, y_{1}),(x_{2}, y_{2}),…,(x_{n}, y_{n})}$ where each <em>x</em> has many predictor variables that can be written as $x_{i} = (x_{i1},x_{i2},..,x_{ip})^{T}$. The goal is to find $\hat f$ such that $Y \approx \hat f (X)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Parametric-methods-for-estimating-f">Parametric methods for estimating f<a class="anchor-link" href="#Parametric-methods-for-estimating-f">¶</a>
<a href="#Parametric-methods-for-estimating-f" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>Parametric methods take a <strong>model based</strong> approach (<strong>deterministic</strong>).
We make an assumption about the functional form of <em>f</em> (whether it is linear, non linear, higher order, logistic etc). For instance, if we assume that <em>f</em> is linear, then</p>
$$
Y \approx f(X) = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + .. + \beta_{p}X_{p}
$$<p>we only need to find $p+1$ coefficients. Through training or fitting (using methods like <em>ordinary least squares</em>), we can estimate the coefficients.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Notes</strong></p>
<ul>
<li>parametric methods are an approximation of the true functional form of <em>f</em>.</li>
<li>simpler (lower order, less flexible) models may lead to poorer estimates of <em>f</em>
</li>
<li>more flexible (higher order, complex) models may lead to <strong>overfitting</strong>.</li>
<li>Since the model is trained on a subset of values, it might be very different from true nature of <em>f</em>. Hence the model developed is only valid for the range of data it was trained on.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Non-parametric-methods-for-estimating-f">Non parametric methods for estimating f<a class="anchor-link" href="#Non-parametric-methods-for-estimating-f">¶</a>
<a href="#Non-parametric-methods-for-estimating-f" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>Non parametric methods <strong>avoid assuming the functional form of f</strong>. However, these methods require <strong>a very large</strong> number of observations since they do not try to reduce the phenomenon to a model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="General-concepts">General concepts<a class="anchor-link" href="#General-concepts">¶</a>
<a href="#General-concepts" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p><strong>Model interpretability and complexity</strong>: The more complex a model is (higher order more flexible models, decision trees..), the less interpretable it is.</p>
<p><strong>Supervised vs Unsupervised algorithms</strong>: Supervised methods are used when both the <code>predictor</code> and <code>response</code> variables can be measured and data is available. Unsupervised methods are used when little is known about the data and only <code>predictor</code> variables are available. Unsupervised are best when put to classification / clustering problems.</p>
<p><strong>Regression vs Classification</strong>: When the <code>response</code> variable is <code>quantitative</code> and continuous, the problem is considered a regression. When the <code>response</code> is <code>qualitative</code> and falls within categories, then the problem is a classification problem. Howerver, this distinction is not really solid as many algorithms can be used for both.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Model-accuracy---Regression-problems">Model accuracy - Regression problems<a class="anchor-link" href="#Model-accuracy---Regression-problems">¶</a>
<a href="#Model-accuracy---Regression-problems" class="headerlink" title="Permalink to this heading">¶</a></h3>
<h4 id="Measuring-quality-of-fit">Measuring quality of fit<a class="anchor-link" href="#Measuring-quality-of-fit">¶</a>
<a href="#Measuring-quality-of-fit" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>Here the deviation between <code>predicted</code> and actual values is measured. In regression, <code>Mean Squared Error (MSE)</code> is commonly used.</p>
$$
MSE = \frac{1}{n} \sum_{i=1}^{n}(y_{i} - \hat f(x_{i}))^2
$$<p>The MSE obtained is called <strong>training MSE</strong>. When used against unseen test data, we get <strong>test MSE</strong>. Our objective is to choose the method with lowest <em>test MSE</em>. There is no guarantee that a low training MSE will yield a low test MSE.</p>
<p>A fundamental property in statistical learning is <strong>as model flexibility increases, training MSE might decrease, but test MSE might not</strong>. When a given learning method yields a <strong>small training MSE</strong> but a <strong>large test MSE</strong>, we are <strong>overfitting</strong> the data. This is because, our data might have noise from <strong>irreducible error</strong> and the model is trying to fit it.</p>
<h4 id="Bias-variance-trade-off">Bias variance trade-off<a class="anchor-link" href="#Bias-variance-trade-off">¶</a>
<a href="#Bias-variance-trade-off" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>If you plot the <strong>test MSE</strong> against <strong>model flexibility</strong>, it follows a <em>U shaped curve</em>. Thus it first reduces then increases. The expected test MSE for observation $x_{0}$ $E(y_{0} - \hat f(x_{0}))$ can be decomposed to <code>3</code> fundamental quantities: (a) the <strong>variance of</strong> $\hat f(x_{0})$, (b) <strong>the squared bias</strong> of $\hat f(x_{0})$ and (c) <strong>variance of irreducible error</strong> $\epsilon$. Thus:</p>
$$
E(y_{0} - \hat f(x_{0}))^{2} = Var(\hat f(x_{0})) + [Bias(\hat f(x_{0}))]^{2} + Var(\epsilon)
$$<p>Thus, to reduce the expected test MSE, we need to reduce both the <strong>variance of $\hat f$</strong> and <strong>bias of $\hat f$</strong>.</p>
<p><strong>Variance</strong> refers to the amount by which $\hat f$ would change if we estimated it using a different training dataset. Ideally, $\hat f$ should not change much if a slightly different data set is used. A statistical learning method with high variance would yield a very different $\hat f$ for different training data sets. <strong>Higher the model flexibility, the higher is its variance</strong> as the model closely fits the training data.</p>
<p><strong>Bias</strong> refers to the error introduced by approximating a real-life problem. Generally, <strong>higher model flexibility, the lower is the bias</strong>. Thus as we use more flexible methods, the variance will increase and bias would decrease.</p>
<p>As model <strong>flexibility increases</strong>, the <strong>bias reduces faster</strong> than the rate at which <strong>variance increases</strong>. Thus, the <strong>test MSE</strong> drops initially before increasing (<code>U shape</code>). This relationship is called the <strong>bias variance trade-off</strong> and the objective is to pick the model flexibility that has the least of both.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Model-accuracy---Classification-problems">Model accuracy - Classification problems<a class="anchor-link" href="#Model-accuracy---Classification-problems">¶</a>
<a href="#Model-accuracy---Classification-problems" class="headerlink" title="Permalink to this heading">¶</a></h3>
<h4 id="Measuring-quality-of-classification">Measuring quality of classification<a class="anchor-link" href="#Measuring-quality-of-classification">¶</a>
<a href="#Measuring-quality-of-classification" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p><code>error rate</code> is used to quantify the errors in classification. It is the ratio of <code>sum of misclassifications</code> to <code>number of observations</code>.</p>
$$
error rate = \frac{1}{n}\sum_{i=1}^{n}I(y_{i} \ne \hat y_{i})
$$<p>where $\hat y_{i}$ is predicted class label for ith observation using $\hat f$. When computed against training data, this yields <strong>training error rate</strong>. When computed for test data, this yields <strong>test error rate</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Bayes-Classifier">Bayes Classifier<a class="anchor-link" href="#Bayes-Classifier">¶</a>
<a href="#Bayes-Classifier" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>Bayes classifier is a simple but idealistic classifier. It assigns each observation to the <strong>most likely class</strong> given its predictor class. This can be written using <strong>conditional probability</strong> as below:</p>
$$
P(Y=j \ | \ X=x_{0})
$$<p>Thus, the error rate with <strong>Bayes classifier</strong> becomes the average of (1 - max probability for different classes). The Bayes error rate is analogous to *<em>irreducible error</em>.</p>
<h4 id="KNN-classifier">KNN classifier<a class="anchor-link" href="#KNN-classifier">¶</a>
<a href="#KNN-classifier" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>In reality, Bayes classifier is not possible as the conditional probability is unknown. Instead, algorithms attempt to derive the conditional probability. One such is KNN.</p>
<p>The KNN classifier identifies <strong>K</strong> points in training data that are closest to test observation $x_{0}$, represented at $N_{0}$. It then <strong>estimates</strong> conditional proabability for class $j$ as the fraction of points in $N_{0}$ whose classes equal $j$. This can be written as:</p>
$$
P(Y = j \ | \ X = x_{0}) = \frac{1}{K} \sum_{i \in N_{0}} I(y_{i} = j)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When <strong>K=1</strong>, the decision boundary is of highest flexibility and <strong>overfits</strong> with low bias and high variance. When <strong>K</strong> is very large, it <strong>underfits</strong> with low flexibility. It has high bias and low variance. As in regression, with classification, increasing flexibility reduces the training error, but does not affect test error rate.</p>

</div>
</div>
</div>
</div>
    </div>
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2021         <a href="mailto:atma.mani@outlook.com">Atma Mani</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>