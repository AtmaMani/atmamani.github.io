<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Understanding Gradient Descent | Atma's blog</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#d4567b">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../rss.xml">
<link rel="canonical" href="https://atmamani.github.io/projects/ml/coursera-gradient-descent/">
<link rel="icon" href="../../../scatter-16.png" sizes="16x16">
<link rel="icon" href="../../../scatter-32.png" sizes="32x32">
<link rel="icon" href="../../../scatter-64.png" sizes="64x64">
<link rel="icon" href="../../../scatter-128.png" sizes="128x128">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-113202945-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113202945-1');
</script><meta name="author" content="Atma Mani">
<meta property="og:site_name" content="Atma's blog">
<meta property="og:title" content="Understanding Gradient Descent">
<meta property="og:url" content="https://atmamani.github.io/projects/ml/coursera-gradient-descent/">
<meta property="og:description" content="Linear regression
Cost functions
The linear regression estimation function (hypothesis function) can be written as $h_{\theta} (x) = \theta_{0} + \theta_{1}x$. The cost function for this equation can ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-05-02T14:22:08-07:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://atmamani.github.io/">

                <span id="blog-title">Atma's blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Projects <b class="caret"></b></a>
            <ul class="dropdown-menu">
<li>
<a href="../../math/">Learn math with Python</a>
                    </li>
<li>
<a href="../../stats/">Learn stats with Python</a>
                    </li>
<li>
<a href="../">Machine Learning projects</a>
                    </li>
<li>
<a href="../../dl/">Deep Learning projects</a>
                    </li>
<li>
<a href="../../spatial/">Spatial analysis</a>
                    </li>
<li>
<a href="../../thermal/">Thermal remote sensing</a>
                    </li>
<li>
<a href="../../mwrs/">Microwave remote sensing</a>
                    </li>
<li>
<a href="../../fun/">Fun projects</a>
            </li>
</ul>
</li>
<li>
<a href="../../../blog/">Blog</a>
                </li>
<li>
<a href="../../../talks/">Talks</a>
                </li>
<li>
<a href="../../../books/">Books</a>
            </li>
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Cheatsheets <b class="caret"></b></a>
            <ul class="dropdown-menu">
<li>
<a href="../../../cheatsheets/learning-resources/">General learning resources</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_datatypes/">Python datatypes</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_conditional_execution/">Python conditional execution</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_functions/">Python functions</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_iterations/">Python iterations</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_exception_handling/">Python exception handling</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_classes/">Python classes</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_memory/">Python memory, ref counts, garbage collection</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_mutability/">Python - mutability and immutability</a>
                    </li>
<li>
<a href="../../../images/regex-mindmap.jpg">Python regex</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/conda_cheat_sheet/">Conda basics</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/numpy/numpy_cheat_sheet_1/">NumPy basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/numpy/numpy_cheat_sheet_2/">NumPy array slicing, dicing, searching</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_1/">Pandas basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_2/">Pandas multilevel index, <br>    missing data, aggregation, merging</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_3/">Productivity with Pandas</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_data_viz_1/">Pandas data visualization</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_1/">Matplotlib basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_2/">Matplotlib log scales, ticks, scientific</a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_geo/">Geographical plotting with Basemap - matplotlib toolkit</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_1/">Seaborn dist, joint, pair, rug plots</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_2/">Seaborn categorical - bar, count, <br>violin, strip, swarm plots</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_3/">Seaborn matrix, regression - heatmap, <br> cluster, regression</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_4/">Seaborn grids &amp; custom - pair, facet grids <br> customization</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_cufflinks_cheat_sheet_1/">Plotly introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_cufflinks_cheat_sheet_2/">Plotly - interactive plotting</a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_geographical_plotting_1/">Plotly - geographic plotting</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/r_cheat_sheet_1/">R basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/octave-1/">Octave / MATLAB - basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/octave-2/">Octave - handling data</a>
                    </li>
<li>
<a href="../../../cheatsheets/js_essentials/">Javascript essentials</a>
                    </li>
<li>
<a href="../../../cheatsheets/latex-1/">Latex introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/docker-1/">Docker introduction</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-1/">GeoPandas - IO, projections, plotting</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-2/">GeoPandas - GP, IO, interactive plotting, geocoding</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-3/">GeoPandas - spatial overlays, topology</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-4/">GeoPandas - PySal, OSM data IO</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/open-geo-raster-1/">Rasterio - IO, plotting, histograms</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/open-geo-raster-2/">Rasterio - hyperspectral, SAM</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/reading-multidim-data-using-opengeotools/">Reading multi-dimensional data using open geo tools</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/postgis-1/">PostGIS - introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/postgis-2/">PostGIS - SQLAlchemy, GeoAlchemy, GeoPandas</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/web-development/">Web Development</a>
                    </li>
<li>
<a href="../../../cheatsheets/html/">HTML basics</a>
            </li>
</ul>
</li>
<li>
<a href="../../../apps/">Apps</a>
                </li>
<li>
<a href="../../../rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- DuckDuckGo custom search --><form method="get" id="search" action="https://duckduckgo.com/" class="navbar-form pull-left">
<input type="hidden" name="sites" value="https://atmamani.github.io/"><input type="hidden" name="k8" value="#444444"><input type="hidden" name="k9" value="#D51920"><input type="hidden" name="kt" value="h"><input type="text" name="q" maxlength="255" placeholder="Search…" class="span2" style="margin-top: 4px;"><input type="submit" value="DuckDuckGo Search" style="visibility: hidden;">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Understanding Gradient Descent</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <h2 id="linear-regression">Linear regression<a href="#linear-regression" class="headerlink" title="Permalink to this heading">¶</a></h2>
<h3 id="cost-functions">Cost functions<a href="#cost-functions" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>The linear regression estimation function (hypothesis function) can be written as $h_{\theta} (x) = \theta_{0} + \theta_{1}x$. The cost function for this equation can be written as</p>
<p>$$
J(\theta_{0}, \theta_{1}) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2
$$</p>
<p>The core of the cost function is the squared difference between prediction and truth in $y$. In other words, this can be written as</p>
<p>$$
J(\theta_{0}, \theta_{1}) = \frac{1}{2m} \sum_{i=1}^{m} (\hat y_{i} - y_{i})^2
$$</p>
<p>We square the error as that is a common way of measuring loss. We sum the loss for each value of $y$, get the average and then half the average. The <strong>squared error cost function</strong> is pretty common and works well for linear regressions. We halve the error function for convenience later when we take the derivative of the function and the <code>2</code> gets cancelled out. However, the objective of the estimation function is choosing values of $\theta_{0} and \theta_{1}$ such that they minimize the cost function.</p>
<h3 id="minimizing-cost-functions">Minimizing cost functions<a href="#minimizing-cost-functions" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>To understand how to minimize the cost functions, let us simplify the above regression to a state where intercept is <code>0</code>. Thus the estimation / hypothesis function changes to</p>
<p>$$
J(\theta_{1}) = \frac{1}{2m} \sum_{i=1}^{m}(h_{\theta}(x_{i}) - y_{i})^2
$$
which reduces to
$$
J(\theta_{1}) = \frac{1}{2m} \sum_{i=1}^{m}(\theta_{1}x_{i} - y_{i})^2
$$</p>
<p>Next, we solve for the cost function for different values of $\theta_{1}$ and plot them in a graph as shown below:</p>
<p><img src="../../../images/ml-cost-function1.png" width="550"></p>
<p>The cost function takes shape of a parabola, with a clear minima.</p>
<h4 id="minimizing-multidimensional-cost-functions">Minimizing multidimensional cost functions<a href="#minimizing-multidimensional-cost-functions" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>In the previous example, we assumed $\theta_{0}=0$. If that was not the case, then we need to minimize the residuals / cost function while changing values of both the variables. This leads to a 3D plot as shown below:
<img src="../../../images/ml-cost-function2.png" width="450"></p>
<p>Another way to represent the cost function is via contour plots as shown below:</p>
<p><img src="../../../images/ml-cost-function3.png" width="550"></p>
<p>Points along same contour have same values of error/loss for different values of $\theta_{0}$ and $\theta_{1}$. The objective is to find the lowest point in the contours - which has the lowest error/loss and find its parameters.</p>
<p>In a multiple regression problem, there are several predictor variables. Thus the loss function is hard to visualize as there now multiple dimensions, one for coefficient of predictor variable + intercept term. An algorithmic way of minimizing the cost function is called <strong>gradient descent</strong>.</p>
<h3 id="gradient-descent">Gradient descent<a href="#gradient-descent" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>Gradient descent is an algorithm that is used to minimize the loss function. It is also used widely in many machine learning problems. The idea is, to start with arbitrary values for $\theta_{0}$ and $\theta_{1}$, keep changing them little by little until we reach minimal values for the loss function $J(\theta_{0}, \theta_{1})$.</p>
<p>The following graphic shows the distribution of the loss function. The GD algorithm starts at an arbitrary point for $\theta_{0}$ and $\theta_{1}$, takes small steps, at each step determining the direction of travel and stride length, and arrives as the <strong>local minima</strong>. The direction is determined by getting the slope of the tangent (derivative) at each point.</p>
<p><img src="../../../images/ml-gradient-descent1.png" width="350"></p>
<p>An interesting feature of GD is, if you started at a different point, you might end up at a <strong>different local minima</strong>. The definition of gradient descent for any <strong>arbitrary equation</strong> is:</p>
<p>$$
\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0}, \theta_{1}) \ (for \; j=0 \;and\; j=1) 
$$
We repeat the above equation until convergence. The $:=$ is assignment operator, $\partial$ is partial differential operator, $\alpha$ is the <strong>learning rate</strong>. $\alpha$ controls the <strong>stride length</strong> in the descent graphic.</p>
<p>Thus, when you have two coefficients, you would compute 
$$
temp0 := \theta_{0} - \alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0}, \theta_{1})
$$
$$
temp1 := \theta_{1} - \alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0}, \theta_{1})
$$
$$
\theta_{0} := temp0
$$
$$
\theta_{1}:= temp1
$$
<strong>Note</strong>: It is important to compute $\theta_{0}, \theta_{1}$ simultaneously (in parallel). You should not compute 1 and substitute its value when computing the next parameter. Intuitively, you are changing both $\theta_{0}, \theta_{1}$ instead of changing just $\theta_{0}$, then the other. This is the principle behind <strong>partial differential equations</strong> -&gt; you are differentiating multiple parameters at the same time, as opposed to <strong>ordinary differential equations</strong> where you differentiate just one variable.</p>
<h4 id="gradient-descent-intuition">Gradient descent intuition<a href="#gradient-descent-intuition" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>For simplicity, let us simplify our solver to minimize over just one coefficient $J(\theta_{1})$. Now strictly speaking, this is a <em>ordinary differential equation</em>, not <em>partial</em>. The equation now becomes</p>
<p>$$
repeat \; until \; convergence
\
\theta_{1} := \theta_{1} - \alpha \frac{d}{d\theta_{1}} J(\theta_{1})
$$</p>
<p>The shape of $J(\theta_{1})$ looks like below:</p>
<p><img src="../../../images/ml-gradient-descent2.png" width="350"></p>
<p>The $\frac{d}{d\theta_{1}} J(\theta_{1})$ term is the <strong>derivative</strong> and gives the slope of the tangent at each point. The direction of the slope changes depending on the position on the curve and will lead the iteration to local minima.</p>
<p>The learning rate $\alpha$ is multiplied by the slope / derivative term. A larger $\alpha$ will lead to an aggressive iteration which may overshoot the minima or even lead to run away divergence. A very small or conservative $\alpha$ will slow down the convergence or might settle for minor minimums as local minima.</p>
<p><img src="../../../images/ml-gradient-descent3.png" width="400"></p>
<p>Further, as we approach the local minima, the slope decreases. Thus even for a fixed $\alpha$, the rate of change will slow down in general, leading to a safe landing at minima. The slope at local minima is <code>0</code>. Thus, once reached, the second term of the equation (right of the minus sign) turns to <code>0</code> and the iteration as converged.</p>
<h4 id="gradient-descent-for-linear-regression">Gradient descent for linear regression<a href="#gradient-descent-for-linear-regression" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>The loss function $J(\theta_{0}, \theta_{1})$ can be expanded out with actual loss function equation from earlier. Thus now the differential equations become:</p>
<p>$$
repeat \; until \; convergence
$$
$$
\theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})
$$
$$
\theta_{1} := \theta_{1} - \alpha \frac{1}{m} \sum_{i=1}^{m} [(h_{\theta}(x_{i}) - y_{i})x_{i}]
$$</p>
<p>Here, m is number of training samples, $\theta_{0}$ is intercept and $\theta_{1}$ is the coefficient 1, $x_{i}$ and $y_{i}$ are values of the training data.</p>
<p>This process of using the full training sample for calculating the GD is called <strong>batch gradient descent</strong>. In the case of linear regression, the shape of the loss function is a <strong>convex function</strong> which looks like a <strong>bowl</strong>. There is only a global minima and <strong>no local minimas</strong>.</p>
    </div>
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2022         <a href="mailto:atma.mani@outlook.com">Atma Mani</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>