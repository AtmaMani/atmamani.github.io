<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Understanding logistic regression | Atma's blog</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#d4567b">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../rss.xml">
<link rel="canonical" href="https://atmamani.github.io/projects/ml/coursera-logistic-reg/">
<link rel="icon" href="../../../scatter-16.png" sizes="16x16">
<link rel="icon" href="../../../scatter-32.png" sizes="32x32">
<link rel="icon" href="../../../scatter-64.png" sizes="64x64">
<link rel="icon" href="../../../scatter-128.png" sizes="128x128">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-113202945-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113202945-1');
</script><meta name="author" content="Atma Mani">
<meta property="og:site_name" content="Atma's blog">
<meta property="og:title" content="Understanding logistic regression">
<meta property="og:url" content="https://atmamani.github.io/projects/ml/coursera-logistic-reg/">
<meta property="og:description" content="A logistic regression is a binary classification alogrithm. Its values are 0 for false cases and 1 for true cases. Before talking about logistic reg, let us consider why not to use linear regression f">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2022-06-08T10:27:44-07:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://atmamani.github.io/">

                <span id="blog-title">Atma’s blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Projects <b class="caret"></b></a>
            <ul class="dropdown-menu">
<li>
<a href="../../math/">Learn math with Python</a>
                    </li>
<li>
<a href="../../stats/">Learn stats with Python</a>
                    </li>
<li>
<a href="../">Machine Learning projects</a>
                    </li>
<li>
<a href="../../dl/">Deep Learning projects</a>
                    </li>
<li>
<a href="../../spatial/">Spatial analysis</a>
                    </li>
<li>
<a href="../../thermal/">Thermal remote sensing</a>
                    </li>
<li>
<a href="../../mwrs/">Microwave remote sensing</a>
                    </li>
<li>
<a href="../../cloud/">Cloud computing</a>
                    </li>
<li>
<a href="../../fun/">Fun projects</a>
            </li>
</ul>
</li>
<li>
<a href="../../../blog/">Blog</a>
                </li>
<li>
<a href="../../../talks/">Talks</a>
                </li>
<li>
<a href="../../../books/">Books</a>
            </li>
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Cheatsheets <b class="caret"></b></a>
            <ul class="dropdown-menu">
<li>
<a href="../../../cheatsheets/learning-resources/">General learning resources</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_datatypes/">Python datatypes</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_numeric_datatypes/">Python numeric datatypes</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_conditional_execution/">Python conditional execution</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_functions/">Python functions</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_iterations/">Python iterations</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_exception_handling/">Python exception handling</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_classes/">Python classes</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_memory/">Python memory, ref counts, garbage collection</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_mutability/">Python - mutability and immutability</a>
                    </li>
<li>
<a href="../../../cheatsheets/python/python_language_optimizations/">Python - language optimizations</a>
                    </li>
<li>
<a href="../../../images/regex-mindmap.jpg">Python regex</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/conda_cheat_sheet/">Conda basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/mamba_cheat_sheet/">Mamba basics</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/numpy/numpy_cheat_sheet_1/">NumPy basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/numpy/numpy_cheat_sheet_2/">NumPy array slicing, dicing, searching</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_1/">Pandas basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_2/">Pandas multilevel index, <br>    missing data, aggregation, merging</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_cheat_sheet_3/">Productivity with Pandas</a>
                    </li>
<li>
<a href="../../../cheatsheets/pandas/pandas_data_viz_1/">Pandas data visualization</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_1/">Matplotlib basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_2/">Matplotlib log scales, ticks, scientific</a>
                    </li>
<li>
<a href="../../../cheatsheets/matplotlib/matplotlib_geo/">Geographical plotting with Basemap - matplotlib toolkit</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_1/">Seaborn dist, joint, pair, rug plots</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_2/">Seaborn categorical - bar, count, <br>violin, strip, swarm plots</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_3/">Seaborn matrix, regression - heatmap, <br> cluster, regression</a>
                    </li>
<li>
<a href="../../../cheatsheets/seaborn/seaborn_cheat_sheet_4/">Seaborn grids <span class="amp">&amp;</span> custom - pair, facet grids <br> customization</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_cufflinks_cheat_sheet_1/">Plotly introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_cufflinks_cheat_sheet_2/">Plotly - interactive plotting</a>
                    </li>
<li>
<a href="../../../cheatsheets/plotly/plotly_geographical_plotting_1/">Plotly - geographic plotting</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/r_cheat_sheet_1/">R basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/octave-1/">Octave / <span class="caps">MATLAB</span> - basics</a>
                    </li>
<li>
<a href="../../../cheatsheets/octave-2/">Octave - handling data</a>
                    </li>
<li>
<a href="../../../cheatsheets/js_essentials/">Javascript essentials</a>
                    </li>
<li>
<a href="../../../cheatsheets/latex-1/">Latex introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/docker-1/">Docker introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/grep-1/">Grep introduction</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-1/">GeoPandas - <span class="caps">IO</span>, projections, plotting</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-2/">GeoPandas - <span class="caps">GP</span>, <span class="caps">IO</span>, interactive plotting, geocoding</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-3/">GeoPandas - spatial overlays, topology</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/geopandas-4/">GeoPandas - PySal, <span class="caps">OSM</span> data <span class="caps">IO</span></a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/open-geo-raster-1/">Rasterio - <span class="caps">IO</span>, plotting, histograms</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/open-geo-raster-2/">Rasterio - hyperspectral, <span class="caps">SAM</span></a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/reading-multidim-data-using-opengeotools/">Reading multi-dimensional data using open geo tools</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/postgis-1/">PostGIS - introduction</a>
                    </li>
<li>
<a href="../../../cheatsheets/open-geo/postgis-2/">PostGIS - SQLAlchemy, GeoAlchemy, GeoPandas</a>
                    </li>
<li>
<a href="../../../"><hr></a>
                    </li>
<li>
<a href="../../../cheatsheets/web-development/">Web Development</a>
                    </li>
<li>
<a href="../../../cheatsheets/html/"><span class="caps">HTML</span> basics</a>
            </li>
</ul>
</li>
<li>
<a href="../../../apps/">Apps</a>
                </li>
<li>
<a href="../../../rss.xml"><span class="caps">RSS</span> feed</a>

                
            </li>
</ul>
<!-- DuckDuckGo custom search --><form method="get" id="search" action="https://duckduckgo.com/" class="navbar-form pull-left">
<input type="hidden" name="sites" value="https://atmamani.github.io/"><input type="hidden" name="k8" value="#444444"><input type="hidden" name="k9" value="#D51920"><input type="hidden" name="kt" value="h"><input type="text" name="q" maxlength="255" placeholder="Search…" class="span2" style="margin-top: 4px;"><input type="submit" value="DuckDuckGo Search" style="visibility: hidden;">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Understanding logistic regression</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>A logistic regression is a binary classification alogrithm. Its values are <code>0</code> for false cases and <code>1</code> for true cases. Before talking about logistic reg, let us consider why not to use linear regression for classification. We could hypothetically use linear reg to predict class probabilities and could theoretically set a threshold, (usually <code>0.5</code>) above which we group to Class 1 and below which we group to Class 2. However this system has flaws. Consider the graphic below:</p>
<p><img src="../../../images/coursera-linearreg-for-classification.jpg" width="450"></p>
<p>The system works well with the hypothesis function in red, until a new valid but outlier data enters the training. Now the regression function is changed to blue and that changes how it classifies borderline cases. Further, regression does not lend well for multi-class problems. Further, regression is likely to predict <code>&gt;1</code> and <code>&lt;0</code> class probabilities which aren’t true.</p>
<h3 id="logistic-regression-model">Logistic regression model<a href="#logistic-regression-model" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>We represent the hypothesis function of logistic regression as $0 \le h_{\theta}(x) \le 1$. The hypothesis function can be represented as:</p>
<p>$$
h_{\theta}(x) = g(\theta^{T}x)
$$
$$
where \; g(z) = \frac{1}{1+e^{-z}} $$</p>
<p>$g(z)$ is a <code>sigmoid</code> function, also called a <code>logistic</code> function, from which the regression gets its name. The hypothesis function looks similar to that of a linear regression, except for the product with the sigmoid function. The shape of the sigmoid function is given by:</p>
<div class="code"><pre class="code literal-block"><span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">gz</span><span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="mi">0</span><span class="o">-</span><span class="n">vals</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">gz</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Sigmoid function'</span><span class="p">);</span>
</pre></div>

<p><img src="../../../images/coursera-sigmoid-fn.png"></p>
<p>As <code>z</code> reaches $\infty$, $g(z)$ asymptotes to 1. The values always range betwen <code>0 &lt; g(z) &lt; 1</code>. Thus, the hypothesis can be rewritten as </p>
<p>$$
h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} $$</p>
<p>$h_{\theta}(x)$ gives us the <strong>probability that output = 1</strong>. Thus, if $h_{\theta}{x} = 0.7$, that means a <code>70%</code> probability the output is <code>1</code> or a true case. Mathematically, this is represented as </p>
<p>$$
h_{\theta}(x) = P(y=1 | x;\theta) = 0.7
$$
which is read as <strong>“probability that y=1, given x, parametertized by $\theta$”</strong>. Since probability adds to <code>1</code>, we can say, the inverse, probability of <code>y=0</code> is <code>0.3</code>: </p>
<p>$$
h_{\theta}(x) = P(y=0 | x;\theta) = 0.3 $$</p>
<h4 id="decision-boundary-of-a-logistic-regression-model">Decision boundary of a logistic regression model<a href="#decision-boundary-of-a-logistic-regression-model" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p><img src="../../../images/coursera-logreg-decision-boundary.png" width="550"></p>
<p>The shape of the logistic function is such that, <code>g(z) &gt; 0.5</code> when <code>z &gt; 0</code> and <code>g(z) &lt; 0.5</code> when <code>z &lt; 0</code>, and <code>g(z) = 0.5</code> when <code>z=0</code>. The <code>z</code> can be expanded as $\theta^{T}x$. Additionally, we can simplify that <code>y=1</code> when $g(z) \ge 0.5$ and <code>y=0</code> when $g(z) &lt; 0.5$.</p>
<p>Now, consider this example dataset. The red cross show <code>y=1</code> case and blue circles show <code>y=0</code> case.:</p>
<p><img src="../../../images/coursera-logreg-decision-boundary2.png" width="550"></p>
<p>We can represent this as $h_{\theta}(x) = g(\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2})$. Solving for theta, say, we find that it is <code>[-3;1;1]</code>. Thus, from previous derivation, we know that <code>y=1</code> when $-3 + x_{1} + x_{2} \ge 0$ as <code>g(z) has to be &gt;= 0</code>. We can simplify it as shown in picture and derive the equation of the decision boundary which is sown in magenta on the pic. The following challenge will explain this better:</p>
<p><img src="../../../images/coursera-logreg-decision-boundary3.png" width="550"></p>
<h4 id="non-linear-decision-boundaries">Non linear decision boundaries<a href="#non-linear-decision-boundaries" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>We can represent non linearity in a linear model by adding additional parameters which are higher order representations of existing parameters, such as $h_{\theta}(x) = g(\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{1}^{2} + \theta_{4}x_{2}^{2})$ as shown here:</p>
<p><img src="../../../images/coursera-logreg-decision-boundary4.png" width="550"></p>
<p>To get the decision boundary, we need to solve for <code>g(z)=0.5</code> case which happens when <code>z=0</code>. Thus, in pic we solve for $x_{1}^{2} + x_{2}^{2} = 1$ which is the equation of a <code>circle</code>.</p>
<p>Using this theory, decision boundaries that take complex shapes can be represented using a linear model and can be solved using logistic regression.</p>
<h3 id="cost-function-for-logistic-regression">Cost function for logistic regression<a href="#cost-function-for-logistic-regression" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>The same cost function that we had for linear regression would apply for logistic regression, however, it leads to a non-convex loss function. The <span class="caps">GD</span> algorithm would fail to arrive at the global minima. Thus, we derive a new cost function as:</p>
<p>$$
J(\theta) = \frac{1}{m}\sum_{i=1}^{m}cost(h_{\theta}(x),y)
$$
where,
$$
cost(h_{\theta}(x), y) = \begin{cases}
                            -log(h_{\theta}(x)) <span class="amp">&amp;</span> y=1 \
                            -log(1-h_{\theta}(x)) <span class="amp">&amp;</span> y=0
                        \end{cases} $$</p>
<p>Note, <code>log(0) = inf</code>, <code>log(-1) = nan</code>, <code>log(1) = 0</code>. Thus, we can plot this cost term as follows:</p>
<div class="code"><pre class="code literal-block"><span class="n">xvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">y1vals</span> <span class="o">=</span> <span class="mi">0</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">xvals</span><span class="p">)</span>
<span class="n">y0vals</span> <span class="o">=</span> <span class="mi">0</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">xvals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xvals</span><span class="p">,</span> <span class="n">y1vals</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'y=1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xvals</span><span class="p">,</span> <span class="n">y0vals</span><span class="p">,</span> <span class="s1">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'y=0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Loss functions of logistic regression'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Hypothesis: $h</span><span class="se">\\</span><span class="s1">theta(x)$'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">);</span>
</pre></div>

<p><img src="../../../images/coursera-logistic-reg-costfn.png"></p>
<p>From the graph, when <code>h(x)=1</code> for <code>y=1</code>, the cost term is <code>0</code> (blue line). As predicted class reduces and approaches <code>0</code>, the cost raises to <code>inf</code>. Likewise, when <code>h(x)=0</code> for <code>y=0</code>, the cost is also <code>0</code>. However as the predicted value increases, the cost also increases penalizing the wrong prediction.</p>
<p>Instead of having two equations, the cost term can be simplified into the following:</p>
<p>$$
cost(h_{\theta}x,y) = -ylog(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x)) $$</p>
<p>plugging the cost term in the cost function, we get:</p>
<p>$$
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}y_{i}log(h_{\theta}(x_{i})) + (1-y_{i})log(1-h_{\theta}(x_{i})) $$</p>
<h3 id="gradient-descent-optimization-for-logistic-regression">Gradient descent optimization for logistic regression<a href="#gradient-descent-optimization-for-logistic-regression" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>To find the values of $\theta$ at the global optima, we need to differentiate the cost function written earlier. This turns out to be</p>
<p>repeat until convergence:
$$
\theta_{j} := \theta_{j} - \alpha\sum_{i=1}^{m}(h_{\theta}(x_{i}) - y_{i})x_{i,j} $$</p>
<p>The above update rule, is just the same we had for <strong>linear regression</strong>. Thus, <span class="caps">GD</span> for linear and logistic regression is the same. What has changed is the hypothesis and the cost functions. A vectorized implementation of <span class="caps">GD</span> is</p>
<p>$$
\theta := \theta - \frac{\alpha}{m}X^{T}(g(X\theta) - \vec{y}) $$</p>
<h3 id="using-logistic-regression-for-multiple-classes">Using logistic regression for multiple classes<a href="#using-logistic-regression-for-multiple-classes" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>While the most use cases of logistic reg is to predict boolean classes, we can extend it to a multi-class problem using a technique called <strong>one-vs-all</strong>. </p>
<p><img src="../../../images/coursera-logistic-multiclass.png" width="550"></p>
<p>We essentially build multiple models, equalling the number of classes. Each model predicts the probability that given value will fall within the class it is trained to predict. Finally we find the class with <strong>max probability</strong> and assign it to the prediction.</p>
<h4 id="ocr-on-mnist-digits-database-using-multi-class-logistic-regression"><span class="caps">OCR</span> on <span class="caps">MNIST</span> digits database using multi-class logistic regression<a href="#ocr-on-mnist-digits-database-using-multi-class-logistic-regression" class="headerlink" title="Permalink to this heading">¶</a></h4>
<p>The <span class="caps">MNIST</span> database has <code>14</code> million images of handdrawn digits. We work with a subset of <code>5000</code> images. Each image is <code>20x20</code> pixels. When laid out as a column vector (which is how Neural Nets and log reg algorithms will read it), we get a <code>1x400</code> row vector. A sample of 100 images is below:</p>
<p><img src="../../../images/coursera-logreg-mnist1.png/" width="400"></p>
<p>Here, we build a multi-class regularized logistic regression model to solve this classification problem. We train this multi-class logistic regression model by iterating a simple log reg model for each class. Each iteration will produce a set of $\theta$ values which predict the probability for that class. Finally we will assemble all theta into a 2D matrix.</p>
<p>The weights / theta is a 2D matrix (unlike a vector for simple logistic reg), where each row is the weight vector for a particular class. Since <strong>each pixel is considered an input feature</strong> and since we have <code>10</code> classes to predict, we get theta as $\Theta_{10x401}$ matrix. <code>401</code> because we add <code>1</code> bias feature to <code>400</code> features which is obtained by flattening the <code>20x20</code> image into a <code>400x1</code> vector.</p>
<p>During the training process, we need to represent the output class not as digits, but as one-hot encoded vector since that is what log reg understands. Finally during the prediction phase, we will translate the one-hot encoded prediction into the actual class label, which is the predicted digit.</p>
<p>The full <span class="caps">MATLAB</span>/<span class="caps">OCTAVE</span> implementation can be found <a href="https://github.com/AtmaMani/pyChakras/tree/master/ml/coursera-ml-matlabonline/machine-learning-ex/ex3">here</a>.</p>
    </div>
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2022         <a href="mailto:atma.mani@outlook.com">Atma Mani</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>