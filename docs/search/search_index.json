{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Atma's blog","text":"<p>I am a staff software engineer, specializing in backend development at One Concern, where I lead the development of spatial data pipelines and mapping APIs. Previously, I worked as a principal and lead product engineer for the ArcGIS API for Python at Esri and before that as a remote sensing scientist at the Indian Space Research Organization. I over 15 years of experience in private, academic, and government research institutions across different countries, applying various facets of geospatial technology. I went to school at College of Engineering, Anna University, India for my undergard, and University of Northern Iowa for my masters.</p> <p>In this personal website, you can find some of my past projects, articles, talks and some teaching resources I developed.</p> <p>Find me on Linked In Find me on GitHub</p>"},{"location":"apps/","title":"Web apps","text":""},{"location":"apps/#full-stack-python-web-apps","title":"Full stack Python web apps","text":"<p>Python never ceases to impress me. On one end of the spectrum, you can use it to build simulations, process images and signals and perform data science. On the other end, you can quickly and easily build a full stack web app that is complete with a front-end, REST API and a backend, all in Python. Below are some of my full-stack Python based web apps.</p> <p>Serverless APIs:</p> <ul> <li>Embed maps into your photos</li> <li>Use Deep Learning to predict if a picture is Donut or Bagel</li> <li>Create a map of all incoming web requests to a Lambda function</li> <li>Visualize Google Pluscodes for USA and the World</li> </ul> <p>Learning resources</p> <ul> <li>Before you get started, you need to know a bit about RESTful APIs. My article on the design principles behind RESTful APIs may be helpful</li> <li>Getting started with building RESTful APIs in Python with Flask</li> <li>Then take a look at this example fullstack Flask based Python web app that is published on Heroku.</li> <li>If publishing on AWS Lambda is your preferred route, then take a look at this FastAPI to Lambda function application.</li> </ul>"},{"location":"apps/#hazard-monitoring-apps","title":"Hazard monitoring apps","text":"<ul> <li>SoCal active fire map</li> <li>SoCal active earthquakes</li> <li>USA wild fire hazard potential</li> </ul>"},{"location":"apps/#maps-and-apps-that-i-love","title":"Maps and Apps that I love","text":"<p>For the love of geography, below is a set of maps and apps (mostly built by someone else) that encourage spatial thinking</p> <ul> <li> <p>Urban observatory. A fantastic app to visually compare up to 3 different cities. The app presents 3 map windows (for 3 cities) and locks the map scale, allowing you to observe the size (and magnitude) of those cities. Load up cities in the same country are across the world to see if Los Angeles is really larger than Mumbai and New York. Settle that argument you had in the bar, forever.</p> </li> <li> <p>The living land. An Esri produced story map journeying through the changes mankind has made to the land surface of this planet. With beautiful charts, maps, animations, drone footage this story provides a compelling and an unbiased view of the livable land and how mankind has put it to use.</p> </li> </ul>"},{"location":"apps/firemap/","title":"Up-to-date forest fire information","text":"<p>Data for this map is sourced from FireWhat.com's data and map on ArcGIS.com.</p> <p>See this map in full screen here</p> iFrames are not supported on this page."},{"location":"apps/flask-notes/","title":"Flask notes","text":"<p>post to AWS using zappa.</p>"},{"location":"apps/flask-notes/#server-stack","title":"Server stack","text":"<ul> <li>Web server - load balancing, SLL endpoint, eg: nginx, apache</li> <li>http server - forwards dynamic content requests, servers dynamic content, eg: uWSGI, Gunicorn</li> <li>app server - communicates with back-end res like db, process dynamic queries. eg: Flask, Django, CherryPy</li> </ul> <p>Ending a REST route with a trailing slash will support whether or not the request comes with a trailing slash.</p> <p>When accepting a request with parameters from a user, ensure you scrub the inputs before passing it into a db or such. This is because people can inject sql and can damage or hack the backend.</p> <p>Like <code>templates</code>, <code>static</code> is a reserved and known folder into which you can store pics and files. Flask will just serve it out.</p> <p>Use <code>from flask import url_for</code> and use it when you need to dynamically compose an internal URL dynamically.</p> <p>When accepting a POST endoing, use <code>request.form['password']</code> instead of <code>request.args.get('param')</code>.</p> <p>Use <code>redirect(&lt;new_url_here&gt;, 302)</code> to redirect.</p>"},{"location":"apps/flask-notes/#login","title":"Login","text":"<p><code>pip install Flask-Login</code> library is a simple one. This library is aware of sessions and can manage that. When storing passwords, use password hashes.</p> <pre><code>from werkzeug.security import generate_password_hash\nsecret_pass = generate_password_hash('123')\n</code></pre> <p>You will compare the hash of user password with a dynamic hash when user presents it.</p>"},{"location":"apps/flask-notes/#users","title":"Users","text":"<p><code>flask_login.UserMixin</code> find what a Mixin is. Find if subclass is the right term.</p>"},{"location":"apps/flask-notes/#zappa","title":"Zappa","text":"<p>Zappa makes is very easy to push to AWS. It will zip up the app, push to S3 bucket, deploy via AWS Lambda, create AWS API Gateway endpoint, adds an AWS Cloudwatch rule that keeps Lambda 'warm'. The cloudwatch will keep the lambda running by pinging it every 4 mins or so.</p>"},{"location":"apps/flask-notes/#resources","title":"Resources","text":"<p>Corey Schafer's 15 part Flask on YouTube.</p>"},{"location":"apps/fullstack-python-webapp-1/","title":"Building and publishing a fullstack Python webapp on Heroku","text":"<p>Thermos repo Get this project from the thermos GitHub repo</p> <p>Experiments with a Python's Flask based web server. </p> <p>This tiny project shows how to build RESTful APIs with Flask. You can define endpoints and the code that needs to be executed when invoked. You can send messages via the REST API and act on it in your Python code.</p>"},{"location":"apps/fullstack-python-webapp-1/#quickstart","title":"Quickstart","text":""},{"location":"apps/fullstack-python-webapp-1/#creating-your-dev-env","title":"Creating your dev env","text":"<p>To run, first clone this repo and enter its folder in terminal. Then clone  the dev env using the <code>requirements.txt</code> file. To do so first create a conda env by running:</p> <pre><code>conda create env --name flask2 python\n</code></pre> <p>then install the dependencies by running:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"apps/fullstack-python-webapp-1/#running-this-project","title":"Running this project","text":"<p>All endpoints are defined in a single <code>app.py</code> which is what should be run by the web server. After you activate and install the dependencies, from the terminal run:</p> <pre><code>python -m app\n</code></pre> <p>which will start the web server (usually at http://localhost:5000)and give you an address to hit. There is a simple HTML frontpage that you can use as UI or you can interact with the REST endpoints programmatically. You can also hit the production version at https://atma-thermos.herokuapp.com</p>"},{"location":"apps/fullstack-python-webapp-1/#architecture-of-this-project","title":"Architecture of this project","text":"<p>The goal of this project is to demonstrate building web apps using Python. It demonstrates using <code>Flask</code> to design and implement RESTful APIs, <code>arcgis</code> for geocoding, <code>NASA APIS</code> to call out to an external process, <code>HTML templates</code> to demo injecting content dynamically into web pages and finally <code>sqlalchemy</code> to perform CRUD ops on a database and expose those operations via a REST API.</p>"},{"location":"apps/fullstack-python-webapp-1/#1-apppy","title":"1. <code>app.py</code>","text":"<p>This is the main file. The web server runs this file. All REST endpoints and handlers for each are defined here. For simple operations, the business logic is defined right in the handler. For others, it is abstracted away in separate Python files that are imported.</p> <p>The <code>/</code> route defines what happens when the landing page of the app is called. If you send a GET request, then a HTML page is displayed. This page is defined in the <code>templates/index.html</code>. As you can see from the code, I am able to use Python to inquire the IP address, machine name, system path etc and pass that to the HTML page. The advantage of this is, which ever server runs this app, the details will be populated at run time. This pattern comes in handy later.</p> <p>The <code>eyeFromAbove</code> resource is probably the most complex. When called via a GET, it returns a HTML page with some UI elements. Users can enter values and hit submit which will send a POST call to the same endpoint. When it receives a POST call, it performs the actual operation of geocoding the address and calling out to NASA and Google Earth Engine apps to get a Landsat satellite image of the coordinates. Once it downloads this image (into <code>\\eye_in_sky_queries</code>), it returns the user a HTML page which dynamically displays this satellite image.</p>"},{"location":"apps/fullstack-python-webapp-1/#2-executorpy-and-geocode_toolpy","title":"2. <code>executor.py</code> and <code>geocode_tool.py</code>","text":"<p>These files abstract the logic needed for generating unique random numbers and geocoding addresses to get coordinate information. The <code>geocode_tool.py</code> uses <code>arcgis</code> package for this.</p>"},{"location":"apps/fullstack-python-webapp-1/#3-address_db_modelpy-and-dbopspy","title":"3. <code>address_db_model.py</code> and <code>dbops.py</code>","text":"<p>This part deals with database operations. The model file creates a <code>sqlite</code> db on disk, declares a table and its schema. The rest is handled in the <code>dbops.py</code> file. This file has logic to Create, Read, Update, Delete (CRUD) records. The db file is called <code>addresses.db</code> and the table is called <code>gAddress</code>. Neither of these is required to talk to the DB because <code>sqlalchemy</code> abstracts DB interaction. Thus, the workflow once DB is created is to create a <code>session</code> that connects to this db with a little bit of boilerplate code. Thereafter, I pass around the <code>session</code> variable whenever I need to perform CRUD ops. Only the <code>app.py</code> file can create the session. All other files will use this session to work with the db.</p> <p>The <code>app.py</code> exposes an <code>/addresses</code> endpoint which accepts GET, POST, PUT, DELETE and an index URL endpoint. Ops like geocoding will automatically write a record to the table. In addition, users can call the addresses endpoint via POST to push a bunch of addresses into the db. GET will read all entries. PUT will update and DELETE will remove individual records.</p>"},{"location":"apps/fullstack-python-webapp-1/#4-unittests","title":"4. <code>\\unittests</code>","text":"<p>A critical component of this repo is the unittests. Written using Python's built-in <code>unittest</code> module, these test each endpoint making REST calls. You can run the tests against development as well as production, just change the <code>base_address</code> property.</p> <p>I cannot emphasize the importance of tests enough. I practically learnt about FLASK and how to read arguments vs form data by debugging my failing tests. It was thrilling to attach debugger to both my server and my client code and see calls made and handled.</p>"},{"location":"apps/fullstack-python-webapp-1/#publishing-to-heroku","title":"Publishing to Heroku","text":"<p>These are the general steps</p> <ol> <li>Install all dependencies using <code>pip</code> as conda support is not matured yet.</li> <li>Freeze dependencies using <code>pip freeze -r requirements.txt</code></li> <li>Create a <code>Procfile</code> with contents <code>web: gunicorn app:app</code>. Here we switch from Flask server to Gunicorn which is a light weight, but production ready server.</li> <li>Install Heroku CLI, create an account, make keys, login to client.</li> <li>Create Heroku app as <code>heroku apps:create atma-thermos</code>. Note: you need to name it differently as the name I chose is taken by me.</li> <li>Test locally using <code>heroku local web</code>.</li> <li>Commit your changes. Push to both remotes.</li> </ol> <pre><code>git push origin master\ngit push heroku master\n</code></pre> <p>The last statement of pushing to heroku triggers the actual job of building your app on serverside. Heroku will unpack this repo, set up the env using requirements.txt, then execute the Procfile instructions. This will lead to your app running at the designated address it gave you when you ran the <code>create</code> command.</p> <p>Look at the appendix at the bottom to see the full terminal build output.</p>"},{"location":"apps/fullstack-python-webapp-1/#mistakes","title":"Mistakes","text":"<ol> <li>When authoring the requirements, I forgot to mention that <code>arcgis</code> should be a lite install. This resulted in heroku trying to install the full package. But it looks like it had no problems :)</li> </ol>"},{"location":"apps/fullstack-python-webapp-1/#appendix-build-outputs","title":"Appendix - build outputs","text":"<pre><code>(flask2) thermos (master) $ git push origin master\nEnumerating objects: 15, done.\nCounting objects: 100% (15/15), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (10/10), done.\nWriting objects: 100% (11/11), 171.85 KiB | 10.11 MiB/s, done.\nTotal 11 (delta 5), reused 0 (delta 0)\nremote: Resolving deltas: 100% (5/5), completed with 3 local objects.\nTo github.com:AtmaMani/thermos.git\n   f6304ec..ef5ee7f  master -&gt; master\n(flask2) thermos (master) $ git remote -v\nheroku  https://git.heroku.com/atma-thermos.git (fetch)\nheroku  https://git.heroku.com/atma-thermos.git (push)\norigin  git@github.com:AtmaMani/thermos.git (fetch)\norigin  git@github.com:AtmaMani/thermos.git (push)\n(flask2) thermos (master)* $ git push heroku master\nEnumerating objects: 59, done.\nCounting objects: 100% (59/59), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (52/52), done.\nWriting objects: 100% (59/59), 784.82 KiB | 23.78 MiB/s, done.\nTotal 59 (delta 18), reused 12 (delta 2)\nremote: Compressing source files... done.\nremote: Building source:\nremote: \nremote: -----&gt; Python app detected\nremote: -----&gt; Installing python-3.6.9\nremote: -----&gt; Installing pip\nremote: -----&gt; Installing SQLite3\nremote: -----&gt; Installing requirements with pip\nremote:        Collecting arcgis==1.7.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ef/ae/6d3f73ed54b243584bc9b0b888f7b0751e255737482a1989345747f4d81b/arcgis-1.7.0.tar.gz (1.7MB)\nremote:        Collecting certifi==2019.9.11 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 2))\nremote:          Downloading https://files.pythonhosted.org/packages/18/b0/8146a4f8dd402f60744fa380bc73ca47303cccf8b9190fd16a827281eac2/certifi-2019.9.11-py2.py3-none-any.whl (154kB)\nremote:        Collecting chardet==3.0.4 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 3))\nremote:          Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\nremote:        Collecting Click==7.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 4))\nremote:          Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\nremote:        Collecting Flask==1.1.1 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 5))\nremote:          Downloading https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl (94kB)\nremote:        Collecting gunicorn==19.9.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 6))\nremote:          Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\nremote:        Collecting idna==2.8 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 7)\nremote:          Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\nremote:        Collecting itsdangerous==1.1.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 8))\nremote:          Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\nremote:        Collecting Jinja2==2.10.3 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 9))\nremote:          Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl (125kB)\nremote:        Collecting MarkupSafe==1.1.1 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 10))\nremote:          Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\nremote:        Collecting pytz==2019.3 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 11))\nremote:          Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\nremote:        Collecting requests==2.22.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 12))\nremote:          Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\nremote:        Collecting six==1.12.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 13))\nremote:          Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nremote:        Collecting SQLAlchemy==1.3.10 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 14))\nremote:          Downloading https://files.pythonhosted.org/packages/14/0e/487f7fc1e432cec50d2678f94e4133f2b9e9356e35bacc30d73e8cb831fc/SQLAlchemy-1.3.10.tar.gz (6.0MB)\nremote:        Collecting urllib3==1.25.6 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 15))\nremote:          Downloading https://files.pythonhosted.org/packages/e0/da/55f51ea951e1b7c63a579c09dd7db825bb730ec1fe9c0180fc77bfb31448/urllib3-1.25.6-py2.py3-none-any.whl (125kB)\nremote:        Collecting Werkzeug==0.16.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 16))\nremote:          Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\nremote:        Collecting ipywidgets&gt;=7 (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/56/a0/dbcf5881bb2f51e8db678211907f16ea0a182b232c591a6d6f276985ca95/ipywidgets-7.5.1-py2.py3-none-any.whl (121kB)\nremote:        Collecting widgetsnbextension&gt;=3 (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/6c/7b/7ac231c20d2d33c445eaacf8a433f4e22c60677eb9776c7c5262d7ddee2d/widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2MB)\nremote:        Collecting pandas&gt;=0.23 (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\nremote:        Collecting numpy (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/0e/46/ae6773894f7eacf53308086287897ec568eac9768918d913d5b9d366c5db/numpy-1.17.3-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\nremote:        Collecting matplotlib (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\nremote:        Collecting keyring (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/b1/08/ad1ae7262c8146bee3be360cc766d0261037a90b44872b080a53aaed4e84/keyring-19.2.0-py2.py3-none-any.whl\nremote:        Collecting jupyterlab (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/56/96/5629fec605f0d320f3857241e84704e533ee54eb2aa87c0b69c34bbcc3f2/jupyterlab-1.2.1-py2.py3-none-any.whl (6.4MB)\nremote:        Collecting pyshp&lt;2,&gt;=1.2.11 (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/c8/ac/8e9adb8e0dadabbb3b708d38a83119ee42115d9f8fd88858261f5bec50f0/pyshp-1.2.12.tar.gz (193kB)\nremote:        Collecting traitlets&gt;=4.3.1 (from ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl (75kB)\nremote:        Collecting ipython&gt;=4.0.0; python_version &gt;= \"3.3\" (from ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/81/2e/59cdacea6476a4c21b7c090a91250ffbcd085900f5eb9f4e4d68dd2ee4e3/ipython-7.9.0-py3-none-any.whl (775kB)\nremote:        Collecting ipykernel&gt;=4.5.1 (from ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/e1/92/8fec943b5b81078399f969f00557804d884c96fcd0bc296e81a2ed4fd270/ipykernel-5.1.3-py3-none-any.whl (116kB)\nremote:        Collecting nbformat&gt;=4.2.0 (from ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/da/27/9a654d2b6cc1eaa517d1c5a4405166c7f6d72f04f6e7eea41855fe808a46/nbformat-4.4.0-py2.py3-none-any.whl (155kB)\nremote:        Collecting notebook&gt;=4.4.1 (from widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f5/69/d2ffaf7efc20ce47469187e3a41e6e03e17b45de5a6559f4e7ab3eace5e1/notebook-6.0.2-py3-none-any.whl (9.7MB)\nremote:        Collecting python-dateutil&gt;=2.6.1 (from pandas&gt;=0.23-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\nremote:        Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 (from matplotlib-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/a3/c4/7828cf9e71ce8fbd43c1e502f3fdd0498f069fcf9d1c268205ce278ae201/pyparsing-2.4.4-py2.py3-none-any.whl (67kB)\nremote:        Collecting kiwisolver&gt;=1.0.1 (from matplotlib-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\nremote:        Collecting cycler&gt;=0.10 (from matplotlib-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\nremote:        Collecting entrypoints (from keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ac/c6/44694103f8c221443ee6b0041f69e2740d89a25641e62fb4f2ee568f2f9c/entrypoints-0.3-py2.py3-none-any.whl\nremote:        Collecting secretstorage; sys_platform == \"linux\" (from keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/82/59/cb226752e20d83598d7fdcabd7819570b0329a61db07cfbdd21b2ef546e3/SecretStorage-3.1.1-py3-none-any.whl\nremote:        Collecting jupyterlab-server~=1.0.0 (from jupyterlab-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/78/98/5b87b9d38176bd98f23b58a8fb730e5124618d68571a011abbd38ad4a842/jupyterlab_server-1.0.6-py3-none-any.whl\nremote:        Collecting tornado!=6.0.0,!=6.0.1,!=6.0.2 (from jupyterlab-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/30/78/2d2823598496127b21423baffaa186b668f73cd91887fcef78b6eade136b/tornado-6.0.3.tar.gz (482kB)\nremote:        Collecting decorator (from traitlets&gt;=4.3.1-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/8f/b7/f329cfdc75f3d28d12c65980e4469e2fa373f1953f5df6e370e84ea2e875/decorator-4.4.1-py2.py3-none-any.whl\nremote:        Collecting ipython-genutils (from traitlets&gt;=4.3.1-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\nremote:        Collecting pygments (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/5c/73/1dfa428150e3ccb0fa3e68db406e5be48698f2a979ccbcec795f28f44048/Pygments-2.4.2-py2.py3-none-any.whl (883kB)\nremote:        Collecting backcall (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/84/71/c8ca4f5bb1e08401b916c68003acf0a0655df935d74d93bf3f3364b310e0/backcall-0.1.0.tar.gz\nremote:        Collecting jedi&gt;=0.10 (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/55/54/da994f359e4e7da4776a200e76dbc85ba5fc319eefc22e33d55296d95a1d/jedi-0.15.1-py2.py3-none-any.whl (1.0MB)\nremote:        Collecting prompt-toolkit&lt;2.1.0,&gt;=2.0.0 (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\nremote:        Collecting pickleshare (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\nremote:        Collecting pexpect; sys_platform != \"win32\" (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/0e/3e/377007e3f36ec42f1b84ec322ee12141a9e10d808312e5738f52f80a232c/pexpect-4.7.0-py2.py3-none-any.whl (58kB)\nremote:        Collecting jupyter-client (from ipykernel&gt;=4.5.1-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/13/81/fe0eee1bcf949851a120254b1f530ae1e01bdde2d3ab9710c6ff81525061/jupyter_client-5.3.4-py2.py3-none-any.whl (92kB)\nremote:        Collecting jsonschema!=2.5.0,&gt;=2.4 (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ce/6c/888d7c3c1fce3974c88a01a6bc553528c99d3586e098eee23e8383dd11c3/jsonschema-3.1.1-py2.py3-none-any.whl (56kB)\nremote:        Collecting jupyter-core (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/fb/82/86437f661875e30682e99d04c13ba6c216f86f5f6ca6ef212d3ee8b6ca11/jupyter_core-4.6.1-py2.py3-none-any.whl (82kB)\nremote:        Collecting Send2Trash (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/49/46/c3dc27481d1cc57b9385aff41c474ceb7714f7935b1247194adae45db714/Send2Trash-1.5.0-py3-none-any.whl\nremote:        Collecting pyzmq&gt;=17 (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/75/89/6f0ea51ffa9c2c00c0ab0460f137b16a5ab5b47e3b060c5b1fc9ca425836/pyzmq-18.1.0-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\nremote:        Collecting nbconvert (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/79/6c/05a569e9f703d18aacb89b7ad6075b404e8a4afde2c26b73ca77bb644b14/nbconvert-5.6.1-py2.py3-none-any.whl (455kB)\nremote:        Collecting terminado&gt;=0.8.1 (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/a7/56/80ea7fa66565fa75ae21ce0c16bc90067530e5d15e48854afcc86585a391/terminado-0.8.2-py2.py3-none-any.whl\nremote:        Collecting prometheus-client (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/b3/23/41a5a24b502d35a4ad50a5bb7202a5e1d9a0364d0c12f56db3dbf7aca76d/prometheus_client-0.7.1.tar.gz\nremote:        Collecting cryptography (from secretstorage; sys_platform == \"linux\"-&gt;keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/45/73/d18a8884de8bffdcda475728008b5b13be7fbef40a2acc81a0d5d524175d/cryptography-2.8-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\nremote:        Collecting jeepney (from secretstorage; sys_platform == \"linux\"-&gt;keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/0a/4c/ef880713a6c6d628869596703167eab2edf8e0ec2d870d1089dcb0901b81/jeepney-0.4.1-py3-none-any.whl (60kB)\nremote:        Collecting json5 (from jupyterlab-server~=1.0.0-&gt;jupyterlab-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/30/44/062543d4a6718f99d82e5ecf9140dbdee8a03122f2c34fbd0b0609891707/json5-0.8.5-py2.py3-none-any.whl\nremote:        Collecting parso&gt;=0.5.0 (from jedi&gt;=0.10-&gt;ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/a3/bd/bf4e5bd01d79906e5b945a7af033154da49fd2b0d5b5c705a21330323305/parso-0.5.1-py2.py3-none-any.whl (95kB)\nremote:        Collecting wcwidth (from prompt-toolkit&lt;2.1.0,&gt;=2.0.0-&gt;ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/7e/9f/526a6947247599b084ee5232e4f9190a38f398d7300d866af3ab571a5bfe/wcwidth-0.1.7-py2.py3-none-any.whl\nremote:        Collecting ptyprocess&gt;=0.5 (from pexpect; sys_platform != \"win32\"-&gt;ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\nremote:        Collecting importlib-metadata (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f6/d2/40b3fa882147719744e6aa50ac39cf7a22a913cbcba86a0371176c425a3b/importlib_metadata-0.23-py2.py3-none-any.whl\nremote:        Collecting pyrsistent&gt;=0.14.0 (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/30/86/53a88c0a57698fa228db29a4000c28f4124823010388cb7042fe6e2be8dd/pyrsistent-0.15.5.tar.gz (107kB)\nremote:        Collecting attrs&gt;=17.4.0 (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\nremote:        Collecting pandocfilters&gt;=1.4.1 (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/4c/ea/236e2584af67bb6df960832731a6e5325fd4441de001767da328c33368ce/pandocfilters-1.4.2.tar.gz\nremote:        Collecting mistune&lt;2,&gt;=0.8.1 (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/09/ec/4b43dae793655b7d8a25f76119624350b4d65eb663459eb9603d7f1f0345/mistune-0.8.4-py2.py3-none-any.whl\nremote:        Collecting bleach (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ab/05/27e1466475e816d3001efb6e0a85a819be17411420494a1e602c36f8299d/bleach-3.1.0-py2.py3-none-any.whl (157kB)\nremote:        Collecting testpath (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/1b/9e/1a170feaa54f22aeb5a5d16c9015e82234275a3c8ab630b552493f9cb8a9/testpath-0.4.4-py2.py3-none-any.whl (163kB)\nremote:        Collecting defusedxml (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/06/74/9b387472866358ebc08732de3da6dc48e44b0aacd2ddaa5cb85ab7e986a2/defusedxml-0.6.0-py2.py3-none-any.whl\nremote:        Collecting cffi!=1.11.3,&gt;=1.8 (from cryptography-&gt;secretstorage; sys_platform == \"linux\"-&gt;keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/49/72/0d42f94fe94afa8030350c26e9d787219f3f008ec9bf6b86c66532b29236/cffi-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (397kB)\nremote:        Collecting zipp&gt;=0.5 (from importlib-metadata-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/74/3d/1ee25a26411ba0401b43c6376d2316a71addcc72ef8690b101b4ea56d76a/zipp-0.6.0-py2.py3-none-any.whl\nremote:        Collecting webencodings (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\nremote:        Collecting pycparser (from cffi!=1.11.3,&gt;=1.8-&gt;cryptography-&gt;secretstorage; sys_platform == \"linux\"-&gt;keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\nremote:        Collecting more-itertools (from zipp&gt;=0.5-&gt;importlib-metadata-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/45/dc/3241eef99eb45f1def35cf93af35d1cf9ef4c0991792583b8f33ea41b092/more_itertools-7.2.0-py3-none-any.whl (57kB)\nremote:        Installing collected packages: six, decorator, ipython-genutils, traitlets, pygments, backcall, parso, jedi, wcwidth, prompt-toolkit, pickleshare, ptyprocess, pexpect, ipython, tornado, jupyter-core, python-dateutil, pyzmq, jupyter-client, ipykernel, more-itertools, zipp, importlib-metadata, pyrsistent, attrs, jsonschema, nbformat, MarkupSafe, Jinja2, Send2Trash, entrypoints, pandocfilters, mistune, webencodings, bleach, testpath, defusedxml, nbconvert, terminado, prometheus-client, notebook, widgetsnbextension, ipywidgets, numpy, pytz, pandas, pyparsing, kiwisolver, cycler, matplotlib, pycparser, cffi, cryptography, jeepney, secretstorage, keyring, json5, jupyterlab-server, jupyterlab, pyshp, arcgis, certifi, chardet, Click, Werkzeug, itsdangerous, Flask, gunicorn, idna, urllib3, requests, SQLAlchemy\nremote:          Running setup.py install for backcall: started\nremote:            Running setup.py install for backcall: finished with status 'done'\nremote:          Running setup.py install for tornado: started\nremote:            Running setup.py install for tornado: finished with status 'done'\nremote:          Running setup.py install for pyrsistent: started\nremote:            Running setup.py install for pyrsistent: finished with status 'done'\nremote:          Running setup.py install for pandocfilters: started\nremote:            Running setup.py install for pandocfilters: finished with status 'done'\nremote:          Running setup.py install for prometheus-client: started\nremote:            Running setup.py install for prometheus-client: finished with status 'done'\nremote:          Running setup.py install for pycparser: started\nremote:            Running setup.py install for pycparser: finished with status 'done'\nremote:          Running setup.py install for pyshp: started\nremote:            Running setup.py install for pyshp: finished with status 'done'\nremote:          Running setup.py install for arcgis: started\nremote:            Running setup.py install for arcgis: finished with status 'done'\nremote:          Running setup.py install for SQLAlchemy: started\nremote:            Running setup.py install for SQLAlchemy: finished with status 'done'\nremote:        Successfully installed Click-7.0 Flask-1.1.1 Jinja2-2.10.3 MarkupSafe-1.1.1 SQLAlchemy-1.3.10 Send2Trash-1.5.0 Werkzeug-0.16.0 arcgis-1.7.0 attrs-19.3.0 backcall-0.1.0 bleach-3.1.0 certifi-2019.9.11 cffi-1.13.2 chardet-3.0.4 cryptography-2.8 cycler-0.10.0 decorator-4.4.1 defusedxml-0.6.0 entrypoints-0.3 gunicorn-19.9.0 idna-2.8 importlib-metadata-0.23 ipykernel-5.1.3 ipython-7.9.0 ipython-genutils-0.2.0 ipywidgets-7.5.1 itsdangerous-1.1.0 jedi-0.15.1 jeepney-0.4.1 json5-0.8.5 jsonschema-3.1.1 jupyter-client-5.3.4 jupyter-core-4.6.1 jupyterlab-1.2.1 jupyterlab-server-1.0.6 keyring-19.2.0 kiwisolver-1.1.0 matplotlib-3.1.1 mistune-0.8.4 more-itertools-7.2.0 nbconvert-5.6.1 nbformat-4.4.0 notebook-6.0.2 numpy-1.17.3 pandas-0.25.3 pandocfilters-1.4.2 parso-0.5.1 pexpect-4.7.0 pickleshare-0.7.5 prometheus-client-0.7.1 prompt-toolkit-2.0.10 ptyprocess-0.6.0 pycparser-2.19 pygments-2.4.2 pyparsing-2.4.4 pyrsistent-0.15.5 pyshp-1.2.12 python-dateutil-2.8.1 pytz-2019.3 pyzmq-18.1.0 requests-2.22.0 secretstorage-3.1.1 six-1.12.0 terminado-0.8.2 testpath-0.4.4 tornado-6.0.3 traitlets-4.3.3 urllib3-1.25.6 wcwidth-0.1.7 webencodings-0.5.1 widgetsnbextension-3.5.1 zipp-0.6.0\nremote: \nremote: -----&gt; Discovering process types\nremote:        Procfile declares types -&gt; web\nremote: \nremote: -----&gt; Compressing...\nremote:        Done: 131.7M\nremote: -----&gt; Launching...\nremote:        Released v3\nremote:        https://atma-thermos.herokuapp.com/ deployed to Heroku\nremote: \nremote: Verifying deploy... done.\nTo https://git.heroku.com/atma-thermos.git\n * [new branch]      master -&gt; master\n</code></pre>"},{"location":"apps/quakemap/","title":"Live earthquake map from USGS","text":"<p>If you cannot view the map on this page, click here to open it in full screen.</p>"},{"location":"apps/usa-pluscodes/","title":"USA Pluscode 4 map","text":""},{"location":"apps/usa-pluscodes/#pluscode4-for-usa","title":"Pluscode4 for USA","text":"<p>Go fullscreen here</p> iFrames are not supported on this page."},{"location":"apps/usa-pluscodes/#pluscode4-symbolized-by-number-of-man-made-structures","title":"Pluscode4 symbolized by number of man-made structures","text":"<p>Go fullscreen here</p> iFrames are not supported on this page."},{"location":"apps/usa-pluscodes/#pluscode4-for-the-world","title":"Pluscode4 for the World","text":"<p>Go fullscreen here</p> iFrames are not supported on this page."},{"location":"apps/wildfire-hazard/","title":"What is my wildfire hazard score?","text":"<p>The map below shows the wildfire hazard potential (or score) for any community. The layer is out of the Esri LivingAtlas of the world. If you would like to know more about this resource, read this blog and this story map.</p> <p>\"A score of 5 is very high risk and a score between 0-1 is likely non-burnable area such as water or asphalt. People that live in areas of moderate to high risk are more likely to be affected by wildfire events including evacuations, higher home insurance, and bad air quality due to smoke.\"</p> <p>View the map in full screen here</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2017/10/26/a-whole-new-way-to-experience-gis/","title":"A whole new way to experience GIS","text":"<p>The ArcGIS API for Python is easy to learn and extremely useful for data scientists, GIS administrators, and GIS analysts. One of the features that makes this API so powerful is its integration with Jupyter Notebook. Jupyter Notebook is a web-based integrated development environment (IDE) for executing Python code. Unlike other traditional IDEs that are designed for developers, Jupyter Notebook provides a simple and easy-to-use interface that encourages the Read-Eval-Print Loop (REPL) process that is central to learning how to code in Python... Read more here</p>"},{"location":"blog/2010/01/26/articles-on-remote-sensing/","title":"Articles on Remote Sensing","text":"<p>Below are some blogs, tutorials that I authored on remote sensing while I worked as a scientist at the Indian Space Research Org.</p> <ul> <li>Shadow within shadow</li> <li>Programming: A tool read and analyze HDF5 data</li> <li>Essentials of image processing</li> <li>Programming: A look at Optimum Index Factor algorithm to choose best bands for visualization</li> <li>Microwave remote sensing concepts</li> </ul>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/","title":"AWS Lambda Functions - a Quick Start guide","text":"<p>Lambda functions from AWS sit at the heart of its serverless compute services. It lets you run code without you as a developer having to procure, host, maintain and secure servers on the cloud. All you need to worry about is just the code and the business logic. This guide will help you get started creating simple lambda functions.</p> <p>AWS promotes several services under its serverless platform as shown below: </p> <p>To create a lambda function, you need not learn a new language. You can code in one of many supported languages, including Python, Java, Node.JS etc. Your code can perform traditional compute as well as use AWS libraries to talk to the rest of AWS platform services. Scaling (compute scaling, network scaling, IO throughput scaling) happens automatically based on demand. System logs are written to AWS Cloudwatch service. As with any serverless compute service on AWS, you don't pay for idle time. Before we create a new function, we need to clear some concepts:</p> <ul> <li>AWS Lambda Functions - a Quick Start guide<ul> <li>Concepts<ul> <li>Lambda function permissions</li> <li>Lambda event sources</li> <li>Lifecycle of a lambda function</li> <li>Architecture of a lambda function</li> <li>Principles of a good lambda function</li> <li>Lambda configurations</li> </ul> </li> <li>Authoring lambda functions</li> </ul> </li> <li>Lambda - advanced concepts<ul> <li>Runtime, Handler, Memory, Timeouts</li> <li>Lambda layers<ul> <li>Building a deployment package for a layer</li> <li>Sharing layers</li> </ul> </li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#concepts","title":"Concepts","text":""},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-function-permissions","title":"Lambda function permissions","text":"<p>There are two types of permissions - a. what is allowed to invoke the function, and b. what the function is allowed to do (or talk to).</p> <p></p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-event-sources","title":"Lambda event sources","text":"<p>Events are the triggers that cause a lambda function to run. For instance, a file being stored in S3, a cloud watch event etc. are event sources. Broadly, these triggers can be classified into two types: a. Push events (where the source is external to the fn) and b. Poll events (where the lambda will poll a service on a set interval).</p> <p></p> <p>Push events can be <code>synchronous</code> or <code>asynchronous</code>. Sync events expect an immediate response, while async can do not. Async are suited for batch processing.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lifecycle-of-a-lambda-function","title":"Lifecycle of a lambda function","text":"<p>From the AWS training site:  1. When a function is first invoked, an execution environment is launched and bootstrapped. Once the environment is bootstrapped, your function code executes. Then, Lambda freezes the execution environment, expecting additional invocations.</p> <ol> <li> <p>If another invocation request for the function is made while the environment is in this state, that request goes through a warm start. With a warm start, the available frozen container is thawed and immediately begins code execution without going through the bootstrap process.</p> </li> <li> <p>This thaw and freeze cycle continues as long as requests continue to come in consistently. But if the environment becomes idle for too long, the execution environment is recycled.</p> </li> <li> <p>A subsequent request starts the lifecycle over, requiring the environment to be launched and bootstrapped. This is a cold start.</p> </li> </ol> <p></p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#architecture-of-a-lambda-function","title":"Architecture of a lambda function","text":"<p>This is a simple lambda function that returns the current date:</p> <pre><code>import json\nfrom datetime import datetime\n\nnow_date = datetime.now().strftime('%y-%m-%d')\n\ndef lambda_handler(event, context):\n    # TODO implement\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda! Now the time is: ' + now_date)\n    }\n</code></pre> <p>The filename is <code>lambda_function.py</code> and the invocation is to <code>lambda_function.lambda_handler</code>.</p> <p>At the core of a lambda function is the <code>handler(event, context)</code> method. The <code>event</code> object can either be AWS generated obj (when AWS services invoke) or custom user-defined obj. The <code>context</code> obj provides information about the current execution, such as remaining time etc.</p> <p>You author lambda functions in 3 ways:</p> <ol> <li>use the Lambda Management Console web app, which is based off the Cloud9 web IDE service.</li> <li>Upload code package after you author it using your IDE of choice</li> <li>Upload code package to a S3 bucket and give Lambda the url to the code.</li> </ol> <p>Uploading to S3 bucket might be suitable if your package is &gt;10 MB in size, or if the code is part of a CICD pipeline.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#principles-of-a-good-lambda-function","title":"Principles of a good lambda function","text":"<p>AWS Lambda developer guide has more information on best practices. Below is the gist:</p> <ul> <li>Functions in your code should be modular, testable and stateless. Separate out business logic from the handler function. If you can identify two separate tasks in your function, break them down and create two separate functions if that's possible. This makes it modular, just like a microservice.</li> <li>To benefit from warm start, store data locally in <code>/tmp</code> directory. But don't assume it is always available.</li> <li>To write data permanently, use DynamoDB service which is serverless and has millisecond latency.</li> <li>To take advantage of CloudWatch, use Python's <code>logging</code> module. Simple print statements also would do and they are captured in CloudWatch.</li> <li>To pass sensitive information, use environment variables.</li> <li>While recursion might be elegant in regular programming, it can lead to uncontrolled behavior in lambda functions. So, avoid them.</li> </ul>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-configurations","title":"Lambda configurations","text":"<p>Lambda functions are billed for memory and duration of execution. The default memory is <code>128 MB</code> and you can request up to <code>2 GB</code>. When you request for a larger size, you get proportionally higher compute power and also a higher cost rate. Services are billed for memory + execution duration (the max of duration is set as timeout limit). Default timeout is <code>3 sec</code> and the current max is <code>15 min</code>. Thus, you need to optimize for cost of higher memory and cost of exec duration. Sometimes, a higher memory might end up costing less, because it finishes in shorter duration. This comes down to profiling your function and understanding how you can speed it up.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#authoring-lambda-functions","title":"Authoring lambda functions","text":"<p>To create a lambda function, create an AWS account (which needs to be different from your Amazon shopping account). If you are a new user, you get <code>12</code> months of free service in addition to certain services that have an always free tier. Checkout https://aws.amazon.com/lambda/pricing/ to understand pricing. In my experience, they first <code>50,000</code> executions are practically free, so it is safe to experiment with.</p> <p>Steps:</p> <ol> <li> <p>From AWS console, search for and click on 'Lambda'. This opens the Lambda console. From the menu on the left, click on 'Function' which brings you to the layout shown below: </p> </li> <li> <p>Click on 'Create Function'. You can choose from one of 3 options, for now, choose 'Author from scratch'. Give a name to the function <code>myFirstFn</code> or such. Choose runtime as <code>python3.7</code> or similar. Choose a permission, the default setting will do for now.</p> </li> <li> <p>In the Lambda console, you have different sections. The 'Designer' lets you choose layers which are Python library layers, more of this later. For now you can choose a Trigger as <code>API gateway</code>. Triggers define what will invoke your function. I don't have a destination in this example, but examples are calling another 'lambda' or some other AWS service. There aren't as many destinations as there are for triggers.  If you click on the 'API Gateway', you get a URL that is public facing. AWS has set up a web server, a server framework, exposed it to the internet, generated a route that is specific for this function and given you in that URL.</p> </li> <li> <p>The Function code section gives you the Cloud9 based IDE. It feels similar to JupyterLab interface and lets you develop Python code. By convention, the lambda service looks for a file by name <code>lambda_function.py</code> and a function called <code>lambda_handler(event, context)</code> within it for invocation. You can follow the convention, which the console stubs out for you, or change this in the Basic settings section. The <code>return</code> statement should return an object that is web friendly - such as JSON or HTML. Any <code>print</code> or <code>logging</code> module statements are captured in CloudWatch logs and linked for you for debugging. </p> </li> <li> <p>Hit Save and Test to ensure it runs as you expect. This is it. You have created the 'hello-world' of lambda functions and its running.</p> </li> </ol>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-advanced-concepts","title":"Lambda - advanced concepts","text":""},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#runtime-handler-memory-timeouts","title":"Runtime, Handler, Memory, Timeouts","text":"<p>Hit the 'Edit' button on 'Basic settings' to customize as shown below: </p> <p>The 'Handler' part allows you to specify your <code>main.py</code> file and the function within to invoke. This is helpful if you deviate from the lambda convention of what you call the handler function.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-layers","title":"Lambda layers","text":"<p>The Amazon Machine Image for <code>python3.7</code> Lambda comes with bare Python and <code>boto</code> library. Nothing else. Any library that your app needs (such as <code>requests</code> or <code>flask</code>) has to be bundled in your code. If you have more than a few functions that have similar dependency stack, you are essentially repeating the same packages in each function. Besides, the dependency package for my apps can easily exceed the <code>10MB</code> upload limit and the lambda console would not let me live edit my files using the Cloud9 console. A solution to all of this is to create a lambda layer which contains all your dependencies. </p> <p>Once you publish a layer, you can link to it from your function and just like that, <code>import</code> your dependencies. Lambda service will take care of laying it and wiring up your <code>Python</code> path. To create a layer, from the lambda menu on the left, choose 'Layers' to enter the layers console. </p> <p>Click on 'Create Layer', to give it a name, upload your files as a .zip file or from a S3 bucket. Choose runtimes. You can choose more than one.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#building-a-deployment-package-for-a-layer","title":"Building a deployment package for a layer","text":"<p>The most important thing to remember here is to build your deployment package in an environment that is identical to the lambda service itself. This implies, building the package in a Linux OS (better if you get the same Docker image lambda uses). After several attempts, using <code>pipenv</code> instead of <code>conda</code> and <code>pip3</code> to install packages worked best for me.</p> <p><code>pipenv</code> allows you to create a folder as your deployment environment. Then, <code>pip3 install &lt;pkg name&gt; --target .</code> allows you to install the packages in the current directory. Go ahead and install all dependencies. Then  bundle them up as a .zip file and you got yourself a deployment package.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#sharing-layers","title":"Sharing layers","text":"<p>Per the AWS doc, you can share a layer you create with other users. All they need is the <code>arn</code> number. However only those functions deployed in your AWS Region can use the layer. If you want it universally available, you may have to publish it in all regions.</p> <p>Just like a typical software library, you can version a layer. This makes it great to keep layers updated as newer versions are released.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#conclusion","title":"Conclusion","text":"<p>This was a quickstart guide teaching what lambda functions are and how to get started with them. Next up, Azure functions.</p>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/","title":"AWS Lambda Functions - authoring with Docker images","text":"<p>At re:Invent 2020, AWS announced support for authoring, shipping and deploying the popular serverless Lambda services via Docker images. Further, they allow images up to <code>10GB</code> in size. As multiple authorities noted, this is a game changer, particularly for the scientific Python community as this would allow us to author machine learning and even deep learning inferencing functions using AWS Lambdas. This blog takes a quick look at authoring a \"hello-world\" style lambda using Docker.</p> <p>To understand how Docker works or how to build Docker images, checkout my Docker 101 cheatsheet. Having said that, you do not need to know much about Docker or containerization for simple functions.</p>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#the-lambda-runtime-images","title":"The Lambda runtime images","text":"<p>AWS provides the runtime images for different languages, including Python. The images are shared at <code>DockerHub: amazon/aws-lambda-provided</code> and <code>ECR Public: public.ecr.aws/lambda/provided</code>. As expected, the base image is a flavor of linux called Amazon Linux.</p>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#the-ric-and-the-rie","title":"The RIC and the RIE","text":"<p>The Python Docker images come with two important components pre-loaded. The RIC - Runtime Interface Client provides the interface between Lambda (infrastructure) &amp; the function code. Think about the invoker from API gateway that runs your handler function. The RIE - Runtime Interface Emulator is a tool that allows you to test the code locally.</p>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#high-level-workflow","title":"High-level workflow","text":"<p>The steps involved in publishing a Lambda function using Docker images will look like below</p> <ol> <li>Download base images in your language of choice</li> <li>Package your code + dependencies + resource files into the image using Docker CLI</li> <li>Push the image into AWS ECR - AWS Elastic Container Registry. Note: You are not charged storage for these images.</li> <li>Create a function, choose 'Container Image' and the rest should be similar</li> </ol>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#gotchas","title":"Gotchas","text":"<ul> <li>Image max size is <code>10 GB</code></li> <li>Application code should run on a read-only file system. Only the <code>/tmp</code> dir is writable with <code>512 MB</code> storage</li> <li>The container is instantiated with a user with least set of permissions. Ensure app code conforms with this.</li> <li>Container settings</li> </ul> <pre><code>ENTRYPOINT \u2013 Specifies the absolute path to the entry point of the application.\nCMD \u2013 Specifies parameters that you want to pass in with ENTRYPOINT.\nWORKDIR \u2013 Specifies the absolute path to the working directory.\nENV \u2013 Specifies an environment variable for the Lambda function. \n</code></pre>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#steps","title":"Steps","text":"<ul> <li>Install Docker for desktop and AWS CLI</li> <li>Create your regular Lambda function into a folder called <code>app</code>. Store your <code>app.py</code>, <code>requirements.txt</code> files into it. See sample below:</li> </ul> <p>app.py:</p> <pre><code>try:\n    import json\n    import sys\n    import requests\n    print(\"All imports ok ...\")\nexcept Exception as e:\n    print(\"Error Imports : {} \".format(e))\n\n\ndef lambda_handler(event, context):\n\n    print(\"Hello AWS!\")\n    print(\"event = {}\".format(event))\n    return {\n        'statusCode': 200,\n        'message': 'hello from docker lambda'\n    }\n</code></pre> <p>requirements.txt:</p> <pre><code>requests\n</code></pre> <ul> <li>Create a docker file as below:</li> </ul> <pre><code>FROM public.ecr.aws/lambda/python:3.8\n\n# Copy function code\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\n# Install the function's deps\nCOPY requirements.txt  .\nRUN pip3 install -r requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n# set the CMD to handler\nCMD [ \"app.lambda_handler\" ]\n</code></pre> <ul> <li>Build Docker image using syntax <code>docker build -t &lt;username/imagename:tag&gt; &lt;build context&gt;</code>. For example: <code>docker build -t atmamani/aws-lambda-docker:v1 .</code>. This prints an output like below:</li> </ul> <pre><code>[+] Building 20.7s (9/9) FINISHED                                                                                                 \n =&gt; [internal] load build definition from Dockerfile                                                                         0.0s\n =&gt; =&gt; transferring dockerfile: 44B                                                                                          0.0s\n =&gt; [internal] load .dockerignore                                                                                            0.0s\n =&gt; =&gt; transferring context: 2B                                                                                              0.0s\n =&gt; [internal] load metadata for public.ecr.aws/lambda/python:3.8                                                            0.4s\n =&gt; [internal] load build context                                                                                            0.0s\n =&gt; =&gt; transferring context: 81B                                                                                             0.0s\n =&gt; [1/4] FROM public.ecr.aws/lambda/python:3.8@sha256:44c0de45aa1927eecd519ad48faa27fe3318717160b5f7560475d12abea7b427     18.0s\n =&gt; =&gt; resolve public.ecr.aws/lambda/python:3.8@sha256:44c0de45aa1927eecd519ad48faa27fe3318717160b5f7560475d12abea7b427      0.0s\n =&gt; =&gt; sha256:20666df3e004b212ed97db3bb9bc7cc0251d842f8b672f1722dc55c0d5f45367 74.99kB / 74.99kB                             0.8s\n =&gt; =&gt; sha256:f7ad8276137021bb0b0cbd6826ec86b0f04c78367e486f3c1728e389d1d400bc 415B / 415B                                   0.8s\n =&gt; =&gt; sha256:44c0de45aa1927eecd519ad48faa27fe3318717160b5f7560475d12abea7b427 1.58kB / 1.58kB                               0.0s\n =&gt; =&gt; sha256:80342c69b467fef0607f90fd75c631fb351da1049b71faaaf98c8c1ce859c7b2 3.00kB / 3.00kB                               0.0s\n =&gt; =&gt; sha256:af4b61775d4cf0f587278cfbc2cbbc9afc3480f5fed5a455a9289c2c387c7187 100.74MB / 100.74MB                           7.5s\n =&gt; =&gt; sha256:bb2e44738d79a6afd429e2a103f081ad47783de7a6f7305664aee8a2e9e3976b 3.32MB / 3.32MB                               2.4s\n =&gt; =&gt; sha256:891c7fcaabb6a1090e28f4d967f1e423aa7cfd1217267b7e9bfe6636cd7b08ef 54.77MB / 54.77MB                            10.0s\n =&gt; =&gt; sha256:c1ae0f49cb65052f0d9674682af2b3bbe09c34fa37004930ef2e8019b76fa2d5 16.10MB / 16.10MB                             6.6s\n =&gt; =&gt; extracting sha256:af4b61775d4cf0f587278cfbc2cbbc9afc3480f5fed5a455a9289c2c387c7187                                    4.9s\n =&gt; =&gt; extracting sha256:20666df3e004b212ed97db3bb9bc7cc0251d842f8b672f1722dc55c0d5f45367                                    0.0s\n =&gt; =&gt; extracting sha256:f7ad8276137021bb0b0cbd6826ec86b0f04c78367e486f3c1728e389d1d400bc                                    0.0s\n =&gt; =&gt; extracting sha256:bb2e44738d79a6afd429e2a103f081ad47783de7a6f7305664aee8a2e9e3976b                                    0.1s\n =&gt; =&gt; extracting sha256:891c7fcaabb6a1090e28f4d967f1e423aa7cfd1217267b7e9bfe6636cd7b08ef                                    3.0s\n =&gt; =&gt; extracting sha256:c1ae0f49cb65052f0d9674682af2b3bbe09c34fa37004930ef2e8019b76fa2d5                                    1.5s\n =&gt; [2/4] COPY app.py /var/task                                                                                              0.1s\n =&gt; [3/4] COPY requirements.txt  .                                                                                           0.0s\n =&gt; [4/4] RUN pip3 install -r requirements.txt --target \"/var/task\"                                                          2.0s\n =&gt; exporting to image                                                                                                       0.1s\n =&gt; =&gt; exporting layers                                                                                                      0.1s\n =&gt; =&gt; writing image sha256:ac4783629d43f0d1f2c641133419bf47b52531b7535bc2ad25dca73ab126f4ce                                 0.0s \n =&gt; =&gt; naming to docker.io/atmamani/aws-lambda-docker:v1                                                                     0.0s\n</code></pre> <ul> <li>Run the container locally using the command:</li> </ul> <pre><code>(base) \u279c  app git:(master) docker run -p 9000:8080 atmamani/aws-lambda-docker:v1\n</code></pre> <ul> <li>You can test by posting <code>curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'</code></li> <li>If you notice any errors, you can edit the files and rebuild the image and update the tag. THen you can retest the application.</li> <li>Authenticate the AWS CLI by following this help.</li> <li>Pass the auth from AWS CLI to Docker CLI, so Docker can tag and later push the images. Docker CLI normally pushes to the default DockerHub registry. However, for Lambda to work, you need to push to Amazon ECR registry. In ECR, I used the web UI to make a repository called <code>ml2web</code>.</li> <li>Run below, where you replace <code>--region us-west-2</code> with your AWS region. You also replace <code>--profile &lt;aws_profile_name&gt;</code> with your AWS CLI profile name. If you are using just 1 account for all (don't do this), then you can skip this argument. The Docker username needs to be <code>AWS</code> always. The URL like string is the name of your ECR registry.</li> </ul> <pre><code>sudo aws ecr get-login-password --region us-west-2 --profile &lt;aws_profile_name&gt; | docker login --username AWS --password-stdin &lt;your_ECR_registry_name&gt;.dkr.ecr.us-west-2.amazonaws.com\n</code></pre> <p>This threw an error saying permission denied, however it still works for me.</p> <ul> <li> <p>Then tag your image using the command <code>docker tag atmamani/aws-lambda-docker:v2 &lt;your_ecr_name&gt;.dkr.ecr.us-west-2.amazonaws.com/ml2web:v2</code></p> </li> <li> <p>Then push to ECR using the command: <code>docker push &lt;your_ECR_name&gt;.dkr.ecr.us-west-2.amazonaws.com/ml2web:v2</code></p> </li> </ul> <pre><code>The push refers to repository [your_ecr_name.dkr.ecr.us-west-2.amazonaws.com/ml2web]\n6b5cd20ea590: Pushed \naef77c8c9312: Pushed \n623b1ac50c4e: Pushed \na1f8e0568112: Pushed \nbcf453d1de13: Pushed \nf6ae2f36d5d7: Pushed \n5959c8f9752b: Pushed \n3e5452c20c48: Pushed \n9c4b6b04eac3: Pushed \nv2: digest: sha256:abcdefgh_some_sha size: 2206\n</code></pre> <ul> <li>Finally, use the Lambda web UI to create a new function. The only difference is, you choose 'container image' in the radio and provide the URI to the ECR registry. You can then browse for your image and tag and choose it.</li> </ul>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#sources","title":"Sources","text":"<ul> <li>Runtime images</li> <li>Using container images with lambda</li> <li>Announcement blog</li> <li>Creating images using AWS base images</li> </ul>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/","title":"Building AWS Lambda Functions with SAM CLI","text":"<p>The SAM (Serverless Application Model) allows you to build complex and production ready lambda functions that are performant yet light-weight. If you are new to serverless computing or lambda functions, checkout my quick start guide. This article helps you with getting started with SAM CLI.</p> <p>The SAM CLI provides you a development environment that is identical to lambda execution environment. This is possible via a Docker image. Thus before installing SAM, you need a few prerequisites met:</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#installing-sam-cli","title":"Installing SAM CLI","text":"<p>The prerequisites are</p> <ul> <li>Docker desktop</li> <li>Shared drive space between host and Docker</li> <li>Homebrew</li> </ul> <p>The SAM installation guide walks you through the steps involved for various platforms. After installation, you need to configure it with your AWS account info as explained here</p> <pre><code>&gt;&gt; sam --version\nSAM CLI, version 1.31.0\n</code></pre> <p>When installing SAM using <code>brew</code>, I noticed the install is global and not restricted to a particular <code>conda</code> env.</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#setting-up-a-sam-app","title":"Setting up a SAM APP","text":"<p>SAM makes it easy to get started with guided CLI set ups and deploys. To start a project, run <code>sam init</code> and answer the questions as shown below:</p> <pre><code>(aws_lambda_default) \u279c  sam-try git:(master) sam init\nWhich template source would you like to use?\n    1 - AWS Quick Start Templates\n    2 - Custom Template Location\nChoice: 1\nWhat package type would you like to use?\n    1 - Zip (artifact is a zip uploaded to S3)  \n    2 - Image (artifact is an image uploaded to an ECR image repository)\nPackage type: 1\n\nWhich runtime would you like to use?\n    1 - nodejs14.x\n    2 - python3.9\n    3 - ruby2.7\n    4 - go1.x\n    5 - java11\n    6 - dotnetcore3.1\n    7 - nodejs12.x\n    8 - nodejs10.x\n    9 - python3.8\n    10 - python3.7\n    11 - python3.6\n    12 - python2.7\n    13 - ruby2.5\n    14 - java8.al2\n    15 - java8\n    16 - dotnetcore2.1\nRuntime: 9\n\nProject name [sam-app]: try-sam-app\n\nCloning from https://github.com/aws/aws-sam-cli-app-templates\n\nAWS quick start application templates:\n    1 - Hello World Example\n    2 - EventBridge Hello World\n    3 - EventBridge App from scratch (100+ Event Schemas)\n    4 - Step Functions Sample App (Stock Trader)\n    5 - Elastic File System Sample App\nTemplate selection: 1\n\n    -----------------------\n    Generating application:\n    -----------------------\n    Name: try-sam-app\n    Runtime: python3.8\n    Dependency Manager: pip\n    Application Template: hello-world\n    Output Directory: .\n\n    Next steps can be found in the README file at ./try-sam-app/README.md\n</code></pre> <p>This creates a folder by the name you provided for the project name during the interactive setup. The structure of the project looks like below:</p> <pre><code>(aws_lambda_default) \u279c  sam-try git:(master) \u2717 exa -T try-sam-app -R -I venv\ntry-sam-app\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 events\n\u2502  \u2514\u2500\u2500 event.json\n\u251c\u2500\u2500 hello_world\n\u2502  \u251c\u2500\u2500 __init__.py\n\u2502  \u251c\u2500\u2500 app.py             # The main app which has the lambda_handler fn\n\u2502  \u2514\u2500\u2500 requirements.txt   # pip install pkgs\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 template.yaml         # cloud formation template pre populated\n\u2514\u2500\u2500 tests\n   \u251c\u2500\u2500 __init__.py\n   \u251c\u2500\u2500 integration\n   \u2502  \u251c\u2500\u2500 __init__.py\n   \u2502  \u2514\u2500\u2500 test_api_gateway.py\n   \u251c\u2500\u2500 requirements.txt\n   \u2514\u2500\u2500 unit\n      \u251c\u2500\u2500 __init__.py\n      \u2514\u2500\u2500 test_handler.py\n</code></pre> <p>You can now edit the files from here. For more info on this, skip to the SAM spec model topic.</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#build-sam-app","title":"Build SAM app","text":"<p>The app can be built by calling <code>sam build</code> from inside the app dir (which has the <code>template.json</code> file.)</p> <pre><code>(aws_lambda_default) \u279c  try-sam-app git:(master) \u2717 sam build\nBuilding codeuri: ~/.../try-sam-app/hello_world runtime: python3.8 metadata: {} functions: ['HelloWorldFunction']\nRunning PythonPipBuilder:ResolveDependencies\nRunning PythonPipBuilder:CopySource\n\nBuild Succeeded\n\nBuilt Artifacts  : .aws-sam/build\nBuilt Template   : .aws-sam/build/template.yaml\n\nCommands you can use next\n=========================\n[*] Invoke Function: sam local invoke\n[*] Deploy: sam deploy --guided\n</code></pre>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#testing-app-locally","title":"Testing App locally","text":"<p>There are 2 ways to test the app locally. One is the simpler <code>invoke</code> command which builds the app in a Docker image, calls the handler (based on what's defined in <code>events.json</code>), reports the output and exits. It looks like below:</p> <pre><code>(aws_lambda_default) \u279c  try-sam-app git:(master) \u2717 sam local invoke\nInvoking app.lambda_handler (python3.8)\nImage was not found.\nRemoving rapid images for repo public.ecr.aws/sam/emulation-python3.8\nBuilding image...................\nSkip pulling image and use local one: public.ecr.aws/sam/emulation-python3.8:rapid-1.31.0.\n\nMounting ~/.../try-sam-app/.aws-sam/build/HelloWorldFunction as /var/task:ro,delegated inside runtime container\nSTART RequestId: e027b54f-86ea-4a31-83ed-ce09d83b9053 Version: $LATEST\nEND RequestId: e027b54f-86ea-4a31-83ed-ce09d83b9053\nREPORT RequestId: e027b54f-86ea-4a31-83ed-ce09d83b9053  Init Duration: 0.11 ms  Duration: 98.97 ms  Billed Duration: 100 ms Memory Size: 128 MB Max Memory Used: 128 MB \n{\"statusCode\": 200, \"body\": \"{\\\"message\\\": \\\"hello world\\\"}\"}%                                                                                                                                            (aws_lambda_default) \u279c  try-sam-app git:(master) \u2717 \n</code></pre> <p>The second method is to use the app interactively: Run <code>sam local start-api</code></p> <pre><code>&gt;&gt; sam local start-api\nMounting HelloWorldFunction at http://127.0.0.1:3000/hello [GET]\nYou can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template\n2021-09-27 16:12:15  * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)\n\n&gt;&gt; curl http://127.0.0.1:3000/hello\n{\"message\": \"hello world\"}%\n</code></pre> <p>Now, if you go to http://127.0.0.1:3000/hello, you get back the default \"hello world\" message.</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#deploying-to-production","title":"Deploying to production","text":"<p>Now that we have verified the local build works (using <code>sam local start-api</code>), we can use SAM to deploy the function to production. This is achieved by calling <code>sam deploy</code>. The first time this is called, SAM recommends calling it using the guided approach -&gt; <code>sam deploy --guided</code>. This takes you through a series of questions which helps set up the deployment or cloud formation stack. Once deployment is complete, it returns with a publicly invocable URL that you can distribute to your users.</p> <p>Often, you might find that some set up which works in the local dev environment breaks in production. You might end up patching the function multiple times. During such cases, you can simply call <code>sam deploy</code> or better, <code>sam deploy --no-confirm-changeset</code> and let SAM build, configure the changesets and deploy them to production. The function is restarted and will get the updates.</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#sam-specification-the-model","title":"SAM specification (the model)","text":"<p>The model is specified in a template while in YAML format as shown below:</p> <pre><code>Transform: AWS::Serverless-2016-10-31\n\nGlobals:\n  set of globals\n  Defines whether product is a Function, API, SimpleTable, HttpApi\n\nDescription:\n  String\n\nMetadata:\n  template metadata\n\nParameters:\n  set of parameters # values to pass to the template at runtime\n\nMappings:\n  set of mappings # kvp to pass based on conditions\n\nConditions:\n  set of conditions # conditions, logic statements\n\nResources:\n  set of resources # stack resources such as EC2 instance or S3 bucket info or details of the lambda runtime\n\nOutputs:\n  set of outputs # describe the values returned when you view the stack's properties\n</code></pre> <p>Only the <code>Transform</code> and <code>Resources</code> is required. Since this template is derived from AWS CloudFormation template, there are some overlaps. For example, below is the template created by the hello world application</p> <pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: &gt;\n  try-sam-app\n\n  Sample SAM Template for try-sam-app\n\nGlobals:\n  Function:\n    Timeout: 3\n\nResources:\n  HelloWorldFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: hello_world/\n      Handler: app.lambda_handler\n      Runtime: python3.7\n      Events:\n        HelloWorld:\n          Type: Api\n          Properties:\n            Path: /hello\n            Method: get\n\nOutputs:\n  HelloWorldApi:\n    Description: \"API Gateway endpoint URL for Prod stage for Hello World function\"\n    Value: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\"\n  HelloWorldFunction:\n    Description: \"Hello World Lambda Function ARN\"\n    Value: !GetAtt HelloWorldFunction.Arn\n  HelloWorldFunctionIamRole:\n    Description: \"Implicit IAM Role created for Hello World function\"\n    Value: !GetAtt HelloWorldFunctionRole.Arn\n</code></pre> <p>This blog is a quick overview of authoring, testing and deploying as simple Python based Lambda function using the SAM CLI.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/","title":"Building data science projects using Azure-ML stack","text":"<p>This wiki covers the steps involved in building a data science project using Azure Machine Learning Workbench product. This also covers the steps involved in productionizing the model as a web service and accessing it over HTTP using its REST API.</p> <p>Azure ML consists of two major parts  - Azure ML portal  - Azure ML Workbench</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#azure-ml-workbench","title":"Azure ML Workbench","text":"<p>Azure ML provides a cloud infrastructure to turn your machine learning projects into production code with the scalability and availability of its cloud infrastructure. As of this blog, Azure itself does not have ML as Service, but it allows you to turn your projects into one.</p> <p>The Workbench is a desktop application + Python libraries with which you build your ML experiments, taking it from data cleaning, model building to refinement. It provides frameworks to </p> <ul> <li>access datasets</li> <li>build models with defined inputs and outputs</li> <li>refine models by tuning hyperparameters and export model parameters and learning artifacts</li> </ul> <p>The Workbench has a run history that allows you to visualize the run results as a time-series and helps you visualize the effects of each run.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#runtime-environments","title":"Runtime environments","text":"<p>The Workbench supports at-least 3 different runtimes.</p> <ul> <li><code>local</code> - This is your local anaconda Python environment (or R env)</li> <li><code>docker-python</code> - This encapsulates the <code>local</code> environment as a Docker container and runs it with this container. This container could run locally, on host OS, or can run on a remote machine as well (such as a VM or HDInsights cluster).</li> <li><code>docker-pyspark</code> - similar, but is useful for distributed computation using Spark.</li> </ul>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#deployment-environments","title":"Deployment environments","text":"<p>The output from Workbench is your Docker image. Azure ML helps you to deploy this image on Azure Container Services and get a <code>REST</code> API for predictions and inference during production.</p> <p>The ML model consists of   - model file (or dir of such files)  - Python file implementing model scoring function  - Conda dependency file (.yml)  - runtime environment file  - schema file for REST API parameters  - manifest file (auto generated) for building Docker image</p> <p></p> <p>Azure ML Model Management console allows your register and track your models like a version control system.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#azure-ml-portal","title":"Azure ML portal","text":"<p>Azure dashboard contains all Azure sub products, including the ML services. Sign in with free Outlook account or Office 365 account and you get <code>$200</code> in free credits.</p> <p>In the console, search for Machine learning experimentation (which is in preview at the time of this article). This service / dashboard provides an environment for the rest of this article.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#walk-through","title":"Walk through","text":"<p>An important process is to write the Python code (in scripts and notebooks) by using <code>azureml</code> Python library for data access, logging and printing. This is the hook for the Workbench UX to interpret model refinement and display them in the dashboard.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#initialization","title":"Initialization","text":"<ol> <li>Log into the ML portal, create a new experiment. Help image</li> <li>Install Workbench. Note: You need macOS, Win 10 or higher for Docker to run. Then sign into the workbench using the same account. Your experimentation account shows up on Workbench. Help image</li> <li>Create a new project, pick from a template if necessary. Help image</li> </ol>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#model-development","title":"Model development","text":"<p>Your Azure ML projects are primarily <code>git</code> repositories. When you open an existing template project, the <code>home</code> tab renders the <code>readme.md</code> file.</p> <ol> <li>Add datasets - Click on the data tab and plus sign. Use wizard to add data sources. If data is tabular, the Workbench attempts to display it as a table and provide some preliminary statistics.</li> </ol> <p>ML Workbench wants to keep a record of all your EDA and data wrangling steps, to aid reproducibility. Hence it stores a <code>.dsource</code> to record how data is input and a <code>.dprep</code> to record the transformations performed on the data.</p> <ol> <li> <p>A <code>.dsource</code> file gets created that contains connections to your dataset and how your file is read.</p> </li> <li> <p>Any data preparation performed by using the built-in prepare tool is stored in <code>.dprep</code> file.</p> </li> </ol> <p>At any time, you can do the same actions in Python, or turn the UX actions into Python code by right clicking the <code>.dprep</code> or <code>.dsource</code> file and generating code file. The sample code that reads from <code>.dsource</code> and returns a pandas DataFrame is below. Script proceeds to run data preparation using <code>.dprep</code> file.</p> <pre><code>from azureml.dataprep import datasource\nfrom azureml.dataprep import package\n\ndf = datasource.load_datasource('iris.dsource')\n# df.head(10)\n\n# column-prep.dprep is my data prep file name.\ndf = package.run('column-prep.dprep', dataflow_idx=0)\ndf.head(10)\n</code></pre> <ol> <li>Serialize your model and its weights using <code>pickle</code> library</li> </ol> <pre><code>with open('./outputs/model.pkl', 'wb') as mh:\n    pickle.dump(classifier, mh)\n</code></pre> <ol> <li> <p>Save your plots as <code>.png</code> files into the <code>./outputs</code> folder. The Workbench picks this up automatically. For each run, the Workbench stores the outputs within the run numbered folder. From the run dashboard, you can download a file later for any run.</p> </li> <li> <p>Choose runtime as <code>docker-python</code>. The Docker base image is specified in the <code>aml_config/docker.compute</code> and run configurations, Python dependencies in <code>docker-python.runconfig</code> files. Workbench first builds a Docker image from base image, installs dependencies and then runs the file.</p> </li> </ol>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#operationalization","title":"Operationalization","text":"<p>Once model is developed, you need to create a schema file that contains the inputs and output of the prediction service. This service is a Python file that reads the pickled model file, accepts inputs, performs predictions and returns the results.</p> <p>The <code>azureml</code> Python library has methods such as <code>azureml.api.realtime.services.generate_schema()</code> to generate schema. To specify the data types, use <code>azureml.api.schema.dataTypes.DataTypes</code> and <code>azureml.api.schema.sampleDefinition.SampleDefinition</code> to specify a sample input. Documentation on these is very thin and is left to user experimentation.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#notes","title":"Notes","text":"<ul> <li>It appears that using Workbench UX to read and clean data into DataFrames is optional. You can as well do all in Python in standard way and create a DF from a .csv file.</li> <li>You accept script parameters as command line arguments. The Workbench UX provides a generic args text box into which you can type the values when running from UX.</li> <li>The text you want persisted in the logs should be sent to Azure ML logger</li> </ul> <pre><code>from azureml.logging import get_azureml_logger\nrun_logger = get_azureml_logger()\n\nrun_logger.log('text')\n</code></pre> <ul> <li> <p>Write your scripts such that there is <code>1</code> main Python file which in turn calls other files that are necessary.</p> </li> <li> <p><code>control_log</code> file contains the running log of the job with details injected by Workbench</p> </li> <li><code>driver_log</code> file contains the print statements</li> </ul> <p>Sources  - Azure ML help</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/","title":"Building RESTful APIs with Flask in Python","text":"<p>This article demonstrates how to quickly build a RESTful API in Python using the Flask library. To know about RESTful APIs, read the article on Design principles behind RESTful APIs.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#boiler-plate","title":"Boiler plate","text":"<p>There is very little boiler plate necessary when defining a flask app. Something as limited as:</p> <pre><code>from flask import Flask\nfrom datetime\n\n# define a variable to hold you app\napp = Flask(__name__)\n\n# define your resource endpoints\napp.route('/')\ndef index_page():\n    return \"Hello world\"\n\napp.route('/time', methods=['GET'])\ndef get_time():\n    return str(datetime.datetime.now())\n\n# server the app when this file is run\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>In the code above, you have defined 2 endpoints - a root <code>/</code> landing page and a <code>/time</code> endpoint. You can run this app from terminal as <code>python -m &lt;filename&gt;</code> which will start the webserver and give you an IP address to go or call.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#passing-arguments","title":"Passing arguments","text":"<p>When calling an endpoint, users can send the server some information. This info can be sent via <code>GET</code> or <code>POST</code> calls. By following HTTP verbs, if user wants to collect info back, they should send it via <code>GET</code>, else if the call is to perform some operation on the server side, then use <code>POST</code>, <code>PUT</code>, <code>DELETE</code>.</p> <pre><code>from flask import request # used to parse payload\napp.route('/hello')\ndef welcome_message():\n    \"\"\"\n    Called as /hello?name='value'\n    \"\"\"\n    # if user sends payload to variable name, get it. Else empty string\n    name = request.get('name', '') \n    return f'Welcome {name}'\n</code></pre> <p>Note, sometimes your web app needs to make calls to other resources on the web. For this you can use the <code>requests</code> library. The <code>request</code> module of Flask used in the snippet is not to be confused with the <code>requests</code> library.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#rendering-html-pages","title":"Rendering HTML pages","text":"<p>Flask allows you to define a <code>templates</code> directory and put HTML pages into it. In these HTML pages, you can define variables and pass values to be displayed when rendering the HTML pages. This totally expands your possibilities without having to write a line of JS code.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;Thermos - building webapis with flask&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;p&gt;\n    A little bit about the machine running this site:\n\n    &lt;h4&gt;OS type: {{os_type}}; OS name: {{os_name}}&lt;/h4&gt;\n&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Put the above HTML page (saved as <code>index.html</code>) into a <code>templates</code> directory. Then you can render this page as</p> <pre><code>from flask import render_template\napp.route('/welcomePage')\ndef welcome_page():\n\n    return render_template('index.html',\n                            'os_type' = sys.platform,\n                            'os_name' = os.name)\n</code></pre>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#getting-user-input-via-html-pages","title":"Getting user input via HTML pages","text":"<p>Not only are HTML pages good to display content on a UI, they are good to collect user input for processing. The snippet below defines a resource that when called via <code>GET</code>, will show an HTML page with form controls. If accessed via <code>POST</code>, it performs the actual processing defined by the resource.</p> <pre><code>@app.route('/eyeFromAbove', methods=['GET', 'POST'])\ndef eye_from_above():\n    \"\"\"\n    If GET request - Displays a html page with box to enter address\n    if POST request - renders output html with image from satellite\n    :return:\n    \"\"\"\n    if request.method == 'POST':\n        # User could have used the web UI or called the endpoint headless\n\n        # If user used the web UI, then address comes as form data\n        if request.form is not None:\n            address = request.form.get('address', None)\n            date = request.form.get('date', None)\n        else:\n            # request data comes via args\n            address = request.args.get('address', None)\n            date = request.args.get('date', None)\n        if address:\n            geocode_dict = geocode_address_executor(address)\n            lon = geocode_dict['location']['x']\n            lat = geocode_dict['location']['y']\n\n        else: # when no address is loaded\n            flash('No address specified')\n            return redirect(request.url)\n\n        base_url = 'https://api.nasa.gov/planetary/earth/imagery'\n        # for some reason, NASA server rejects the GET request if I send data over payload\n        if date:\n            full_url = f'{base_url}/?lat={lat}&amp;lon={lon}&amp;date={date}&amp;cloud_score=False&amp;api_key={key}'\n        else:\n            full_url = f'{base_url}/?lat={lat}&amp;lon={lon}&amp;cloud_score=False&amp;api_key={key}'\n\n\n        # construct the query and get download url\n        # resp = requests.get(base_url, params)\n        resp = requests.get(full_url)\n\n        if resp.status_code == 200:\n            resp_dict = json.loads(resp.text)\n        else:\n            return json.dumps({'error':resp.text})\n\n        # Download the image from Google Earth Engine API.\n        img_resp = requests.get(resp_dict['url'])\n        if img_resp.status_code == 200:\n            img_filename = address.replace(' ','_').replace('-','').replace('.','').replace('*','').replace(',','')\n            with open(f'eye_in_sky_queries/{img_filename}.png', 'wb') as img_handle:\n                img_handle.write(img_resp.content)\n        else:\n            return json.dumps({'error':img_resp.text})\n\n        # render the HTML page\n        return render_template('eye_from_above.html',\n                               media_type='image',\n                               media_url = os.path.join('/','eye_in_sky_queries',img_filename+'.png'),\n                               img_date = resp_dict['date'],\n                               img_id = resp_dict['id'],\n                               img_dataset = resp_dict['resource']['dataset'])\n\n    elif request.method == 'GET':\n        # case when page is loaded on browser\n        return '''\n            &lt;!doctype html&gt;\n            &lt;title&gt;Enter address to view&lt;/title&gt;\n            &lt;h1&gt;Enter the address to get image of&lt;/h1&gt;\n            &lt;form method=post enctype=multipart/form-data&gt;\n              &lt;input type=text name=address value=address&gt;\n              &lt;input type=text name=date value='2013-12-17'&gt;\n              &lt;input type=submit value=Submit&gt;\n            &lt;/form&gt;\n            '''\n\n# this resource is needed to render images from disk. Needed for the template HTML pages\n@app.route('/eye_in_sky_queries/&lt;filename&gt;')\ndef uploaded_file(filename):\n    return send_from_directory('eye_in_sky_queries',\n                               filename)\n</code></pre> <p>Now, to display the image, we need this part in the template HTML file</p> <pre><code>&lt;body&gt;\n    {% if media_type == 'image' %}\n        &lt;img src={{media_url}} class=\"img-fluid\" alt=\"Responsive image\"&gt;\n&lt;/body&gt;\n</code></pre>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#database-crud-operations","title":"Database CRUD operations","text":"<p>One of the popular use cases of RESTful web services is, to perform operations on backend database via the web. In the snippet below, I am using <code>sqlalchemy</code> library to  create an in-memory <code>sqlite</code> database and perform CRUD - Create, Read, Update, Delete operations on it.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#sqlalchemy-in-2-minutes","title":"<code>sqlalchemy</code> in 2 minutes","text":"<p>Sqlalchemy library is an ORM (Object Relatinal Mapper) which allows representing database elements as Python objects. In addition, it allows you to connect to a DB and perform CRUD ops in addition to others.</p> <p>Connecting to a DB. In this case a sqllite db is created. If you want the db in-memory, then specify <code>memory</code> as location.</p> <pre><code>from sqlalchemy import create_engine\nengine = create_engine('sqlite:///filename.db')\n</code></pre> <p>The next step is to establish a session to this newly created db.</p> <pre><code>from sqlalchemy.orm import sessionmaker\nDBSession = sessionmaker(bind=engine)\nmy_session = DBSession()\n</code></pre> <p>The next step is a little boiler plate and can be seen in many examples using sqlalchemy.</p> <pre><code>from sqlalchemy.ext.declarative import declarative_base\nBase = declarative_base()\n</code></pre> <p>Next step is to create a Model - this represents the table, columns, rows as Python objects.</p> <pre><code>from sqlalchemy import column, Integer, Numeric, String\n\nclass Puppy(Base): #must inherit from declarative base\n    __tablename__ = 'puppy'\n\n    puppy_id = Column(Integer, primary_key=True)\n    puppy_name = Column(String[30])\n    # ... other columns\n</code></pre> <p>Finnaly, we need to create the table define above on the db.</p> <pre><code>Base.metadata.create_all(engine)\n</code></pre>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#inserting-rows","title":"Inserting rows","text":"<p>Create an instance of the class you created that inherited the <code>declarative_base</code>.</p> <pre><code>puppy1 = Puppy(puppy_name = 'nemo') #pass the columns info to the constructor\n\n# insert row\nsession.add(puppy1)\nsession.commit()\n</code></pre> <p>Since, we did not pass <code>puppy_id</code>, the Primary key, the session knows this is a new row and it has to be created. Else, to edit an existing row, you still do an <code>add</code> but have the primary key specified.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#user-facing-crud-api","title":"User facing CRUD API:","text":"<p>Below is an example endpoint that can perform all 4 CRUD operations:</p> <pre><code>@app.route('/addresses/&lt;int:id&gt;', methods=['GET', 'PUT', 'DELETE'])\ndef address_id_handler(id):\n    \"\"\"\n    GET - called as /addresses/25\n    PUT - called to update as /addresses/25?address='abc'&amp;lat=25&amp;lon=89\n    DELETE - called as /addresses/25\n    :param id:\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        return jsonify(read_address(session, address_id=id))\n\n    elif request.method == 'PUT':\n        address = request.form.get('address','dummy')\n        lat = request.form.get('lat',0.1)\n        lon = request.form.get('lon',0.1)\n        update_address(session, address_id=id, search_string=address, lat=lat, lon=lon)\n        return jsonify({'success': True})\n\n    elif request.method == 'DELETE':\n        delete_address(session, id)\n</code></pre> <p>If you called <code>/addresses/&lt;id&gt;</code> with an existing <code>id</code> via <code>GET</code>, you perform a Read operation. Calling with a new <code>id</code> via <code>PUT</code> will do a Create operation. However, calling with an existing <code>id</code> via <code>PUT</code> will do an Update operation. Finally, <code>DELETE</code> will Delete that record from the DB.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#conclusion","title":"Conclusion","text":"<p>To view the full app in action, checkout the apps section section. To see its source code, see this GitHub repo.</p>"},{"location":"blog/2018/12/04/coding-standards-for-jupyter-notebooks/","title":"Coding Standards for Jupyter Notebooks","text":"<p>Jupyter Notebook has become incredibly popular among data scientists and general users of Python and R. While the Jupyter framework is liberal and lets you be creative, it would benefit you, your team, and your readers if you define a structure and follow it. Based on my experience as developer evangelist and the author of public-facing notebooks for the last three years, I share in this article the patterns I recommend for writing data science samples using Jupyter Notebook. Read more here</p>"},{"location":"blog/2015/03/15/demystifying-python-jargon-datascience/","title":"Demystifying Python jargon found in Data Science","text":"<p>For someone trying to start out using Python for data analysis, the diversity of terminology can be daunting. This article lists out some of the popular terms you hear going around, groups and explains them in a context.. Read more here</p>"},{"location":"blog/2021/10/12/deploying-deep-learning-inference-as-serverless-functions/","title":"Donut or Not! - Deploying Deep Learning Inference as Serverless Functions","text":"<p>There is a common myth that to perform deep learning, one needs high compute, GPU enabled devices. While this is true to a degree, when training deep learning models, it is often just as possible to perform inference using simple CPU based architectures in compute and memory constrained environments - such as serverless functions. This blog takes you through my journey of deploying a simple donut vs bagel vs vada classifier as a AWS Lambda function! </p>"},{"location":"blog/2021/10/12/deploying-deep-learning-inference-as-serverless-functions/#checkout-the-function-at-bitlydonutornot","title":"Checkout the function at bit.ly/donutornot.","text":"<p>At a high-level, this application consists of a deep learning model that was trained using Fastai using a <code>resnet32</code> backbone and exported as a PyTorch model. The model is invoked by a REST API defined using FastAPI library. The whole service is packaged into a Docker image and pushed up to Amazon's AWS ECR container registry. This is then used by a AWS Lambda service to create a function and exposes it using an AWS API gateway, which gives a URL to the service. The HTML front-end is created using Jinja2 templating, something that is common and well established when using micro-frameworks like Flask or FastAPI (as in this case).</p> <p>The picture below shows the technology stack that powers this app.</p> <p></p>"},{"location":"blog/2021/10/12/deploying-deep-learning-inference-as-serverless-functions/#how-to-guide","title":"How to guide","text":"<p>Surprisingly, the training aspects of this model was relatively simpler and predictable. However, it took a lot of reading, trial and error and experimentation to get the serverless application running. The GitHub repo at https://github.com/AtmaMani/donut_or_not has all the set up and deployment information. So, if that's what you are looking for, please head there.</p>"},{"location":"blog/2021/10/12/deploying-deep-learning-inference-as-serverless-functions/#things-i-learnt","title":"Things I learnt","text":"<p>I wanted to write this blog to share my experience creating this app and also take stock of the current state of things when it comes to serverless deep learning. So, here are some observations.</p> <ul> <li>This project was possible because AWS announced support for authoring Lambda functions using Docker images. Not just any images, but they provided the exact same images they use behind the scenes. AWS also expanded the image size limit to <code>10GB</code>. Since the base images are slim to begin with, this large size allowance permits developers to install most of the libs needed for performing data science.</li> <li>AWS SAM CLI makes it possible to create a template suitable for machine learning. While it gives us a framework, I found this to be bare-bones and not sufficiently furnished for someone to get started quickly, particularly for someone who is not a cloud engineer.</li> <li>It took me multiple attempts to figure out how to get the dependencies successfully install on the image. I finally settled with an approach of specifying certain deps in the <code>requirements.txt</code> and certain others to install directly via <code>Docker RUN</code> command. This seems approach seems fragile to me.</li> <li>Once the dependencies were installed, it took several attempts to get the FastAPI service run via Docker. I had problems understanding how to codify my endpoints in SAM's <code>template.yml</code> file. Since my app allows users to upload a JPG (a binary file), I had to read through several StackExchange comments and SAM's GitHub issues to figure out how to specify the MIME type. It appeared quirky and I would have liked this supported out-of-the-box, as it is with Heroku.</li> <li>Once the FastAPI service started running, I ran into a few permission denial issues. Apparently, Lambda runs the container with a user with minimal privileges, unlike the local runtime emulator. Thus, I ended up with new errors that crop up in production, but not in local dev env.</li> </ul> <p>However, this was a great learning experience and now I have a template to follow, if I were to deploy additional models to production. I have a method to perform deep learning inference via a low-cost, low-maintenance, on-demand manner using serverless cloud infrastructure.</p> <p>I hope this blog and GitHub repo is useful for others who are attempting to do the same.</p>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/","title":"Design principles behind RESTful APIs","text":"<p>RESTful stands for \"Representational State Transfer\". Consider this as a concept and a pattern of building client-server APIs. I have been building Python APIs that consume some popular RESTful APIs for the past 5+ years. This article outlines the aspects of a thoughtful and well-designed REST API. Some of the aspects here are from the perspective of a consumer, not the maker of RESTful APIs.</p>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#what-makes-a-web-api-restful","title":"What makes a web API RESTful?","text":"<ul> <li>Presence of a strong separation between client and server</li> <li>Server should not remember previous actions of the client, ergo, be what people call \"stateless\".</li> <li>Each request from client should be treated independently and as if it is the first request. This goes with the previous point about server being stateless. Thus, the onus of sending context is upon the client, not the server to remember.</li> <li>However, when designing anything for a user, you need to make it as simple as possible. Thus, servers can exchange authentication and context for <code>tokens</code> with the client. This allows for a stateless design, but gives users a stateful experience (such as a shopping website remembering what user added to shopping cart) to the client / user.</li> <li>Responses from server can be marked as cacheable or non-cacheable. This way, client does not have to talk to server for the same requests.</li> <li>Server should provide a uniform interface, no matter what the client is(mobile vs desktop vs another server).</li> </ul> <p>Some advanced features of RESTful APIs</p> <ul> <li>If the logic is complex, you can implement a layered system where clients interact with say, 'Server A'. Then, Server A would interact with many other servers to fulfill the request and give results back to client. Client does not have to know how to talk to the rest of the servers to accomplish this.</li> <li>code on demand: Server might optionally send executable code (like JS) to run on client to fulfil a request.</li> </ul>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#http-request-structure","title":"HTTP Request structure","text":"<p>Next, let us talk about what gets exchanged between the client and the server. A request from client to server consists of 3 parts:</p> <ol> <li>header - which contains:<ol> <li>A request line. This line contains a HTTP verb, a URI and HTTP version number. An example syntax of request line is <code>GET /index.html HTTP 1.1</code></li> <li>optional request headers which appear as kvp. For instance <code>Accept: image/gif, image/jpeg, */*</code>, <code>Accept-Language: en-us</code>.</li> </ol> </li> <li>blank line</li> <li>body (optional) - body can contain any additional information, such as auth info etc.<ol> <li>for instance <code>puppyId=12345&amp;name=Fido+Lava</code></li> </ol> </li> </ol>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#http-response-structure","title":"HTTP Response structure","text":"<p>Similar to a request, the response from server to client consists of 3 same parts</p> <ol> <li>a header, which contains<ol> <li>status line which has the status code, HTTP version</li> <li>optional response headers</li> </ol> </li> <li>blank line</li> <li>body (optional). The body contains what the client asked for. For instance a mp3 file, image file or an html page.</li> </ol>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#making-restful-apis-elegant","title":"Making RESTful APIs elegant","text":"<p>The rules listed above are just the skeleton. Although not strictly needed, when APIs use the the following design patterns, they become easy for the end user to predict and understand the architecture of the backend system they are working with.</p> <p>URI design</p> <ol> <li>URI should take name of resource, not the action to take on them. For instance, <code>/tickets/4</code> is good, while <code>getTicketInfo/4</code> is not. Remember, URIs should be nouns, not verbs.</li> <li>URI should be a plural form for each resource name. For instance, <code>/puppies</code>, <code>/tickets</code>. Depending on the resource needed for the backend, calling the plural form of a resource (<code>/items/</code>) can list all the items, or give a summary info.</li> <li>Use HTTP verbs to indicate action. The verbs are <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, <code>HEAD</code>, <code>OPTIONS</code>, <code>TRACE</code>, <code>CONNECT</code>.<ol> <li><code>HEAD</code> and <code>GET</code> can be called by web crawlers and search engines, so ensure these resources can only get information and not modify.</li> <li><code>GET</code>, <code>POST</code> resources typically give information to client</li> <li><code>PUT</code>, <code>DELETE</code> resources can add, update, delete resources on the server. Thus we cover the <code>CRUD</code> operations using HTTP verbs.</li> </ol> </li> <li>Use HTTP status codes such as <code>404</code>, <code>401</code> along with error messages appropriately.</li> <li>Expose child resources progressively. For instance, if <code>/items/</code> does not expose all the items and only provides a summary or count, then expose a <code>/items/search</code> for client to search the backend database. Finally, ensure <code>/search/</code> accepts well known SQL syntax queries, don't make a custom one.</li> <li>Give the benefit of doubt to the client. Build the REST handler to be fault tolerant, error tolerant by building the validation logic on backend, and not on the client side.</li> </ol> <p>API versioning</p> <ol> <li>Versioning allows to maintain backward compatibility when you build a permanent web API.</li> <li>You can either add version in the URI, as <code>/puppies/v2/resource</code> or in the header of the call.</li> </ol>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#conclusion","title":"Conclusion","text":"<p>To learn more about RESTful APIs, go   - restfulapi.net  - Udacity course on Flask</p> <p>To look at an example of building RESTful APIs, go to my blog article Building RESTful APIs with Flask in Python.</p>"},{"location":"blog/2017/07/20/devops-today/","title":"DevOps Today","text":"<p>As software development matures as an industry, there arose a need to identify and a group a set of tasks and folks, typically in back office, that are required to keep the rest of the software development shop running. These are the folks that keep the servers spinning, repositories and build process going, plumb and connect the various automation tools into a giant contraption. The industry came around to classify the various roles as <code>DevOps</code> short for Developers in Operations.</p> <p>This blog is a short compendium of tools used by DevOps today (early 2017).</p>"},{"location":"blog/2017/07/20/devops-today/#version-control-systems","title":"Version control systems","text":"<p> Called VCS in short form, this is where the sourcecode comes to live. Many popular VCS exist today, but you are more likely to have heard about <code>git</code> or <code>github</code> which are arguably the most popular today. VCS mark every change made to the source code and record who changed it, why and how each changes are linked to one another in a time line. In a large shop, about 50 or so developers can edit the same file yet stay sane and not overwrite or trample on other's work.</p> <p>As the product reaches an important milestone, the team would pull the source code into a build process, convert it into a finished product so the sales team can sell it to customers. But today, in the world of agile software development the philosophy is to build often, ship often and get feedback and implement it immediately thereby reducing the risk of catastrophic failure or going irrelevant. Thus, the shipping process which was once in two years, now has to be performed every day! To enable this the DevOps use two of the following tools in abundance.</p>"},{"location":"blog/2017/07/20/devops-today/#containerization","title":"Containerization","text":"<p>Traditionally software was sold in CD-ROMs which customers would buy and install in their computer. The software was as much a physical entity as it got to be. When the user installed, the software made tangible changes to the customer's operating systems as it copied numerous files into different places and took anywhere from an hour to several. This process was slow, error prone and made it very hard to build software for different operating systems (Windows vs MacOS vs Linux vs Solaris). As the internet and cloud computing came about, server software was installed on computers running on the cloud which the customers accessed as a service, hence the name <code>Saas</code> Software as a Service. With SaaS when the demand rose, the same software had to be installed on additional machines to meet the demand, then removed when the surge dropped. This process of mustering additional machines had to be performed in a matter of seconds if the company wanted to give a seamless experience to the customers and the traditional sense of shipping software as an installable file quickly became impossible.</p> <p>The industry tried various solutions for this problem, one of which was to pre-create additional computers with the necessary software installed and have them turn on only when demand rose. But this meant a number of your assets are sitting idle for the most part and used sub-optimally. To combat this problem, a concept of virtualization was invented. In virtualization, the software and all the necessary dependencies including the operating system are compressed into an <code>image</code> file. When the demand spiked, any computer hardware can be quickly programmed to power up that image and serve the software. This was way faster than installing the full software and is used widely even today. </p> <p>As the containerization technology matured, the DevOps engineers found ways to shed the weight of the image. They invented a leaner method where they hand pick only the necessary parts of the operating sytem and the dependencies and the actual software and put them into something called <code>containers</code>. Containers turned out to be much smaller than images and they were able to power up and run much faster than virtual machines. Further, containers would run the same no matter the flavor of the host computer's architecture, which meant, software engineers can build software for linux and have them sold to a customer which is primarily a Windows shop.</p>"},{"location":"blog/2017/07/20/devops-today/#docker","title":"Docker","text":"<p>.. image:: /images/docker_logo.png</p> <p><code>Docker</code> is one of the most popular containerization technologies. A <code>docker repository</code> contains a bunch of such container images that a software sells. The <code>docker registry</code> is a place where such repositories are hosted so customers can search for, buy and use. The <code>docker hub</code> and more recently <code>docker store</code> are public websites which host registries.</p> <p>When the customer wants to use a software shipped as a <code>docker image</code>, he/she has to install a <code>docker engine</code> which will act as a host to power up the <code>docker images</code> into <code>containers</code>. Increasingly customers identify a powerful computer which they designate for docker engine so it can power up a number of containers which the rest of the company can use as a service. In such cases they use <code>docker swarm</code> which is a cluster of docker engines running in <code>swarm mode</code>.</p> <p>Some companies might lease a cloud from <code>docker cloud</code> and run their containers on hardware leased from docker company while other larger companies can by <code>docker datacenter</code> software so they can power up an entire datacenter using docker based technology.</p>"},{"location":"blog/2017/07/20/devops-today/#continuous-integration","title":"Continuous integration","text":"<p> At the turn of this decade (2011), the software industry embraced a new concept of reducing the gap between software build process and customer involvement. In the old water fall model, the build process had definite stages of planning, constructions, testing and deployment. Often with the pace at which technology evolved, the software being built went out of date before it got delivered. To solve this problem, the industry adopted an <code>agile</code> framework where the build cycles are reduced to a short 3 week period and at the end of it, a fully build software was shipped to the customer. The customer would use, provide feedback which the development team would work on and in another 3 weeks, send out an increment.</p> <p>This model works well, but it also means some tasks which were performed once a year such as build, test, etc. has to be performed every day and in some cases every hour. Thus the devops engineers devised a procedure where at every time a change is made to the source code (or at set intervals) the entire source code gets compiled, built into a finished product, run tests against it and pushed into the deployment phase. This is called <code>continuous integration</code>. To orchestrate this complex process a number of tools are available in the market, some of which are <code>Jenkins</code>, <code>CircleCI</code>, <code>TravisCI</code>, <code>TeamCity</code> etc.</p>"},{"location":"blog/2017/07/20/devops-today/#chef","title":"Chef","text":"<p>Continuous integration solved the problem of keeping the agile software development in a loop. However, when the business model is to sell software as a service, devop engineers had to also automate the process of updating the service with the latest software, something I call <code>agile deployment</code>. To meet these needs, devop engineers need an all overseeing automation platform which can orchestrate the entire process of <code>agile development</code> and <code>agile deployment</code>. <code>Chef</code> is one such product that is popular in the industry today. Though a set of <code>cookbooks</code>, engineers can automate the creation and deployment of containers that result from the continuous integration framework.</p>"},{"location":"blog/2017/07/20/devops-today/#conclusion","title":"Conclusion","text":"<p>Thus this blog outlined some of the popular tools in the devops tool belt. By no means this is a complete or exhaustive list and I have grossly generalized a lot of the processes involved. However the idea of software development in today's world is portrayed to the best accuracy as possible.</p>"},{"location":"blog/2021/02/04/embedding-maps-in-photos/","title":"Where was this photo taken? A Python based web-app to extract location information and embed maps in photos","text":"<p>This is a draft, to be completed soon. Python full-stack web app created for Esri Dev Summit 2021.</p>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/","title":"A Gentle Introduction to the AWS Platform","text":""},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#aws-platform-from-30k-view","title":"AWS platform from 30k view","text":"<p>The AWS platform is an offering from Amazon and by far is considered the industry leader in this segment. It is available in 190 countries around the world and is used by hundreds of thousands of business and governmental bodies around the world.</p>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#regions-and-availability-zones","title":"Regions and availability zones","text":"<p>An AWS region is a physical location in the world, where there can be multiple availability zones. Availability zones consist of one or more discrete data centers, each with redundant power, networking, comms, housed in separate facilities. There are over <code>60</code> availability zones within over <code>20</code> regions around the world.</p>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#components-of-the-platform","title":"Components of the platform","text":"<p>The sheer number of services available from the AWS platform can be quite dizzying. I made the schematic below to refer to the most commonly used services by data scientists:</p> <p></p> <p>For convenience, the services are grouped under a set of broad categories such as analytics, application integration, database, compute, IoT... etc.</p>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#analytics","title":"Analytics","text":"<p>Below are some popular services under the analytics bucket</p> <ol> <li>Athena - interactive query service that makes it easy to analyze data in S3 using SQL</li> <li>EMR - managed Hadoop framework that makes it easy to process vast amounts of data across dynamically scalable EC2 instances.</li> <li>CloudSearch - a search solution for your website or app</li> <li>Elasticsearch - search service</li> <li>Kinesis - analyze real-time and streaming data</li> <li>Redshift - fast, scalable data warehouse with 10x performance using ML, massively parallel query execution, columnar storage on high performance disk.</li> <li>QuickSight - cloud-powered BI service</li> <li>Data Pipeline - process and move data b/w different AWS compute and storage services</li> <li>Glue - fully managed ETL service</li> <li>Lake Formation - service to set up a secure data lake in days.</li> </ol>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#compute","title":"Compute","text":"<ol> <li>EC2 - Elastic Compute Cloud - the service that changed it all. Allows you to pay for only what you consume. 3 types of instances: On-demand (pay as you go), Reserved (dedicated resources at 70% discount) and Spot (utilize unused capacity at 80% discount)</li> <li>Elastic cloud registry - Docker container registry</li> <li>Elastic container service - container orchestration service</li> <li>Elastic container service for Kubernetes</li> <li>Lightsail - VPC with AWS</li> <li>Batch - batch compute jobs on AWS</li> <li>Beanstalk - deploy scalable web apps developed with Python, .NET, PHP, Node.js etc</li> <li>Fargate - compute engine for AWS that allows you to run containers without having to manage servers or clusters.</li> <li>Lambda - run code without provisioning servers. No charge when code is not running.</li> <li>Outposts - bring native AWS services, infrastructure, operating models to any data center / on-premises facility.</li> </ol>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#database","title":"Database","text":"<ol> <li>Aurora - MySQL, PostgreSQL compatible relational database engine.</li> <li>DynamoDB - key-value and document database that delivers single-digit millisecond performance at scale.</li> <li>Neptune - graph database.</li> </ol>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#machine-learning","title":"Machine Learning","text":"<ol> <li>SageMaker - fully managed platform for quickly building, training, deploying ML models at scale.</li> <li>SageMaker Ground Truth - service offers access to public and private human labelers and provides them with built-in workflows and interfaces to build training data. Lowers cost by <code>70%</code>. A model in parallel keeps learning from human labelers and can offer labeling service.</li> <li>Comprehend - NLP service to find insights in text.</li> <li>Lex - build conversational interfaces into any app. It is the same service that powers Alexa.</li> <li>Polly - text to lifelike speech. Supports <code>47</code> voices in <code>24</code> languages.</li> <li>Rekognition - image analysis service that can detect objects, scenes, faces, compare faces, visual search etc.</li> <li>Translate - translation service</li> <li>Transcribe - automatic speech recognition (ASR). Allows you to transcribe files in S3 and add a textual search of audio / conversation files.</li> <li>Elastic inference - attach low-cost GPU powered inference to EC2 and SageMaker instances.</li> </ol>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/","title":"A Gentle Introduction to Cloud Computing","text":"<p>Computing mindset for the 2010s</p> <p>Cloud computing took off in the decade of 2010s. Up until then, when people wanted to run an application, they had to buy computers, databases, switches, network, domains, software, hire IT staff to deploy and maintain anything on the internet. This is similar to learning everything about electricity before you can learn to turn on and off the power switch. Cloud computing changed all of this and allowed developers to build things on the internet without having to worry about hardware and networking.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#what-is-cloud-computing","title":"What is cloud computing?","text":"<p>Cloud computing is renting of resources like storage, CPU, GPU from a server farm. You only pay for what you use - time or storage.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#3-compute-patterns","title":"3 compute patterns","text":"<p>We can group compute patterns on the cloud to 3 categories:</p> <ul> <li>Virtual Machines - where you rent VMs of certain guest OS and manage all the software, OS updates, driver updates etc on it.</li> <li>Containers - similar to VMs, but lack a separate guest OS. All the app logic and data is bundled into an image that is run directly on the host OS</li> <li>Serverless - this is just code, not even images. You write your app to constitute a bunch of functions which are called on demand.</li> </ul> <p></p> <p>Unlike VMs and containers, when using serverless, you only pay for function execution time. In VMs and containers, you pay for run time of container and VM, even if they are idle.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#characteristics-of-cloud-compute","title":"Characteristics of cloud compute","text":"<p>Cloud computing is cost effective as there is no up-front infrastructure cost, no fixed rent, no separate electricity or utility bills for your servers.</p> <p>It is scalable on demand. You can vertically scale by switching to a host machine with more RAM and CPUs or horizontally scale by distributing load to many servers and scaling out.</p> <p>Scaling can be automatic or manual and can be based off a threshold such as CPU load or traffic per second. Thus, the cloud provider can add more resources during peak times and remove unused resources during down time. This nature of scalling is called being elastic.</p> <p>Cloud gives the promise of being current - with respect to security patches, OS updates, hardware improvements etc and reliable - as redundancy is built-in by replicating / backing up periodically. This makes it fault tolerant and recoverable from disasters.</p> <p>All of these happen automatically, in the background, without any interruption to your apps or to the users consuming your apps. Further, cloud makes your app go global as it can replicate content across multiple data centers, allowing for minimal network lag when accessed from across the globe.</p> <p>On top of all this, cloud is secure as they have some of the best firewalls, anti viruses and dedicated team working to thwart attacks. They are also physically secure as they are remote in secure facilities.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#cost-savings-with-cloud-compute","title":"Cost savings with cloud compute","text":"<p>Capital expenditures, such as server costs, storage costs, network costs, backup and DR costs, utility costs are called as <code>CapEx</code> costs. These are typically encountered in the traditional datacenter model and mostly are up-front, but can become recurring as demand changes and technologies shift over time.</p> <p>Operational costs such as building custom features, scaling costs, licensing costs etc are called as <code>OpEx</code> costs.</p> <p>The graphic below illustrates how demand and costs change over time. By moving to the cloud, companies can limit both these costs, while maintaining superior experience for the end users.</p> <p></p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#cloud-deployment-patterns","title":"Cloud deployment patterns","text":"<ol> <li>Public cloud: most common, most simple, everything including hardware, storage, execution is managed by provider. Hardware is typically shared across multiple cloud customers (<code>multitenant</code>). Primary advantage is cost.</li> <li>Private cloud: next common - could be in your premises or actually on the cloud itself (as in <code>VPC</code>). You become the maintainer of the cloud and provision to internal customers. Advantages include highest data ownership, compliance with certain government regulations etc.</li> <li>Hybrid cloud: is a mix of both. Typically, the public cloud contains as much public elements as possible and the private contains the most secure and sensitive information / systems.</li> </ol>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#3-types-of-cloud-services","title":"3 types of cloud services","text":"<ol> <li>IaaS: Infrastructure as a Service: you have complete control over the software stack installed on the servers. The machines / hardware is still managed by the cloud provider, but you manage the host OS, updates, load balancing etc. Think of IaaS as renting hardware. If you want a machine that is dedicated to you, but not necessarily on your premises, then this is the best. The cost is typically pay-as-you-go.</li> <li>PaaS: Platform as a Service: is somewhere in the middle, it provides the entire cloud provider platform to manage the full life-cycle of the product you build, from development to test to deployment.</li> <li>SaaS: Software as a Service: is typically what you build and sell. However, you can subscribe services from the cloud provider as well - for instance AI services such as vision and audio transcribing, email services, storage services etc.</li> </ol> <p>As you thought, each type sits on top of the pervious level, thereby abstracting some level of complexity. The graphic below explains this better.</p> <p></p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#popular-cloud-providers","title":"Popular cloud providers","text":"<p>As of this blog, there are a number of cloud providers out there - Azure, AWS, Rackspace, Linode, Heroku, GCC etc. However, AWS from Amazon and Azure from Microsoft are industry leaders and many other cloud providers actually build on top of these providers. To know more about these, checkout my other blogs on this topic.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#references","title":"References","text":"<ol> <li>Azure - principles of cloud computing</li> </ol>"},{"location":"blog/2017/10/27/harness-the-power-of-gis-python-api/","title":"Harness the Power of GIS with the ArcGIS API for Python","text":"<p>Link: https://www.esri.com/about/newsroom/arcuser/harness-the-power-of-gis-with-the-arcgis-api-for-python/?rmedium=esri_com_redirects01&amp;rsource=/esri-news/arcuser/fall-2017/harness-the-power-of-gis-arcgis-api-for-python</p> <p>The ArcGIS API for Python is a new Python library for working with maps and geospatial data that is powered by Web GIS, whether online or on-premises. It is a Pythonic API that uses Python best practices in its design and employs standard Python constructs and data structures with clean, readable idioms... Read more here</p>"},{"location":"blog/2018/10/26/house-hunting-the-datascientist-way/","title":"House hunting\u200a\u2014\u200athe data scientist way","text":"<p>At some point in time, each of us would have went through the process of either renting or buying a house. Whether we realize or not, a lot of factors we consider important are heavily influenced by location. In this article, we apply the data wrangling capabilities of scientific Python ecosystem and geospatial data visualization &amp; analysis capabilities of the ArcGIS platform to build a model that will help shortlist good properties (houses). Read more here</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/","title":"How many snakes do you need? - An introduction to concurrency and parallelism in Python","text":""},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#performance-matters","title":"Performance matters","text":"<p>At some point, every Python developer wonders if it's their program that is slow, or Python that is slow. In most cases, it is their program itself. Although Python gets a bad rap for being slower than compiled languages like C, C++, developers can utilize concurrency and parallelism to see significant gains.</p> <p>So, what's the difference between concurrency and parallelism?</p> <ul> <li>Concurrency is the virtue of tasks not holding up one another and letting the program to progress. Think of blocking vs non-blocking.</li> <li>Parallelism is the practice of breaking a large task into smaller subtasks that can run in parallel, at the same time.</li> </ul> <p>Both concurrency and parallelism share implementation aspects. Concurrency is more about how a program is structured vs parallelism is doing things in parallel. Thus, concurrency is not inherently parallel, but doing things at the same time (by switching context from one thread to another). In general, UI design cares more about concurrency where as big data and scientific data processing cares more about parallelism.</p> <p>It is also relevant to discuss another paradigm in computing called asynchronous (vs synchronous) processing. A program is set to be asynchronous, if does not expect the caller to wait until it finishes execution. Programs accomplish this by providing a <code>jobid</code> for each task which the caller can poll at intervals to know if the task finished or not. Thus, the caller can submit a job to the async program, proceed to do other stuff and then use the result when it really needs it. Thus, even though a calling program may not implement parallelism, it can now run tasks concurrently by using async programming model.</p> <p>Another way to differentiate concurrency from parallelism is to simply say, parallelism involves threads in different processors, which allows them to execute at the same time, whereas concurrency is multiple threads on the same process, which although are non-blocking, they do not strictly execute all at the same time. However, these threads still individually make progress since the process will cycle through them.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#threads-vs-processes","title":"Threads vs Processes","text":"<p>A process is an instance of your program that is being executed. It has <code>3</code> elements - the code, the data (variables) used by the code and the state of the process (execution context). Each process has its own address space and don't typically talk to each other.</p> <p>Each process can have one or more threads. Threads are lightweight as they share the address space with other threads within the same process and is not treated as a separate entity by the host OS.</p> <p>Typically, operating systems cycle through threads in a round-robin fashion. The thread that gets to execute is called the main thread. But how long will it be the main thread? In Python, a thread will execute until</p> <ul> <li>it is finished</li> <li>until it is waiting for some IO</li> <li>starts a sleep</li> <li>has been running for <code>15ms</code></li> </ul> <p>after which, the OS will switch to another in its queue before returning back to it.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#threading","title":"Threading","text":"<p>A thread in Python can be created using the <code>Thread</code> class from the <code>threading</code> module. A thread can be in one of 3 states.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#thread-states","title":"Thread states","text":"<p>When a thread is created, it just exists, remains in an un-started state. Once you call the <code>start()</code> method, it goes to one of 3 states</p> <ul> <li>Runnable - a state where the processor can execute it</li> <li>Running - currently being the <code>active</code> thread</li> <li>Waiting - where it is blocked by another process or thread. The <code>join()</code> method allows you to represent this dependency.</li> </ul> <p>The scheduler moves the thread through these states. The thread remains alive until its <code>run()</code> method terminates, upon which, it becomes dead. The graphic below (from Hunt J. (2019)) explains these states quite well.</p> <p></p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#instantiating-the-thread-class","title":"Instantiating the Thread class","text":"<p>The constructor for Thread looks like below</p> <pre><code>class threading.Thread(group=None, target=None, \n                      name=None, args=(), kwargs={},\n                      daemon=None)\n</code></pre> <p><code>group</code> is reserved, primarily to be used when <code>ThreadGroup</code> class is implemented. <code>target</code> accepts a callable object (like a method). The <code>run()</code> method of the Thread object will call that that object. <code>name</code> is the name of the thread and by convention takes the form <code>Thread-N</code> where N is a number. <code>args</code> collects the arguments to pass to the callable object and <code>kwargs</code> does the same but with named arguments.</p> <p>So, a simple example could be:</p> <pre><code>from threading import Thread\n\ndef simple_worker(greet='hello'):\n    print(greet)\n\nt1 = Thread(target=simple_worker)\nprint(t1.is_alive())  # prints False\nt1.start()  # prints hello\nprint(t1.native_id)   # prints the thread ID.\n</code></pre> <p>Typically, when you execute in a separate thread, you will notice the kernel's main becomes free even if the thread you spawned is still running. For example see this:</p> <pre><code>from time import sleep\n\ndef worker():\n    for i in range(0,10):\n        print('.', end='', flush=True)\n        sleep(1)\n\nprint('Starting')\n\nt2 = Thread(target=worker, name='t2')\nt2.start()\nprint('\\nDone')\n</code></pre> <p>will print</p> <pre><code>Starting\n.\nDone\n.......\n</code></pre> <p>See <code>Done</code> got printed before all the dots got printed, which means the execution proceeded right along with the next steps. The worker did not block the main thread.</p> <p>You can make one thread wait for another using the <code>join()</code> method. If you add a <code>t2.join()</code> after <code>t2.start()</code>, it would wait for the worker to finish and then print <code>Done</code>.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#inspecting-threads","title":"Inspecting threads","text":"<p><code>threading.enumerate()</code> will print all the threads that are running:</p> <pre><code>[&lt;_MainThread(MainThread, started 4515802560)&gt;,\n &lt;Thread(Thread-2, started daemon 123145471606784)&gt;,\n &lt;Heartbeat(Thread-3, started daemon 123145488396288)&gt;,\n &lt;HistorySavingThread(IPythonHistorySavingThread, started 123145506258944)&gt;,\n &lt;ParentPollerUnix(Thread-1, started daemon 123145523585024)&gt;\n &lt;Thread(t2, started 123145540374528)&gt;]  ## -&gt; my thread\n</code></pre> <p>and <code>threading.current_thread()</code> will print the currently active thread:</p> <pre><code>&lt;_MainThread(MainThread, started 4515802560)&gt;\n</code></pre>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#multiple-threads","title":"Multiple threads","text":"<p>You can start multiple threads for the same worker function. Each thread will invoke the function once, but will retain its own local heap space as can be seen below:</p> <pre><code>from time import sleep\nimport random\n\ndef worker():\n    mark = ['a','b','c','d','e','f','g']\n    prefix=random.choice(mark)  # will choose a prefix at random\n\n    for i in range(0,10):\n        print(prefix+str(i), end=\" \", flush=True)\n        sleep(1)\n\nprint('Starting')\n\nta = Thread(target=worker, name='ta')\ntb = Thread(target=worker, name='tb')\ntc = Thread(target=worker, name='tc')\nta.start()\ntb.start()\ntc.start()\nprint('\\nDone')\n</code></pre> <p>will print</p> <pre><code>Starting\na0 d0e0  \nDone\na1d1e1   a2d2e2   e3d3a3   e4d4a4   e5 d5a5  e6a6d6   a7e7d7   e8d8a8   e9d9a9\n</code></pre> <p>In this case, <code>a</code>,<code>d</code>,<code>e</code> are the prefixes each thread chose. As you see in the print, the threads don't go off in sequence as the prefix arrive mixed in the prints. The same thread never prints in succession, which would mean as soon as the sleep is encountered, the main thread switches to the other threads and so on until each thread has to eventually finish the sleep time.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#establishing-dependency","title":"Establishing dependency","text":"<p>The operating system reserves the right to schedule your threads. Thus as a programmer, you do not have the ability to deterministically know when your threads will start and finish. However, in real life, you need certain workers to finish before you can proceed to the next step. You establish this using <code>join()</code> methods off <code>Thread</code> objects. See example below:</p> <pre><code>\"\"\" Two threads cooking soup \"\"\"\nimport threading\nimport time\n\nclass ChefOlivia(threading.Thread):\n    def __init__(self):\n        super().__init__()\n    def run(self):\n        print('Olivia started &amp; waiting for sausage to thaw...')\n        time.sleep(3)\n        print('Olivia is done cutting sausage.')\n\n# main thread\nif __name__ == '__main__':\n    print(\"Barron started &amp; requesting Olivia's help.\")\n    olivia = ChefOlivia()\n    print('  Olivia alive?:', olivia.is_alive())\n\n    print('Barron tells Olivia to start.')\n    olivia.start()\n    print('  Olivia alive?:', olivia.is_alive())\n\n    print('Barron continues cooking soup.')\n    time.sleep(0.5)\n    print('  Olivia alive?:', olivia.is_alive())\n\n    print('Barron patiently waits for Olivia to finish and join...')\n    olivia.join()\n    print('  Olivia alive?:', olivia.is_alive())\n\n    print('Barron and Olivia are both done!')\n</code></pre> <p>which prints</p> <pre><code>Barron started &amp; requesting Olivia's help.\n  Olivia alive?: False\nBarron tells Olivia to start.\nOlivia started &amp; waiting for sausage to thaw...\n  Olivia alive?: True\nBarron continues cooking soup.\n  Olivia alive?: True\nBarron patiently waits for Olivia to finish and join...\nOlivia is done cutting sausage.\n  Olivia alive?: False\n</code></pre> <p>In the example above, <code>Olivia</code> is the worker thread, on which the main thread waits before proceeding to a certain step. The example also shows how to inherit a <code>Thread</code> class. When doing this, you only override two methods - the constructor and <code>run()</code> which the scheduler will call once the thread is in Runnable state.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#daemon-threads","title":"Daemon threads","text":"<p>Normally, when a program spawns child threads, the program needs to wait until the child completes (that is, if you don't already have a dependency established using a <code>join()</code>). This may not always be ideal and you might want to kill off the child when the main program exits. You can establish this behavior using <code>daemon</code> threads. Daemon threads are created by passing that parameter to the constructor or by setting the <code>daemon</code> property to <code>True</code>. Once started, you cannot change a normal thread to a daemon thread.</p> <p>In the example below, if you did not set <code>daemon=True</code>, the child thread would run forever causing the program to never terminate.</p> <pre><code>import threading\nimport time\n\ndef kitchen_cleaner():\n    while True:\n        print('Olivia cleaned the kitchen.')\n        time.sleep(1)\n\nif __name__ == '__main__':\n    olivia = threading.Thread(target=kitchen_cleaner, daemon=True)\n    olivia.start()\n\n    print('Barron is cooking...')\n    time.sleep(0.6)\n    print('Barron is cooking...')\n    time.sleep(0.6)\n    print('Barron is done!')\n</code></pre> <p>which prints</p> <pre><code>Olivia cleaned the kitchen.\nBarron is cooking...\nBarron is cooking...\nOlivia cleaned the kitchen.\nBarron is done!\n</code></pre> <p>The daemon thread is quit abruptly when the main thread terminates. Thus you should be careful what kinds of ops are relegated to a daemon thread. Good options are using daemon for heartbeat, garbage collection, license checks etc.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#multiprocessing","title":"Multiprocessing","text":"<p>Multiprocessing is running jobs concurrently on multiple processors or cores. This allows developers to truly use modern compute hardware, allowing tasks to run truly in parallel. This mode of computing is useful in data analytics, image processing, animation and gaming.</p> <p>Similar to threading, Python provides a <code>multiprocessing</code> module and a <code>Process</code> class. This can be used to run a callable object such as a function in a separate process. Dependency between processes can be expressed using <code>join()</code> methods. Processes created this way are directly managed by the operating system. Processes are much more heavier and take resources to spin up compared to threads. The advantage though is the ability to exploit multiple cores. </p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#instantiating-a-process","title":"Instantiating a <code>Process</code>","text":"<p>Constructing a process looks like below:</p> <pre><code>class multiprocess.Process(group=None, target=None, name=None,\n                            args=(), kwargs={}, daemon=None)\n</code></pre> <p><code>group</code> is reserved as in threading and is to be used along with the threading API. <code>target</code> accepts a callable object, which will be invoked by the <code>run()</code> instance method. <code>name</code> is the process name, <code>args</code> accepts a tuple to pass to the called function and <code>kwargs</code> does the same with named args. <code>daemon</code> represents whether the process needs to be run as a background daemon.</p> <p>The <code>Process</code> class provides useful methods and properties</p> <ul> <li><code>start()</code> which arranges for the <code>run()</code> to be started in a separate process</li> <li><code>join([timeout in sec])</code> to join the current process with another. Current is blocked until timeout or the blocking process ends.</li> <li><code>is_alive()</code></li> <li><code>name</code> - the process's name. Has no semantics, can be useful for debugging</li> <li><code>daemon</code> - bool flag</li> <li><code>pid</code> - process ID</li> <li><code>exitcode</code> - <code>None</code> if process has not terminated or value</li> <li><code>terminate()</code> - to terminate a process</li> <li><code>kill()</code> - same as terminate, slight differences based on OS.</li> <li><code>close()</code> - releases all resources used by the process. Raises <code>ValueError</code> if the process is still running.</li> </ul>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#using-a-pool","title":"Using a Pool","text":"<p>Since creating processes are expensive, one option is to reuse processes within a given application. The <code>Pool</code> class represents a pool of worker processes and has methods that allow tasks to be offloaded to these worker processes. A Pool can be created as below:</p> <pre><code>class multiprocessing.pool.Pool(processes=None, initializer=None,\n                                initargs=(), maxtasksperchild=None,\n                                context=None)\n</code></pre> <p>where <code>processes</code> is the number of workers to use. Default is <code>os.cpu_count()</code>. <code>initializer(*initargs)</code> is used to represent the method to call and its arguments. <code>maxtasksperchild</code> is the number of tasks a worker can complete. If None, the worker will live as long as the pool.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#poolmap-pattern","title":"<code>Pool.map()</code> pattern","text":"<p>An example of using the Pool is shown below:</p> <pre><code>from multiprocessing import Pool\n\ndef worker(x):\n    print('In worker with: ',x)\n    sleep(2)\n    return x*x\n\ndef main():\n    with Pool(processes=4) as pool:\n        print(pool.map(worker, [0,1,2,3,4,5]))\n\n\nif __name__=='__main__':\n    main()\n\n# output\nIn worker with:  1\nIn worker with:  2\nIn worker with:  0\nIn worker with:  3\nIn worker with:  4\nIn worker with:  5\n[0, 1, 4, 9, 16, 25]\n</code></pre> <p>Best practice is to close the <code>Pool</code> object after use, so the <code>with as</code> statement is used in the example above. In the example above, only <code>4</code> processes were created, but <code>6</code> tasks were given. In this case, excess tasks should wait until running processes finish. The <code>map()</code> function returns an iterable, List in this case. Notice: the output order matches the input order despite things running in parallel.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#poolimap_unordered-pattern","title":"<code>Pool.imap_unordered()</code> pattern","text":"<p>If output order does not matter, then you can use <code>imap_unordered()</code> function instead, which gives you a performance improvement. The above program can be modified as shown below:</p> <pre><code>def main():\n    with Pool(processes=4) as pool:\n        for res in pool.imap_unordered(worker, [0,1,2,3,4,5]):\n            print(res)\n\n# output:\nIn worker with:  0\nIn worker with:  1\nIn worker with:  2\nIn worker with:  3\nIn worker with:  4\nIn worker with:  5\n0\n9\n1\n4\n16\n25\n</code></pre> <p>Notice the mismatch between input numbers and their squares.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#poolapply_asyc-pattern","title":"<code>Pool.apply_asyc()</code> pattern","text":"<p>The <code>apply_async()</code> method allows for tasks to be executed asynchronously. This way, the main process can spawn off processes in a pool and continue to progress. The pool will munch the data. Results can be collected through a callback function or by using a blocking <code>get()</code> method.</p> <p>Example using blocking <code>get()</code> method:</p> <pre><code>from multiprocessing import Pool\n\ndef collect_results(result):\n    print('In collect results: ', result)\n\n\ndef worker(x):\n    print('In worker: ',x)\n    sleep(2)\n    return x*x\n\n\ndef main():\n    with Pool(processes=2) as pool:\n        # blocking workflow:\n        res = pool.apply_async(worker, [2])\n        print('Blocking result: ' + res.get(timeout=5))\n\nif __name__ == '__main__':\n    main()\n\n# output:\nIn worker:  2\nBlocking result: 4\n</code></pre>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#conclusion","title":"Conclusion","text":"<p>This concludes the introduction to concurrency in Python. While this article explains the concepts of threads and processes with examples, it is no way close to explaining how to build a program to use these concepts. That is for another part of this article. In the subsequent parts of the article, we will see the need for locks, barriers, synchronization. We will also look into concurrency patterns such as <code>futures</code> and libraries such as <code>asyncio</code> and review their applications.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#references","title":"References","text":"<ul> <li>Hunt J. (2019) Introduction to Concurrency and Parallelism. In: Advanced Guide to Python 3 Programming. Undergraduate Topics in Computer Science. Springer, Cham. https://doi.org/10.1007/978-3-030-25943-3_29</li> </ul>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/","title":"How many snakes do you need? - Parallel computing architectures","text":"<p>The part 1 of this blog series introduced a quick start to working with threads and processes in Python. This article covers some concepts in parallel computing.</p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#parallel-processing-architectures","title":"Parallel processing architectures","text":"<p>A commonly used system to classify processor architecture is Flynn's Taxonomy. This distinguishes <code>4</code> classes of computer architecture based on <code>2</code> factors: the number of concurrent instruction streams and number of data streams.</p> <p></p> <ul> <li>SISD: Single Instruction Single Data is the simplest of architectures, which operates sequentially using a single processor.</li> <li>SIMD: Single Instruction Multiple Data architecture is a parallel computer with multiple processing units. Each processor execute the same instructions but on different data blocks. </li> </ul> <p></p> <p>This type of architecture is suited for programs that perform the same set of operations on large data sets (like image processing). Most GPUs operate on SIMD. Here, each processor is executing the same instruction, at the same time.</p> <ul> <li>MISD: Multiple Instructions Single Data is one where multiple processes operate on the same data element. This type does not make much sense and is not very common.</li> </ul> <p></p> <ul> <li>MIMD: Multiple Instructions Multiple Data, is one where multiple processors can work on different sets of instructions on different pieces of data.</li> </ul> <p></p> <p>This type of architecture is the most common and is seen everywhere from multi-core phones &amp; computers, supercomputers etc.</p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#mimd-architecture","title":"MIMD architecture","text":"<p>The MIMD is further subdivided into SPMD and MPMD models.</p> <ul> <li>SPMD: Single Program Multiple Data looks a lot like SIMD, however, each processor is executing a copy of the same program but asynchronously and different tasks of the program. SPMD is the most common stye of parallel programming.</li> </ul> <p></p> <ul> <li>MPMD: Multiple Programs, Multiple Data is when processors are each executing separate programs on different pieces of data. MPMD usually involves an orchestrator node and multiple worker nodes. MPMD is not as common as SPMD but suitable for programs that exhibit functional decomposition ability.</li> </ul> <p></p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#shared-vs-distributed-memory","title":"Shared vs Distributed memory","text":"<p>In a parallel computing architecture, the access to data could fall under one of two categories - shared vs distributed. In a shared model, all processors have access to a shared pool of memory. If one of the processes changes some data, all other processes see the change. The shared model is further subdivided into categories: Uniform Memory Access (UMA) and Non-Uniform Memory Access (NUMA)</p> <p>There are many types of UMA architectures, but the most common is the Symmetric Multiprocessing System (SMP). In a SMP, each processor is connected to a single shared memory using a system bus. In modern computers, each processor has its own cache which is a smaller, faster piece of memory that only it can see. </p> <p></p> <p>Processors use cache to store data it frequently works with. However, when a process updates a value in the cache, that needs to be flown to the shared memory before another process reads that value. This phenomena is called cache coherency and is handled by the processor hardware.</p> <p>The other type of shared memory architecture is the NUMA which is made by connecting multiple SMP together over a system bus. The access to memory is non-uniform as some processors can have access quicker than others as it takes longer to read data over a bus.</p> <p></p> <p>Shared memory architectures are easier to program with, but they don't always scale well. Adding more processors to the system adds more memory but introduces the complexity of cache coherency and puts a strain on the system bus. Further, the programmer needs to synchronize memory access.</p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#distributed-memory-architecture","title":"Distributed memory architecture","text":"<p>In a distributed memory architecture, each processor has its own memory with its own address space. Thus a global address space does not exist. The processors are connected over a network (like ethernet) and if a process makes a change to the data, it is not seen by other processes unless the programmer explicitly defines how data is to be communicated between nodes.</p> <p></p> <p>The advantage of dist. memory arch is that it is easily scalable. This structure makes it cost-effective to use commodity hardware to build large distributed systems.</p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#references","title":"References","text":"<ul> <li>Python - Parallel and concurrent programming part 1</li> </ul>"},{"location":"blog/2021/03/30/how-many-snakes-python-concurrency-3/","title":"How many snakes do you need? - Challenges in parallel computing","text":"<p>Concurrency and parallelism as seen earlier can speed up your Python application. However, when not careful, they can crop up a bunch of specific bugs and challenges. We will cover some of those in this article.</p>"},{"location":"blog/2021/03/30/how-many-snakes-python-concurrency-3/#data-race","title":"Data race","text":"<p>A common problem with concurrency is when multiple workers attempt to read the same location in memory and when at-least one of them is attempting to change the value.</p> <pre><code>import threading\n\ngarlic_count = 0\n\ndef shopper():\n    global garlic_count\n    print('Current garlic count: ', garlic_count)\n    for i in range(10_000_000):  # 10 million\n        garlic_count += 1\n\nif __name__ == '__main__':\n    barron = threading.Thread(target=shopper)\n    olivia = threading.Thread(target=shopper)\n    barron.start()\n    olivia.start()\n    barron.join()\n    olivia.join()\n    print('We should buy', garlic_count, 'garlic.')\n</code></pre> <p>The script above spawns <code>2</code> worker threads that increment the garlic count by <code>10</code> million. Ideally, they each are supposed to increment <code>10</code> million, resulting in a final count of <code>20</code> million. However, when you run it, you end up with this:</p> <pre><code>Current garlic count:  0\nCurrent garlic count:  83186\nWe should buy 13724492 garlic.\n</code></pre> <p>which is a classic case of a data race.</p>"},{"location":"blog/2021/03/30/how-many-snakes-python-concurrency-3/#mutex-mutual-exclusion","title":"Mutex (Mutual Exclusion)","text":"<p>A Critical Section is the part of your code where multiple threads share the same data in memory. To prevent data corruption, the task of updating a critical section needs to be done uninterrupted - meaning as an atomic transaction. This ensures all other threads would read the updated value of the critical section and not stale values.</p> <p>In programming, this is accomplished using Mutex or Mutual Exclusion. A mutex is a lock and only the thread possessing it will have access to update a shared resource / critical section. Only after the thread in possession releases a mutex, another thread can gain possession.</p> <p>Acquiring of a mutex (the lock) is atomic, meaning it appears as an indivisible action to other threads even thought it may internally involve multiple steps. Thus no other thread can interrupt the thread acquiring the lock.</p> <p>To rectify the previous code, we add create a mutex object using <code>threading.Lock()</code>. Then we call the <code>acquire()</code> and <code>release()</code> methods on the Mutex object appropriately when editing a shared resource. The snippet below rectifies the shopping cart code we had earlier.</p> <pre><code>import threading\n\ngarlic_count = 0\npencil = threading.Lock() # create a Mutex\n\ndef shopper(pencil):\n    global garlic_count\n    pencil.acquire()\n    for i in range(10_000_000):\n        garlic_count += 1\n    pencil.release()\n\nif __name__ == '__main__':\n    barron = threading.Thread(target=shopper, kwargs={'pencil':pencil})\n    olivia = threading.Thread(target=shopper, kwargs={'pencil':pencil})\n    barron.start()\n    olivia.start()\n    barron.join()\n    olivia.join()\n    print('We should buy', garlic_count, 'garlic.')\n</code></pre> <p>This has spawn two threads as earlier, but the results prints <code>20,000,000</code> correctly:</p> <pre><code>We should buy 20000000 garlic.\n</code></pre>"},{"location":"blog/2021/03/30/how-many-snakes-python-concurrency-3/#locks-and-reentrant-mutex","title":"Locks and Reentrant Mutex","text":"<p>What happens when a thread locks a resource, does not unlock and tries to lock again? Well, a thread cannot lock a resource if it is already locked, so it just ends up waiting for its turn to lock, which will never come. This situation is called a deadlock.</p> <p>Deadlocks pose a major limitation when writing recursive code. A reentrant mutex solves this problem by allowing a thread to lock a resource multiple times. However, the same thread should unlock the resource the corresponding number of times to properly release it.</p> <p>In Python, you create a reentrant lock using <code>threading.RLock()</code> API.  One difference between <code>Lock()</code> and <code>RLock()</code>, when a resource is locked using <code>Lock()</code>, any thread can release it. However, only the thread that locked the reentrant lock can unlock it and it needs to do as many number of times it locked originally.</p>"},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/","title":"Imagery in ArcGIS ecosystem","text":"<p>ArcGIS apps give you access to work imagery data from a variety of file formats. The goal is to unify the differences in image characteristics (spatial, spectral, temportal resolutions), file formats (local - different types of image formats, mosaic and web). However, it is useful to understand the basics. This page does not teach you remote sensing or spatial analysis, it just gives you a roadmap to navigate the software.</p>"},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/#file-formats","title":"File formats","text":"<p>ArcGIS with the help of GDAL system, supports over 400 file formats. ArcGIS Pro extends this support by giving you templates for common imagery providers and satellite/aerial platforms. Thus you can access imagery from  - local file on disk (.tiff, .img, .fgdb raster etc)  - local file from mosaic dataset (a collection of images)  - from an Image service (consider this as a mosaic dataset served over HTTP).</p>"},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/#arcgis-imagery-products-and-terms","title":"ArcGIS Imagery products and terms","text":""},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/#mosaic-dataset","title":"Mosaic dataset","text":"<p>You can create a MD inside your fgdb, then add rasters to it. During this process, you can choose templates to adapt it for different types of satellite images. During this step, you will build <code>pyramids</code> which build tiles for different scales for display purposes and you will build <code>overviews</code> which or low resolution snapshots of the images for viewing them at full extent.</p> <p>A MD consists of   - boundary - extent of all images (union)  - footprint - extent of individual images  - image - images with data</p> <p>A MD is like a catalog of all relevant images. MD is highly scalable, you can dynamically change the mosaic order. </p>"},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/#raster-products","title":"Raster Products","text":"<p>This is a virtual product. For well known satellite images, Pro will dynamically create a new file in the <code>catalog</code>`project` pane that appears to be a composite of all the bands. This is virtual, but can be used like a regular file on disk within Pro. If you expand, you will notice some commonly used band combinations and specific names for those (like cloud, water, vegetation, soil cover etc.)</p>"},{"location":"blog/2007/01/26/my-old-blog-posts/","title":"My old blog posts","text":"<p>These are some blogs I wrote when blogging used to be a thing.</p> <ul> <li>Fun: Why Kochadiiyaan is not an anachronism</li> <li>Finance: Nuances in buying a new car</li> <li>Geospatial: Satellite remote sensing at the consumer end</li> <li>Empirical modeling: Correct until proven otherwise</li> <li>Finance: Nuances in buying a used car</li> <li>Productivity: New age tools for the researcher</li> <li>Travel: Above the abode of clouds</li> <li>Fiction: Are you the martian the world is looking for?</li> <li>Global food crisis - the urban animal's work of art</li> <li>Travel: A Bangalore fly-by</li> <li>Ideas: Solar spheres</li> <li>Ideas: Interlinking of Indian rivers</li> <li>Ideas: Color of sound</li> </ul>"},{"location":"blog/2019/05/31/open-geospatial-world/","title":"The state of the Open Geospatial World in 2019","text":"<p>This article represents my takeaways from FOSS4GNA 2019 conference. FOSS4G - free and open source software for geospatial industry is part of the larger OSGeo organization.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#executive-summary","title":"Executive summary","text":""},{"location":"blog/2019/05/31/open-geospatial-world/#big-data","title":"Big data","text":"<p>Managing big data is the talk of the town. A number of players are solving different aspects of geospatial big data problem. - Element84 is working on Cumulus with NASA for ETL of RS data on AWS cloud. - GeoTrellis is working on scalable raster analysis on Apache Spark. - GeoMesa for vector data on top of Accumulo, Hbase, Cassandra db etc. - PanGeo is working on a hosted notebook server solution for analyzing big geospatial raster datasets. - OmniSci core is a high GPU database query engine. - Google Earth Engine has been revived and being maintained.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#imagery-analysis","title":"Imagery analysis","text":"<p>It was surprising to see raster and satellite imagery analysis be spoken at forefront. It was common understanding that we are about to witness an explosion in availability of Earth Observation (EO) images (via dove, cube sats, drone images, NASA and other international space agency satellites). And a number of products were focussed on solving the data analysis challenge such as GeoTrellis, PanGeo, Google Earth Engine.</p> <p>Efficient tiling engines - GeoTrellis, other providers build tiles off raster outputs and serve that. The idea is serving WMTS is less CPU intensive than WMS (which is dynamic)</p> <p>NASA appears to be a big customer of FOSS4G on the cloud. Companies like Element84, Pangeo seem to be born out of NASA's transition to cloud.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#data-discoverability","title":"Data discoverability","text":"<p>The amount of data collected is projected to grow exponentially. However, today, different vendors use custom or sometimes proprietary metadata files, making indexing and searching difficult. Thus, STAC - SpatioTemporal Asset Catalog is a common specification created to describe a range of geospatial datasets (Imagery, SAR, Point Clouds, Scientific data, FMV etc.).</p> <p>Even though STAC is relatively new (release in April 2018), a number of APIs, applications have already adopted this standard. For instance, sat-api of sat utils project from Developmentseed was used as a premier example of embracing STAC</p> <p>NASA's transition (specifically the NASA ACCESS 2017 grant) appears to drive creation of new standards like STAC. New cloud-native data formats such as Cloud optimized GeoTIFF, Zarr (Z arryas for nD data similar to netCDF) are being worked upon.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#other-observations","title":"Other observations","text":"<p>Even though the FOSS ecosystem appears fragmented, a number of libraries effectively self-organize and reuse one another. For instance, intake (which specializes in data discovery, loading, management, dissemination)  uses Xarray which uses rasterio wich again uses GDAL for loading rasters.</p> <p>Python is everywhere. Most scientific analysis presentations used Python APIs to some extent.</p> <p>FOSS is struggling to maintain funding and a number of core projects have a very tiny team that's building them.</p> <p>While the above is about what was seen at FOSS4G, it is also important to note what was missing. There was little mention about native smartphone applications. I could not find a framework that allowed platform agnostic (or not) development of runtime applications. There wasn't an equivalent to ArcGIS Runtime stack. My colleague noted the absence of the term <code>Data Science</code>. While this crowd is certainly technical and does advanced machine learning, they don't to use or associate with the term Data Science.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#points-to-ponder","title":"Points to ponder","text":"<ol> <li>Should ArcGIS Image Server evaluate Spark stack for distributed raster processing (like it's existing GeoAnalytics stack)?</li> <li>A participant noted that ArcGIS Online needs an easy, one step imagery publishing functionality. The barrier to analyzing raster is to purchase and set up ArcGIS Desktop or ArcGIS Enterprise. Further, the AGS JS API cannot display and work with local rasters. These shortcomings raise the barrier when users need to analyze their own imagery datasets. FOSS on the other hand has identified this shortcoming and products like GeoTiff.io were created which allows you to quickly analyze your local rasters right in your browser.</li> </ol>"},{"location":"blog/2019/05/31/open-geospatial-world/#products-seen-in-the-wild-at-foss4gna","title":"Products seen in the wild at FOSS4GNA","text":""},{"location":"blog/2019/05/31/open-geospatial-world/#cesium-gis","title":"Cesium GIS:","text":"<p>|  | Ceasium is primarily for 3D GIS. Products include     - ion - cloud platform     - 3D tiling pipeline     - SDK     - 3D Content - imagery, 3D Tiles etc.  |</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#crunchy-data","title":"Crunchy data","text":"<p>|  | Crunchy is enterprise postgreSQL leader. They got postgres for cloud etc. As is, this is not a FOSS4G company. Just that Paul Ramsey works there. |</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#geosurgeio-no-website-yet","title":"[Geosurge.io] - no website yet.","text":"<p>|  | Works on http://app.geotiff.io/ a UX for analyzing raster data on the browser using JS, client side computations |</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#omnisci","title":"Omnisci","text":"<p> Omnisci has GPU based high speed sql queries. Their chief evangelist said \"Speed is so high, that we don't worry about spaital indexing\". Go to https://www.omnisci.com/demos/ and https://omnisci.cloud/accounts/login/?next=/ to spin up a trial omnisci cloud for you.</p> <ul> <li>Omnisci's core is FOSS, but higher stacks are SAAS, something similar to carto.com.</li> <li>Omnisci Immerse is similar to Insights for ArcGIS product.</li> <li>They talk about \"VAST data\" (volume, agility, spatio temporal data) and their platform is uniquely positioned to analyze that.</li> <li>From a climate science talk: A typical data analysis workflow is to use omnisci (cloud) for cleaning big data in the cloud. Then download to disk for Jupyter Notebooks and Python libraries like seaborn, geopandas, xarray to visualize, explore and analyze.</li> <li>Below is an architecture diagram of Omnisci cloud:</li> <li></li> </ul>"},{"location":"blog/2019/05/31/open-geospatial-world/#google-earth-engine-oss-server","title":"Google Earth Engine OSS server:","text":"<p> GEE is not dead, it is being maintained at http://www.opengee.org/. The talk showed how to build tilecache for local datasets, load on GEE. But the speaker ran into many issues and it looked cumbersome to work with.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#geotrellis","title":"GeoTrellis","text":"<p> is a Scala Lib to work with raster data using Apache spark. It can do fast IO, map algebra, R2V, V2R conversions. It can render outputs to PNGs. It can perform dynamic computations as well as batch processing. The diagram below shows their overall architecture: </p> <p>Some recent additions allow GeoTrellis to build vector tiles off vector data, work with point cloud data, streaming data and they are working to enable GeoTrellis speak to GeoServer.</p> <p>Geotrellis had a lot of contributors and speakers from Azavea and the theme of this talk was 'cloud native GIS apps' as opposed to  Service Oriented Architecture (SOA) of traditional GIS Server apps.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#geotrellisrasterframes","title":"Geotrellis/RasterFrames","text":"<p> is spark DF for raster data. It allows to perform spatiotemporal queries, map algebra ops on rasters along side Spark ML algorithms. </p> <p>The PyRasterFrames provides Python bindings for Geotrellis scala API. PyRF is based on PySpark API. Their MAML (map algebra modeling language) was shown off significantly.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#azavea","title":"Azavea","text":"<p> appears to sponsor most of GeoTrellis work, their demo servers and speakers at this conference. Most of Azavea's talks were packed.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#azavearastervision","title":"Azavea/rasterVision","text":"<p> framework is something to be noted. This API builds on top of Tensorflow and supports workflows such as classification, object detection and semantic segmentation of satellite images. </p> <p>Raster Vision's goal seems to be well defined. The why? article clearly outlines how Azavea pictures this library as a plumber and how it benefits in creating a repeatable and deployable deep learning pipeline.</p> <p>In these ways, raster vision is aligned with the 'learn' module of the ArcGIS API for Python.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#geomesa","title":"GeoMesa","text":"<p> is for large-scale spatial querying, analytics on distributed systems. SpatioTemportal indexing on top of Accumulo, Hbase, Cassandra db etc for vector data.</p> <p>I did not collect more information about this product. I left that to my expert colleagues from database teams.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#aws","title":"AWS","text":"<p> Arguably the elephant in the room. AWS was one of the biggest sponsors of this conference and its Geo Data lead Joe Flasher did a pretty good job of being a technical evangelist and also supporting a lot of his customers (including Esri) in a number of other talks.</p> <p>Why is AWS interested in FOSS4G? Simply because its customers are. NASA and DigitalGlobe are two of its biggest customers and AWS is pretty vested in supporting them.</p> <p>Joe spoke about https://registry.opendata.aws/ which contains all open data including spatial. Some subsets from the site are</p> <ul> <li>https://registry.opendata.aws/tag/satellite-imagery/</li> <li>https://registry.opendata.aws/tag/geospatial/</li> </ul> <p>The landing page https://aws.amazon.com/earth/ is geared toward its efforts in the geospatial sector. This page features a number of talks on this topic from customers of AWS. (Esri's Peter Becker's talk from reInvent is featured here).</p> <p></p> <p>He spoke about AWS Snowball edge which is a portable AWS cloud that gets shipped out to disaster response centers for their local VPC. During such calamities, (for customers like element84), they start a data checkout into the snowball edge. STAC is the protocol for how data is laid out.</p> <p></p> <p>AWS is also building a ground station network for satellite data reception. Thus straight from satellite into AWS infrastructure. This has been functional since 2018 Nov.</p> <p> AWS and a few other companies sponsor the https://spacenet.ai/ contains ML challenges to be solved in the GeoAI community.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#here-technologies","title":"Here technologies","text":"<p> showed off their geospatial platform. It looked good, but I didn't go into the details. Their emphasis however, seemed to be around managing mapping data from and for autonomous vehicles.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#speaker-slide-decks-and-other-resources","title":"Speaker slide decks and other resources","text":"<p>There isn't a formal place where the decks are shared. These are the ones I manage to source from Twitter and other similar sources  - Paul Ramsey's keynote on FOSS lifecycle  - Gretchen's blog post  - My slide deck on 'Let's take the machines house hunting'</p> <p>Resources:  - GIS Carpentry This teaches EDA using R. The root page for all geosaptial data carpentry is here.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/","title":"Practical scrum for agile teams","text":"<p>In this blog, I introduce the concept of scrumming, which is used widely by agile software development teams. I draw upon my introspections of practicing scrum for 3 years.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#table-of-contents","title":"Table of contents","text":"<ul> <li>Misconceptions</li> <li>What is scrum?<ul> <li>History of scrum</li> <li>Core tenets of scrum</li> </ul> </li> <li>The scrum framework<ul> <li>4 formal scrum events</li> <li>Roles in a scrum<ul> <li>Scrum team</li> <li>Product Owner \\(PO\\)<ul> <li>PO responsibilities</li> </ul> </li> <li>Development team</li> <li>Scrum Master \\(SM\\)</li> </ul> </li> <li>Management tools and metrics in scrum<ul> <li>Product backlog</li> <li>User stories \\(PBIs\\)</li> <li>Tasks</li> <li>Epics</li> <li>PBI estimates</li> <li>Sprint velocity</li> <li>The burn down chart</li> </ul> </li> </ul> </li> <li>Using scrum to plan your projects</li> <li>Lessons learned</li> </ul>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#misconceptions","title":"Misconceptions","text":"<p>What scrum is not? - not just a software development process. It is independent of industry, in fact can be applied even in your household.</p> <p>Scrum is not all about the stand-up meetings - although that is sometimes trademark of teams that follow scrum. The stand up meetings is a means to improve communication and camaraderie through regular in-person or video conference meetings.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#what-is-scrum","title":"What is scrum?","text":"<p>It's a framework for teams that want to be self-organizing, agile and adapt quickly to changing requirements and deliver in short manageable time line.</p> <p>It is surprising to observe even how considerably large projects can be broken down into smaller pieces and delivered in half or quarter of the projected time and expense by adopting scrum methodology.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#history-of-scrum","title":"History of scrum","text":"<p>Founded on empirical process control theory or empiricism, scrum believes knowledge comes from experience and emphasizes on making decisions based on what is known. Scrum employs iterative, incremental approach to improve predictability and control risk. There are 3 important principles of empirical process control theory:  - Transparency - a common metric to measure progress and a shared definition of done  - Inspection - regular and frequent inspection of progress is required. But inspection should never get in the way of work or stop or modify the project.  - Adaptation - if inspection leads to a discovery of something in deviation then an adjustment should be made at the earliest.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#core-tenets-of-scrum","title":"Core tenets of scrum","text":"<p>Let us look at a few process management techniques that take a systems approach to understand where scrum fits in:  * Lean - The goal of lean is simple: minimize the time between customer request and fulfillment by continually improving and reducing non-value adding work.  * Agile - Uses business value as primary measure of progress. Reduces risk through continuous delivery  * Scrum - a wrapper around agile and is a process to practice agile delivery / development. Scrum is industry and technology agnostic.</p> <p>In this light, it is relevant to discuss about the key points of the Agile manifesto: An agile process values:  - Individual and interactions over processes and tools. Teams are self-organizing and self-solving all the way. Teams are self-contained as they figure out what to do and also how to do.  - Working software over detailed doc explaining the eccentricities and caveats while using the product. Customers pay for value, which should be provided by the main product and not by a bunch of explanations about the product. Scrum lays emphasis on developing a 'definition of done' and arriving at a consensus with the customer / stakeholder. This <code>done</code> definition could be such that   - Customer collaboration over contract negotiation. Most successful businesses attribute their success to one thing - listening and providing what the customer wants. This can be achieved only involving the customer / stakeholder not only in the beginning but also at regular intervals during the development life cycle.  - Responding to change (agile) over following a process. In today's world with increased access to real-time information in all fronts, companies can continue to remain on top only by embracing change and adapting to it. This means, as your customers constantly adapt to change, so do you. By delivering working software in increments and but periodically collaborating with the customer, scrum helps you to be agile to these changes. As needs change, your development team can change trajectory without much losses or overheads. </p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#the-scrum-framework","title":"The scrum framework","text":"<p>Now, we are getting into the details of scrum. Let us visit some concepts about scrum that will help us better understand how to adopt it:</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#4-formal-scrum-events","title":"4 formal scrum events","text":"<p>Although scrum encourages daily stand-up meetings, it discourages longer more formal events. It recommends each team to organically figure out at what frequency they meet and for how long they discuss. Having said, below are four formal events that scrum teams practice:  - sprint planning - a meeting where the entire team plans the work to be done during that sprint / iteration  - daily scrum - a short daily meeting where entire team takes stock of work done the previous day  - sprint review - a meeting toward the end of the sprint where the product delivered incrementally is visited and reviewed.  - sprint retrospective - another meeting toward the end of the sprint with emphasis on interpersonal relationships, productivity where the team discusses their development methodology, impedances and figure what can be improved.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#roles-in-a-scrum","title":"Roles in a scrum","text":"<p>Scrum believes in teams composed of multi-disciplinary and cross-functional team members such that all the talent and skills required to finish the product are self-contained within the team. As product complexities grow, inevitably each of your team members start becoming subject matter experts specializing in a narrow functional area. Yet this introduces a unique risk in the form of single point failure. Thus scrum encourages developing a healthy mix of multi-disciplinary team members who are sufficiently acquainted in areas other than their own. Some managers call these as T members (since their spread both horizontally and also vertically). Thus whenever a shortage of resource or skill set arises, the existing team can expand and fulfill the gap without immediately becoming brittle. In this light, each of the team member fit into one of the following roles:</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#scrum-team","title":"Scrum team","text":"<p>A team consists of   - product owner  - development team  - scrum master</p> <p>Scrum teams are self organizing - they decide how to work on things without external direction, cross functional - have all competencies to finish the project without relying on other teams</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#product-owner-po","title":"Product Owner (PO)","text":"<p>Manages product backlog (more on this later) - furnishing with details, prioritizing it, exposing it to all team members.</p> <p>Only the PO has the right to change the backlog and the entire organization must respect this decision. The development team works from this backlog and no one can inject a different set of requirements bypassing the PO.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#po-responsibilities","title":"PO responsibilities","text":"<ul> <li>Manage profitability and ROI (return on investment). A PO does this by prioritizing the backlog to maximize the business value delivered. To start with, the PO must arrive at a way to determine the value delivered, a metric of sorts</li> <li>Call for releases. PO decides what constitutes a minimum shippable product. PO also has liberty to move the release time line forward or backward to maximize the ROI.</li> <li>Guides product development. PO would establish, nurture and communicate the vision. Knows what to build and in what sequence (note, how to build is not a requirement for PO. The development team figures this out).</li> </ul>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#development-team","title":"Development team","text":"<p>This amazing team is self organizing, not even the scrum master tells how to turn the backlog into products. Scrum calls everyone a developer no matter the work performed by the person. No sub teams can exist within a scrum team. 3-9 is a valid team size not including PO and SM unless they also act as development team.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#scrum-master-sm","title":"Scrum Master (SM)","text":"<p>Ensure scrum is understood and practiced. SM plays a servant-leader role in helping teams be self organizing and helping PO with backlog grooming, arrangement, interfacing with external influencers etc. The scrum master usually drives the team meetings.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#management-tools-and-metrics-in-scrum","title":"Management tools and metrics in scrum","text":"<p>Scrum provides a set of tools, terms and metrics to organize and manage your project. Let us visit them briefly here:</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#product-backlog","title":"Product backlog","text":"<p>The backlog contains the list of all requirements. Each item (a PBI - product backlog item) in the backlog is written as a user story. The PO prioritizes the backlog and arranges the items in the order in which it has to be finished. The order of backlog should maximize the ROI.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#user-stories-pbis","title":"User stories (PBIs)","text":"<p>Each PBI is written as a user story. User stories are of format:</p> <pre><code>As a &lt;role&gt;,\nI can &lt;feature / function&gt;,\nso that &lt;goal / value&gt;.\n</code></pre> <p>For example, the user story for backup camera in a car can be written as</p> <p>As a driver, I can use the back up camera when I am reversing the car so I can see what's behind the car more clearly than what my rear view mirrors show.</p> <p>What is management without acronyms :) Lets throw one for user stories.. A user story is good if its INVESTable: Independent, Negotiable, Valuable, Estimatable, Small and Testable. Resist from writing a user story until you have a clear understanding of what you are building.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#tasks","title":"Tasks","text":"<p>Each PBI written as a user story will have a set of clearly defined tasks which need to be finished to call that PBI <code>done</code>. The scrum master encourages the team to come up with a definition of done. This definition can vary slightly between each PBI. The purpose of this definition to establish and maintain the quality of the product being delivered over several sprints.</p> <p>Tasks are usually technical in nature and they are collectively created as a team during the scrum planning meeting.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#epics","title":"Epics","text":"<p>When a bunch of user stories come together, they form an epic. Consider an epic as a large feature that is composed of many sub features that together make a functionality complete and usable. In the case of the backup camera we user story we visited earlier, an epic would be a collection of few more user stories such as:</p> <p>Backup camera Epic:</p> <ol> <li>Turn on back up camera when put in reverse</li> <li>Enhance back up camera feed with steering predictors / guides</li> <li>Alert with audio and steering wheel vibrators when a moving object is detected in back up camera feed</li> <li>Alert lane departures using back up camera feed.</li> </ol> <p>Next, let us discuss about some metrics to quantify and estimate progress of a scrum team.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#pbi-estimates","title":"PBI estimates","text":"<p>Scrum believes in breaking down your product releases (or epics) into smaller identical building blocks called PBIs. No matter how you try, not all of your PBIs would be of identical nature in terms of complexity and effort. Further, some of your PBIs may be diagonally opposite that they may nothing in common. Some of your PBIs may be so new to everyone in your team that nobody knows exactly how to finish it. Then how do you compare and estimate their effort, provide a delivery date and bill the customer?</p> <p>One way to estimate is treat the PBIs as abstract entities and judge them relative to one another in terms of time or effort required to finish them. As a team, during the sprint planning meeting you sort arrange the PBIs in ascending order of effort involved. It is much easier to compare a PBI against another PBI which your team has completed and evaluate comparatively if it requires the same time, lesser or greater time. Then you assign effort in abstract terms such as T shirt sizes (S, M, L, XL, XXL, ..) or Fibonacci number sequence (1, 2, 3, 5, 8, 13 ..).</p> <p>Thus, in the backup camera example from earlier, a team may evaluate</p> <ol> <li>Turn on back up camera when put in reverse --&gt; S or 2</li> <li>Enhance back up camera feed with steering predictors / guides --&gt; M or 3</li> <li>Alert with audio and steering wheel vibrators when a moving object is detected in back up camera feed --&gt; L or 5</li> <li>Alert lane departures using back up camera feed. --&gt; M or 3</li> </ol> <p>Time and effort estimation are much accurate as long as you work with smaller numbers (S, M or 1, 2, 3 ..) and they get fuzzy and uncertain once they go beyond 8 or XL size. If you find yourself with PBIs that are in larger end of effort spectrum, try to break it into smaller pieces.</p> <p>Since you write the PBIs as user stories, the PBI estimates are also called as <code>story points</code>.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#sprint-velocity","title":"Sprint velocity","text":"<p>Once you start to quantify your PBIs with story points, you can calculate how much you manage to finish during a sprint. Remember, a PBI is considered finished only after it meets the <code>definition of done</code> explained earlier. This number which is the sum of finished PBI story points is your sprint velocity.</p> <p>In the backup camera example, if your team finishes the first two PBIs in a sprint, then your <code>velocity</code> is <code>2 + 3 = 5</code>. If your sprint consisted of 3 weeks then you may understand that is the time required for finishing an effort 5.</p> <p>Just like the story points, the velocity is highly subjective to each team. Two scrum teams cannot compare their velocities since it is arbitrary in nature. However, the velocity is a useful metric as it allows you to estimate how many PBIs can be completed in a typical sprint.</p> <p>From the example above, if your sprint velocity is a <code>5</code>, then you may estimate that you need a minimum of <code>2</code> more sprints to finish the remaining 5 and 3 story point PBIs. Once that epic is finished, the PO can call for a release cycle and release the full product.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#the-burn-down-chart","title":"The burn down chart","text":"<p>Now that you have enough metrics to play with, you can create a chart to visualize them. Simply plot the time (perhaps in the form of sprints) in X axis and PBI story points for an epic in the Y axis. Next plot your the velocity for each sprint as a line chart, you get yourself a burn down chart.</p> <p></p> <p>The chart helps you to inspect the pace and estimate when the epic would get done. You can take precautionary measures if you anticipate requiring more time.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#using-scrum-to-plan-your-projects","title":"Using scrum to plan your projects","text":"<p>Now that you are familiar with the terminologies and tools involved, let us review how to apply them in project management. In the agile management approach, you plan for releases in three levels. </p> <ol> <li>First is the <code>release plan</code>. Here, you arrange the PBIs and or epics in the order of importance or value. The PO takes a call on what to include and what to leave out. All the PBIs go into the product backlog list.</li> <li>Each release is composed of one or more <code>sprints</code>. A sprint is a short two to three week period where the entire scrum team comes together, works on one or two PBIs and finishes it. During the <code>sprint planning</code> meeting, the scrum master coaches the development team to work with the PO and pick the PBIs they want to finish during that sprint. These PBIs enter the <code>sprint backlog</code>.</li> <li>During the sprint, the team meets for a short 15 min stand-up meeting where each member states what they worked on, plan to work and if they have anything impeding their progress. The scrum master takes responsibility to clear these impedences. The product owner clarifies any questions the team has.</li> </ol> <p>At the end of the sprint, in a <code>sprint review</code> meeting the team demonstrates the finished product increment to the stake holders and get their sign off. This is also a chance for the stake holders to request for new features or modify the goal of the project based on changing market needs. If such a case arises, the product owner adds them to the product backlog. At no cost can the stake holders interfere a scrum team during a development cycle.</p> <p>Also at the end of the sprint, the team convenes for a <code>sprint retrospective</code> to discuss how the iteration went, what can be improved or changed. The team either proceeds to or reconvenes to plan the next sprint in a <code>sprint planning</code> meeting. The cycle repeats with the team working with product owner to pick the PBIs they want to finish in that sprint cycle and so on.</p> <p>Over this course, each team member have clearly defined roles and know what they are responsible for. The product owner continues to interface with the stake holders, get new requirements, keep the product backlog organized and prioritized by business value. The scrum master continues to perform the servant-leader role and coaches the team toward peak performance. The SM ensures the team is safe from external disturbances or injection of work items. The development team continues make progress by working on as few PBIs possible at a time as a team. They finish the PBIs to meet the <code>done</code> specification before proceeding to work on a new PBI. If a task is left undone, the team members feel free to grab it and work on it. There is no ownership of duties, but there is ownership of the product as a whole.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#lessons-learned","title":"Lessons learned","text":"<ol> <li>Don't compare velocities between teams</li> <li>your velocity will change as members are added or removed from your team</li> <li>velocity changes seasonally as your members go on vacations, fall sick, or other tasks impede them</li> <li>when you break a larger PBI down to smaller ones, you almost always realize they break down into larger than expected pieces. For instance, I have noticed, breaking a 8 will not simply yield a 5 and 3 story point PBI, but often into two 5s or if we are lucky into three 3s. This is not bad, since it benefits you to over estimate time required than otherwise.</li> </ol>"},{"location":"blog/2017/07/09/arcgis-api-for-python-v1-2/","title":"ArcGIS API for Python v1.2 is Here!","text":"<p>Since its official debut in winter 2016, the ArcGIS API for Python has been a huge hit. The API caters to a wide spectrum of ArcGIS users ranging from GIS administrators, DevOps, content publishers, GIS analysts, data scientists, developers and power users. The API provided these users with a Pythonic representation of their GIS... Read more here</p>"},{"location":"blog/2017/12/22/arcgis-api-for-python-v1-3/","title":"Turbocharge your Python scripts with ArcGIS API for Python v1.3","text":"<p>We are pleased to announce the newest release of the ArcGIS API for Python (version 1.3) ahead of the holiday season. This version packs some serious enhancements for GIS administration, content management, and performing spatial analysis... Read more here</p>"},{"location":"blog/2019/11/26/python-distributed-ml/","title":"Python and distributed machine learning","text":"<p>In today's computing world, machine learning is hitting a performance block. More and more companies want to run them on-demand, instead of as batch processes and want their ML models to deliver results in real-time. Often, the datasets are big-data. Thus, the ML frameworks that data scientsits learnt (<code>pandas</code>, <code>scikit-learn</code>, <code>pyTorch</code>, <code>keras</code>) and know to use don't scale well in this fast production environment or is too cumbersome to implement. This blog explores the approaches the ML, DevOps, HPC industry has arrived at in 2019.</p> <p>We can broadly categorize the efforts into two buckets depending on processor architecture. Companies that are vested in CPU (such as Intel) have their own solution and companies vested in GPU (such as NVIDIA) have their own solutions. Let us explore them:</p>"},{"location":"blog/2019/11/26/python-distributed-ml/#category-1-distributed-cpu-processing","title":"Category 1: distributed CPU processing","text":"<p>Intel has the BigDL that allows running DL models on CPUs using a Spark cluster.</p>"},{"location":"blog/2019/11/26/python-distributed-ml/#category-2-distributed-gpu-processing","title":"Category 2: distributed GPU processing","text":"<p>NVIDIA has created RapidsAI which allows running end-to-end ML on GPUs. Running DL on NVIDIA GPU is well known. How RapidsAI differs is, it provides GPU optimized versions of most popular traditional ML models that can see termendous speed-ups on GPU infrastrucure. Below are some of the components of Rapids AI.</p>"},{"location":"blog/2019/11/26/python-distributed-ml/#cuspatial","title":"cuSpatial","text":"<p>Summary: cuSpatial is a Python library that allows you to perform highly performant spatial computations on GPUs. At the moment, it only has about 10 capabilities / tools. When compared with FOSS4G Python libraries, it provides 1000x to 100,000x fold speed improvements on millions of records.</p> <p>Details: <code>cuSpatial</code> is a <code>C++</code> library accelerated on GPUs using NVIDIA <code>CUDA</code>, <code>cuDF</code> the RAPIDS DataFrame library. It provides GPU acceleration to common spatial, spatio-temporal ops such as point-in-polygon tests, distances, trajectory clustering etc. Speed-ups are in the order of 10x to 10,000x.</p> <p> cuSpatial technology stack</p> <p>cuSpatial data is laid out as columns in GPU memory for fast data processing using GPU data parallelism. cuSpatial data can seamlessly interoperate with cuDF. Thus, for ETL, users will use cuDF to first clean non-spatial fields and then pass it to cuSpatial for spatial filtering.</p> <p>The example quoted in the medium blog article explains how cuDF and cuSpatial is used to identify objects in video camera feeds, geolocate the objects from image frame to spatial coordinates for downstream GIS analysis.</p> <p>cuSpatial allows reading of data from relational formats - CSV, Parquet etc. and GIS formats like shapefiles. It also allows CPU datastructures like NumPy Arrays to flow into GPU data structures in a transparent fashion.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/","title":"A short tour of Open Geospatial Tools of the scientific Python ecosystem","text":"<p>One of my favorite statistics is that about 80% of data in the world, contain some element that is spatial. For instance, take the list of gas stations in a city or restaurants in a city, revenue from medical industry, there is always some element of this data that can be categorized as being spatial - be it locations, routes the products take, cost variations in gas prices etc.</p> <p>This spatial relationship is of significant interest to me and I have been analyzing them for over a decade and have been building software to analyze them for more than half of the past decade. While majority of what I built were proprietary, this blog is a look at what is available in the open-source ecosystem and when to use which tool.</p> <p>Something that is common and beautiful about the opensource Python data analysis ecosystem is, a number of these libraries have self-organized themselves around a common platform or standard of interoperability. You will find many libraries interchangeably using each other for parts they are good at, emphasizing the 'dont reinvent the wheel' philosophy. For then end user, even though the number of libraries is large, it is possible to mark a clear boundary of what lib does what functionality and why it is needed.</p> <p>This blog is just a high level overview to navigate this python-open-geospatial community and packages.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#data-io","title":"Data IO","text":"<p>Fiona is a GDAL Python wrapper and is for reading vector data. Fiona is written to be a clean Pythonic wrapper at the cost of performance and memory usage.</p> <p>An alternate is PyShape which reads and writes Esri Shape files in pure Python vs Fiona which is a GDAL wrapper for libs in C.</p> <p>OSMnx is a library used to retrieve network and vector data from Open Street Maps database. This library pairs well with networkx library that can perform network analysis</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#raster-data-io","title":"Raster data IO","text":"<p>rasterio is a popular library from Mapbox that is a Pythonic wrapper over GDAL Python bindings. This library makes it easy to read satellite images, DEMs (and all formats supported by GDAL) quickly into a numpy array. In addition, it has some handy methods such as plotting, histogram to quickly visualize the images.</p> <p>Once in numpy, you can perform any analysis over raw arrays. However, rasterio provides modules to perform several different processing such as Georeferencing, masking, mosaicing, reprojecting, resampling and writing to disk.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#geometry-and-projection-engines","title":"Geometry and projection engines","text":"<p>Geopandas provides a spatial extension to the famous pandas library. It uses shapely for geometry, fiona for reading vector data, descartes and matplotlib for plotting. Geopandas allows spatial operations that would otherwise require PostGIS.</p> <p>Shapely is a Python package for manipulation and analysis of geometries. Shapely cannot IO datasets, but can help in projections. Shapely is a Pythonic wrapper to GEOS (Geometry Engine, Open Source), which in turn is a <code>C++</code> port of Java Topology Suite. You can already see a heavy re-use of libraries. Shapely forms the geometry backbone of geopandas</p> <p>Pyproj is used by Geopandas to perform conversions to and between different <code>crs</code> (coordinate reference system). In addition, PyCRS appears to be an alternate implementation in pure Python for the same functionality.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#spatial-indexing","title":"Spatial indexing","text":""},{"location":"blog/2018/06/06/python-geospatial-world/#data-wrangling","title":"Data wrangling","text":"<p>Geopandas supports pretty much all data wrangling provided by pandas. Thus you can treat it as a regular data frame and work with its non-spatial columns.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#visualization","title":"Visualization","text":"<p>Descartes uses Shapely to convert them to matplotlib paths and patches. Thus it uses matplotlib for plotting geometries. The <code>GeoDataFrame.plot()</code> method of GeoPandas directly integrates with <code>Descartes</code> making it easy to directly plot your data frames.</p> <p>Folium uses Leaflet.js to plot an interactive map on the Jupyter notebook. It is pretty slick and looks great. Folium, in its current release does not seem to support reading Geopandas dataframes. You need to serialize it to a GeoJSON and then add it to the map. Although this can happen in memory, it may become problematic for large datasets. Another option to look into is IpyLeaflet.</p> <p>GeoViews is a spatial extension for Holoviews viz project. Geoviews is based on Cartopy and renders the plot either using matplotlib or bokeh (interactive) general plotting libraries. Cartopy is a common library that is used by many other high level plotting libraries.</p> <p>Geoplot has an ambitious goal of being a seaborn like library for plotting spatial data. Similar at GeoViews, it is an extension to Cartopy and matplotlib.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#sharing-web-gis","title":"Sharing - web GIS","text":""},{"location":"blog/2018/06/06/python-geospatial-world/#spatial-analysis","title":"Spatial analysis","text":"<p>Geopandas allows for basic overlay and set operations. You can do  - intersection,   - union,   - symmetrical difference,   - difference,   - buffer  - dissolve and aggregation  - attribute and spatial joins  - geocoding using google, bing, googlev3, yahoo, mapquest, openmapquest.</p> <p>A number of data wrangling such as reclassification is possible through regular pandas expressions and functionality.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#pysal","title":"Pysal","text":"<p>Python spatial analysis library, is the go-to for spatial data analysis in open source world.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#geocoding","title":"Geocoding","text":"<p><code>geopy</code> is a Pythonic wrapper for a number of different providers (including Google, Esri, Yahoo, Mapzen etc.) <code>geopandas</code> integrates with <code>geopy</code> and returns the hits as a <code>geodataframe</code>, how neat! Another option is <code>geocoder</code></p>"},{"location":"blog/2018/06/06/python-geospatial-world/#conclusion","title":"Conclusion","text":"<p>By no means this is an exhaustive list, and that is the beauty of open source ecosystem. There are parallel efforts around the world to improve existing packages and building simpler and more powerful packages. Here is a blog that lists a few more packages for Python geospatial work.</p> <p>Learning resources  - Geohackweek has a nice tutorial for vector, raster data processing using open source Python packages.  - Python GIS course by Henrikki Tenkanen</p>"},{"location":"blog/2017/01/08/scrum-crash-course-1/","title":"A crash course on Scrum part 1 - an agile project management methodology","text":"<p>Companies and organizations have never felt the need to innovate and adapt quickly to changes in market conditions like they face today. This requirement boils down to each individual team in your organization. Thankfully methodologies like agile help us with a framework to accomplish that. Scrum is one such methodology that has gained traction in the last few years. Scrum is simple and sticks with you once you start following it for a few cycles. However there are a lot of terms involved which might feel foreign, especially if you are only familiar with the water flow model of project management. Hence, in this brief two part article series, I aim to get you up to speed with the tools, terms and metrics involved in scrum... Read more here</p>"},{"location":"blog/2017/01/08/scrum-crash-course-2/","title":"A crash course on Scrum part 2 - an agile project management methodology","text":"<p>This is the second part of a two part series on \"Scrum\" - an agile project management methodology. Read the part 1 of this article here. Part 1 gave you an introduction to what scrum is, why use it and the various roles members of a scrum team play. The part 2 below gets more technical and elaborates on metrics and tools used in scrum for tracking progress and estimation... Read more here</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/","title":"Building stand-alone command line tools using Python","text":"<p>You have built a neat little tool using Python. Following modern software building paradigms, you built your tool to be user friendly, used a number of FOSS libraries instead of reinventing the wheel, etc. Now you are ready to ship and you face the dilemma of whether to package it as a conda package or PyPI package or whether to send it as a GitHub repository. What if your user does not want to even install Python on their computer?</p> <p>Fear not, you can build truly self-contained stand-alone applications using PyInstaller. The rest of this blog walks you through an example.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#what-is-pyinstaller","title":"What is PyInstaller","text":"<p>From PyInstaller's page:      PyInstaller bundles a Python application and all its dependencies into a single package. The user can run the packaged app without installing a Python interpreter or any modules. PyInstaller supports Python 2.7 and Python 3.3+, and correctly bundles the major Python packages such as numpy, PyQt, Django, wxPython, and others. PyInstaller is tested against Windows, Mac OS X, and Linux.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#how-to-install-pyinstaller","title":"How to install PyInstaller","text":"<p>Before using PyInstaller, you need to install it in your active Python environment. If you are not already using, I would highly recommend using <code>conda</code> to isolate your various dev environments. You can install conda from here and once installed, you can create a new environment using the command below:</p> <pre><code>conda create --name stand_alone_app_project python=3.5\n</code></pre> <p>Here, I am creating a new environment called <code>stand_alone_app_project</code> and installing a Python 3.5 kernel in it. To use this environment, I need to activate it - as shown below</p> <pre><code>source activate stand_alone_app_project\n</code></pre> <p>Environments are folders at the end of the day and activation pretty much adds the path to this folder to your system path. Activation is transient and is only applicable for the life of that terminal's session. Once activated, install all the packages you need for your script to work. Next, install PyInstaller using PIP (an older package manager for Python)</p> <pre><code>pip install PyInstaller\n</code></pre>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#building-stand-alone-applications","title":"Building stand-alone applications.","text":"<p>PyInstaller allows you to create apps in two formats - as folder with dependencies and an application or option 2 - is to create a fully packaged single executable file. I prefer the latter as it resembles closely what my end users would want and also allows to run the tool from a flash drive, without any installation. In both cases, you will package a Python runtime and all dependencies.</p> <p>To build an app and package it into a folder, run</p> <pre><code>pyinstaller your_main_script.py\n</code></pre> <p>To build it as a single file executable, run</p> <pre><code>pyinstaller your_main_script.py --onefile\n</code></pre> <p>Upon running this, if there are no errors, PyInstaller creates a <code>dist</code> and a <code>build</code> folder. You ship the contents of the <code>dist</code> to your end users.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#building-for-different-os","title":"Building for different OS","text":"<p>To build the executable for different OS, you need to run the steps on each OS of choice. I was able to build the same script tool into a Windows <code>exe</code> and a Unix <code>app</code> for Mac. Since Python is platform independent, doing this was a breeze and the app behaved identical on both platforms.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#errors-and-what-to-do","title":"Errors and what to do","text":"<p>Your first few times with PyInstaller will be rough. You will run into errors unless you are packaging a <code>hello world.py</code> kind of script. In my case, I was packaging a tool that relied on <code>requests</code> package and PyInstaller could not understand this package. After a brief search on Stack Exchange, I was able to figure out that I needed to install <code>urllib3</code> on Mac to get it working. Similarly, on Windows, I needed to install <code>pypiwin32</code>. The latter is true no matter what package you use.</p> <p>Although PyInstaller aims to recursively parse the <code>import</code> statements in your script and all its dependencies to figure out the libraries to package, it might fall short if you are using irregular imports. In such cases, read their appropriately named When things go wrong article.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#conclusion","title":"Conclusion","text":"<p>Conda and Pip are excellent mechanisms to ship your Python code. Yet, they are suitable if you are building libraries and your audience is fellow developers. However, if you are creating an application for end users, then you need to ship it as an executable. Python still falls short in this area compared to other languages like C++, Java, .NET. We are yet to find an IDE that will build your script into an executable (similar to what visual studio does out of the box since ages). Tools like py2exe and PyInstaller are great as they attempt to solve this gap. Yet, this is only the beginning and I am sure the Python community would greatly benefit if we can expand this area.</p> <p>If you are looking for an example project, checkout my command line app at https://github.com/AtmaMani/vector_style_manager</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/","title":"Takeaways from Strata Data Conference - NewYork City Sep 2019","text":"<p>I had the opportunity to attend 2 days of Strata Data Conference last month (9/25-26) at the Javits Convention center in Manhattan, NYC. This is my report of the conference and my opinion on a few things I noticed. If you want the gist, please read just the summary.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#executive-summary","title":"Executive summary","text":"<p>I noticed 3 important things</p> <ol> <li> <p>The number of hosted notebook solutions is only increasing. LinkedIn is building Darwin and Uber is building a Data Science Workbench. Both these firms admitted the difficulty in defining the skill set of a typical \u2018data scientist\u2019 and which tools, features and frameworks to provide. Uber decided to build two products \u2013 Michelangelo \u2013 a highly scalable ML engine, but with very limited features meant for a narrow user base and another product called Data Science Workbench for the broader set of users. LinkedIn acknowledged the challenges in building a hosted notebook solution. What was clear from these talks is, although the number of notebook products is proliferating, it is not necessarily simplifying. Both these products have a complicated setup requiring experts and are for internal users at this point.</p> </li> <li> <p>The next big thing in deep learning might be distributed DL on Spark based CPU clusters. Intel is building a BigDL library for distributed DL on Spark. Most organizations of today are likely to have server farms with CPU than with GPU dedicated for DL and might be interested in utilizing this existing infrastructure for DL.</p> </li> <li> <p>Several companies are building data catalogs or data management software for big data warehouses. Features include \u2013 building data manifests, data usage graphs, flagging &amp; removal of PII. The overall theme is, following the big data hype, a number of companies have started collecting large amounts of business process data. They do not have a way to make sense of this massive collection or sometimes, even know what data they have. These vendors are building products to help manage these data warehouses.</p> </li> </ol>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#interesting-talks-i-attended","title":"Interesting talks I attended","text":""},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#linkedin-darwin-a-hosted-notebook-product","title":"LinkedIn \u2013 Darwin \u2013 a hosted notebook product","text":"<p>Darwin is a hosted notebook product for internal use. They plan to release this as open source project later. The challenges they are trying to solve is similar to our AGS notebook project.</p> <p> </p> <p>Darwin tries to provide a self-service environment. User\u2019s have access to private data, either local datasets or hosted on server. Analysts can write in Python or R and can share their notebooks across the team. They can version their notebooks, but they don\u2019t yet support reviewing of notebooks, just a git integration. Finally, users can schedule the notebooks or \u2018productionize by other means\u2019 (which wasn\u2019t clear from their description).</p> <p>Below is the architecture of Darwin:</p> <p></p> <p>At the core, Darwin is based off JupyterHub. The notebook container image has the notebook, Python / R libraries and sparkmagic. A persistent storage is made available (not sure if mounted) via a storage service. The notebook instance connects to a Spark farm via an Apache Livy service and Jupyter sparkmagic. Data Science users of Darwin are encouraged to execute jobs on this Spark infrastructure (via Livy) thereby overcoming memory limits of their containers.</p> <p>Users can also pick their Docker Image from an internal registry (artifactory was mentioned). The Docker containers of the Jupyter Hub are run on a Kubernetes cluster. The architecture of Jupyter Hub looked familiar to the default setup in Hub docs. The farm they execute processing against contains Spark (as seen earlier), Tensorflow, and some internal tools like Pinot analytic engine.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#democratizing-ml-at-uber","title":"Democratizing ML at Uber","text":"<p>Uber spoke about Michelangelo a machine learning task engine. Their motivation: Internally, most teams found it difficult to get started with using ML to solve problems. They found it hard to access data and to collaborate. While the teams can build a ML solution using tools of their preference (scikit-learn, R, other tools), they found it hard to take it to production and scale it up. Only a very few teams had the know-how of building ML algorithms and the skill of deploying them in HPCs in a distributed and horizontally scalable manner. Further, teams that were capable were building custom solution and the DevOps and SREs found it hard to provide a simpler scaling solution to all data science teams. There wasn\u2019t a standard to collaborate or replicate.</p> <p></p> <p>The speaker acknowledged the difference between data analysts and scientists if fuzzy and that building a comprehensive platform that can support varied use cases is not quite possible. Thus, Uber chose to build a ML platform that is specialized, narrow and only serves a narrow audience. Motto: build for a few, build for production.</p> <p></p> <p>Key features of Michelangelo</p> <p></p> <p>For all other users and use cases that Michelangelo did not serve, the built another product called \u2018Data Science Workbench.\u2019 DSW is a web-based GUI for self-service data science. It provides a notebook interface but has support for r-studio and shiny.</p> <p>Uber then spoke about PyML, a Python library that sits at the intersection of Michelangelo and DSW. It allows users to deploy their custom ML solutions to production on Michelangelo. For models that require big-data, users build and train directly on Michelangelo\u2019s Spark infrastructure. They are limited to a finite number of algorithms. For smaller datasets, users build using the library of their preference (scikit-learn for instance), package using PyML and deploy it as a Docker image on Michelangelo\u2019s farm.</p> <p>Today\u2019s Uber is a large company (about 19,000 as of 2019) with several data science teams. The speaker shared some learnings in adopting ML:</p> <p>How they \u2018democratized ML\u2019 at Uber:</p> <ul> <li>They started an internal conference for ML: Uber ML conf, and continued the conversation via brown bag events</li> <li>They held regular and repeat ML education events and bootcamps</li> <li>They marketed heavily championed the new way of doing ML \u2013 DSW for interactive dev and Michelangelo for deployment to production</li> <li>Coordinated planning with teams as new features were being planned</li> </ul> <p>Design philosophy behind these two tools:</p> <ul> <li>Build for experts, but design for less technical users (which I interpreted as: allow customization, but start with intuitive UI and have good defaults)</li> <li>Build for unknown</li> <li>Don\u2019t force workflows</li> <li>Develop iteratively, but constantly align with long term vision</li> <li>Build a community of scientists &amp; non-scientists / technical &amp; less technical users. Do not build separate products for these user groups.</li> </ul>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#my-takeaways-from-these-talks","title":"My Takeaways from these talks:","text":"<p>It was clear that big Silicon Valley firms like to build their own data science platforms. This desire for custom tools is so deep that, when I asked LinkedIn why they did not make use of Azure notebooks (since MS bought LinkedIn), they simply said, we started with this effort before the acquisition. What was also clear is, building a custom data science platform is a very challenging business.  There are several open source components and a variety of ways these can be architected.</p> <p>Smaller firms and companies whose core product are not software development, are better off buying a hosted notebook solution (such as AGS Notebooks), than attempting to build one.</p> <p>It is very hard to identify which features to include in a data science platform. Skillset and tools of preference amongst people with title \u2018data scientist\u2019 is so varied even within these two firms, let alone the broad data science industry. Uber and LinkedIn admitted the proliferation of Python libraries, ML and DL frameworks, their cascading dependencies is a challenge.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#companies-and-products-seen-at-the-expo","title":"Companies and products seen at the Expo","text":""},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#intel-and-bigdl-distributed-dl-on-spark","title":"Intel and BigDL \u2013 distributed DL on Spark:","text":"<p>Intel is a leading chip maker for general purpose computing but was late to the game when it came to making GPUs for deep learning. Most DL frameworks of today support Nvidia GPUs. Most organizations realize the power of DL, but to implement large scale DL solutions, they need to invest in new GPU powered hardware (either on prem or hired on the cloud). However, most of these orgs are likely to have generic server infrastructure (CPU powered) in place for existing business processes. Thus, Intel is building BigDL \u2013 a distributed deep learning library on Apache Spark. At the booth, Intel staff demonstrated performance benchmarks for running DL training on GPU vs distributed CPU.</p> <p></p> <p>Intel\u2019s booth had a unique layout. Under a large Intel banner, I found 6 separate stations where business partners of Intel set up their booths. I spoke to</p> <ul> <li>H20.ai \u2013 automatic feature engineering, training and model development. It is an intelligent assistant for data scientists. The sales person was a former GIS analyst and was happy to meet someone from Esri.</li> <li>OneConvergence \u2013 another cloud native DL platform</li> <li>Domino datalab -  another data science platform</li> </ul>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#semoss-open-source-browser-based-ui-driven-analysis","title":"SEMOSS: open source, browser based, UI driven analysis","text":"<p>It provides Python and R shells that allow users to write custom scripts, execute on a server back-end and get the results updated back on the browser. This is a challenging feature to achieve (maintaining data context) and their demo was of this feature was impressive. SEMOSS provides limited geospatial capabilities \u2013 viz and geocoding at best. For geocoding, they make use of Esri World Geocoding service.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#alteryx-gui-based-self-service-data-analysis-platform","title":"Alteryx: GUI based, self-service data analysis platform","text":"<p>Alteryx has many features including geospatial analysis, but the staff at the booth were not equipped to talk to those aspects. Their emphasis was on the self-service nature.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#data-catalog-companies","title":"Data catalog companies","text":"<p>Several companies are building data catalogs or data management software for your big data warehouse. Features include \u2013 building data manifests, data usage graphs, flagging &amp; removal of PII. The overall theme is, following the big data hype, a number of companies have started collecting large amounts of business process data. They do not have a way to make sense of this massive collection and these vendors are building products to help manage these data warehouses.</p> <p>Okera (data cataloger), Cryptonumerics (PII identification), Alation (data catalog), Waterline Data (data catalog), Matillion (data management), Dremio (fast data queries), Immuta (data governance), Ataccama (data cleanser/curator), Collibra (data lineage), Alluxio (data orchestration).</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#summary","title":"Summary","text":"<p>I found my time at Strata productive and informative. I wish I could have spent a bit more time at the Esri Booth, but the people I spoke to were thrilled to hear the analytical applications possible with spatial data. It was interesting to meet and talk to people that crossed over from being a GIS analyst to the Financial industry. The challenges that Uber and LinkedIn\u2019s shared while building notebook and ML solutions were insightful.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/","title":"The Big Data Ecosystem","text":"<p>Big data is the talk of the town. What is it? What are the components? Let us demystify some of these jargons</p> <p></p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#spark","title":"Spark","text":"<p>Open source - Apache. New, but very widely used.  - flexible alternative to MapReduce  - Data stored in HDFS, Cassandra  - not replace Hadoop</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#hadoop-and-hadoop-ecosystem","title":"Hadoop and Hadoop ecosystem","text":"<p>Hadoop - way to distribute very large files over multiple machines. Uses HDFS (Hadoop File System). HDFS will duplicate data for fault tolerance. Hadoop uses MapReduce which allows computation on that data. Together <code>MapReduce + HDFS = Hadoop</code></p> <p>HDFS - 128 MB blocks of data that is duplicated and split across multiple nodes. Small blocks allow massive parallelization.</p> <p>Hadoop has a <code>schema on read</code> whereas RDBMS has a <code>schema on write</code> logic. HDFS does not use RDBMS tables, data files are generally .txt or binary files.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#hdfs","title":"HDFS","text":"<p>HDFS is written in Java and sits on top of a standard OS of that machine. HDFS is great only if your data files are quite large, like <code>1GB</code> and not if they are tiny - like <code>1kb</code>.</p> <p>Writing data into HDFS is interesting. All datasets are to be replicated <code>3</code> times. You talk to the co-ordinator node which tells which nodes to write / read data from. You write to the first node which tells you where to put the other 2 replicas. This format of processing is called <code>pipeline</code> processing in HDFS.</p> <p>When a node fails, the co-ordinator finds which copies of data were lost and it instructs the remaining nodes to replicate those blocks amongst the remaining nodes so all blocks of data are now replicated <code>3</code> times.</p> <p>Data Locality is important in HDFS for speed. Since data is replicated 3 times, the coordinator can instruct different processing operations to be performed on different nodes. Priority is given to that node which has the max data blocks required for that data operation, since working on data in that node is much faster than reading data from network.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#mapreduce","title":"MapReduce","text":"<p>Allows computation across Hadoop nodes. It uses a job tracker on main node and task trackers on all slave nodes. Task trackers allocate CPU and memory and monitor the tasks.</p> <p>Developed by Google in 2004 which led to development of Hadoop. MapReduce was hard to master and use, requires lot of Java code. Building complex jobs is very hard.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#yarn","title":"YARN","text":"<p>Yarn manages compute resources and sits on top of HDFS. Yarn stands for Yet Another Resource Manager.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#data-loaders","title":"Data loaders.","text":""},{"location":"blog/2017/12/10/the-big-data-ecosystem/#sqoop","title":"SQOOP","text":"<p><code>SQOOP = SQL + HadOOP</code> for ETL (Extract, Transform, Load), get data from SQL like tables into Hadoop</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#flume","title":"FLUME","text":"<p>To get streaming data into Hadoop</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#data-searching","title":"Data searching","text":""},{"location":"blog/2017/12/10/the-big-data-ecosystem/#hive","title":"HIVE","text":"<p>Accessing data from HDFS is via a <code>.java</code> program and not a <code>sql</code> like in a RDBMS. A number of flavors of Hadoop came out that answered this limit. For instance, Facebook created <code>Hive</code> which allowed you to write standard <code>sql</code> which will be converted to <code>java</code> for Hadoop. Hive reduces 1000s of lines of Java into SQL, rather the reverse.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#pig","title":"PIG","text":"<p>Same as HIVE, but higher level language and is anologous to <code>PL/SQL</code>.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#impala","title":"IMPALA","text":"<p>Developed as Hive and Pig were not just as fast. So for low latency sql, Impala was created and it bypasses MapReduce as it communicates directly with HDFS</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#solr","title":"SOLR","text":"<p>If you wan to Google (search) your Hadoop data. You can pipe SQOOP and FLUME into SOLR on the way into HDFS so it indexes on the fly for great search.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#spark_1","title":"SPARK","text":"<p>Apache open source framework that allows performing batch and real-time data processing on HDFS. It was built by learning the difficulties with MapReduce. It can be considered as a general purpose compute engine.</p> <p>Integrates very well if Python, Scala, Java. New products like <code>Spark Streaming</code> is designed for developer ease of use.</p> <p></p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#rdd","title":"RDD","text":"<p><code>Resilient Distributed Datasets</code> representation of data in object format and allow computation on them. They contain data lineage. <code>Transformation</code>s are what you do to <code>RDD</code> that result in creation of a new <code>RDD</code>. <code>Action</code>s is questions / searches on the <code>RDD</code> to get your answer.</p> <p>Spark does all work as lazy evaluations and work on the data only when necessary. Further, <code>data lineage</code> allows them to rebuild data or RDD that is lost in the event of a fault.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#gfs-google-file-system","title":"GFS - Google File System","text":"<p>A very early form of a distributed file system. It had a <code>3</code> time data replication. With more nodes, Google improved both their compute and storage power. It built MapReduce to work on this GFS.</p> <p>After Google published their white paper on this, Hadoop was born out of a custom implementation of this.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#commercial-packages","title":"Commercial packages","text":"<p>Since there are numerous alternates, some companies bundle them and sell as their products with some additional value added components.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#mapr","title":"MapR","text":"<p>MapR stack comprises <code>MapR = HDFS + YARN + SPARK + SPARK SQL</code>.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#resources","title":"Resources","text":"<ol> <li>Apache Hadoop &amp; Big data 101</li> <li>Hadoop, SQL comparison</li> <li>What is Hadoop</li> <li>Understanding HDFS using Legos</li> <li>What is Apache Spark?</li> <li>Apache Spark vs MapReduce</li> </ol>"},{"location":"blog/2018/01/26/tutorials-for-arcgis/","title":"A few tutorials I authored for ArcGIS","text":"<p>Below is a non-exhaustive list of tutorials that I authored for ArcGIS. The objective of these tutorials is to demonstrate spatial data analysis using the scientific computing ecosystem of Python and the ArcGIS stack.</p> <ul> <li>Analysis: Fighting California forest fires using spatial analysis</li> <li>Analysis: Creating hurricane tracks using Geoanalytics</li> <li>Analysis: Analyzing New York City taxi data using big data tools</li> <li>Analysis: Finding suitable spots for placing heart defibrillator equipments in public</li> <li>Automation: Publishing SDs, Shapefiles and CSVs</li> <li>Automation: Publishing packages as web layers</li> <li>Automation: Publishing web maps and web scenes</li> <li>Automation: Using and updating GIS content</li> <li>Automation: Updating features in a feature layer</li> <li>Automation: Batch creation of Groups</li> <li>Automation: Your first notebook</li> <li>Automation: Clone Portal users, groups and content</li> <li>Overwriting feature layers</li> <li>Most guide pages here</li> </ul>"},{"location":"books/","title":"Books","text":"<ul> <li>2021: Climate modeling work featured in the technology showcase of Esri's GIS for Science flip book<ul> <li></li> </ul> </li> <li> <p>2020: Reviewed and contributed to the 9th chapter of Advanced Python Scripting for ArcGIS Pro book.</p> <ul> <li></li> </ul> </li> <li> <p>2019: Reviewed and contributed to the 2nd edition of Python Scripting for ArcGIS Pro book.</p> <ul> <li></li> </ul> </li> <li>2019: Contributed to chapter 13 of GIS for Science. See the article here and the web app here<ul> <li></li> </ul> </li> <li>2018: Reviewer for Essential Python.<ul> <li></li> </ul> </li> </ul>"},{"location":"books/2018-essential-python/","title":"Essential Python","text":"<p>Lucky to be a reviewer on a Python book by my friend Ravi Chityala. Get the book on Amazon: Essential Python.</p> <p></p>"},{"location":"books/2019-py-scripting/","title":"Python Scripting for ArcGIS Pro","text":"<p>Reviewed and contributed to the 2nd edition of Python Scripting for ArcGIS Pro book.</p> <p></p>"},{"location":"books/2020-adv-py-scripting/","title":"Advanced Python Scripting for ArcGIS Pro","text":"<p>Reviewed and contributed to the 9th chapter of Advanced Python Scripting for ArcGIS Pro book.</p> <p></p>"},{"location":"books/2021-gis-for-science/","title":"GIS for Science - Technology Showcases","text":"<p>Climate modeling work featured in the technology showcase of Esri's GIS for Science flip book </p> <p></p>"},{"location":"books/2021-gis-for-science/#jupyter-notebooks-of-the-analysis-work","title":"Jupyter Notebooks of the analysis work","text":"<ul> <li>Part 1: Preparing larger-than-memory hurricane data using Dask and GeoAnalytics</li> <li>Part 2: EDA on hurricane tracks</li> <li>Part 3: Does intensity of hurricanes increase over time?</li> </ul>"},{"location":"projects/","title":"Projects","text":"<p>Use the menubar to find relevant projects</p>"},{"location":"projects/cloud/","title":"Cloud computing","text":"<p>Some useful resources for building on the cloud. This page will primarily contain resources on AWS and GCP.</p>"},{"location":"projects/cloud/#pages","title":"Pages","text":"<ul> <li>Argo for Kubernetes</li> <li>Google cloud platform</li> <li>Intro to Kubernetes</li> <li>Kubernetes objects and specification</li> <li>Helm charts</li> <li><code>kubectl</code> commands</li> <li><code>gsutil</code> commands</li> </ul>"},{"location":"projects/cloud/#a-little-bit-of-history","title":"A little bit of history","text":"<p>Cloud is a general purpose definition used to refer to everything from compute, storage, networking and security on the cloud. Cloud is the current paradigm shift in computing. The shift of the early 21st century. Arguably, AI is the second paradigm shift. The shift to cloud computing involves a leap-of-faith move for most companies and those which embrace this are set to survive this tech era. The tech space is always evolving. The evolution here is especially noticeable due to its speed. The practice of moving to newer (and better) technologies involves abandoning the old ways of building software systems and rebuilding them on newer paradigms. This practice is called the burning-platform-effect. While the name sounds sounds ominous, the move is essential for survival in today's tech scene.</p> <p>As companies migrated from on-premises to cloud, they saw a switch from capital expenditures (CapEx) to operational expenditures (OpEx). This shift meant lower up-front cost and leveled the field for big and small companies to start up and expand globally. While earlier, you needed to have geo-political presence if you wanted to spread multi-nationally, today you can rent hardware / platform from cloud providers and accomplish the same. This benefit also meant, smaller, leaner companies can have an out-sized impact if they leveraged the power of cloud (and had everything else right - such as product market fit, demand, incentives, marketing, funding, staffing etc.)</p> <p>The most profound effect of cloud tech is the vast amounts of data that is collected and consumed. Increasingly, humankind is switching from \"connecting to online\" to \"living online\". The result is the highly accurate and highly personalized predictive models that can be built. This leads us to the second paradigm shift - Artificial Intelligence.</p> <p>AI requires not-only vast amounts of data, but also commiserate compute power. AI's compute demand has led to newer generations of processors be developed. The newer GPU based processors (such as TPU) break Moore's law. Instead of being 2x powerful, each subsequent generation is ~50x powerful.</p>"},{"location":"projects/cloud/#cloud-is-in-layers","title":"Cloud is in layers","text":"<ul> <li>IaaS: Infrastructure as a Service - is the rawest form of cloud offering. Users get access to compute, storage and networking. They pay for what they allocate.</li> <li>PaaS: Platform as a Service - is an offering that wraps a layer over IaaS. PaaS is a bunch of managed services and users pay for what they consume rather than allocate.</li> <li>SaaS: Software as a Service - can be thought of a layer on top of PaaS. This is typically managed, server-less and oriented toward consumers. Most web applications can be considered as SaaS. Users pay for a service model / subscription tier rather than allocation or consumption.</li> </ul>"},{"location":"projects/cloud/argo/","title":"Argo workflows for Kubernetes","text":""},{"location":"projects/cloud/argo/#what-is-argo","title":"What is Argo?","text":"<p>Argo helps make Kubernetes more accessible to everyone. It provides services for creating workflows and jobs that build on Kubernetes. Argo is composed of the following services:</p> <ul> <li>Argo Workflows - orchestrate parallel jobs on K8. Represent workflows as DAGs and easily run compute intensive jobs.</li> <li>Argo CD - uses git repo as the source of truth and builds the deployment env to conform to the repo. Config is via a <code>YAML</code> file or <code>Helm</code> package.</li> <li>Argo Events - dependency manager that is events based. It can hook up and listen to sources like AWS SNS, SQS, GCP PubSub and execute workflows.</li> </ul>"},{"location":"projects/cloud/argo/#why-argo","title":"Why Argo?","text":"<p>Argo is a compelling solution for those that already build on K8. Argo does not reinvent K8 features, instead builds on them. It enables implementing each step in the workflow as a container. It provides artifact management that allows to specify the output from any step as input to another. Since everything is as containers, the entire workflow, including each step and their interactions can be managed as source code (in YAML). This is called container native workflow management. Thus a workflow that runs on one Argo env will run exactly the same on another, allowing for better portability.</p>"},{"location":"projects/cloud/argo/#argo-cli-commands","title":"Argo CLI commands","text":""},{"location":"projects/cloud/argo/#list-workflows-argo-list","title":"List workflows <code>argo list</code>","text":"<pre><code>$ argo list -n &lt;namespace&gt; &lt;flags&gt;\n$ argo list -n flood --running  # will list all running workflows in the flood namespace\n$ argo list -n flood --completed  # for completed wf\n</code></pre> <p>example output:</p> <pre><code>NAME                                               STATUS                AGE   DURATION   PRIORITY\ningest-weather-data-compass-lisflood-japan-xb7pm   Running               4m    4m         0\nflood-pipeline-ps8rf                               Running               42m   41m        0\njp-3hr-live-cgtrb                                  Running               44m   44m        0\njp-3hr-hist-lc89r                                  Running               2d    2d         0\n</code></pre>"},{"location":"projects/cloud/argo/#create-submit-workflow-using-kubectl","title":"Create / submit workflow using <code>kubectl</code>","text":"<p>You can use argo CLI or <code>kubectl</code> to submit or create workflows.</p> <pre><code>(base) \u279c  ~ k create -n argo-local -f wf-resource-template-localfile.yaml \nworkflow.argoproj.io/wf-resource-tmpl-55s7b created\n</code></pre>"},{"location":"projects/cloud/argo/#argo-workflow-definition-files","title":"Argo workflow definition files","text":"<p>Below is a sample argo workflow from argo doc website. A template starts by declaring the version it is based off, followed by <code>kind</code> and <code>metadata</code> (name, etc). The <code>spec</code> is the most important part. Spec in-turn has two main parts, <code>entrypoint</code> and <code>templates</code>.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-  # Name of this Workflow\nspec:\n  entrypoint: whalesay        # Defines \"whalesay\" as the \"main\" template\n  templates:\n  - name: whalesay            # Defining the \"whalesay\" template\n    container:\n      image: docker/whalesay\n      command: [cowsay]\n      args: [\"hello world\"]   # This template runs \"cowsay\" in the \"whalesay\" image with arguments \"hello world\"\n</code></pre> <p>The <code>templates</code> section accepts an array of objects. In Yaml, you prefix each element in an array of objects with a <code>-</code>. For, an array of elements, you enclose elements within <code>[]</code> in a single line or in a broken line.</p> <p>Argo has the following template types:</p> <ul> <li>Container: specs to schedule a container. This follows the same spec used by K8s. So you can cross use the specs</li> <li>Script: convenience wrapper around a container and follows the same spec. It has an additional <code>script</code> field which you can specify which file to be executed.</li> <li>Resource: template to modify and operate on K8s resources</li> <li>Suspend: to suspend operations</li> </ul>"},{"location":"projects/cloud/argo/#container-template","title":"Container template","text":"<p>An example container template is shown below. Notice the container object within the templates section of the spec which defines this is a container template.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: wf-container-templ-\nspec:\n  entrypoint: container-template\n  templates:\n    - name: container-template\n      container:\n        image: python:3.8-slim\n        command: [echo]\n        args: [\"Hello, from within container running argo workflow\"]\n</code></pre>"},{"location":"projects/cloud/argo/#script-template","title":"Script template","text":"<p>A script template inherits from container template. It is a convenience wrapper to allow execution of scripts like Python. Note the script object which takes up the place of container:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: wf-script-tmpl-\nspec:\n  entrypoint: script-template\n  templates:\n    - name: script-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"This script is embedded into the template and is executed\")\n</code></pre> <p>In the case above, the Python script is embedded right into the workflow yaml file.</p>"},{"location":"projects/cloud/argo/#resource-template","title":"Resource template","text":"<p>A resource template is used to act on K8s or Argo resources, such as create child workflows. As an example below, we write a resource template to spawn another argo workflow that executes a Python script (using a script template). Notice the resource object within the template section of the workflow yaml. The manifest field takes an entire script template. There is one small caveat - the metadata uses <code>name</code> instead of <code>generateName</code>.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: wf-resource-tmpl-\nspec:\n  entrypoint: resource-template\n  templates:\n    - name: resource-template\n      resource:\n        action: create\n        manifest: |\n          apiVersion: argoproj.io/v1alpha1\n          kind: Workflow\n          metadata:\n            name: wf-res-spawn\n          spec:\n            entrypoint: script-template\n            templates:\n              - name: script-template\n                script:\n                  image: python:3.8-slim\n                  command: [python]\n                  source: |\n                    print(\"WF spawned by another res wf.\")\n</code></pre>"},{"location":"projects/cloud/argo/#template-invocation","title":"Template invocation","text":"<p>Argo provides invoker templates that can invoke other workflows. There are two types of invoker templates:</p> <ul> <li>Steps: Defines a list of steps. Inner steps will run in parallel and outer lists will run sequentially</li> <li>DAG: Defines the tasks in a directed acyclic graph. A DAG specifies the interdependencies between tasks. This allows Argo to know which tasks can be run sequentially and which in parallel.</li> </ul>"},{"location":"projects/cloud/argo/#steps-template","title":"Steps template","text":"<p>The steps template resembles other templates seen so far, with the <code>steps</code> object in place of <code>resource</code> or <code>script</code>. The <code>steps</code> object accepts an array of objects, each with a <code>name</code> and <code>template</code> property. In the example below, the step template defines 3 steps, followed by a script template that has the actual logic for each of the steps.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: wf-steps-template-serial\nspec:\n  entrypoint: steps-template\n  templates:\n    - name: steps-template\n      steps:\n      - - name: step1\n          template: task-template\n      - - name: step2\n          template: task-template\n      - - name: step3\n          template: task-template\n\n    - name: task-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"Task - hello\")\n</code></pre> <p>Outer steps: Note the double dash <code>- -</code> prefix for each of the step element in the template. The double dash signify these are outer tasks and by design, the execute in serial.</p> <p>Inner steps: Parallel exec: In the case above, all the steps can execute in parallel as there is no inter-dependency between them. To make them execute in parallel, you remove a dash and indent the step as shown below:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: wf-steps-template-parallel\nspec:\n  entrypoint: steps-template\n  templates:\n    - name: steps-template\n      steps:\n      - - name: step1  # outer step\n          template: task-template\n        - name: step2  # inner step (single dash)\n          template: task-template \n        - name: step3  # inner step\n          template: task-template\n      - - name: step4  # outer step\n          template: task-template\n\n    - name: task-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"Task - hello\")\n</code></pre> <p>When executed, the workflow looks like below. Note that steps1, 2, and 3 can run in parallel.</p> <p></p> <p>and the timeline tab looks like below:</p> <p></p>"},{"location":"projects/cloud/argo/#suspend-template","title":"Suspend template","text":"<p>This template can be used to add a pause / sleep timer between steps in a workflow. See example below:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: wf-suspend-template\nspec:\n  entrypoint: steps-template\n  templates:\n    - name: steps-template\n      steps:\n      - - name: step1  # outer step\n          template: task-template\n        - name: step2  # inner step (single dash)\n          template: task-template \n      - - name: delay  # adds the delay\n          template: suspend-template\n      - - name: step4  # outer step\n          template: task-template\n\n    - name: task-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"Task - hello\")\n\n    - name: suspend-template\n      suspend:  # The suspend template\n        duration: \"10s\"\n</code></pre>"},{"location":"projects/cloud/argo/#dag-template","title":"DAG template","text":"<p>The DAG template solves the same workflow representation as the steps template. Instead of the you specifying which tasks to run in sequence or parallel, in a DAG, you flip the problem and specify which tasks have a dependency on which other task. The argo executor then attempts to run all tasks in parallel, except when blocked by a dependency. In a DAG, the steps are now called tasks.</p> <p>Below is an example of a DAG that produces a diamond pattern workflow:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: wf-dag-template\nspec:\n  entrypoint: dag-template\n  templates:\n    - name: dag-template\n      dag:\n        tasks:\n          - name: task1\n            template: task-template\n          - name: task2\n            template: task-template \n            dependencies: [task1]\n          - name: task3\n            template: task-template\n            dependencies: [task1]  # makes a binary branch pattern\n          - name: task4\n            template: task-template\n            dependencies: [task2, task3]  # closes dag with a diamond pattern\n\n    - name: task-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"Task - hello\")\n</code></pre> <p></p>"},{"location":"projects/cloud/argo/#resources","title":"Resources","text":"<ul> <li>Medium.com: What is Argo and how it works on GKE</li> <li>Argo blog: Introductory article.</li> <li>Argo version on Dev cloud: 3.3.6</li> </ul>"},{"location":"projects/cloud/gcp-1/","title":"An introduction to Google Cloud Platform - part 1","text":""},{"location":"projects/cloud/gcp-1/#basics-logging-in-and-getting-set-up","title":"Basics - logging in and getting set up.","text":""},{"location":"projects/cloud/gcp-1/#about-gcp","title":"About GCP","text":""},{"location":"projects/cloud/gcp-1/#resource-hierarchy-in-google-cloud","title":"Resource hierarchy in Google Cloud","text":"<p>At the bottom of the stack are Resources which are storage buckets, databases, scripts. These are organized into Projects. One or more projects can be in a folder or sub-folders, which themselves are organized under an Org node. Access, budgeting policies are defined at Org or folder level and sometimes at granular levels such as projects and resources. Policies are inherited downward.</p>"},{"location":"projects/cloud/gcp-1/#gcp-project","title":"GCP \"Project\"","text":"<p>A Google Cloud project is an organizing entity for your Google Cloud resources. Each resources can belong to just 1 project. Projects also contain settings and permissions, which specify security rules and who has access to what resources. Orgs can allocate budgets to projects and set spending alerts so they don't accidentally run up a large GCP bill.</p> <p>Each project has <code>3</code> identifying entities: </p> <ul> <li>Project ID: a unique identifier that is used to link Google Cloud resources and APIs to your specific project. Project IDs are unique across Google Cloud. Project IDs are created by GCP and once created, they are immutable.</li> <li>Project Name: a label created by user. Not unique, can be edited later</li> <li>Project Number: globally unique, created by GCP and cannot be altered by user.</li> </ul>"},{"location":"projects/cloud/gcp-1/#iam","title":"IAM","text":"<p>For smaller orgs, it is sufficient to just manage access via settings on Folders and Projects. However, when companies need to manage access in a sophisticated and granular manner, they can use IAM - Identity and Access Management. An IAM consists of a bunch of Who has access to What. The Who can be a google account, google group, service account or a cloud identity domain and the What part is defined by a Role which is a collection of permissions.</p> <p>There are 3 types of Roles:</p> <ul> <li>basic: Owner, Editor, Viewer, Billing Admin. These are broad by design.</li> <li>predefined: More nuanced such as Instance Admin, VM Admin etc.</li> <li>custom IAM: go crazy, go as narrow as you want.</li> </ul> <p>A special type of account is the Service Account which is tuned for all kinds of automation scenarios. Unlike regular accounts, this uses a cryptographic key instead of a password. Service accounts are also considered resources (not user accounts) and so can be tightly managed using policies.</p>"},{"location":"projects/cloud/gcp-1/#ways-of-accessing-gcp","title":"Ways of accessing GCP","text":"<p>You can access GCP via 4 ways:</p> <ul> <li>Web interface: GUI. Allows you to check health, manage, set budgets. Can also connect to instances via SSH</li> <li>Cloud SDK &amp; Cloud shell: SDK has <code>gcloud</code> main CLI tool, <code>gsutil</code> CLI for storage, <code>bq</code> CLI for big query. SDK is installed on your workstation. Cloud Shell is a CLI running on the cloud and accessed via browser. Is Debian based with <code>5</code> GB persistent storage and loaded with SDK.</li> <li>GCP API: Web and client libraries in different languages - Java, Python, C#, node, Ruby, C++ etc.</li> <li>Cloud console mobile app: certain management, start-stop operations, view budgets, view server status etc, incident management. </li> </ul>"},{"location":"projects/cloud/gcp-1/#networking-on-gcp","title":"Networking on GCP","text":"<p>Each project in GCP has a default VPC (virtual private cloud) configured. The resources within a project can talk to each other via this internal IP Address. By default, the firewall is configured to block all incoming traffic, but allow all out-going traffic from within a project.</p>"},{"location":"projects/cloud/gcp-1/#storage-on-gcp","title":"Storage on GCP","text":"<p>There are multiple storage services on GCP - depending on the type of data being stored and the application that is intended. Below is a list of 5 types of services:</p> <p></p> <p>For Cloud Storage, there are multiple classes of storage, depending on how often you access the data as shown below:</p> <p></p> <p>No matter which class you use, all types allow for global access, no minimum amount, pay as you go rates. Data is always encrypted at rest.</p> <p>Cloud SQL provides fully managed RDBMS including <code>mysql</code>, <code>postgresql</code>, <code>sql server</code>. You can scale up to <code>64</code> cores, <code>400 GB</code> of RAM and <code>30 TB</code> of storage.</p> <p>Cloud spanner is also a fully managed RDBMS, but for high throughput SQL operations including joins, reads, writes. It sounds like you would start with Cloud SQL and upgrade to spanner if your needs warrant that.</p> <p>Firestore - scalable, NoSQL DB where data is stored in documents and stored in collections. Firestore is suitable for web or mobile apps (in addition to other users), allows for offline replication &amp; sync. Cost is fine grained, per read, write, query ops and the amount of data stored.</p> <p>Cloud Bigtable - NoSQL, big data DB. Bigtable is suitable when data is high throughput, exceeds 1TB, either structured or unstructured, supports time-series. Frequently customers that run ML jobs on data use bigtable.</p>"},{"location":"projects/cloud/gcp-1/#7-main-categories-of-services","title":"<code>7</code> main categories of services","text":"<p>There are seven categories of Google Cloud services:</p> <ul> <li>Compute: A variety of machine types that support any type of workload. The different computing options let you decide how much control you want over operational details and infrastructure.</li> <li>Storage: Data storage and database options for structured or unstructured, relational or non relational data.</li> <li>Networking: Services that balance application traffic and provision security rules.</li> <li>Cloud Operations: A suite of cross-cloud logging, monitoring, trace, and other service reliability tools.</li> <li>Tools: Services that help developers manage deployments and application build pipelines.</li> <li>Big Data: Services that allow you to process and analyze large datasets.</li> <li>Artificial Intelligence: A suite of APIs that run specific artificial intelligence and machine learning tasks on Google Cloud.</li> </ul> <p>Here is a cheetsheat containing all GCP services.</p>"},{"location":"projects/cloud/gke-1/","title":"Google Kubernetes Engine - An introduction","text":"<p>Kubernetes is an open-source platform for managing containerized workloads and services. <code>K8s</code> or <code>K8</code> (as it is colloquially known) makes it easy to orchestrate different containers on different hosts, scale them as microservices, deploy rollouts or rollbacks etc. K8s was open-sourced by Google in 2014. Below are some of the advantages of using K8s</p> <ul> <li>Load balancing &amp; service discovery: K8s can expose your a container to outside traffic. If incoming traffic is high, it can load balance and distribute so the deployment is stable.</li> <li>Storage orchestration - allows you to declaratively mount a storage system (local or cloud store)</li> <li>Structured rollout and rollbacks - you can instruct K8s your desired system state and it can control the update of the container images in a controlled manner.</li> <li>Automatic resource allocation - you provide K8s a set of nodes (a cluster) and you specify how much CPU and memory each container needs. K8s takes care of efficiently running all of the containers in the provided node pool.</li> <li>Self-healing - K8s can automatically divert traffic from unresponsive containers, kill them, restart them and advertise only after they recover</li> <li>Secret management - you can store &amp; manage sensitive info like passwords, SSH keys, oAuth tokens separately from container images</li> </ul>"},{"location":"projects/cloud/gke-1/#components-of-a-kubernetes-cluster","title":"Components of a Kubernetes cluster","text":"<ul> <li>Node - a physical machine that runs containers (with on-prem set up). In a cloud set up, this could be a VM.</li> <li>Cluster - a set of such nodes together running an application. A cluster can have a group of nodes that run as  control plane and the remaining running as nodes for containers.</li> <li>Node pool - designate subset of nodes within a cluster.</li> <li>Pod - a process running a container (with a wrapper around it). Pods are the smallest unit of reference in K8. Usually only one container runs in a pod. But you can package more than 1 container, especially if they have to communicate or share resources. A pod can provide a unique IP per pod and different ports for different containers running within it. To interact with a pod, you can use the <code>kubectl</code> command.<ul> <li></li> <li>To see the list of pods in your cluster, run <code>kubectl get pods</code></li> </ul> </li> </ul> <p>The IP address of a pod can change over time, especially as pods are created and terminated over time. Thus if the front-end pods refer to back-end pods by IP, the application may break. This is why you wrap the relevant pods behind a service and give it a fixed IP. As you scale your app, any number of pods can share that IP of that service.</p> <p>You can use <code>kubectl</code> commands to create pods, create load balancer, network etc. However, k8 allows you to declaratively define your architecture in a Yaml file, called a deployment config file.</p>"},{"location":"projects/cloud/gke-1/#the-control-plane","title":"The control plane","text":"<p>The control plane makes decisions about the cluster by detecting and responding to cluster events. The control plane can ideally be run on any node in the cluster. However, by convention, cluster start-up scripts start control plane first and do not run any user containers on that node.</p> <ul> <li>kube-apiserver is the API server component of the control plane which exposes K8 via the K8 API.</li> <li>etcd - is a consistent, highly available key-value store that stores all data about the cluster</li> <li>kube-scheduler - watches newly created pods with no assigned node and selects a node for them to run on. The scheduler will take into account the resource requirements, affinity specifications, data locality etc.</li> <li>kube-controller-manager - fine grained controller component that listens to events happening in the cluster.</li> <li>cloud-controller-manager - allows you to link your k8 cluster into the cloud provider's API.</li> </ul>"},{"location":"projects/cloud/gke-1/#components-of-a-node","title":"Components of a Node","text":"<p>These components run on every node (physical or virtual worker machine) providing a kubernetes runtime environment.</p> <ul> <li>kubelet - agent that runs on every node. It makes sure that containers are running in a pod.</li> <li>kube-proxy - maintains network rules on each node. This network allows communication within the cluster or from outside to your cluster.</li> <li>Container runtime - responsible for running the container.</li> </ul>"},{"location":"projects/cloud/gke-1/#google-kubernetes-engine","title":"Google Kubernetes Engine","text":"<p>GKE is a managed K8 in the cloud. It is a bunch of compute engine instances that are bound together to form a cluster. This cluster can be created using web UI or <code>gcloud</code> CLI. When using GKE, Google provides a built-in load-balancer service. </p>"},{"location":"projects/cloud/gke-2/","title":"Kubernetes objects and specification","text":"<p>A K8s object is a \"record of intent\" which tells K8 how you want the cluster to look like - aka, desired state. You can create and manage these objects by using the Kubernetes API. You typically do so by using the <code>kubectl</code> CLI tool Every K8 object has two fields <code>spec</code> (specification of desired state) and <code>status</code> (current status that k8 system will update by the control plane). The k8 system will work actively to bring the <code>status</code> to match the <code>spec</code>, the desired state.</p>"},{"location":"projects/cloud/gke-2/#k8s-object-spec","title":"K8s Object spec","text":"<p>Below is an example of a Kubernetes deployment in Yaml specification:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n\n  replicas: 2 # tells deployment to run 2 pods matching the template\n  template:\n    metadata:\n      labels:\n        app: nginx\n\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre> <p>You will typically pass this to <code>kubectl</code> as shown below to create a deployment:</p> <pre><code>kubectl apply -f &lt;path to yaml file&gt;\n</code></pre> <p>The following fields are compulsory</p> <ul> <li><code>apiVersion</code>: corresponds to K8s version</li> <li><code>kind</code>: what type of object is this spec for</li> <li><code>metadata</code>: unique identifiers for the object - <code>name</code>, <code>UID</code>, <code>namespace</code></li> <li><code>spec</code>: the state specification</li> </ul> <p>The spec is different for each type of object and can have nested objects. The K8s API ref contains templates that can be used for different object types</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/","title":"Analyzing historic hurricane tracks - Part 1/3","text":"In\u00a0[\u00a0]: Copied! <pre># imports for downloading data from FTP site\nimport os\nfrom ftplib import FTP\n\n# imports to process data using DASK\nfrom dask import delayed\nimport dask.dataframe as ddf\n\n# imports for data analysis and visualization\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# imports to perform spatial aggregation using ArcGIS GeoAnalytics server\nfrom arcgis.gis import GIS\nfrom arcgis.geoanalytics import get_datastores\nfrom arcgis.geoanalytics.summarize_data import reconstruct_tracks\nimport arcgis\n</pre> # imports for downloading data from FTP site import os from ftplib import FTP  # imports to process data using DASK from dask import delayed import dask.dataframe as ddf  # imports for data analysis and visualization import pandas as pd import matplotlib.pyplot as plt %matplotlib inline  # imports to perform spatial aggregation using ArcGIS GeoAnalytics server from arcgis.gis import GIS from arcgis.geoanalytics import get_datastores from arcgis.geoanalytics.summarize_data import reconstruct_tracks import arcgis <p>Establish an anonymous connection to FTP site</p> In\u00a0[4]: Copied! <pre>conn = FTP(host='eclipse.ncdc.noaa.gov')\nconn.login()\n</pre> conn = FTP(host='eclipse.ncdc.noaa.gov') conn.login() Out[4]: <pre>'230 Anonymous access granted, restrictions apply'</pre> <p>Change directory to folder contianing the hurricane files. List the files</p> In\u00a0[5]: Copied! <pre>conn.cwd('/pub/ibtracs/v03r10/all/csv/year/')\nfile_list = conn.nlst()\nlen(file_list)\n</pre> conn.cwd('/pub/ibtracs/v03r10/all/csv/year/') file_list = conn.nlst() len(file_list) Out[5]: <pre>176</pre> <p>Print the top 10 items</p> In\u00a0[6]: Copied! <pre>file_list[:10]\n</pre> file_list[:10] Out[6]: <pre>['Year.1842.ibtracs_all.v03r10.csv',\n 'Year.1843.ibtracs_all.v03r10.csv',\n 'Year.1844.ibtracs_all.v03r10.csv',\n 'Year.1845.ibtracs_all.v03r10.csv',\n 'Year.1846.ibtracs_all.v03r10.csv',\n 'Year.1847.ibtracs_all.v03r10.csv',\n 'Year.1848.ibtracs_all.v03r10.csv',\n 'Year.1849.ibtracs_all.v03r10.csv',\n 'Year.1850.ibtracs_all.v03r10.csv',\n 'Year.1851.ibtracs_all.v03r10.csv']</pre> In\u00a0[1]: Copied! <pre>data_dir = './me'\n</pre> data_dir = './me' In\u00a0[31]: Copied! <pre>if 'hurricanes_raw' not in os.listdir(data_dir):\n    os.mkdir(os.path.join(data_dir,'hurricanes_raw'))\n\nhurricane_raw_dir = os.path.join(data_dir,'hurricanes_raw')\nos.listdir(data_dir)\n</pre> if 'hurricanes_raw' not in os.listdir(data_dir):     os.mkdir(os.path.join(data_dir,'hurricanes_raw'))  hurricane_raw_dir = os.path.join(data_dir,'hurricanes_raw') os.listdir(data_dir) Out[31]: <pre>['Allstorms.ibtracs_all.v03r09.csv',\n 'hurricanes_raw',\n '.nb_auth_file',\n 'Allstorms.ibtracs_all.v03r09.csv.gz']</pre> In\u00a0[11]: Copied! <pre>%%time\nfile_path = hurricane_raw_dir\nfor file in file_list:\n    with open(os.path.join(file_path, file), 'wb') as file_handle:\n        try:\n            conn.retrbinary('RETR ' + file, file_handle.write, 1024)\n            print(f'Downloaded {file}')\n        \n        except Exception as download_ex:\n            print(f'Error downloading {file} + {str(download_ex)}')\n</pre> %%time file_path = hurricane_raw_dir for file in file_list:     with open(os.path.join(file_path, file), 'wb') as file_handle:         try:             conn.retrbinary('RETR ' + file, file_handle.write, 1024)             print(f'Downloaded {file}')                  except Exception as download_ex:             print(f'Error downloading {file} + {str(download_ex)}') <pre>Downloaded Year.1842.ibtracs_all.v03r10.csv\nDownloaded Year.1843.ibtracs_all.v03r10.csv\nDownloaded Year.1844.ibtracs_all.v03r10.csv\nDownloaded Year.1845.ibtracs_all.v03r10.csv\n....Downloaded Year.2015.ibtracs_all.v03r10.csv\nDownloaded Year.2016.ibtracs_all.v03r10.csv\nDownloaded Year.2017.ibtracs_all.v03r10.csv\nCPU times: user 8.63 s, sys: 12.1 s, total: 20.8 s\nWall time: 12min 5s\n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[3]: Copied! <pre>csv_path = os.path.join(hurricane_raw_dir,'Year.2017.ibtracs_all.v03r10.csv')\n</pre> csv_path = os.path.join(hurricane_raw_dir,'Year.2017.ibtracs_all.v03r10.csv') In\u00a0[18]: Copied! <pre>df = pd.read_csv(csv_path)\ndf.head()\n</pre> df = pd.read_csv(csv_path) df.head() Out[18]: IBTrACS -- Version: v03r10 Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude Longitude Wind(WMO) Pres(WMO) Center Wind(WMO) Percentile Pres(WMO) Percentile Track_type Latitude_for_mapping Longitude_for_mapping Current Basin hurdat_atl_lat hurdat_atl_lon hurdat_atl_grade hurdat_atl_wind hurdat_atl_pres td9636_lat td9636_lon td9636_grade td9636_wind td9636_pres reunion_lat reunion_lon reunion_grade reunion_wind reunion_pres atcf_lat atcf_lon atcf_grade atcf_wind atcf_pres mlc_natl_lat mlc_natl_lon mlc_natl_grade mlc_natl_wind mlc_natl_pres ds824_sh_lat ds824_sh_lon ds824_sh_grade ds824_sh_wind ds824_sh_pres ds824_ni_lat ds824_ni_lon ds824_ni_grade ds824_ni_wind ds824_ni_pres bom_lat bom_lon bom_grade bom_wind bom_pres ds824_au_lat ds824_au_lon ds824_au_grade ds824_au_wind ds824_au_pres jtwc_sh_lat jtwc_sh_lon jtwc_sh_grade jtwc_sh_wind jtwc_sh_pres jtwc_wp_lat jtwc_wp_lon jtwc_wp_grade jtwc_wp_wind jtwc_wp_pres td9635_lat td9635_lon td9635_grade td9635_wind td9635_pres ds824_wp_lat ds824_wp_lon ds824_wp_grade ds824_wp_wind ds824_wp_pres jtwc_io_lat jtwc_io_lon jtwc_io_grade jtwc_io_wind jtwc_io_pres cma_lat cma_lon cma_grade cma_wind cma_pres hurdat_epa_lat hurdat_epa_lon hurdat_epa_grade hurdat_epa_wind hurdat_epa_pres jtwc_ep_lat jtwc_ep_lon jtwc_ep_grade jtwc_ep_wind jtwc_ep_pres ds824_ep_lat ds824_ep_lon ds824_ep_grade ds824_ep_wind ds824_ep_pres jtwc_cp_lat jtwc_cp_lon jtwc_cp_grade jtwc_cp_wind jtwc_cp_pres tokyo_lat tokyo_lon tokyo_grade tokyo_wind tokyo_pres neumann_lat neumann_lon neumann_grade neumann_wind neumann_pres hko_lat hko_lon hko_grade hko_wind hko_pres cphc_lat cphc_lon cphc_grade cphc_wind cphc_pres wellington_lat wellington_lon wellington_grade wellington_wind wellington_pres newdelhi_lat newdelhi_lon newdelhi_grade newdelhi_wind newdelhi_pres nadi_lat nadi_lon nadi_grade nadi_wind nadi_pres reunion_rmw reunion_wind_radii_1_ne reunion_wind_radii_1_se reunion_wind_radii_1_sw reunion_wind_radii_1_nw reunion_wind_radii_2_ne reunion_wind_radii_2_se reunion_wind_radii_2_sw reunion_wind_radii_2_nw bom_mn_hurr_xtnt bom_mn_gale_xtnt bom_mn_eye_diam bom_roci atcf_rmw atcf_poci atcf_roci atcf_eye atcf_wrad34_rad1 atcf_wrad34_rad2 atcf_wrad34_rad3 atcf_wrad34_rad4 atcf_wrad50_rad1 atcf_wrad50_rad2 atcf_wrad50_rad3 atcf_wrad50_rad4 atcf_wrad64_rad1 atcf_wrad64_rad2 atcf_wrad64_rad3 atcf_wrad64_rad4 tokyo_dir50 tokyo_long50 tokyo_short50 tokyo_dir30 tokyo_long30 tokyo_short30 jtwc_??_rmw jtwc_??_poci jtwc_??_roci jtwc_??_eye jtwc_??_wrad34_rad1 jtwc_??_wrad34_rad2 jtwc_??_wrad34_rad3 jtwc_??_wrad34_rad4 jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 NaN Year # BB BB NaN YYYY-MM-DD HH:MM:SS NaN deg_north deg_east kt mb NaN % % NaN degrees_north degrees_east NaN deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile mb nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile Quad nmile nmile Quad nmile nmile nmile mb nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 06:00:00 NR -13.70 63.90 0.0 0.0 reunion -100.000 -100.000 main -13.70 63.90 SI -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -13.7 63.9 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 1874-01-11 12:00:00 NR -999. -999. -999. -999. NaN -999. -999. main -13.75 63.86 SI -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -13.7 63.9 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 1874-01-11 18:00:00 NR -999. -999. -999. -999. NaN -999. -999. main -13.88 63.77 SI -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -13.9 63.8 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 <p>The input looks mangled. This is because the file's row <code>1</code> has a header that pandas fails to read. So let us skip that row</p> In\u00a0[19]: Copied! <pre>df = pd.read_csv(csv_path, skiprows=1)\ndf.head()\n</pre> df = pd.read_csv(csv_path, skiprows=1) df.head() Out[19]: Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude Longitude ... jtwc_??_wrad34_rad3 jtwc_??_wrad34_rad4 jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 0 NaN Year # BB BB NaN YYYY-MM-DD HH:MM:SS NaN deg_north deg_east ... nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile 1 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 06:00:00 NR -13.70 63.90 ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 2 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 12:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 3 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 18:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 4 1874011S14064 1874 01 SI MM XXXX874148 1874-01-12 00:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 <p>5 rows \u00d7 200 columns</p> <p>A little better. But the file's 3rd row is also a header. Let us drop that row</p> In\u00a0[20]: Copied! <pre>df.drop(labels=0, axis=0, inplace=True)\ndf.head()\n</pre> df.drop(labels=0, axis=0, inplace=True) df.head() Out[20]: Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude Longitude ... jtwc_??_wrad34_rad3 jtwc_??_wrad34_rad4 jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 1 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 06:00:00 NR -13.70 63.90 ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 2 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 12:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 3 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 18:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 4 1874011S14064 1874 01 SI MM XXXX874148 1874-01-12 00:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 5 1874011S14064 1874 01 SI MM XXXX874148 1874-01-12 06:00:00 NR -14.80 63.30 ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 <p>5 rows \u00d7 200 columns</p> In\u00a0[9]: Copied! <pre>%%time\nfile_path = hurricane_raw_dir\nnum_records = {}\nfor file in file_list:\n    df = pd.read_csv(os.path.join(file_path, file), skiprows=1)\n    num_records[str(file.split('.')[1])] = df.shape[0]\n    \n    df.drop(labels=0, axis=0, inplace=True)\n    df.to_csv(os.path.join(file_path, file))\n    print(f'Processed {file}')\n</pre> %%time file_path = hurricane_raw_dir num_records = {} for file in file_list:     df = pd.read_csv(os.path.join(file_path, file), skiprows=1)     num_records[str(file.split('.')[1])] = df.shape[0]          df.drop(labels=0, axis=0, inplace=True)     df.to_csv(os.path.join(file_path, file))     print(f'Processed {file}') <pre>Processed Year.1842.ibtracs_all.v03r10.csv\nProcessed Year.1843.ibtracs_all.v03r10.csv\nProcessed Year.1844.ibtracs_all.v03r10.csv\nProcessed Year.1845.ibtracs_all.v03r10.csv\n...Processed Year.2013.ibtracs_all.v03r10.csv\nProcessed Year.2014.ibtracs_all.v03r10.csv\nProcessed Year.2015.ibtracs_all.v03r10.csv\nProcessed Year.2016.ibtracs_all.v03r10.csv\nProcessed Year.2017.ibtracs_all.v03r10.csv\nCPU times: user 36.4 s, sys: 3.39 s, total: 39.8 s\nWall time: 46.8 s\n</pre> In\u00a0[3]: Copied! <pre>fld_path = hurricane_raw_dir\ncsv_path = os.path.join(fld_path,'*.csv')\n</pre> fld_path = hurricane_raw_dir csv_path = os.path.join(fld_path,'*.csv') <p>Preemptively, specify the assortment of values that should be treated as null values.</p> In\u00a0[36]: Copied! <pre>%%time\ntable_na_values=['-999.','-999','-999.000', '-1', '-1.0','0','0.0']\nfull_df = ddf.read_csv(csv_path, na_values=table_na_values, dtype={'Center': 'object'})\n</pre> %%time table_na_values=['-999.','-999','-999.000', '-1', '-1.0','0','0.0'] full_df = ddf.read_csv(csv_path, na_values=table_na_values, dtype={'Center': 'object'}) <pre>CPU times: user 1.26 s, sys: 17.6 ms, total: 1.28 s\nWall time: 1.29 s\n</pre> <p>You can query the top few (or bottom few) records as you do on a regular Pandas <code>DataFrame</code> object.</p> In\u00a0[37]: Copied! <pre>full_df.head()\n</pre> full_df.head() Out[37]: Unnamed: 0 Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude ... jtwc_??_wrad34_rad3 jtwc_??_wrad34_rad4 jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 0 1 1842298N11080 1842 1 NI BB NOT NAMED 1842-10-25 06:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 2 1842298N11080 1842 1 NI BB NOT NAMED 1842-10-25 12:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 3 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-25 18:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 4 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-26 00:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 5 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-26 06:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN <p>5 rows \u00d7 201 columns</p> <p>Drop the first, duplicate index column</p> In\u00a0[38]: Copied! <pre>full_df = full_df.drop(labels=['Unnamed: 0'], axis=1)\n</pre> full_df = full_df.drop(labels=['Unnamed: 0'], axis=1) In\u00a0[39]: Copied! <pre>all_columns=list(full_df.columns)\nlen(all_columns)\n</pre> all_columns=list(full_df.columns) len(all_columns) Out[39]: <pre>200</pre> <p>This dataset has <code>200</code> columns. Not all are unique as you can see from the print below:</p> In\u00a0[40]: Copied! <pre>from pprint import pprint\npprint(all_columns, compact=True, width=100)\n</pre> from pprint import pprint pprint(all_columns, compact=True, width=100) <pre>['Serial_Num', 'Season', 'Num', 'Basin', 'Sub_basin', 'Name', 'ISO_time', 'Nature', 'Latitude',\n 'Longitude', 'Wind(WMO)', 'Pres(WMO)', 'Center', 'Wind(WMO) Percentile', 'Pres(WMO) Percentile',\n 'Track_type', 'Latitude_for_mapping', 'Longitude_for_mapping', 'Current Basin', 'hurdat_atl_lat',\n 'hurdat_atl_lon', 'hurdat_atl_grade', 'hurdat_atl_wind', 'hurdat_atl_pres', 'td9636_lat',\n 'td9636_lon', 'td9636_grade', 'td9636_wind', 'td9636_pres', 'reunion_lat', 'reunion_lon',\n 'reunion_grade', 'reunion_wind', 'reunion_pres', 'atcf_lat', 'atcf_lon', 'atcf_grade', 'atcf_wind',\n 'atcf_pres', 'mlc_natl_lat', 'mlc_natl_lon', 'mlc_natl_grade', 'mlc_natl_wind', 'mlc_natl_pres',\n 'ds824_sh_lat', 'ds824_sh_lon', 'ds824_sh_grade', 'ds824_sh_wind', 'ds824_sh_pres', 'ds824_ni_lat',\n 'ds824_ni_lon', 'ds824_ni_grade', 'ds824_ni_wind', 'ds824_ni_pres', 'bom_lat', 'bom_lon',\n 'bom_grade', 'bom_wind', 'bom_pres', 'ds824_au_lat', 'ds824_au_lon', 'ds824_au_grade',\n 'ds824_au_wind', 'ds824_au_pres', 'jtwc_sh_lat', 'jtwc_sh_lon', 'jtwc_sh_grade', 'jtwc_sh_wind',\n 'jtwc_sh_pres', 'jtwc_wp_lat', 'jtwc_wp_lon', 'jtwc_wp_grade', 'jtwc_wp_wind', 'jtwc_wp_pres',\n 'td9635_lat', 'td9635_lon', 'td9635_grade', 'td9635_wind', 'td9635_pres', 'ds824_wp_lat',\n 'ds824_wp_lon', 'ds824_wp_grade', 'ds824_wp_wind', 'ds824_wp_pres', 'jtwc_io_lat', 'jtwc_io_lon',\n 'jtwc_io_grade', 'jtwc_io_wind', 'jtwc_io_pres', 'cma_lat', 'cma_lon', 'cma_grade', 'cma_wind',\n 'cma_pres', 'hurdat_epa_lat', 'hurdat_epa_lon', 'hurdat_epa_grade', 'hurdat_epa_wind',\n 'hurdat_epa_pres', 'jtwc_ep_lat', 'jtwc_ep_lon', 'jtwc_ep_grade', 'jtwc_ep_wind', 'jtwc_ep_pres',\n 'ds824_ep_lat', 'ds824_ep_lon', 'ds824_ep_grade', 'ds824_ep_wind', 'ds824_ep_pres', 'jtwc_cp_lat',\n 'jtwc_cp_lon', 'jtwc_cp_grade', 'jtwc_cp_wind', 'jtwc_cp_pres', 'tokyo_lat', 'tokyo_lon',\n 'tokyo_grade', 'tokyo_wind', 'tokyo_pres', 'neumann_lat', 'neumann_lon', 'neumann_grade',\n 'neumann_wind', 'neumann_pres', 'hko_lat', 'hko_lon', 'hko_grade', 'hko_wind', 'hko_pres',\n 'cphc_lat', 'cphc_lon', 'cphc_grade', 'cphc_wind', 'cphc_pres', 'wellington_lat', 'wellington_lon',\n 'wellington_grade', 'wellington_wind', 'wellington_pres', 'newdelhi_lat', 'newdelhi_lon',\n 'newdelhi_grade', 'newdelhi_wind', 'newdelhi_pres', 'nadi_lat', 'nadi_lon', 'nadi_grade',\n 'nadi_wind', 'nadi_pres', 'reunion_rmw', 'reunion_wind_radii_1_ne', 'reunion_wind_radii_1_se',\n 'reunion_wind_radii_1_sw', 'reunion_wind_radii_1_nw', 'reunion_wind_radii_2_ne',\n 'reunion_wind_radii_2_se', 'reunion_wind_radii_2_sw', 'reunion_wind_radii_2_nw',\n 'bom_mn_hurr_xtnt', 'bom_mn_gale_xtnt', 'bom_mn_eye_diam', 'bom_roci', 'atcf_rmw', 'atcf_poci',\n 'atcf_roci', 'atcf_eye', 'atcf_wrad34_rad1', 'atcf_wrad34_rad2', 'atcf_wrad34_rad3',\n 'atcf_wrad34_rad4', 'atcf_wrad50_rad1', 'atcf_wrad50_rad2', 'atcf_wrad50_rad3', 'atcf_wrad50_rad4',\n 'atcf_wrad64_rad1', 'atcf_wrad64_rad2', 'atcf_wrad64_rad3', 'atcf_wrad64_rad4', 'tokyo_dir50',\n 'tokyo_long50', 'tokyo_short50', 'tokyo_dir30', 'tokyo_long30', 'tokyo_short30', 'jtwc_??_rmw',\n 'jtwc_??_poci', 'jtwc_??_roci', 'jtwc_??_eye', 'jtwc_??_wrad34_rad1', 'jtwc_??_wrad34_rad2',\n 'jtwc_??_wrad34_rad3', 'jtwc_??_wrad34_rad4', 'jtwc_??_wrad50_rad1', 'jtwc_??_wrad50_rad2',\n 'jtwc_??_wrad50_rad3', 'jtwc_??_wrad50_rad4', 'jtwc_??_wrad64_rad1', 'jtwc_??_wrad64_rad2',\n 'jtwc_??_wrad64_rad3', 'jtwc_??_wrad64_rad4']\n</pre> <p>Reading the metadata from NOAA NCDC site, we find sensor measurements get unique columns if they are collected by a different agency. Thus we find multiple pressure, wind speed, latitude, longitude, etc. columns with different suffixes and prefixes. Data is sparse as it gets distributed between these columns. For our geospatial analysis, it suffices if we can merge these columns together and get location information from the co-ordinates.</p> In\u00a0[41]: Copied! <pre>lat_columns = [x for x in all_columns if 'lat' in x.lower()]\nlon_columns = [x for x in all_columns if 'lon' in x.lower()]\nfor x in zip(lat_columns, lon_columns):\n    print(x)\n</pre> lat_columns = [x for x in all_columns if 'lat' in x.lower()] lon_columns = [x for x in all_columns if 'lon' in x.lower()] for x in zip(lat_columns, lon_columns):     print(x) <pre>('Latitude', 'Longitude')\n('Latitude_for_mapping', 'Longitude_for_mapping')\n('hurdat_atl_lat', 'hurdat_atl_lon')\n('td9636_lat', 'td9636_lon')\n('reunion_lat', 'reunion_lon')\n('atcf_lat', 'atcf_lon')\n('mlc_natl_lat', 'mlc_natl_lon')\n('ds824_sh_lat', 'ds824_sh_lon')\n('ds824_ni_lat', 'ds824_ni_lon')\n('bom_lat', 'bom_lon')\n('ds824_au_lat', 'ds824_au_lon')\n('jtwc_sh_lat', 'jtwc_sh_lon')\n('jtwc_wp_lat', 'jtwc_wp_lon')\n('td9635_lat', 'td9635_lon')\n('ds824_wp_lat', 'ds824_wp_lon')\n('jtwc_io_lat', 'jtwc_io_lon')\n('cma_lat', 'cma_lon')\n('hurdat_epa_lat', 'hurdat_epa_lon')\n('jtwc_ep_lat', 'jtwc_ep_lon')\n('ds824_ep_lat', 'ds824_ep_lon')\n('jtwc_cp_lat', 'jtwc_cp_lon')\n('tokyo_lat', 'tokyo_lon')\n('neumann_lat', 'neumann_lon')\n('hko_lat', 'hko_lon')\n('cphc_lat', 'cphc_lon')\n('wellington_lat', 'wellington_lon')\n('newdelhi_lat', 'newdelhi_lon')\n('nadi_lat', 'nadi_lon')\n</pre> <p>In this dataset, if data is collected by 1 agency, the corresponding duplicate columns from other agencies are empty. However there may be exceptions. Hence we define a custom function that will pick median value for a row, from a given list of columns. This way, we can consolidate latitude / longitude information from all the agencies.</p> In\u00a0[42]: Copied! <pre>def pick_median_value(row, col_list):\n    return row[col_list].median()\n</pre> def pick_median_value(row, col_list):     return row[col_list].median() In\u00a0[43]: Copied! <pre>%%time\nfull_df['latitude_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = lat_columns)\n</pre> %%time full_df['latitude_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = lat_columns) <pre>CPU times: user 56.9 ms, sys: 5.31 ms, total: 62.2 ms\nWall time: 58.3 ms\n</pre> In\u00a0[44]: Copied! <pre>%%time\nfull_df['longitude_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = lon_columns)\n</pre> %%time full_df['longitude_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = lon_columns) <pre>CPU times: user 58.3 ms, sys: 5.43 ms, total: 63.7 ms\nWall time: 59.1 ms\n</pre> <p>With <code>dask</code>, the above operation was delayed and stored in a queue. It has not been evaluated yet. Next, let us evaluate for <code>5</code> records and print output. If results look good, we will merge all remaining related columns together.</p> In\u00a0[45]: Copied! <pre>%%time\nfull_df.head(5)\n</pre> %%time full_df.head(5) <pre>CPU times: user 137 ms, sys: 6.17 ms, total: 143 ms\nWall time: 141 ms\n</pre> Out[45]: Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude Longitude ... jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 latitude_merged longitude_merged 0 1842298N11080 1842 1 NI BB NOT NAMED 1842-10-25 06:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.885 79.815 1 1842298N11080 1842 1 NI BB NOT NAMED 1842-10-25 12:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.810 78.890 2 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-25 18:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.795 77.910 3 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-26 00:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.795 76.915 4 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-26 06:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.805 75.820 <p>5 rows \u00d7 202 columns</p> <p>The results look good. Two additional columns(<code>latitude_merged</code>, <code>longitude_merged</code>) have been added. Thus by merging related columns, the redundant sparse columns can be removed and thereby simplifying the dimension of the input dataset.</p> <p>Now that this prototype looks good, we will proceed by identifying the lists of remaining columns that are redundant and can be merged.</p> In\u00a0[46]: Copied! <pre>from copy import deepcopy\ncolumns_tracker = deepcopy(all_columns)\nlen(columns_tracker)\n</pre> from copy import deepcopy columns_tracker = deepcopy(all_columns) len(columns_tracker) Out[46]: <pre>200</pre> <p>From the <code>columns_tracker</code> list, let us remove the redundant columns we already identified for location columns:</p> In\u00a0[47]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in lat_columns]\ncolumns_tracker = [x for x in columns_tracker if x not in lon_columns]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in lat_columns] columns_tracker = [x for x in columns_tracker if x not in lon_columns] len(columns_tracker) Out[47]: <pre>142</pre> <p>Thus, we have reduced the number of columns from <code>200</code> to <code>142</code>. We will progressively reduce this while retaining key information.</p> In\u00a0[48]: Copied! <pre># pick all columns that have 'wind' in name\nwind_columns = [x for x in columns_tracker if 'wind' in x.lower()]\n\n# based on metadata doc, we decide to eliminate percentile and wind distance columns\ncolumns_to_eliminate = [x for x in wind_columns if 'radii' in x or 'percentile' in x.lower()]\n\n# trim wind_columns by removing the ones we need to eliminate\nwind_columns = [x for x in wind_columns if x not in columns_to_eliminate]\nwind_columns\n</pre> # pick all columns that have 'wind' in name wind_columns = [x for x in columns_tracker if 'wind' in x.lower()]  # based on metadata doc, we decide to eliminate percentile and wind distance columns columns_to_eliminate = [x for x in wind_columns if 'radii' in x or 'percentile' in x.lower()]  # trim wind_columns by removing the ones we need to eliminate wind_columns = [x for x in wind_columns if x not in columns_to_eliminate] wind_columns Out[48]: <pre>['Wind(WMO)',\n 'hurdat_atl_wind',\n 'td9636_wind',\n 'reunion_wind',\n 'atcf_wind',\n 'mlc_natl_wind',\n 'ds824_sh_wind',\n 'ds824_ni_wind',\n 'bom_wind',\n 'ds824_au_wind',\n 'jtwc_sh_wind',\n 'jtwc_wp_wind',\n 'td9635_wind',\n 'ds824_wp_wind',\n 'jtwc_io_wind',\n 'cma_wind',\n 'hurdat_epa_wind',\n 'jtwc_ep_wind',\n 'ds824_ep_wind',\n 'jtwc_cp_wind',\n 'tokyo_wind',\n 'neumann_wind',\n 'hko_wind',\n 'cphc_wind',\n 'wellington_wind',\n 'newdelhi_wind',\n 'nadi_wind']</pre> In\u00a0[49]: Copied! <pre>%%time\nfull_df['wind_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = wind_columns)\n</pre> %%time full_df['wind_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = wind_columns) <pre>CPU times: user 56.7 ms, sys: 4.92 ms, total: 61.6 ms\nWall time: 57.6 ms\n</pre> In\u00a0[50]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in wind_columns]\ncolumns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in wind_columns] columns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate] len(columns_tracker) Out[50]: <pre>106</pre> In\u00a0[51]: Copied! <pre># pick all columns that have 'pres' in name\npressure_columns = [x for x in columns_tracker if 'pres' in x.lower()]\n\n# from metadata, we eliminate percentile and pres distance columns\nif columns_to_eliminate:\n    columns_to_eliminate.extend([x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()])\nelse:\n    columns_to_eliminate = [x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()]\n\n# trim wind_columns by removing the ones we need to eliminate\npressure_columns = [x for x in pressure_columns if x not in columns_to_eliminate]\npressure_columns\n</pre> # pick all columns that have 'pres' in name pressure_columns = [x for x in columns_tracker if 'pres' in x.lower()]  # from metadata, we eliminate percentile and pres distance columns if columns_to_eliminate:     columns_to_eliminate.extend([x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()]) else:     columns_to_eliminate = [x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()]  # trim wind_columns by removing the ones we need to eliminate pressure_columns = [x for x in pressure_columns if x not in columns_to_eliminate] pressure_columns Out[51]: <pre>['Pres(WMO)',\n 'hurdat_atl_pres',\n 'td9636_pres',\n 'reunion_pres',\n 'atcf_pres',\n 'mlc_natl_pres',\n 'ds824_sh_pres',\n 'ds824_ni_pres',\n 'bom_pres',\n 'ds824_au_pres',\n 'jtwc_sh_pres',\n 'jtwc_wp_pres',\n 'td9635_pres',\n 'ds824_wp_pres',\n 'jtwc_io_pres',\n 'cma_pres',\n 'hurdat_epa_pres',\n 'jtwc_ep_pres',\n 'ds824_ep_pres',\n 'jtwc_cp_pres',\n 'tokyo_pres',\n 'neumann_pres',\n 'hko_pres',\n 'cphc_pres',\n 'wellington_pres',\n 'newdelhi_pres',\n 'nadi_pres']</pre> In\u00a0[52]: Copied! <pre>%%time\nfull_df['pressure_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = pressure_columns)\n</pre> %%time full_df['pressure_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = pressure_columns) <pre>CPU times: user 122 ms, sys: 5.33 ms, total: 127 ms\nWall time: 123 ms\n</pre> In\u00a0[53]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in pressure_columns]\ncolumns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in pressure_columns] columns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate] len(columns_tracker) Out[53]: <pre>78</pre> <p>Notice the length of <code>columns_tracker</code> is reducing progressively as we identify redundant columns.</p> In\u00a0[54]: Copied! <pre># pick all columns that have 'grade' in name\ngrade_columns = [x for x in columns_tracker if 'grade' in x.lower()]\ngrade_columns\n</pre> # pick all columns that have 'grade' in name grade_columns = [x for x in columns_tracker if 'grade' in x.lower()] grade_columns Out[54]: <pre>['hurdat_atl_grade',\n 'td9636_grade',\n 'reunion_grade',\n 'atcf_grade',\n 'mlc_natl_grade',\n 'ds824_sh_grade',\n 'ds824_ni_grade',\n 'bom_grade',\n 'ds824_au_grade',\n 'jtwc_sh_grade',\n 'jtwc_wp_grade',\n 'td9635_grade',\n 'ds824_wp_grade',\n 'jtwc_io_grade',\n 'cma_grade',\n 'hurdat_epa_grade',\n 'jtwc_ep_grade',\n 'ds824_ep_grade',\n 'jtwc_cp_grade',\n 'tokyo_grade',\n 'neumann_grade',\n 'hko_grade',\n 'cphc_grade',\n 'wellington_grade',\n 'newdelhi_grade',\n 'nadi_grade']</pre> In\u00a0[55]: Copied! <pre>%%time\nfull_df['grade_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = grade_columns)\n</pre> %%time full_df['grade_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = grade_columns) <pre>CPU times: user 54.9 ms, sys: 5.59 ms, total: 60.5 ms\nWall time: 56.4 ms\n</pre> In\u00a0[56]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in grade_columns]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in grade_columns] len(columns_tracker) Out[56]: <pre>52</pre> In\u00a0[57]: Copied! <pre># pick all columns that have 'eye' in name\neye_dia_columns = [x for x in columns_tracker if 'eye' in x.lower()]\neye_dia_columns\n</pre> # pick all columns that have 'eye' in name eye_dia_columns = [x for x in columns_tracker if 'eye' in x.lower()] eye_dia_columns Out[57]: <pre>['bom_mn_eye_diam', 'atcf_eye', 'jtwc_??_eye']</pre> In\u00a0[58]: Copied! <pre>%%time\nfull_df['eye_dia_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = eye_dia_columns)\n</pre> %%time full_df['eye_dia_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = eye_dia_columns) <pre>CPU times: user 53.6 ms, sys: 4.74 ms, total: 58.4 ms\nWall time: 54.8 ms\n</pre> In\u00a0[59]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in eye_dia_columns]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in eye_dia_columns] len(columns_tracker) Out[59]: <pre>49</pre> <p>We are down to <code>49</code> columns, let us visualize what those look like.</p> In\u00a0[60]: Copied! <pre>pprint(columns_tracker, width=119, compact=True)\n</pre> pprint(columns_tracker, width=119, compact=True) <pre>['Serial_Num', 'Season', 'Num', 'Basin', 'Sub_basin', 'Name', 'ISO_time', 'Nature', 'Center', 'Track_type',\n 'Current Basin', 'reunion_rmw', 'bom_mn_hurr_xtnt', 'bom_mn_gale_xtnt', 'bom_roci', 'atcf_rmw', 'atcf_poci',\n 'atcf_roci', 'atcf_wrad34_rad1', 'atcf_wrad34_rad2', 'atcf_wrad34_rad3', 'atcf_wrad34_rad4', 'atcf_wrad50_rad1',\n 'atcf_wrad50_rad2', 'atcf_wrad50_rad3', 'atcf_wrad50_rad4', 'atcf_wrad64_rad1', 'atcf_wrad64_rad2',\n 'atcf_wrad64_rad3', 'atcf_wrad64_rad4', 'tokyo_dir50', 'tokyo_short50', 'tokyo_dir30', 'tokyo_short30', 'jtwc_??_rmw',\n 'jtwc_??_poci', 'jtwc_??_roci', 'jtwc_??_wrad34_rad1', 'jtwc_??_wrad34_rad2', 'jtwc_??_wrad34_rad3',\n 'jtwc_??_wrad34_rad4', 'jtwc_??_wrad50_rad1', 'jtwc_??_wrad50_rad2', 'jtwc_??_wrad50_rad3', 'jtwc_??_wrad50_rad4',\n 'jtwc_??_wrad64_rad1', 'jtwc_??_wrad64_rad2', 'jtwc_??_wrad64_rad3', 'jtwc_??_wrad64_rad4']\n</pre> <p>Based on metadata shared by data provider, we choose to retain only the first <code>11</code> columns. We add the rest to the list <code>columns_to_eliminate</code>.</p> In\u00a0[61]: Copied! <pre>columns_to_eliminate.extend(columns_tracker[11:])\npprint(columns_to_eliminate, width=119, compact=True)\n</pre> columns_to_eliminate.extend(columns_tracker[11:]) pprint(columns_to_eliminate, width=119, compact=True) <pre>['Wind(WMO) Percentile', 'reunion_wind_radii_1_ne', 'reunion_wind_radii_1_se', 'reunion_wind_radii_1_sw',\n 'reunion_wind_radii_1_nw', 'reunion_wind_radii_2_ne', 'reunion_wind_radii_2_se', 'reunion_wind_radii_2_sw',\n 'reunion_wind_radii_2_nw', 'Pres(WMO) Percentile', 'reunion_rmw', 'bom_mn_hurr_xtnt', 'bom_mn_gale_xtnt', 'bom_roci',\n 'atcf_rmw', 'atcf_poci', 'atcf_roci', 'atcf_wrad34_rad1', 'atcf_wrad34_rad2', 'atcf_wrad34_rad3', 'atcf_wrad34_rad4',\n 'atcf_wrad50_rad1', 'atcf_wrad50_rad2', 'atcf_wrad50_rad3', 'atcf_wrad50_rad4', 'atcf_wrad64_rad1',\n 'atcf_wrad64_rad2', 'atcf_wrad64_rad3', 'atcf_wrad64_rad4', 'tokyo_dir50', 'tokyo_short50', 'tokyo_dir30',\n 'tokyo_short30', 'jtwc_??_rmw', 'jtwc_??_poci', 'jtwc_??_roci', 'jtwc_??_wrad34_rad1', 'jtwc_??_wrad34_rad2',\n 'jtwc_??_wrad34_rad3', 'jtwc_??_wrad34_rad4', 'jtwc_??_wrad50_rad1', 'jtwc_??_wrad50_rad2', 'jtwc_??_wrad50_rad3',\n 'jtwc_??_wrad50_rad4', 'jtwc_??_wrad64_rad1', 'jtwc_??_wrad64_rad2', 'jtwc_??_wrad64_rad3', 'jtwc_??_wrad64_rad4']\n</pre> In\u00a0[62]: Copied! <pre>len(full_df.columns)\n</pre> len(full_df.columns) Out[62]: <pre>206</pre> In\u00a0[63]: Copied! <pre>columns_to_drop = lat_columns + lon_columns + wind_columns + pressure_columns + \\\n                    grade_columns + eye_dia_columns+columns_to_eliminate\nlen(columns_to_drop)\n</pre> columns_to_drop = lat_columns + lon_columns + wind_columns + pressure_columns + \\                     grade_columns + eye_dia_columns+columns_to_eliminate len(columns_to_drop) Out[63]: <pre>189</pre> In\u00a0[34]: Copied! <pre>full_df.visualize()\n</pre> full_df.visualize() Out[34]: <p>Below we perform execute all column merge and column drop operations that we have queued so far. We store the resulting <code>DataFrame</code> in a new variable.</p> In\u00a0[\u00a0]: Copied! <pre>if 'hurricanes_merged' not in os.listdir(data_dir):\n    os.mkdir(os.path.join(data_dir,'hurricanes_merged'))\n\nmerged_csv_path = os.path.join(data_dir, 'hurricanes_merged')\n</pre> if 'hurricanes_merged' not in os.listdir(data_dir):     os.mkdir(os.path.join(data_dir,'hurricanes_merged'))  merged_csv_path = os.path.join(data_dir, 'hurricanes_merged') In\u00a0[64]: Copied! <pre>%%time\nmerged_df = full_df.drop(columns_to_drop, axis=1)\nmerged_df.to_csv(os.path.join(merged_csv_path, 'hurr_dask_*.csv'))\n</pre> %%time merged_df = full_df.drop(columns_to_drop, axis=1) merged_df.to_csv(os.path.join(merged_csv_path, 'hurr_dask_*.csv')) <pre>CPU times: user 43min 46s, sys: 6min 3s, total: 49min 49s\nWall time: 45min 45s\n</pre> <p>The <code>save()</code> operation spawns several workers that compute in parallel. Notice the ouput file name contains a wildcard (<code>*</code>). This allows Dask to both read data in chunks and write outputs in chunks. The save operation will result in creating a number of processed CSV files whose names are prefixed with <code>hurr_dask</code> and suffixed with a number.</p> In\u00a0[65]: Copied! <pre>merged_df.shape\n</pre> merged_df.shape Out[65]: <pre>(348703, 17)</pre> In\u00a0[7]: Copied! <pre>gis = GIS(\"home\")\n</pre> gis = GIS(\"home\") <p>Get the geoanalytics datastores and search it for the registered datasets:</p> In\u00a0[20]: Copied! <pre># Query the data stores available\ndatastores = get_datastores()\nbigdata_fileshares = datastores.search()\nbigdata_fileshares\n</pre> # Query the data stores available datastores = get_datastores() bigdata_fileshares = datastores.search() bigdata_fileshares Out[20]: <pre>[&lt;Datastore title:\"/enterpriseDatabases/AGSDataStore_ds_41yu9aqv\" type:\"egdb\"&gt;,\n &lt;Datastore title:\"/bigDataFileShares/hurricanes_dask_csv\" type:\"bigDataFileShare\"&gt;,\n &lt;Datastore title:\"/nosqlDatabases/AGSDataStore_nosqldb_tcs_yjayxzc4\" type:\"nosql\"&gt;,\n &lt;Datastore title:\"/nosqlDatabases/AGSDataStore_bigdata_bds_wuqp7mbg\" type:\"nosql\"&gt;,\n &lt;Datastore title:\"/rasterStores/LocalRasterDS\" type:\"rasterStore\"&gt;]</pre> <p>The dataset <code>hurricanes_dask_csv</code> data is registered as a big data file share with the Geoanalytics datastore, so we can reference it:</p> In\u00a0[21]: Copied! <pre>data_item = bigdata_fileshares[1]\n</pre> data_item = bigdata_fileshares[1] <p>If there is no big data file share for hurricane track data registered on the server, we can register one that points to the shared folder containing the CSV files.</p> <pre><code>data_item = datastores.add_bigdata(\"Hurricane_tracks\", r\"\\\\path_to_hurricane_data\")</code></pre> <p>Once a big data file share is registered, the GeoAnalytics server processes all the valid file types to discern the schema of the data, including information about the geometry in a dataset. If the dataset is time-enabled, as is required to use some GeoAnalytics Tools, the manifest reports the necessary metadata about how time information is stored as well.</p> <p>This process can take a few minutes depending on the size of your data. Once processed, querying the manifest property will return a schema. As you can see from below, the schema is contains the columns we merged using DASK previously.</p> In\u00a0[22]: Copied! <pre>datasets = data_item.manifest['datasets']\nlen(datasets)\n</pre> datasets = data_item.manifest['datasets'] len(datasets) Out[22]: <pre>1</pre> In\u00a0[23]: Copied! <pre>[dataset['name'] for dataset in datasets]\n</pre> [dataset['name'] for dataset in datasets] Out[23]: <pre>['hurricanes_merged']</pre> In\u00a0[24]: Copied! <pre>datasets[0]\n</pre> datasets[0] Out[24]: <pre>{'name': 'hurricanes_merged',\n 'format': {'quoteChar': '\"',\n  'fieldDelimiter': ',',\n  'hasHeaderRow': True,\n  'encoding': 'UTF-8',\n  'escapeChar': '\"',\n  'recordTerminator': '\\n',\n  'type': 'delimited',\n  'extension': 'csv'},\n 'schema': {'fields': [{'name': 'col_1', 'type': 'esriFieldTypeBigInteger'},\n   {'name': 'Serial_Num', 'type': 'esriFieldTypeString'},\n   {'name': 'Season', 'type': 'esriFieldTypeBigInteger'},\n   {'name': 'Num', 'type': 'esriFieldTypeBigInteger'},\n   {'name': 'Basin', 'type': 'esriFieldTypeString'},\n   {'name': 'Sub_basin', 'type': 'esriFieldTypeString'},\n   {'name': 'Name', 'type': 'esriFieldTypeString'},\n   {'name': 'ISO_time', 'type': 'esriFieldTypeString'},\n   {'name': 'Nature', 'type': 'esriFieldTypeString'},\n   {'name': 'Center', 'type': 'esriFieldTypeString'},\n   {'name': 'Track_type', 'type': 'esriFieldTypeString'},\n   {'name': 'Current Basin', 'type': 'esriFieldTypeString'},\n   {'name': 'latitude_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'longitude_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'wind_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'pressure_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'grade_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'eye_dia_merged', 'type': 'esriFieldTypeDouble'}]},\n 'geometry': {'geometryType': 'esriGeometryPoint',\n  'spatialReference': {'wkid': 4326},\n  'fields': [{'name': 'longitude_merged', 'formats': ['x']},\n   {'name': 'latitude_merged', 'formats': ['y']}]},\n 'time': {'timeType': 'instant',\n  'fields': [{'name': 'ISO_time', 'formats': ['yyyy-MM-dd HH:mm:ss']}],\n  'timeReference': {'timeZone': 'UTC'}}}</pre> In\u00a0[25]: Copied! <pre>search_result = gis.content.search(\"\", item_type = \"big data file share\")\nsearch_result\n</pre> search_result = gis.content.search(\"\", item_type = \"big data file share\") search_result Out[25]: <pre>[&lt;Item title:\"bigDataFileShares_hurricanes_dask_csv\" type:Big Data File Share owner:amani001&gt;]</pre> In\u00a0[26]: Copied! <pre>data_item = search_result[0]\ncleaned_csv = data_item.layers[0]\ncleaned_csv\n</pre> data_item = search_result[0] cleaned_csv = data_item.layers[0] cleaned_csv Out[26]: <pre>&lt;Layer url:\"https://datascience-arcpy.esri.com/server/rest/services/DataStoreCatalogs/bigDataFileShares_hurricanes_dask_csv/BigDataCatalogServer/hurricanes_merged\"&gt;</pre> In\u00a0[27]: Copied! <pre>agg_result = reconstruct_tracks(cleaned_csv, \n                                track_fields='Serial_Num',   # the Hurricane id number\n                                method='GEODESIC', output_name='hurricane_tracks_aggregated_ga')\n</pre> agg_result = reconstruct_tracks(cleaned_csv,                                  track_fields='Serial_Num',   # the Hurricane id number                                 method='GEODESIC', output_name='hurricane_tracks_aggregated_ga') <pre>Submitted.\nExecuting...\nExecuting (ReconstructTracks): ReconstructTracks \"Feature Set\" Serial_Num Geodesic # # # # # # \"{\"serviceProperties\": {\"name\": \"hurricane_tracks_aggregated_ga\", \"serviceUrl\": \"https://datascience-arcpy.esri.com/server/rest/services/Hosted/hurricane_tracks_aggregated_ga/FeatureServer\"}, \"itemProperties\": {\"itemId\": \"545adb07c4ba4da7b72ddbe47bd275d2\"}}\" \"{\"defaultAggregationStyles\": false}\"\nStart Time: Thu Nov 15 21:57:30 2018\nUsing URL based GPRecordSet param: https://datascience-arcpy.esri.com/server/rest/services/DataStoreCatalogs/bigDataFileShares_hurricanes_dask_csv/BigDataCatalogServer/hurricanes_merged\n{\"messageCode\":\"BD_101028\",\"message\":\"Starting new distributed job with 352 tasks.\",\"params\":{\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"0/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"0\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"177/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"177\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"241/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"241\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"308/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"308\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"352/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"352\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101081\",\"message\":\"Finished writing results:\"}\n{\"messageCode\":\"BD_101082\",\"message\":\"* Count of features = 12757\",\"params\":{\"resultCount\":\"12757\"}}\n{\"messageCode\":\"BD_101083\",\"message\":\"* Spatial extent = {\\\"xmin\\\":-180,\\\"ymin\\\":-68.5,\\\"xmax\\\":180,\\\"ymax\\\":81}\",\"params\":{\"extent\":\"{\\\"xmin\\\":-180,\\\"ymin\\\":-68.5,\\\"xmax\\\":180,\\\"ymax\\\":81}\"}}\n{\"messageCode\":\"BD_101084\",\"message\":\"* Temporal extent = Interval(MutableInstant(1842-10-25 06:00:00.000),MutableInstant(2017-06-13 06:00:00.000))\",\"params\":{\"extent\":\"Interval(MutableInstant(1842-10-25 06:00:00.000),MutableInstant(2017-06-13 06:00:00.000))\"}}\n</pre> <pre>{\"messageCode\":\"BD_101054\",\"message\":\"Some records have either missing or invalid geometries.\"}\n</pre> <pre>{\"messageCode\":\"BD_101054\",\"message\":\"Some records have either missing or invalid geometries.\"}\nSucceeded at Thu Nov 15 21:57:54 2018 (Elapsed Time: 24.21 seconds)\n</pre> In\u00a0[28]: Copied! <pre>agg_tracks_layer = agg_result.layers[0]\nagg_fields = [f.name for f in agg_tracks_layer.properties.fields]\npprint(agg_fields, compact=True)\n</pre> agg_tracks_layer = agg_result.layers[0] agg_fields = [f.name for f in agg_tracks_layer.properties.fields] pprint(agg_fields, compact=True) <pre>['Serial_Num', 'COUNT', 'COUNT_col_1', 'SUM_col_1', 'MIN_col_1', 'MAX_col_1',\n 'MEAN_col_1', 'RANGE_col_1', 'SD_col_1', 'VAR_col_1', 'COUNT_Season',\n 'SUM_Season', 'MIN_Season', 'MAX_Season', 'MEAN_Season', 'RANGE_Season',\n 'SD_Season', 'VAR_Season', 'COUNT_Num', 'SUM_Num', 'MIN_Num', 'MAX_Num',\n 'MEAN_Num', 'RANGE_Num', 'SD_Num', 'VAR_Num', 'COUNT_Basin', 'ANY_Basin',\n 'COUNT_Sub_basin', 'ANY_Sub_basin', 'COUNT_Name', 'ANY_Name', 'COUNT_ISO_time',\n 'ANY_ISO_time', 'COUNT_Nature', 'ANY_Nature', 'COUNT_Center', 'ANY_Center',\n 'COUNT_Track_type', 'ANY_Track_type', 'COUNT_Current_Basin',\n 'ANY_Current_Basin', 'COUNT_latitude_merged', 'SUM_latitude_merged',\n 'MIN_latitude_merged', 'MAX_latitude_merged', 'MEAN_latitude_merged',\n 'RANGE_latitude_merged', 'SD_latitude_merged', 'VAR_latitude_merged',\n 'COUNT_longitude_merged', 'SUM_longitude_merged', 'MIN_longitude_merged',\n 'MAX_longitude_merged', 'MEAN_longitude_merged', 'RANGE_longitude_merged',\n 'SD_longitude_merged', 'VAR_longitude_merged', 'COUNT_wind_merged',\n 'SUM_wind_merged', 'MIN_wind_merged', 'MAX_wind_merged', 'MEAN_wind_merged',\n 'RANGE_wind_merged', 'SD_wind_merged', 'VAR_wind_merged',\n 'COUNT_pressure_merged', 'SUM_pressure_merged', 'MIN_pressure_merged',\n 'MAX_pressure_merged', 'MEAN_pressure_merged', 'RANGE_pressure_merged',\n 'SD_pressure_merged', 'VAR_pressure_merged', 'COUNT_grade_merged',\n 'SUM_grade_merged', 'MIN_grade_merged', 'MAX_grade_merged',\n 'MEAN_grade_merged', 'RANGE_grade_merged', 'SD_grade_merged',\n 'VAR_grade_merged', 'COUNT_eye_dia_merged', 'SUM_eye_dia_merged',\n 'MIN_eye_dia_merged', 'MAX_eye_dia_merged', 'MEAN_eye_dia_merged',\n 'RANGE_eye_dia_merged', 'SD_eye_dia_merged', 'VAR_eye_dia_merged',\n 'TRACK_DURATION', 'globalid', 'OBJECTID', 'END_DATETIME', 'START_DATETIME']\n</pre> <p>To get the number of hurricanes the reconstruct tracks tool identified, we run a query on the aggregation layer and get just the number of records.</p> In\u00a0[7]: Copied! <pre>agg_tracks_layer.query(return_count_only=True)\n</pre> agg_tracks_layer.query(return_count_only=True) Out[7]: <pre>12362</pre>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#analyzing-historic-hurricane-tracks-part-13","title":"Analyzing historic hurricane tracks - Part 1/3\u00b6","text":"<p>Hurricanes are large swirling storms that produce winds of speeds <code>74</code> miles per hour (<code>119</code> kmph) or higher. When hurricanes make a landfall, they produce heavy rainfall, cause storm surges and intense flooding. Often hurricanes strike places that are dense in population, causing devastating amounts of death and destruction throughout the world.</p> <p>Since the recent past, agencies such as the National Hurricane Center have been collecting quantitative data about hurricanes. In this study we use meteorological data of hurricanes recorded in the past <code>169</code> years to analyze their location, intensity and investigate if there are any statistically significant trends. We also analyze the places most affected by hurricanes and what their demographic make up is. We conclude by citing releavant articles that draw similar conclusions.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#part-1-prepare-hurricane-data","title":"Part 1 - prepare hurricane data\u00b6","text":"<p>This notebook covers part 1 of this study. In this notebook we download data from NCEI portal, do extenstive pre-processing in the form of clearing headers, merging redundant columns and aggregate the observations into hurricane tracks.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#download-hurricane-data-from-ncei-ftp-portal","title":"Download hurricane data from NCEI FTP portal\u00b6","text":"<p>The National Centers for Environmental Information, formerly National Climatic Data Center shares the historic hurricane track datasets at ftp://eclipse.ncdc.noaa.gov/pub/ibtracs/v03r09/all/csv/. We use the <code>ftplib</code> Python library to login in and download these datasets.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#download-each-file-into-hurricanes_raw-directory","title":"Download each file into <code>hurricanes_raw</code> directory\u00b6","text":""},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#process-csv-files-by-removing-header-rows","title":"Process CSV files by removing header rows\u00b6","text":"<p>The CSV files have multiple header rows. Let us start by processing one of the files as an example</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#automate-across-all-files","title":"Automate across all files\u00b6","text":"<p>Now we need to repeat the above cleaning steps across all CSV files. In the steps below, we will read all CSV files, drop the headers and write to disk. This step is necessary as it will ease subsequent processing using the DASK library.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#cleaning-hurricane-observations-with-dask","title":"Cleaning hurricane observations with Dask\u00b6","text":"<p>The data collected from NOAA NCDC source is just too large to clean with Pandas or Excel. With <code>350,000 x 200</code> in dense matrix, this data is larger than memory for a normal computer. Hence traditional packages such as Pandas cannot be used as they expect data to fit fully in memory.</p> <p>Thus, in this part of the study, we use Dask, a distributed data analysis library. Functionally, Dask provides a <code>DataFrame</code> object that behaves similar to a traditional pandas <code>DataFrame</code> object. You can perform slicing, dicing, exploration on them. However transformative operations on the <code>DataFrame</code> get queued and are operated only when necessary. When executed, Dask will read data in chunks, distribute it to workers (be it cores on a single machine or multiple machines in a cluster set up) and collect the data back for you. Thus, DASK allows you to work with any larger than memory dataset as it performs operations on chunks of it, in a distributed manner.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#read-input-csv-data","title":"Read input CSV data\u00b6","text":"<p>As mentioned earlier DASK allows you to work with larger than memory datasets. These datasets can reside as one large file or as multiple files in a folder. For latter, DASK allows you to just specify the folder containing the datasets as input. In turn, it provides you a single <code>DataFrame</code> object that represents all your datasets combined together. The operations you perform on this <code>DataFrame</code> get queued and executed only when necessary.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-all-location-columns","title":"Merge all location columns\u00b6","text":"<p>Below we prototype merging location columns. If this succeeds, we will proceed to merge all remaining columns</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-similar-columns","title":"Merge similar columns\u00b6","text":"<p>To keep track of which columns have been accounted for, we will duplicate the <code>all_columns</code> list and remove ones that we have identified.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-wind-columns","title":"Merge wind columns\u00b6","text":"<p>Wind, pressure, grade are some of the meteorological observations this dataset contains. To start off, let us identify the wind columns:</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-pressure-columns","title":"Merge pressure columns\u00b6","text":"<p>We proceed to identify all <code>pressure</code> columns. But before that, we update the <code>columns_tracker</code> list by removing those we identified for wind:</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-grade-columns","title":"Merge grade columns\u00b6","text":""},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-eye-diameter-columns","title":"Merge eye diameter columns\u00b6","text":""},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#identify-remaining-redundant-columns","title":"Identify remaining redundant columns\u00b6","text":""},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#drop-all-redundant-columns","title":"Drop all redundant columns\u00b6","text":"<p>So far, we have merged similar columns together and collected the lists of redundant columns to drop. Below we compile them into a single list.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#perform-delayed-computation","title":"Perform delayed computation\u00b6","text":"<p>In Dask, all computations are delayed and queued. The <code>apply()</code> functions called earlier are not executed yet, however respective columns have been created as you can see from the DataFrame display above. In the cells below, we will call <code>save()</code> to make Dask compute on this larger than memory dataset.</p> <p>Calling <code>visualize()</code> on the delayed compute operation or the <code>DataFrame</code> object will plot the dask task queue as shown below. The graphic below provides a glimpse on how Dask distributes its tasks and how it reads this 'larger than memory dataset' in chunks and operates on them.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#preview-results","title":"Preview results\u00b6","text":"<p>Once Dask realizes a delayed computation, it returns the result as an in-memory Pandas DataFrame object. Thus, the <code>merged_df</code> variable represents a Pandas <code>DataFrame</code> object with <code>348,703</code> records and <code>17</code> columns.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#creating-hurricane-tracks-using-geoanalytics","title":"Creating hurricane tracks using Geoanalytics\u00b6","text":"<p>The data collected so far are a set of Point observations representing all hurricanes recorded in the last <code>169</code> years. To make sense of these points, we need to connect them together to create a track for each hurricane. This part of the notebook uses ArcGIS GeoAnalytics server to reconstruct such hurricane tracks. The GeoAnalytics server is capable of processing on massive datasets in a scalable and distributed fashion.</p> <p>The DASK process merged redundant columns together and ouput a folder full of CSV files. The GeoAnalytics server is also capabale of accepting a folder of datasets as 1 dataset and working on them. Thus in this part, we register that folder as a datastore and on the GeoAnalytics server for processing.</p> <p>Reconstruct tracks: Reconstruct tracks is a type of data aggregation tool available in the <code>arcgis.geoanalytics</code> module. This tool works with a layer of point features or polygon features that are time enabled. It first determines which points belong to a track using an identification number or identification string. Using the time at each location, the tracks are ordered sequentially and transformed into a line representing the path of movement. The map below shows a subset of the point datasets.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#create-a-data-store","title":"Create a data store\u00b6","text":"<p>For the GeoAnalytics server to process your big data, it needs the data to be registered as a data store. In our case, the data is in multiple CSV files and we will register the folder containing the files as a data store of type <code>bigDataFileShare</code>.</p> <p>Let us connect to an ArcGIS Enterprise</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#perform-data-aggregation-using-reconstruct-tracks-tool","title":"Perform data aggregation using reconstruct tracks tool\u00b6","text":"<p>When you add a big data file share, a corresponding item gets created in your GIS. You can search for it like a regular item and query its layers.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#execute-reconstruct-tracks-tool","title":"Execute reconstruct tracks tool\u00b6","text":"<p>The <code>reconstruct_tracks()</code> function is available in the <code>arcgis.geoanalytics.summarize_data</code> module. In this example, we are using this tool to aggregate the numerous points into line segments showing the tracks followed by the hurricanes. The tool creates a feature layer item as an output which can be accessed once the processing is complete.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#analyze-the-result-of-aggregation","title":"Analyze the result of aggregation\u00b6","text":"<p>The reconstruct tracks produces summary statistics such as <code>MIN</code>, <code>MAX</code>, <code>MEAN</code>, <code>MEDIAN</code>, <code>RANGE</code>, <code>SD</code>, <code>VAR</code>, <code>SUM</code>, for numeric columns and <code>COUNT</code> for ordinal columns during the aggregation process. Let us list the fields in this dataset to view them.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook we observed how to download meteorological data over FTP from NEIC website. The data came in <code>169</code> CSV files for the past <code>169</code> years. We sanitized it initially using Pandas to remove bad header rows. We then used DASK library to read all <code>169</code> files as a single file and merged data from redundant columns. This pre-processing resulted in multiple output CSV files covering a total of 348k records and 17 columns.</p> <p>This data was fed to the ArcGIS GeoAnalytics server for aggregation. The reconstruct tracks tool on GeoAnalytics server reduced this point dataset into hurricane tracks (lines) and during this aggregation, it calculated summary statistics for the numerical columns. The tool identified <code>12,362</code> individual hurricanes from the past <code>169</code> years.</p> <p>In Part 2 of this study, we will visualize and explore this dataset to understand the prevelance, duration of hurricanes and the communities affected by hurricanes worldwide.</p> <p>In Part 3, we will analyze this aggregated result comprehensively, answer important questions such as, does the intensity of hurricanes increase over time and draw conclusions.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/","title":"Analyzing hurricane tracks - Part 2/3","text":"<p>Import the libraries necessary for this notebook.</p> In\u00a0[1]: Copied! <pre># import ArcGIS Libraries\nfrom arcgis.gis import GIS\nfrom arcgis.geometry import filters\nfrom arcgis.geocoding import geocode\nfrom arcgis.features.manage_data import overlay_layers\nfrom arcgis.geoenrichment import enrich\n\n# import Pandas for data exploration\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# import plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# import display tools\nfrom pprint import pprint\nfrom IPython.display import display\n\n# import system libs\nfrom sys import getsizeof\n</pre> # import ArcGIS Libraries from arcgis.gis import GIS from arcgis.geometry import filters from arcgis.geocoding import geocode from arcgis.features.manage_data import overlay_layers from arcgis.geoenrichment import enrich  # import Pandas for data exploration import pandas as pd import numpy as np from scipy import stats  # import plotting libraries import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline  # import display tools from pprint import pprint from IPython.display import display  # import system libs from sys import getsizeof In\u00a0[3]: Copied! <pre>gis = GIS('home')\n</pre> gis = GIS('home') In\u00a0[2]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n</pre> import warnings warnings.filterwarnings('ignore') In\u00a0[4]: Copied! <pre>hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga')[0]\nhurricane_fl = hurricane_tracks_item.layers[0]\n</pre> hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga')[0] hurricane_fl = hurricane_tracks_item.layers[0] <p>The GeoAnalytics step calculated summary statistics of all numeric fields. However only a few of the columns are of interest to us.</p> In\u00a0[5]: Copied! <pre>pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80)\n</pre> pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80) <pre>['objectid', 'serial_num', 'count', 'count_col_1', 'sum_col_1', 'min_col_1',\n 'max_col_1', 'mean_col_1', 'range_col_1', 'sd_col_1', 'var_col_1',\n 'count_season', 'sum_season', 'min_season', 'max_season', 'mean_season',\n 'range_season', 'sd_season', 'var_season', 'count_num', 'sum_num', 'min_num',\n 'max_num', 'mean_num', 'range_num', 'sd_num', 'var_num', 'count_basin',\n 'any_basin', 'count_sub_basin', 'any_sub_basin', 'count_name', 'any_name',\n 'count_iso_time', 'any_iso_time', 'count_nature', 'any_nature', 'count_center',\n 'any_center', 'count_track_type', 'any_track_type', 'count_current_basin',\n 'any_current_basin', 'count_latitude_merged', 'sum_latitude_merged',\n 'min_latitude_merged', 'max_latitude_merged', 'mean_latitude_merged',\n 'range_latitude_merged', 'sd_latitude_merged', 'var_latitude_merged',\n 'count_longitude_merged', 'sum_longitude_merged', 'min_longitude_merged',\n 'max_longitude_merged', 'mean_longitude_merged', 'range_longitude_merged',\n 'sd_longitude_merged', 'var_longitude_merged', 'count_wind_merged',\n 'sum_wind_merged', 'min_wind_merged', 'max_wind_merged', 'mean_wind_merged',\n 'range_wind_merged', 'sd_wind_merged', 'var_wind_merged',\n 'count_pressure_merged', 'sum_pressure_merged', 'min_pressure_merged',\n 'max_pressure_merged', 'mean_pressure_merged', 'range_pressure_merged',\n 'sd_pressure_merged', 'var_pressure_merged', 'count_grade_merged',\n 'sum_grade_merged', 'min_grade_merged', 'max_grade_merged',\n 'mean_grade_merged', 'range_grade_merged', 'sd_grade_merged',\n 'var_grade_merged', 'count_eye_dia_merged', 'sum_eye_dia_merged',\n 'min_eye_dia_merged', 'max_eye_dia_merged', 'mean_eye_dia_merged',\n 'range_eye_dia_merged', 'sd_eye_dia_merged', 'var_eye_dia_merged',\n 'track_duration', 'end_datetime', 'start_datetime']\n</pre> <p>Below we select the following fields for the rest of this analysis.</p> In\u00a0[9]: Copied! <pre>fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',\n                   'any_name', 'mean_latitude_merged', 'mean_longitude_merged',\n                   'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',\n                   'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',\n                   'end_datetime', 'start_datetime']\n</pre> fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',                    'any_name', 'mean_latitude_merged', 'mean_longitude_merged',                    'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',                    'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',                    'end_datetime', 'start_datetime'] In\u00a0[10]: Copied! <pre>%%time\nall_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True)\n</pre> %%time all_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True) <pre>CPU times: user 1.12 s, sys: 318 ms, total: 1.43 s\nWall time: 4.5 s\n</pre> In\u00a0[11]: Copied! <pre>all_hurricanes_df.shape\n</pre> all_hurricanes_df.shape Out[11]: <pre>(12362, 17)</pre> <p>There are <code>12,362</code> hurricanes identified by GeoAnalytics aggregate tracks tool. To get an idea about this aggregated dataset, call the <code>head()</code> method.</p> In\u00a0[12]: Copied! <pre>all_hurricanes_df.head()\n</pre> all_hurricanes_df.head() Out[12]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged min_pressure_merged min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration 0 {\"paths\": [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 NaN 1854.0 1 NaN NaN 1854-02-08 06:00:00 1.296000e+08 1 {\"paths\": [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 NaN 1859.0 2 NaN 10.0 1859-08-24 12:00:00 1.728000e+08 2 {\"paths\": [[[-23.19999999999999, 12.1000000000... NA UNNAMED NA 50.0 1853-09-12 18:00:00 NaN 130.0 26.982000 -51.776000 924.0 1853.0 3 53.0 90.0 1853-08-30 00:00:00 1.058400e+09 3 {\"paths\": [[[59.80000000000001, -15.5], [59.49... SI XXXX856017 MM 13.0 1856-04-05 18:00:00 NaN NaN -20.185385 59.573077 NaN 1856.0 4 NaN NaN 1856-04-02 18:00:00 2.592000e+08 4 {\"paths\": [[[99.60000000000002, -11.5], [98.30... SI NOT NAMED WA 13.0 1861-03-15 18:00:00 NaN NaN -12.940769 94.183846 NaN 1861.0 5 NaN NaN 1861-03-12 18:00:00 2.592000e+08 <p>To better analyze this data set, the date columns need to be changed to a format that Pandas understands better. This is accomplished by calling <code>to_datetime()</code> method and passing the appropriate time columns.</p> In\u00a0[14]: Copied! <pre>all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime'])\nall_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime'])\nall_hurricanes_df.index = all_hurricanes_df['start_datetime']\nall_hurricanes_df.head()\n</pre> all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime']) all_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime']) all_hurricanes_df.index = all_hurricanes_df['start_datetime'] all_hurricanes_df.head() Out[14]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged min_pressure_merged min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration start_datetime 1854-02-08 06:00:00 {\"paths\": [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 NaN 1854.0 1 NaN NaN 1854-02-08 06:00:00 1.296000e+08 1859-08-24 12:00:00 {\"paths\": [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 NaN 1859.0 2 NaN 10.0 1859-08-24 12:00:00 1.728000e+08 1853-08-30 00:00:00 {\"paths\": [[[-23.19999999999999, 12.1000000000... NA UNNAMED NA 50.0 1853-09-12 18:00:00 NaN 130.0 26.982000 -51.776000 924.0 1853.0 3 53.0 90.0 1853-08-30 00:00:00 1.058400e+09 1856-04-02 18:00:00 {\"paths\": [[[59.80000000000001, -15.5], [59.49... SI XXXX856017 MM 13.0 1856-04-05 18:00:00 NaN NaN -20.185385 59.573077 NaN 1856.0 4 NaN NaN 1856-04-02 18:00:00 2.592000e+08 1861-03-12 18:00:00 {\"paths\": [[[99.60000000000002, -11.5], [98.30... SI NOT NAMED WA 13.0 1861-03-15 18:00:00 NaN NaN -12.940769 94.183846 NaN 1861.0 5 NaN NaN 1861-03-12 18:00:00 2.592000e+08 <p>The track duration and length columns need to be projected to units (days, hours, miles) that are meaningful for analysis.</p> In\u00a0[15]: Copied! <pre>all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000\nall_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24)\n</pre> all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000 all_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24) In\u00a0[21]: Copied! <pre>map1 = gis.map('USA')\nmap1\n</pre> map1 = gis.map('USA') map1 Out[21]: In\u00a0[17]: Copied! <pre>all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map1, \n                                                             renderer_type='u',\n                                                             col='any_basin',\n                                                            cmap='prism')\n</pre> all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map1,                                                               renderer_type='u',                                                              col='any_basin',                                                             cmap='prism') Out[17]: <pre>True</pre> <p>The map above draws a set of <code>500</code> hurricanes chosen at random. You can visualize the Spatially Enabled DataFrame object with different types of renderers. In the example above a unique value renderer is applied on the basin column. You can switch the map to 3D mode and view the same on a globe.</p> In\u00a0[22]: Copied! <pre>map2 = gis.map()\nmap2.mode= '3D'\nmap2\n</pre> map2 = gis.map() map2.mode= '3D' map2 Out[22]: In\u00a0[27]: Copied! <pre>all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map2, \n                                                             renderer_type='u',\n                                                             col='any_basin',\n                                                            cmap='prism')\n</pre> all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map2,                                                               renderer_type='u',                                                              col='any_basin',                                                             cmap='prism') In\u00a0[18]: Copied! <pre>ax = sns.distplot(all_hurricanes_df['min_season'], kde=False, bins=50)\nax.set_title('Number of hurricanes recorded over time')\n</pre> ax = sns.distplot(all_hurricanes_df['min_season'], kde=False, bins=50) ax.set_title('Number of hurricanes recorded over time') Out[18]: <pre>Text(0.5,1,'Number of hurricanes recorded over time')</pre> <p>The number of hurricanes recorded increases steadily until <code>1970</code>. This could be due to advances in geospatial technologies allowing scientists to better monitor hurricanes. However, after <code>1970</code> we notice a reduction in the number of hurricanes. This is in line with what scientists observe and predict.</p> In\u00a0[19]: Copied! <pre>fig1, ax1 = plt.subplots(1,2, figsize=(12,5))\n\nbasin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1[0])\nbasin_ax.set_title('Number of hurricanes per basin')\nbasin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',\n                          'Eastern Pacicifc', 'North Indian','Southern Pacific',\n                          'South Atlantic'])\n\nsub_basin_ax = all_hurricanes_df['any_sub_basin'].value_counts().plot(kind='bar', ax=ax1[1])\nsub_basin_ax.set_title('Number of hurricanes per sub basin')\nsub_basin_ax.set_xticklabels(['MM','North Atlantic','Bay of Bengal','Western Australia',\n                             'Eastern Australia', 'Carribean Sea', 'Gulf of Mexico',\n                             'Arabian Sea', 'Central Pacific'])\nsub_basin_ax.tick_params()\n</pre> fig1, ax1 = plt.subplots(1,2, figsize=(12,5))  basin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1[0]) basin_ax.set_title('Number of hurricanes per basin') basin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',                           'Eastern Pacicifc', 'North Indian','Southern Pacific',                           'South Atlantic'])  sub_basin_ax = all_hurricanes_df['any_sub_basin'].value_counts().plot(kind='bar', ax=ax1[1]) sub_basin_ax.set_title('Number of hurricanes per sub basin') sub_basin_ax.set_xticklabels(['MM','North Atlantic','Bay of Bengal','Western Australia',                              'Eastern Australia', 'Carribean Sea', 'Gulf of Mexico',                              'Arabian Sea', 'Central Pacific']) sub_basin_ax.tick_params() <p>Thus, most hurricanes occur in Wester Pacific basin. This is the region that is east of China, Phillipines and rest of South East Asia. This is followed by South Indian which spans from west of Australia to east of Southern Africa. North Atlantic basin which is the source of hurricanes in the continental United States ranks as the third busiest hurricane basin.</p> <p>Pandas provides a handy API called <code>value_counts()</code> to count unique occurrences. We use that below to count the number of times each hurricane name has been used. We then print the top <code>25</code> most frequently used names.</p> In\u00a0[20]: Copied! <pre># Get the number of occurrences of top 25 hurricane names\nall_hurricanes_df['any_name'].value_counts()[:25]\n</pre> # Get the number of occurrences of top 25 hurricane names all_hurricanes_df['any_name'].value_counts()[:25] Out[20]: <pre>NOT NAMED          4099\nUNNAMED            1408\n06B                  31\n05B                  30\n04B                  30\n09B                  30\n07B                  29\n08B                  29\n10B                  29\n03B                  28\n01B                  27\n12B                  26\n11B                  23\n13B                  23\n02B                  22\n14B                  17\nSUBTROP:UNNAMED      16\nIRMA                 15\nFLORENCE             15\n02A                  14\nJUNE                 13\nALICE                13\nOLGA                 13\nSUSAN                13\nFREDA                13\nName: any_name, dtype: int64</pre> <p>Names like <code>FLORENCE</code>, <code>IRMA</code>, <code>OLGA</code>.. appear to be more popular. Interestingly all are of female gender. We can take this further to explore at what time periods have the name <code>FLORENCE</code> been used?</p> In\u00a0[21]: Copied! <pre>all_hurricanes_df[all_hurricanes_df['any_name']=='FLORENCE'].index\n</pre> all_hurricanes_df[all_hurricanes_df['any_name']=='FLORENCE'].index Out[21]: <pre>DatetimeIndex(['1953-09-23 12:00:00', '1954-09-10 12:00:00',\n               '1963-07-14 12:00:00', '1967-01-03 06:00:00',\n               '1964-09-05 18:00:00', '1965-09-08 00:00:00',\n               '1973-07-25 00:00:00', '1960-09-17 06:00:00',\n               '1994-11-02 00:00:00', '1969-09-02 00:00:00',\n               '2012-08-03 06:00:00', '1977-09-20 12:00:00',\n               '2000-09-10 18:00:00', '1988-09-07 06:00:00',\n               '2006-09-03 18:00:00'],\n              dtype='datetime64[ns]', name='start_datetime', freq=None)</pre> <p>The name <code>FLORENCE</code> had been used consistently in since the 1950s, reaching a peak in popularity during the 60s.</p> <p>Hurricanes happen when water temperatures (<code>Sea Surface Temperature</code> SST) are warm. Solar incidence is one of the key factors affecting SST and this typically happens during summer months. However, summer happens during different months in northern and southern hemispheres. To visualize this seasonality, we need to group our data by month as well as basin. Thus the snippet below creates a multilevel index grouper in Pandas</p> In\u00a0[22]: Copied! <pre># Create a grouper object\ngrouper = all_hurricanes_df.start_datetime.dt.month_name()\n\n# use grouper along with basin name to create a multilevel groupby object\nhurr_by_basin = all_hurricanes_df.groupby([grouper,'any_basin'], as_index=True)\nhurr_by_basin_month = hurr_by_basin.count()[['count', 'min_pressure_merged']]\nhurr_by_basin_month.head()\n</pre> # Create a grouper object grouper = all_hurricanes_df.start_datetime.dt.month_name()  # use grouper along with basin name to create a multilevel groupby object hurr_by_basin = all_hurricanes_df.groupby([grouper,'any_basin'], as_index=True) hurr_by_basin_month = hurr_by_basin.count()[['count', 'min_pressure_merged']] hurr_by_basin_month.head() Out[22]: count min_pressure_merged start_datetime any_basin April NA 5 2 NI 41 5 SI 242 85 SP 97 74 WP 83 56 <p>Now we turn the index into columns for further processing.</p> In\u00a0[23]: Copied! <pre># turn index into columns\nhurr_by_basin_month.reset_index(inplace=True)\nhurr_by_basin_month.drop('min_pressure_merged', axis=1, inplace=True)\nhurr_by_basin_month.columns = ['month', 'basin', 'count']\nhurr_by_basin_month.head()\n</pre> # turn index into columns hurr_by_basin_month.reset_index(inplace=True) hurr_by_basin_month.drop('min_pressure_merged', axis=1, inplace=True) hurr_by_basin_month.columns = ['month', 'basin', 'count'] hurr_by_basin_month.head() Out[23]: month basin count 0 April NA 5 1 April NI 41 2 April SI 242 3 April SP 97 4 April WP 83 <p>We add the month column back, but this time we will help Pandas understand how to sort months other than by alphabetical order.</p> In\u00a0[64]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(15,7))\nmonth_order = ['January','February', 'March','April','May','June',\n              'July','August','September','October','November','December']\n\nsns.barplot(x='month', y='count', hue='basin', data=hurr_by_basin_month, ax=ax,\n            order=month_order)\n</pre> fig, ax = plt.subplots(1,1, figsize=(15,7)) month_order = ['January','February', 'March','April','May','June',               'July','August','September','October','November','December']  sns.barplot(x='month', y='count', hue='basin', data=hurr_by_basin_month, ax=ax,             order=month_order) Out[64]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2705d908&gt;</pre> <p>The bars in red represent the number of hurricanes in Sounth Indian ocean which spans from west of Australia to east of southern Africa while the brown bars are for Western Pacific which spans east of China and orange bars for North Atlantic. The sinusoidal nature of these bars show the charateristic offset in summer between northern and southern hemispheres. The green bars represent North Indian hurricanes which is dominated by monsoon effect and is seen to be prevalant for most part of the year.</p> In\u00a0[70]: Copied! <pre>agol = GIS(set_active=False)\n</pre> agol = GIS(set_active=False) In\u00a0[107]: Copied! <pre>world_boundaries_item = agol.content.get('57c1ade4fa7c4e2384e6a23f2b3bd254')\nworld_boundaries_item\n</pre> world_boundaries_item = agol.content.get('57c1ade4fa7c4e2384e6a23f2b3bd254') world_boundaries_item Out[107]: World Continents This layer represents the boundaries for the continents of the world. The layer is suitable for display to a largest scale of 1:15,000,000.Feature Layer Collection by esri                         Last Modified: November 11, 2018                         0 comments, 39,101 views                      <p>We will import the <code>overlay_layers</code> tool from <code>manage_data</code> toolset to perform the overlay analysis.</p> In\u00a0[108]: Copied! <pre>boundary_fl = world_boundaries_item.layers[0]\nfrom arcgis.features.manage_data import overlay_layers\n</pre> boundary_fl = world_boundaries_item.layers[0] from arcgis.features.manage_data import overlay_layers In\u00a0[96]: Copied! <pre>%%time\ninland_tracks = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl, \n                               overlay_type='INTERSECT', output_type='INPUT', \n                               output_name='hurricane_landfall_tracks', gis=gis)\n</pre> %%time inland_tracks = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl,                                 overlay_type='INTERSECT', output_type='INPUT',                                 output_name='hurricane_landfall_tracks', gis=gis) <pre>CPU times: user 1.37 s, sys: 231 ms, total: 1.6 s\nWall time: 22min 13s\n</pre> <p>As part of the intersect operation, the output type is specified as <code>Input</code>. Since the input layer is hurricane tracks (a line layer), the result will continue to be a line layer. We can draw this layer on a map to view those hurricanes that have made a landfall and traveled inland.</p> In\u00a0[24]: Copied! <pre>landfall_tracks_map = gis.map(\"USA\")\nlandfall_tracks_map\n</pre> landfall_tracks_map = gis.map(\"USA\") landfall_tracks_map Out[24]: In\u00a0[98]: Copied! <pre>landfall_tracks_map.add_layer(inland_tracks)\n</pre> landfall_tracks_map.add_layer(inland_tracks) <p>We query the landfall tracks layer into a DataFrame. We will then plot a bar chart showing what percent of hurricanes in each basin make a landfall.</p> In\u00a0[27]: Copied! <pre>fields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged', \n                   'min_pressure_merged', 'track_duration','end_datetime', \n                   'start_datetime', 'analysislength']\n\nlandfall_tracks_fl = inland_tracks.layers[0]\n</pre> fields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged',                     'min_pressure_merged', 'track_duration','end_datetime',                     'start_datetime', 'analysislength']  landfall_tracks_fl = inland_tracks.layers[0] In\u00a0[28]: Copied! <pre>landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df\nlandfall_tracks_df.head(3)\n</pre> landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df landfall_tracks_df.head(3) Out[28]: analysislength any_basin any_name end_datetime max_wind_merged min_pressure_merged min_season objectid start_datetime track_duration SHAPE 0 4.376642 NA NOT NAMED -3663424800000 95.0 965.0 1853.0 1 -3664699200000 1.317600e+09 {'paths': [[[-74.47272727299998, 24], [-74.463... 1 117.097286 NA UNNAMED -3645172800000 70.0 NaN 1854.0 2 -3645475200000 2.160000e+08 {'paths': [[[-99.13749999999999, 26.5699999999... 2 256.909588 NA UNNAMED -3645172800000 70.0 NaN 1854.0 3 -3645475200000 2.160000e+08 {'paths': [[[-102.21739130399999, 27.686956522... In\u00a0[30]: Copied! <pre>fig1, ax1 = plt.subplots(1,1, figsize=(12,5))\n\nbasin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1)\nbasin_ax = landfall_tracks_df['any_basin'].value_counts().plot(kind='bar', \n                                                               ax=ax1, \n                                                               cmap='viridis', \n                                                               alpha=0.5)\nbasin_ax.set_title('Number of hurricanes per basin that make landfall')\nbasin_ax.tick_params(axis='x', labelrotation=65)\nbasin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',\n                          'Eastern Pacicifc', 'North Indian','Southern Pacific',\n                          'South Atlantic'])\nbasin_ax.tick_params()\n</pre> fig1, ax1 = plt.subplots(1,1, figsize=(12,5))  basin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1) basin_ax = landfall_tracks_df['any_basin'].value_counts().plot(kind='bar',                                                                 ax=ax1,                                                                 cmap='viridis',                                                                 alpha=0.5) basin_ax.set_title('Number of hurricanes per basin that make landfall') basin_ax.tick_params(axis='x', labelrotation=65) basin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',                           'Eastern Pacicifc', 'North Indian','Southern Pacific',                           'South Atlantic']) basin_ax.tick_params() <p>The bar chart above plots the number of hurricanes (per basin) that made landfall over another bar chart of the total number of hurricanes per basin. From the chart, most hurricanes in the Western Pacific, South Indian, North Atlantic make landfall. Hurricanes in Southern Pacific on the otherhand rarely make landfall. This helps us guage the severity of hurricanes in different geographic basins.</p> In\u00a0[43]: Copied! <pre>landfall_tracks_df['analysislength'].plot(kind='hist', bins=100,\n                                          title='Histogram of distance traveled inland',\n                                         figsize=(12,7), xlim=[-100,2500])\nplt.xlabel('Distance in miles')\n</pre> landfall_tracks_df['analysislength'].plot(kind='hist', bins=100,                                           title='Histogram of distance traveled inland',                                          figsize=(12,7), xlim=[-100,2500]) plt.xlabel('Distance in miles') Out[43]: <pre>Text(0.5,0,'Distance in miles')</pre> <p>Thus, majority of hurricanes travel less than <code>500</code> miles after making a landfall. We can query which are the top <code>50</code> hurricanes that have traveled longest inland.</p> In\u00a0[54]: Copied! <pre># filter the top 50 longest hurricanes (distance traveled inland)\ntop_50_longest = landfall_tracks_df.sort_values(by=['analysislength'], axis=0, ascending=False).head(50)\ntop_50_longest['any_basin'].value_counts().plot(kind='bar')\n</pre> # filter the top 50 longest hurricanes (distance traveled inland) top_50_longest = landfall_tracks_df.sort_values(by=['analysislength'], axis=0, ascending=False).head(50) top_50_longest['any_basin'].value_counts().plot(kind='bar') Out[54]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2c808ef0&gt;</pre> <p>Southern Pacific basin, followed by South Indian basin contains the hurricanes that have traveled longest inland.</p> In\u00a0[59]: Copied! <pre>inland_map = gis.map()\ninland_map\n</pre> inland_map = gis.map() inland_map Out[59]: In\u00a0[58]: Copied! <pre>top_50_longest.spatial.plot(inland_map, renderer_type='u', col='any_basin',cmap='prism')\n</pre> top_50_longest.spatial.plot(inland_map, renderer_type='u', col='any_basin',cmap='prism') Out[58]: <pre>True</pre> <p>Plotting this on the map, we notice hurricanes have traveled longest inland over the east coast of North America, China and Australia. Interestingly, Australia bears landfall of hurricanes from both South Indian and Western Pacific basins.</p> In\u00a0[109]: Copied! <pre>%%time\nlandfall_item = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl, \n                               overlay_type='INTERSECT', output_type='POINT', \n                               output_name='hurricane_landfall_locations', gis=gis)\n</pre> %%time landfall_item = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl,                                 overlay_type='INTERSECT', output_type='POINT',                                 output_name='hurricane_landfall_locations', gis=gis) <pre>CPU times: user 1.43 s, sys: 258 ms, total: 1.69 s\nWall time: 24min 41s\n</pre> In\u00a0[110]: Copied! <pre>landfall_points_fl = landfall_item.layers[0]\n</pre> landfall_points_fl = landfall_item.layers[0] In\u00a0[25]: Copied! <pre>landfall_map = gis.map()\nlandfall_map\n</pre> landfall_map = gis.map() landfall_map Out[25]: In\u00a0[50]: Copied! <pre>landfall_map.add_layer(landfall_item)\n</pre> landfall_map.add_layer(landfall_item) In\u00a0[112]: Copied! <pre>%%time\nfrom arcgis.features.analyze_patterns import calculate_density\nlandfall_density_item = calculate_density(input_layer=landfall_points_fl, radius=30,\n                                          radius_units='Miles', area_units='SquareMiles',\n                                          classification_type='NaturalBreaks', num_classes=7,\n                                          output_name='landfall_density', gis=gis)\n</pre> %%time from arcgis.features.analyze_patterns import calculate_density landfall_density_item = calculate_density(input_layer=landfall_points_fl, radius=30,                                           radius_units='Miles', area_units='SquareMiles',                                           classification_type='NaturalBreaks', num_classes=7,                                           output_name='landfall_density', gis=gis) <pre>CPU times: user 80.9 ms, sys: 15.3 ms, total: 96.2 ms\nWall time: 42.7 s\n</pre> In\u00a0[129]: Copied! <pre>landfall_map = gis.map('Florida, USA')\nlandfall_map\n</pre> landfall_map = gis.map('Florida, USA') landfall_map Out[129]: In\u00a0[128]: Copied! <pre>landfall_map.add_layer(landfall_density_item)\n</pre> landfall_map.add_layer(landfall_density_item) <p>The map here computes the kernel density of landfall locations. It does this by summing the number of landfalls within a radius of <code>30</code> miles and dividing it by the area of this radius. Thus it spreads the number of landfalls over a smooth surface, then classifies this surface into <code>7</code> classes. By performing density anlaysis, we are able to wade through large clouds of landfall points and identify locations that have more landfalls compared to the rest of the world.</p> <p>Let us visualize the density analysis results on a table and as a chart.</p> In\u00a0[130]: Copied! <pre>landfall_density_sdf = landfall_density_item.layers[0].query(as_df=True)\nlandfall_density_sdf.head()\n</pre> landfall_density_sdf = landfall_density_item.layers[0].query(as_df=True) landfall_density_sdf.head() Out[130]: SHAPE analysisarea class objectid value_max_per_squaremile value_min_per_squaremile 0 {\"rings\": [[[53.845677534778474, 81.0534260146... 920.649527 2 1 0.010679 0.00267 1 {\"rings\": [[[24.78192557105382, 79.98490572185... 1896.053559 2 2 0.010679 0.00267 2 {\"rings\": [[[15.16524293599764, 79.45064557546... 162.088581 2 3 0.010679 0.00267 3 {\"rings\": [[[12.600794233316094, 65.2393256814... 183.094538 2 4 0.010679 0.00267 4 {\"rings\": [[[-13.79165699844873, 64.5982135057... 1672.631881 2 5 0.010679 0.00267 In\u00a0[133]: Copied! <pre>ax = landfall_density_sdf['class'].hist()\nax.set_title('Histogram of hurricane landfall densities')\n</pre> ax = landfall_density_sdf['class'].hist() ax.set_title('Histogram of hurricane landfall densities') Out[133]: <pre>Text(0.5,1,'Histogram of hurricane landfall densities')</pre> <p>From the histogram above, we notice there are only a very few places that can be classified as having a high density of hurricane landfalls. Let us analyze these places a bit further.</p> In\u00a0[134]: Copied! <pre>high_density_landfalls = landfall_density_sdf[(landfall_density_sdf['class']==6) | \n                                              (landfall_density_sdf['class']==7)]\n\nhigh_density_landfalls.shape\n</pre> high_density_landfalls = landfall_density_sdf[(landfall_density_sdf['class']==6) |                                                (landfall_density_sdf['class']==7)]  high_density_landfalls.shape Out[134]: <pre>(40, 6)</pre> <p>We have identified <code>40</code> sites worldwide that have a high density of hurricane landfalls based on the anlaysis of data spanning the last <code>169</code> years. Below, we plot them on a map.</p> In\u00a0[146]: Copied! <pre>high_density_landfall_map1 = gis.map('North Carolina')\nhigh_density_landfall_map2 = gis.map('China')\ndisplay(high_density_landfall_map1)\ndisplay(high_density_landfall_map2)\n</pre> high_density_landfall_map1 = gis.map('North Carolina') high_density_landfall_map2 = gis.map('China') display(high_density_landfall_map1) display(high_density_landfall_map2) Out[146]: In\u00a0[145]: Copied! <pre>high_density_landfalls.spatial.plot(map_widget=high_density_landfall_map1, line_width=0, outline_color=0)\nhigh_density_landfalls.spatial.plot(map_widget=high_density_landfall_map2, line_width=0, outline_color=0)\n</pre> high_density_landfalls.spatial.plot(map_widget=high_density_landfall_map1, line_width=0, outline_color=0) high_density_landfalls.spatial.plot(map_widget=high_density_landfall_map2, line_width=0, outline_color=0) Out[145]: <pre>True</pre> <p>The places that turn up are not much of a surprise. We notice the coast of Carolinas in the United States, the states of West Bengal, Orissa in India, several places along the East coast of China, southern tip of Japan and most of the island of Philippines are the places that are most affected on a repeat basis.</p> In\u00a0[210]: Copied! <pre>landfalls_enriched = enrich(high_density_landfalls, data_collections='keyGlobalFacts')\nlandfalls_enriched.head()\n</pre> landfalls_enriched = enrich(high_density_landfalls, data_collections='keyGlobalFacts') landfalls_enriched.head() Out[210]: AVGHHSZ HasData ID OBJECTID_0 SHAPE TOTFEMALES TOTHH TOTMALES TOTPOP aggregationMethod analysisarea apportionmentConfidence class objectid populationToPolygonSizeRating sourceCountry value_max_per_squaremile value_min_per_squaremile 0 2.49 1 0 1 {\"rings\": [[[-75.76583397992073, 36.0687216884... 3514 2815 3500 7014 BlockApportionment:US.BlockGroups 118.233701 2.576 7 170 2.191 US 0.226918 0.122803 1 2.44 1 1 2 {\"rings\": [[[-76.19324209703427, 36.3892777762... 23548 19172 23631 47179 BlockApportionment:US.BlockGroups 1165.144645 2.576 6 174 2.191 US 0.122803 0.067631 2 2.21 1 2 3 {\"rings\": [[[-76.40694615559107, 34.8933493663... 6210 5485 6521 12731 BlockApportionment:US.BlockGroups 731.245665 2.576 6 194 2.191 US 0.122803 0.067631 3 2.70 1 3 4 {\"rings\": [[[130.2448784688346, 32.54260472224... 11118 7816 9969 21087 BlockApportionment:JP.Blocks 106.236904 2.090 6 275 1.695 JP 0.122803 0.067631 4 2.39 1 4 5 {\"rings\": [[[-80.57417529744856, 32.2220486344... 27843 21425 28081 55924 BlockApportionment:US.BlockGroups 255.801403 2.576 6 280 2.191 US 0.122803 0.067631 <p>The <code>enrich()</code> operation accepts the Spatially Enabled <code>DataFrame</code>, performs spatial aggregation and returns another <code>DataFrame</code> with socio-economic and demographic columns added to it. The <code>data_collections</code> parameter decides which additional columns get added.</p> <p>Let us visualize the population that is affected by country. For this, we group by the <code>sourceCountry</code> column and sum up the results. The cell below plots the total number of men, women and households that live within the high density polygons.</p> In\u00a0[216]: Copied! <pre>fig, ax = plt.subplots(1,2, figsize=(15,5))\n# plot bar chart 1\ngrouper1 = landfalls_enriched[['TOTFEMALES','TOTMALES','sourceCountry']].groupby(by='sourceCountry')\ngrouper1.sum().plot(kind='bar', stacked=True, ax=ax[0], \n                    title='Population living within high density areas')\n\n# plot bar chart 2\ngrouper2 = landfalls_enriched[['TOTHH','sourceCountry']].groupby(by='sourceCountry')\ngrouper2.sum().plot(kind='bar', ax=ax[1], \n                    title='Total number of households within high density areas')\n\nplt.tight_layout()\n</pre> fig, ax = plt.subplots(1,2, figsize=(15,5)) # plot bar chart 1 grouper1 = landfalls_enriched[['TOTFEMALES','TOTMALES','sourceCountry']].groupby(by='sourceCountry') grouper1.sum().plot(kind='bar', stacked=True, ax=ax[0],                      title='Population living within high density areas')  # plot bar chart 2 grouper2 = landfalls_enriched[['TOTHH','sourceCountry']].groupby(by='sourceCountry') grouper2.sum().plot(kind='bar', ax=ax[1],                      title='Total number of households within high density areas')  plt.tight_layout() <p>Hurricanes make landfalls on coasts, which invariably are places of dense population. By geo enriching the density maps, we are able to determine that the coasts of China (7 million), Hongkong (7M), India (6M) and Philippines (3M) are at high risk as they suffer repeat landfalls and also support large populations.</p> <p>This information can be used by city planners to better zone the coasts and avoid development along hurricane prone areas.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#analyzing-hurricane-tracks-part-23","title":"Analyzing hurricane tracks - Part 2/3\u00b6","text":"<p>This is the second part to a three part set of notebooks that process and analyze historic hurricane tracks. In the previous notebook we saw</p> <ol> <li>downloading historic hurricane datasets using Python</li> <li>cleaning and merging hurricane observations using DASK</li> <li>aggregating point observations into hurricane tracks using ArcGIS GeoAnalytics server</li> </ol> <p>In this notebook you will analyze the aggregated tracks to answer important questions about prevalance of hurricanes, their seasonality, their density, places where they make landfall and investigate the communities that are most affected.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#access-aggregated-hurricane-data","title":"Access aggregated hurricane data\u00b6","text":"<p>Below, we access the tracks aggregated using GeoAnalytics in the previous notebook.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#query-hurricane-tracks-into-a-spatially-enabled-dataframe","title":"Query hurricane tracks into a Spatially enabled <code>DataFrame</code>\u00b6","text":""},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#exploratory-data-analysis","title":"Exploratory data analysis\u00b6","text":"<p>In this section we perform exploratory analysis of the dataset and answer some interesting questions.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#does-the-number-of-hurricanes-increase-with-time","title":"Does the number of hurricanes increase with time?\u00b6","text":"<p>To understand if number of hurricanes have increased over time, we will plot a histogram of the <code>MIN_Season</code> column.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#how-many-hurricanes-occuer-per-basin-and-sub-basin","title":"How many hurricanes occuer per basin and sub basin?\u00b6","text":"<p>Climate scientists have organized global hurricanes into <code>7</code> basins and a number of sub basins. The snippet below plots groups the data by basin and sub basin, counts the occurrences and plots the frequency in bar charts.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#are-certain-hurricane-names-more-popular","title":"Are certain hurricane names more popular?\u00b6","text":""},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#is-there-a-seasonality-in-the-occurrence-of-hurricanes","title":"Is there a seasonality in the occurrence of hurricanes?\u00b6","text":""},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#what-percent-of-hurricanes-make-landfall","title":"What percent of hurricanes make landfall?\u00b6","text":"<p>While exploring the hurricane data on maps, we noticed their geographic distribution and that they travel long distances over the oceans. Thus, do all hurricanes eventually make landfall? If not, what percent of them do? This is an important question to answer as the threat to human life decreases dramatically when a hurricane does not make a landfall.</p> <p>We will answer this question by performing overlay analysis. For this, we need to intersect the hurricane tracks with world boundary dataset. We will make an anonymous connection to ArcGIS Online to look for a layer published by Esri in living atlas.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#how-far-do-hurricanes-travel-inland-after-landfall","title":"How far do hurricanes travel inland after landfall?\u00b6","text":"<p>Hurricanes in general lose velocity and intensity after they make a landfall. Thus they can only travel a short distance inland. As a result of the overlay analysis, an <code>analysislength</code> column is created. We can plot the histogram of that column to understand how far hurricanes have traveled inland after landfall.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#where-do-hurricanes-make-landfall","title":"Where do hurricanes make landfall?\u00b6","text":"<p>Of equal interest is finding where hurricanes make landfall. From experience we know certain regions are prone to hurricane damage more than the rest. Using spatial data science, we can empirically derive those regions that have statistically more hurricane landfalls compared to the rest.</p> <p>For this, we will repeat the overlay analysis, however this time, we will change the <code>output_type</code> to <code>POINT</code>. The tool will return the points along the coast lines where hurricanes make landfall.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#perform-density-anlaysis-on-hurricane-landfall-locations","title":"Perform density anlaysis on hurricane landfall locations\u00b6","text":"<p>The map above shows hundreds of thousands of points spread around the world. Do all these places have equal probability of being hit by a hurricane? To answer this, we will perform density analysis. The <code>calculate_density</code> tool available under the <code>analyze_patterns</code> toolset is used for this.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#what-is-the-demographics-of-places-with-highest-density-of-landfalls","title":"What is the demographics of places with highest density of landfalls?\u00b6","text":"<p>Now that we have found the places that have the highest desnity of landfalls, we are faced with the question, who lives there? What is the impact of repeat natural calamities on these places? We can answer those questions by geoenriching those polygons with demographic and socio-economic attributes.</p>      Note: Geoenrichment costs ArcGIS Online credits and requires this functionality to be configured in your Portal for ArcGIS.  <p>Below we use the <code>enrich()</code> function to add socio-economic columns to the landfall density <code>DataFrame</code>. For a data collection, we pick <code>keyGlobalFacts</code> which is available for most part of the world.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook we accessed the aggregated hurricane track data from Part 1 as Spatially Enabled <code>DataFrame</code> objects. We visualized these DataFrames both spatially and as charts to reveal interesting information about the global hurricane dataset.</p> <p>We learnt that global hurricanes are classified into <code>7</code> basins, with the most hurricanes occurring over Western Pacific. The North Atlantic basin which affects the continental United States ranks as third busiest basin. The number of hurricanes recorded worldwide has been steadily climbing as technology improves. However, after <code>1970</code>s, we notice a year-over-year reduction in the number of hurricanes. We analyze more of this phenomena in the next part of this study.</p> <p>As for hurricane names, the top spot is a tie between Irma and Florence, with <code>15</code> occurrences for each so far. By turning this DataFrame into a timeseries, we were able to observe a sinusoidal seasonality. The peaks between hurricanes in northern and cyclones in southern hemisphere were offest by about <code>6</code> months, matching the time when summer occurs in these hemispheres. We also noticed that hurricanes over the North Indian basin occur throughout the year as they are influenced by a monsoon phenomena.</p> <p>We then performed overlay analysis to understand where hurricanes make landfall, the path they take once they make landfall. We noticed that the majority of hurricanes make landfall. Once they make a landfall, the majority travel under <code>100</code> miles inland.</p> <p>We extended the overlay analysis to calculate the exact points where landfall occurs. After performing a density analysis on the landfall locations we found places along the coasts of Carolinas in the United States, Orissa, West Bengal in India, several places along the east coast of China, southern tip of Japan and most of Phillippines are affected from a repeat landfall / historical perspective.</p> <p>By geo-enriching the high density places, we were able to understand the number of people that live in these places. China tops the list with most people affected.</p> <p>In the next notebook, we extend this analysis to answer the important question: Does the intensity of hurricanes increase over time?</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/","title":"Analyzing hurricane tracks - Part 3/3","text":"<p>Import the libraries necessary for this notebook.</p> In\u00a0[1]: Copied! <pre># import ArcGIS Libraries\nfrom arcgis.gis import GIS\nfrom arcgis.geometry import filters\nfrom arcgis.geocoding import geocode\nfrom arcgis.features.manage_data import overlay_layers\nfrom arcgis.geoenrichment import enrich\n\n# import Pandas for data exploration\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# import plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# import display tools\nfrom pprint import pprint\nfrom IPython.display import display\n\n# import system libs\nfrom sys import getsizeof\n</pre> # import ArcGIS Libraries from arcgis.gis import GIS from arcgis.geometry import filters from arcgis.geocoding import geocode from arcgis.features.manage_data import overlay_layers from arcgis.geoenrichment import enrich  # import Pandas for data exploration import pandas as pd import numpy as np from scipy import stats  # import plotting libraries import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline  # import display tools from pprint import pprint from IPython.display import display  # import system libs from sys import getsizeof In\u00a0[3]: Copied! <pre>gis = GIS('home')\n</pre> gis = GIS('home') In\u00a0[2]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n</pre> import warnings warnings.filterwarnings('ignore') In\u00a0[4]: Copied! <pre>hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga')[0]\nhurricane_fl = hurricane_tracks_item.layers[0]\n</pre> hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga')[0] hurricane_fl = hurricane_tracks_item.layers[0] <p>The GeoAnalytics step calculated summary statistics of all numeric fields. However only a few of the columns are of interest to us.</p> In\u00a0[5]: Copied! <pre>pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80)\n</pre> pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80) <pre>['objectid', 'serial_num', 'count', 'count_col_1', 'sum_col_1', 'min_col_1',\n 'max_col_1', 'mean_col_1', 'range_col_1', 'sd_col_1', 'var_col_1',\n 'count_season', 'sum_season', 'min_season', 'max_season', 'mean_season',\n 'range_season', 'sd_season', 'var_season', 'count_num', 'sum_num', 'min_num',\n 'max_num', 'mean_num', 'range_num', 'sd_num', 'var_num', 'count_basin',\n 'any_basin', 'count_sub_basin', 'any_sub_basin', 'count_name', 'any_name',\n 'count_iso_time', 'any_iso_time', 'count_nature', 'any_nature', 'count_center',\n 'any_center', 'count_track_type', 'any_track_type', 'count_current_basin',\n 'any_current_basin', 'count_latitude_merged', 'sum_latitude_merged',\n 'min_latitude_merged', 'max_latitude_merged', 'mean_latitude_merged',\n 'range_latitude_merged', 'sd_latitude_merged', 'var_latitude_merged',\n 'count_longitude_merged', 'sum_longitude_merged', 'min_longitude_merged',\n 'max_longitude_merged', 'mean_longitude_merged', 'range_longitude_merged',\n 'sd_longitude_merged', 'var_longitude_merged', 'count_wind_merged',\n 'sum_wind_merged', 'min_wind_merged', 'max_wind_merged', 'mean_wind_merged',\n 'range_wind_merged', 'sd_wind_merged', 'var_wind_merged',\n 'count_pressure_merged', 'sum_pressure_merged', 'min_pressure_merged',\n 'max_pressure_merged', 'mean_pressure_merged', 'range_pressure_merged',\n 'sd_pressure_merged', 'var_pressure_merged', 'count_grade_merged',\n 'sum_grade_merged', 'min_grade_merged', 'max_grade_merged',\n 'mean_grade_merged', 'range_grade_merged', 'sd_grade_merged',\n 'var_grade_merged', 'count_eye_dia_merged', 'sum_eye_dia_merged',\n 'min_eye_dia_merged', 'max_eye_dia_merged', 'mean_eye_dia_merged',\n 'range_eye_dia_merged', 'sd_eye_dia_merged', 'var_eye_dia_merged',\n 'track_duration', 'end_datetime', 'start_datetime']\n</pre> <p>Below we select the following fields for the rest of this analysis.</p> In\u00a0[9]: Copied! <pre>fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',\n                   'any_name', 'mean_latitude_merged', 'mean_longitude_merged',\n                   'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',\n                   'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',\n                   'end_datetime', 'start_datetime']\n</pre> fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',                    'any_name', 'mean_latitude_merged', 'mean_longitude_merged',                    'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',                    'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',                    'end_datetime', 'start_datetime'] In\u00a0[10]: Copied! <pre>%%time\nall_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True)\n</pre> %%time all_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True) <pre>CPU times: user 1.12 s, sys: 318 ms, total: 1.43 s\nWall time: 4.5 s\n</pre> In\u00a0[11]: Copied! <pre>all_hurricanes_df.shape\n</pre> all_hurricanes_df.shape Out[11]: <pre>(12362, 17)</pre> <p>There are <code>12,362</code> hurricanes identified by GeoAnalytics aggregate tracks tool. To get an idea about this aggregated dataset, call the <code>head()</code> method.</p> In\u00a0[12]: Copied! <pre>all_hurricanes_df.head()\n</pre> all_hurricanes_df.head() Out[12]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged min_pressure_merged min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration 0 {\"paths\": [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 NaN 1854.0 1 NaN NaN 1854-02-08 06:00:00 1.296000e+08 1 {\"paths\": [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 NaN 1859.0 2 NaN 10.0 1859-08-24 12:00:00 1.728000e+08 2 {\"paths\": [[[-23.19999999999999, 12.1000000000... NA UNNAMED NA 50.0 1853-09-12 18:00:00 NaN 130.0 26.982000 -51.776000 924.0 1853.0 3 53.0 90.0 1853-08-30 00:00:00 1.058400e+09 3 {\"paths\": [[[59.80000000000001, -15.5], [59.49... SI XXXX856017 MM 13.0 1856-04-05 18:00:00 NaN NaN -20.185385 59.573077 NaN 1856.0 4 NaN NaN 1856-04-02 18:00:00 2.592000e+08 4 {\"paths\": [[[99.60000000000002, -11.5], [98.30... SI NOT NAMED WA 13.0 1861-03-15 18:00:00 NaN NaN -12.940769 94.183846 NaN 1861.0 5 NaN NaN 1861-03-12 18:00:00 2.592000e+08 <p>To better analyze this data set, the date columns need to be changed to a format that Pandas understands better. This is accomplished by calling <code>to_datetime()</code> method and passing the appropriate time columns.</p> In\u00a0[14]: Copied! <pre>all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime'])\nall_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime'])\nall_hurricanes_df.index = all_hurricanes_df['start_datetime']\nall_hurricanes_df.head()\n</pre> all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime']) all_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime']) all_hurricanes_df.index = all_hurricanes_df['start_datetime'] all_hurricanes_df.head() Out[14]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged min_pressure_merged min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration start_datetime 1854-02-08 06:00:00 {\"paths\": [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 NaN 1854.0 1 NaN NaN 1854-02-08 06:00:00 1.296000e+08 1859-08-24 12:00:00 {\"paths\": [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 NaN 1859.0 2 NaN 10.0 1859-08-24 12:00:00 1.728000e+08 1853-08-30 00:00:00 {\"paths\": [[[-23.19999999999999, 12.1000000000... NA UNNAMED NA 50.0 1853-09-12 18:00:00 NaN 130.0 26.982000 -51.776000 924.0 1853.0 3 53.0 90.0 1853-08-30 00:00:00 1.058400e+09 1856-04-02 18:00:00 {\"paths\": [[[59.80000000000001, -15.5], [59.49... SI XXXX856017 MM 13.0 1856-04-05 18:00:00 NaN NaN -20.185385 59.573077 NaN 1856.0 4 NaN NaN 1856-04-02 18:00:00 2.592000e+08 1861-03-12 18:00:00 {\"paths\": [[[99.60000000000002, -11.5], [98.30... SI NOT NAMED WA 13.0 1861-03-15 18:00:00 NaN NaN -12.940769 94.183846 NaN 1861.0 5 NaN NaN 1861-03-12 18:00:00 2.592000e+08 <p>The track duration and length columns need to be projected to units (days, hours, miles) that are meaningful for analysis.</p> In\u00a0[15]: Copied! <pre>all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000\nall_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24)\n</pre> all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000 all_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24) In\u00a0[27]: Copied! <pre>inland_tracks = gis.content.search('hurricane_landfall_tracks')[0]\n\nfields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged', \n                   'min_pressure_merged', 'track_duration','end_datetime', \n                   'start_datetime', 'analysislength']\n\nlandfall_tracks_fl = inland_tracks.layers[0]\n</pre> inland_tracks = gis.content.search('hurricane_landfall_tracks')[0]  fields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged',                     'min_pressure_merged', 'track_duration','end_datetime',                     'start_datetime', 'analysislength']  landfall_tracks_fl = inland_tracks.layers[0] In\u00a0[28]: Copied! <pre>landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df\nlandfall_tracks_df.head(3)\n</pre> landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df landfall_tracks_df.head(3) Out[28]: analysislength any_basin any_name end_datetime max_wind_merged min_pressure_merged min_season objectid start_datetime track_duration SHAPE 0 4.376642 NA NOT NAMED -3663424800000 95.0 965.0 1853.0 1 -3664699200000 1.317600e+09 {'paths': [[[-74.47272727299998, 24], [-74.463... 1 117.097286 NA UNNAMED -3645172800000 70.0 NaN 1854.0 2 -3645475200000 2.160000e+08 {'paths': [[[-99.13749999999999, 26.5699999999... 2 256.909588 NA UNNAMED -3645172800000 70.0 NaN 1854.0 3 -3645475200000 2.160000e+08 {'paths': [[[-102.21739130399999, 27.686956522... In\u00a0[122]: Copied! <pre>all_hurricanes_df.shape\n</pre> all_hurricanes_df.shape Out[122]: <pre>(12362, 19)</pre> In\u00a0[140]: Copied! <pre>missing_data_viz = all_hurricanes_df.replace(0,np.NaN)\nmissing_data_viz = missing_data_viz.replace(-9999.0,np.NaN)\nmissing_data_viz['min_pressure_merged'] = missing_data_viz['min_pressure_merged'].replace(100.0,np.NaN)\n\nplt.figure(figsize=(10,10))\nmissing_data_ax = sns.heatmap(missing_data_viz[['max_wind_merged', 'min_pressure_merged',\n                                                'max_eye_dia_merged', 'track_duration']].isnull(),\n                              cbar=False, cmap='viridis')\nmissing_data_ax.set_ylabel('Years')\nmissing_data_ax.set_title('Missing values (yellow) visualized as a heatmap')\n</pre> missing_data_viz = all_hurricanes_df.replace(0,np.NaN) missing_data_viz = missing_data_viz.replace(-9999.0,np.NaN) missing_data_viz['min_pressure_merged'] = missing_data_viz['min_pressure_merged'].replace(100.0,np.NaN)  plt.figure(figsize=(10,10)) missing_data_ax = sns.heatmap(missing_data_viz[['max_wind_merged', 'min_pressure_merged',                                                 'max_eye_dia_merged', 'track_duration']].isnull(),                               cbar=False, cmap='viridis') missing_data_ax.set_ylabel('Years') missing_data_ax.set_title('Missing values (yellow) visualized as a heatmap') Out[140]: <pre>Text(0.5,1,'Missing values (yellow) visualized as a heatmap')</pre> <p>All three observation columns - wind speed, atmospheric pressure and eye diameter, suffer from missing values. In general as technology improved over time, we were able to collect better data with fewer missing observations. In the sections below we attempt to fill these values using different techniques. We will compare how they fare and pick one of them for rest of the analysis.</p> <p>Technique 1: Drop missing values: An easy way to deal with missing values is to drop those record from analysis. If we were to do that, we lose over a third of the hurricanes.</p> In\u00a0[141]: Copied! <pre>hurricanes_nona = missing_data_viz.dropna(subset=['max_wind_merged','min_pressure_merged'])\nhurricanes_nona.shape\n</pre> hurricanes_nona = missing_data_viz.dropna(subset=['max_wind_merged','min_pressure_merged']) hurricanes_nona.shape Out[141]: <pre>(5857, 19)</pre> <p>Technique 2: Fill using median value: A common technique is to fill using median value (or a different measure of centrality). This technique computes the median of the entire column and applies that to all the missing values.</p> In\u00a0[142]: Copied! <pre>fill_values = {'max_wind_merged': missing_data_viz['max_wind_merged'].median(),\n                'min_pressure_merged': missing_data_viz['min_pressure_merged'].median(),\n              'track_duration_hrs': missing_data_viz['track_duration_hrs'].median()}\nhurricanes_fillna = missing_data_viz.fillna(value=fill_values)\n</pre> fill_values = {'max_wind_merged': missing_data_viz['max_wind_merged'].median(),                 'min_pressure_merged': missing_data_viz['min_pressure_merged'].median(),               'track_duration_hrs': missing_data_viz['track_duration_hrs'].median()} hurricanes_fillna = missing_data_viz.fillna(value=fill_values) <p>Technique 3: Fill by interpolating between existing values: A sophisticated approach is to interploate a missing value based on two of its closest observations.</p> In\u00a0[143]: Copied! <pre>hurricanes_ipl = missing_data_viz\nhurricanes_ipl['max_wind_merged'] = hurricanes_ipl['max_wind_merged'].interpolate()\nhurricanes_ipl['min_pressure_merged'] = hurricanes_ipl['min_pressure_merged'].interpolate()\nhurricanes_ipl['track_duration_hrs'] = hurricanes_ipl['track_duration_hrs'].interpolate()\n</pre> hurricanes_ipl = missing_data_viz hurricanes_ipl['max_wind_merged'] = hurricanes_ipl['max_wind_merged'].interpolate() hurricanes_ipl['min_pressure_merged'] = hurricanes_ipl['min_pressure_merged'].interpolate() hurricanes_ipl['track_duration_hrs'] = hurricanes_ipl['track_duration_hrs'].interpolate() <p>Visualize all 3 techniques</p> <p>To compare how each of these techniques fared, we will plot the histogram of wind speed column after managing for missing values.</p> In\u00a0[144]: Copied! <pre>fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5))\nfig.suptitle('Comparing effects of missing value imputations on Wind speed column', \n             fontsize=15)\n\nhurricanes_nona['max_wind_merged'].plot(kind='hist', ax=ax[0], bins=35, title='Drop null values')\nhurricanes_fillna['max_wind_merged'].plot(kind='hist', ax=ax[1], bins=35, title='Impute with median')\nhurricanes_ipl['max_wind_merged'].plot(kind='hist', ax=ax[2], bins=35, title='Impute via interpolation')\nfor a in ax:\n    a.set_xlabel('Wind Speed')\n</pre> fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5)) fig.suptitle('Comparing effects of missing value imputations on Wind speed column',               fontsize=15)  hurricanes_nona['max_wind_merged'].plot(kind='hist', ax=ax[0], bins=35, title='Drop null values') hurricanes_fillna['max_wind_merged'].plot(kind='hist', ax=ax[1], bins=35, title='Impute with median') hurricanes_ipl['max_wind_merged'].plot(kind='hist', ax=ax[2], bins=35, title='Impute via interpolation') for a in ax:     a.set_xlabel('Wind Speed') <p>Next, we will plot the histogram of atmospheric pressure column after managing for missing values.</p> In\u00a0[145]: Copied! <pre>fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5))\nfig.suptitle('Comparing effects of missing value imputations on Pressure column', \n             fontsize=15)\n\nhurricanes_nona['min_pressure_merged'].plot(kind='hist', ax=ax[0], title='Drop null values')\nhurricanes_fillna['min_pressure_merged'].plot(kind='hist', ax=ax[1], title='Impute with median')\nhurricanes_ipl['min_pressure_merged'].plot(kind='hist', ax=ax[2], title='Impute via interpolation')\nfor a in ax:\n    a.set_xlabel('Atmospheric Pressure')\n</pre> fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5)) fig.suptitle('Comparing effects of missing value imputations on Pressure column',               fontsize=15)  hurricanes_nona['min_pressure_merged'].plot(kind='hist', ax=ax[0], title='Drop null values') hurricanes_fillna['min_pressure_merged'].plot(kind='hist', ax=ax[1], title='Impute with median') hurricanes_ipl['min_pressure_merged'].plot(kind='hist', ax=ax[2], title='Impute via interpolation') for a in ax:     a.set_xlabel('Atmospheric Pressure') <p>Fill using interpolation preserves shape of the original distribution. So it will be used for further anlaysis.</p> In\u00a0[363]: Copied! <pre>ax = all_hurricanes_df['min_season'].hist(bins=50)\nax.set_title('Number of hurricanes per season')\n</pre> ax = all_hurricanes_df['min_season'].hist(bins=50) ax.set_title('Number of hurricanes per season') Out[363]: <pre>Text(0.5,1,'Number of hurricanes per season')</pre> <p>From the previous notebook, we noticed the number of hurricanes recorded has been steadily increasing, partly due to advancements in technology. We notice a reduction in number of hurricanes after 1970s. Let us split this up by basin and observe the the trend is similar.</p> In\u00a0[378]: Copied! <pre>fgrid = sns.FacetGrid(data=all_hurricanes_df, col='any_basin', col_wrap=3,\n                     sharex=False, sharey=False)\nfgrid.map(plt.hist, 'min_season', bins=50)\nfgrid.set_axis_labels(x_var='Seasons', y_var='Frequency of hurricanes')\n</pre> fgrid = sns.FacetGrid(data=all_hurricanes_df, col='any_basin', col_wrap=3,                      sharex=False, sharey=False) fgrid.map(plt.hist, 'min_season', bins=50) fgrid.set_axis_labels(x_var='Seasons', y_var='Frequency of hurricanes') Out[378]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x1a35c9f908&gt;</pre> <p>Plotting the frequency of hurricanes by basin shows a similar trend with the number of hurricanes reducing globally after 1970s. This is consistent with certain studies (1). However this is only one part of the story. Below, we continue to analyze if the nature of hurricanes itself is changing, while the total number may reduce.</p> In\u00a0[266]: Copied! <pre>jgrid = sns.jointplot(x='min_season', y='max_wind_merged', data=hurricanes_ipl,\n             kind='reg', joint_kws={'line_kws':{'color':'green'}}, height=7, space=0.5)\nj = jgrid.annotate(stats.pearsonr)\nj = jgrid.ax_joint.set_title('Does hurricane wind speed increase over time?')\n</pre> jgrid = sns.jointplot(x='min_season', y='max_wind_merged', data=hurricanes_ipl,              kind='reg', joint_kws={'line_kws':{'color':'green'}}, height=7, space=0.5) j = jgrid.annotate(stats.pearsonr) j = jgrid.ax_joint.set_title('Does hurricane wind speed increase over time?') <p>From the plot above, we notice a small positive correlation. Wind speeds are observed to increase with time. The small <code>p-value</code> suggests this correlation (albeit small) is statistically significant. The plot above considers hurricanes across all the basins and regresses that against time. To get a finer picture, we need to split the data by basins and observe the correlation.</p> In\u00a0[329]: Copied! <pre># since there are not many hurricanes observed over South Atlantic basin (SA), \n# we drop it from analysis\nhurricanes_major_basins_df = hurricanes_ipl[hurricanes_ipl['any_basin'].isin(\n                                            ['WP','SI','NA','EP','NI','SP'])]\n</pre> # since there are not many hurricanes observed over South Atlantic basin (SA),  # we drop it from analysis hurricanes_major_basins_df = hurricanes_ipl[hurricanes_ipl['any_basin'].isin(                                             ['WP','SI','NA','EP','NI','SP'])] <p>Define a function that can compute <code>pearson-r</code> correlation coefficient for any two columns across all basins.</p> In\u00a0[322]: Copied! <pre>def correlate_by_basin(column_a, column_b, df=hurricanes_major_basins_df, \n                       category_column = 'any_basin'):\n    # clean data by dropping any NaN values\n    df_nona = df.dropna(subset=[column_a, column_b])\n    \n    # loop through the basins\n    basins = list(df[category_column].unique())\n    return_dict = {}\n    for basin in basins:\n        subset_df = df_nona[df_nona[category_column] == basin]\n        r, p = stats.pearsonr(list(subset_df[column_a]), list(subset_df[column_b]))\n        \n        return_dict[basin] = [round(r,4), round(p,4)]\n    \n    # return correlation coefficient and p-value for each basin as a DataFrame\n    return_df = pd.DataFrame(return_dict).T\n    return_df.columns=['pearson-r','p-value']\n    return return_df\n</pre> def correlate_by_basin(column_a, column_b, df=hurricanes_major_basins_df,                         category_column = 'any_basin'):     # clean data by dropping any NaN values     df_nona = df.dropna(subset=[column_a, column_b])          # loop through the basins     basins = list(df[category_column].unique())     return_dict = {}     for basin in basins:         subset_df = df_nona[df_nona[category_column] == basin]         r, p = stats.pearsonr(list(subset_df[column_a]), list(subset_df[column_b]))                  return_dict[basin] = [round(r,4), round(p,4)]          # return correlation coefficient and p-value for each basin as a DataFrame     return_df = pd.DataFrame(return_dict).T     return_df.columns=['pearson-r','p-value']     return return_df In\u00a0[342]: Copied! <pre>fgrid = sns.lmplot('min_season', 'max_wind_merged', col='any_basin', \n                   data=hurricanes_major_basins_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> fgrid = sns.lmplot('min_season', 'max_wind_merged', col='any_basin',                     data=hurricanes_major_basins_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) <p>From the scatter plots above, we notice the wind speeds in most basins show a slight positive trend, with North Atlantic being an exception. To explore this further, we compute the correlation coefficient and its p-value below.</p> In\u00a0[328]: Copied! <pre>wind_vs_season = correlate_by_basin('min_season','max_wind_merged')\nprint('Correlation coefficients for min_season vs max_wind_merged')\nwind_vs_season\n</pre> wind_vs_season = correlate_by_basin('min_season','max_wind_merged') print('Correlation coefficients for min_season vs max_wind_merged') wind_vs_season <pre>Correlation coefficients for min_season vs max_wind_merged\n</pre> Out[328]: pearson-r p-value SI 0.0686 0.0003 NA -0.1073 0.0000 NI 0.0528 0.0357 WP 0.0891 0.0000 SP 0.3415 0.0000 EP 0.1177 0.0001 <p>The table above displays the correlation coefficient of hurricane wind speed over time. Hurricanes over Southern Pacific basin exhibit a positive trend of increasing wind speeds. The <code>r</code> value over North Atlantic shows a weak negative trend. Since all the <code>p-value</code>s are less than <code>0.05</code>, these correlations are statistically significant.</p> In\u00a0[414]: Copied! <pre>def categorize_hurricanes(row, wind_speed_column='max_wind_merged'):\n    wind_speed = row[wind_speed_column] * 1.152  # knots to mph\n    if 74 &lt;= wind_speed &lt;= 95:\n        return '1'\n    elif 96 &lt;= wind_speed &lt;= 110:\n        return '2'\n    elif 111 &lt;= wind_speed &lt;= 129:\n        return '3'\n    elif 130 &lt;= wind_speed &lt;= 156:\n        return '4'\n    elif 157 &lt;= wind_speed &lt;= 500:\n        return '5'\n</pre> def categorize_hurricanes(row, wind_speed_column='max_wind_merged'):     wind_speed = row[wind_speed_column] * 1.152  # knots to mph     if 74 &lt;= wind_speed &lt;= 95:         return '1'     elif 96 &lt;= wind_speed &lt;= 110:         return '2'     elif 111 &lt;= wind_speed &lt;= 129:         return '3'     elif 130 &lt;= wind_speed &lt;= 156:         return '4'     elif 157 &lt;= wind_speed &lt;= 500:         return '5' In\u00a0[435]: Copied! <pre>hurricanes_major_basins_df['category_str'] = hurricanes_major_basins_df.apply(categorize_hurricanes, \n                                                                              axis=1)\nhurricanes_major_basins_df['category'] = pd.to_numeric(arg=hurricanes_major_basins_df['category_str'],\n                                                      errors='coerce', downcast='integer')\n\nhurricanes_major_basins_df.head(2)\n</pre> hurricanes_major_basins_df['category_str'] = hurricanes_major_basins_df.apply(categorize_hurricanes,                                                                                axis=1) hurricanes_major_basins_df['category'] = pd.to_numeric(arg=hurricanes_major_basins_df['category_str'],                                                       errors='coerce', downcast='integer')  hurricanes_major_basins_df.head(2) Out[435]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged ... min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration track_duration_hrs track_duration_days category category_str start_datetime 1854-02-08 06:00:00 {'paths': [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 ... 1854.0 1 NaN NaN 1854-02-08 06:00:00 129600000.0 36.0 1.5 NaN None 1859-08-24 12:00:00 {'paths': [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 ... 1859.0 2 NaN 10.0 1859-08-24 12:00:00 172800000.0 48.0 2.0 NaN None <p>2 rows \u00d7 21 columns</p> <p>We will create violin and bar plots to visualize the number of hurricane categories over different basins.</p> In\u00a0[429]: Copied! <pre>fig, ax = plt.subplots(1,2, figsize=(15,6))\nvplot = sns.violinplot(x='any_basin', y='category', data=hurricanes_major_basins_df, ax=ax[0])\nvplot.set_title('Number of hurricanes per category in each basin')\n\ncplot = sns.countplot(x='any_basin', hue='category_str', data=hurricanes_major_basins_df,\n             hue_order=['1','2','3','4','5'], ax=ax[1])\ncplot.set_title('Number of hurricanes per category in each basin')\n</pre> fig, ax = plt.subplots(1,2, figsize=(15,6)) vplot = sns.violinplot(x='any_basin', y='category', data=hurricanes_major_basins_df, ax=ax[0]) vplot.set_title('Number of hurricanes per category in each basin')  cplot = sns.countplot(x='any_basin', hue='category_str', data=hurricanes_major_basins_df,              hue_order=['1','2','3','4','5'], ax=ax[1]) cplot.set_title('Number of hurricanes per category in each basin') Out[429]: <pre>Text(0.5,1,'Number of hurricanes per category in each basin')</pre> <p>We notice all basins are capable of generating major hurricanes (over 3). The Eastern Pacific basin appears to have a larger than the proportional number of major hurricanes. Below, we will regress the hurricane category against time to observe if there is a positive trend.</p> In\u00a0[440]: Copied! <pre>kde_regplot = sns.jointplot(x='min_season', y='category', \n                            data=hurricanes_major_basins_df, kind='kde', \n                            stat_func=stats.pearsonr).plot_joint(sns.regplot, \n                                                                 scatter=False)\nkde_regplot.ax_joint.set_title('Scatter plot of hurricane categories over seasons')\n</pre> kde_regplot = sns.jointplot(x='min_season', y='category',                              data=hurricanes_major_basins_df, kind='kde',                              stat_func=stats.pearsonr).plot_joint(sns.regplot,                                                                   scatter=False) kde_regplot.ax_joint.set_title('Scatter plot of hurricane categories over seasons') Out[440]: <pre>Text(0.5,1,'Scatter plot of hurricane categories over seasons')</pre> <p>Even at a global level, we notice a strong positive correlation between hurricane category and seasons. Below, we will split this across basins to observe if the trend holds good.</p> In\u00a0[419]: Copied! <pre>wgrid = sns.lmplot('min_season', 'category', col='any_basin', \n                   data=hurricanes_major_basins_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> wgrid = sns.lmplot('min_season', 'category', col='any_basin',                     data=hurricanes_major_basins_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) In\u00a0[442]: Copied! <pre>category_corr_df = correlate_by_basin('min_season','category', df=hurricanes_major_basins_df)\nprint('Correlation coefficients for min_season vs hurricane category')\ncategory_corr_df\n</pre> category_corr_df = correlate_by_basin('min_season','category', df=hurricanes_major_basins_df) print('Correlation coefficients for min_season vs hurricane category') category_corr_df <pre>Correlation coefficients for min_season vs hurricane category\n</pre> Out[442]: pearson-r p-value SI 0.3314 0.0 NA 0.2161 0.0 NI 0.4612 0.0 WP 0.1849 0.0 SP 0.2908 0.0 EP 0.2919 0.0 <p>Thus, at both global and basin scales, we notice a positive trend in the number of hurricanes of category <code>4</code> and higher, while there is a general reduction in the quantity of hurricanes per season. This is along the lines of several studies [1] [2] [3] [4]. Thus while the total number of hurricanes per season may reduce, we notice an increase in the intensity of them.</p> In\u00a0[330]: Copied! <pre>pgrid = sns.lmplot('min_season', 'min_pressure_merged', col='any_basin', \n                   data=hurricanes_major_basins_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> pgrid = sns.lmplot('min_season', 'min_pressure_merged', col='any_basin',                     data=hurricanes_major_basins_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) In\u00a0[333]: Copied! <pre>pressure_corr_df = correlate_by_basin('min_season','min_pressure_merged')\nprint('Correlation coefficients for min_season vs min_pressure_merged')\npressure_corr_df\n</pre> pressure_corr_df = correlate_by_basin('min_season','min_pressure_merged') print('Correlation coefficients for min_season vs min_pressure_merged') pressure_corr_df <pre>Correlation coefficients for min_season vs min_pressure_merged\n</pre> Out[333]: pearson-r p-value SI -0.0160 0.4027 NA 0.1911 0.0000 NI 0.2551 0.0000 WP -0.0230 0.1606 SP -0.2456 0.0000 EP -0.0270 0.3612 <p>Lower the atmospheric pressure, more intense is the hurricane. Hence we are looking for strong negative correlation between the pressure and season columns. From the charts and table above, we notice South Pacific basin once again tops the list with a mild negative correlation over time. The <code>p-value</code>s of Western Pacific and Eastern Pacific is larger than <code>0.05</code>, so we disregard their correlation coefficients. Over North American and Indian basins, we notice a weak positive correlation.</p> In\u00a0[159]: Copied! <pre>jgrid = sns.jointplot(x='min_season', y='track_duration_days', data=hurricanes_ipl,\n             kind='hex', height=7, space=0.5)\nj = jgrid.annotate(stats.pearsonr)\n\nsns.regplot(x='min_season', y='track_duration_days', data=hurricanes_ipl, \n            ax=jgrid.ax_joint, color='green',scatter=False)\n\nj = jgrid.ax_joint.set_title('Does hurricane duration increase over time?')\n</pre> jgrid = sns.jointplot(x='min_season', y='track_duration_days', data=hurricanes_ipl,              kind='hex', height=7, space=0.5) j = jgrid.annotate(stats.pearsonr)  sns.regplot(x='min_season', y='track_duration_days', data=hurricanes_ipl,              ax=jgrid.ax_joint, color='green',scatter=False)  j = jgrid.ax_joint.set_title('Does hurricane duration increase over time?') <p>At a global scale, we notice an increase in the duration of hurricanes. Below we split this up by basins to get a finer look.</p> In\u00a0[332]: Copied! <pre>lgrid = sns.lmplot('min_season', 'track_duration_days', col='any_basin', \n                   data=hurricanes_major_basins_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> lgrid = sns.lmplot('min_season', 'track_duration_days', col='any_basin',                     data=hurricanes_major_basins_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) In\u00a0[334]: Copied! <pre>linger_time_corr_df = correlate_by_basin('min_season','track_duration_days')\nprint('Correlation coefficients for min_season vs track_duration_days')\nlinger_time_corr_df\n</pre> linger_time_corr_df = correlate_by_basin('min_season','track_duration_days') print('Correlation coefficients for min_season vs track_duration_days') linger_time_corr_df <pre>Correlation coefficients for min_season vs track_duration_days\n</pre> Out[334]: pearson-r p-value SI 0.3577 0.0000 NA -0.0295 0.1738 NI 0.0736 0.0034 WP 0.3627 0.0000 SP 0.2716 0.0000 EP 0.2560 0.0000 <p>At most basins, we notice a positive trend in hurricane track duration. An exception to this is the North Atlantic basin where the <code>p-value</code> is not significant enough to let us draw any conclusion.</p> <p>The trend we notice here could be partly due to technological advancements that allow us to identify and track hurricanes at a very early stage. Hence, to complement this, we will analyze if hurricanes travel longer than usual once the make a landfall.</p> In\u00a0[341]: Copied! <pre>jgrid = sns.jointplot(x='min_season', y='analysislength', data=landfall_tracks_df,\n                      kind='reg', joint_kws={'line_kws':{'color':'green'}}, \n                      height=7, space=0.5, ylim=[0,2000])\nj = jgrid.annotate(stats.pearsonr)\nj = jgrid.ax_joint.set_title('Do hurricanes travel longer inland over time?')\n</pre> jgrid = sns.jointplot(x='min_season', y='analysislength', data=landfall_tracks_df,                       kind='reg', joint_kws={'line_kws':{'color':'green'}},                        height=7, space=0.5, ylim=[0,2000]) j = jgrid.annotate(stats.pearsonr) j = jgrid.ax_joint.set_title('Do hurricanes travel longer inland over time?') In\u00a0[343]: Copied! <pre>lgrid = sns.lmplot('min_season', 'analysislength', col='any_basin', \n                   data=landfall_tracks_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> lgrid = sns.lmplot('min_season', 'analysislength', col='any_basin',                     data=landfall_tracks_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) In\u00a0[344]: Copied! <pre>linger_distance_corr_df = correlate_by_basin('min_season','analysislength', df=landfall_tracks_df)\nprint('Correlation coefficients for min_season vs inland track length')\nlinger_distance_corr_df\n</pre> linger_distance_corr_df = correlate_by_basin('min_season','analysislength', df=landfall_tracks_df) print('Correlation coefficients for min_season vs inland track length') linger_distance_corr_df <pre>Correlation coefficients for min_season vs inland track length\n</pre> Out[344]: pearson-r p-value NA 0.0035 0.8611 NI -0.2320 0.0000 SI -0.0013 0.9684 WP 0.0555 0.0004 SP -0.1722 0.0000 EP -0.0296 0.6344 SA 1.0000 0.0000 <p>When we correlated inland track length over time, we were able to unravel and interesting observation. At basins where there correlation is statistically significant, it is negative (North Indian, Western Pacific, South Pacific). Thus while the duration of hurricanes continues to increase (due to reasons discussed previously), we notice hurricanes travel shorter distances inland. This could be problematic of communities affected as the hurricane could remain stagnant and produce stronger than usual storm surges and precipitation.</p> In\u00a0[443]: Copied! <pre># clean data by dropping any NaN values\nsubset_cols = ['min_season', 'max_wind_merged',\n                'min_pressure_merged', 'track_duration_days', 'category']\n\ndf_nona = hurricanes_major_basins_df.dropna(subset=subset_cols)\n\n# loop through the basins\nbasins = list(df_nona['any_basin'].unique())\nreturn_dict = {}\nfor basin in basins:\n    subset_df =df_nona[df_nona['any_basin'] == basin]\n    row_vector = []\n    for col in subset_cols[1:]:\n        r, p = stats.pearsonr(list(subset_df['min_season']), list(subset_df[col]))\n        if p &lt; 0.05:\n            row_vector.append(round(r,4))\n        else:\n            row_vector.append(pd.np.NaN)\n\n    return_dict[basin] = row_vector\n\n# return as a DataFrame\nreturn_df = pd.DataFrame.from_dict(return_dict, orient='index', columns=subset_cols[1:])\nreturn_df\n</pre> # clean data by dropping any NaN values subset_cols = ['min_season', 'max_wind_merged',                 'min_pressure_merged', 'track_duration_days', 'category']  df_nona = hurricanes_major_basins_df.dropna(subset=subset_cols)  # loop through the basins basins = list(df_nona['any_basin'].unique()) return_dict = {} for basin in basins:     subset_df =df_nona[df_nona['any_basin'] == basin]     row_vector = []     for col in subset_cols[1:]:         r, p = stats.pearsonr(list(subset_df['min_season']), list(subset_df[col]))         if p &lt; 0.05:             row_vector.append(round(r,4))         else:             row_vector.append(pd.np.NaN)      return_dict[basin] = row_vector  # return as a DataFrame return_df = pd.DataFrame.from_dict(return_dict, orient='index', columns=subset_cols[1:]) return_df Out[443]: max_wind_merged min_pressure_merged track_duration_days category NA 0.2171 -0.0904 0.1633 0.2161 SI 0.3156 -0.3608 0.5433 0.3314 NI 0.4456 -0.2234 0.3414 0.4612 WP 0.1755 -0.3666 0.5860 0.1849 SP 0.3028 -0.3508 0.3819 0.2908 EP 0.2820 -0.3510 0.3077 0.2919 <p>We can visualize this correlation table as a heat map to appreciate how the hurricane severity indicators correlate over seasons for each basin.</p> In\u00a0[450]: Copied! <pre>hm_plot = sns.heatmap(return_df, annot=True, linecolor='black', cmap='RdBu_r')\nhm_plot.set_title('Heatmap of correlation coefficients of hurricane severity indicators')\n</pre> hm_plot = sns.heatmap(return_df, annot=True, linecolor='black', cmap='RdBu_r') hm_plot.set_title('Heatmap of correlation coefficients of hurricane severity indicators') Out[450]: <pre>Text(0.5,1,'Heatmap of correlation coefficients of hurricane severity indicators')</pre>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#analyzing-hurricane-tracks-part-33","title":"Analyzing hurricane tracks - Part 3/3\u00b6","text":"<p>This is the third part to a three part set of notebooks that process and analyze historic hurricane tracks. In the previous notebooks we saw</p> <p>Part 1</p> <ol> <li>downloading historic hurricane datasets using Python</li> <li>cleaning and merging hurricane observations using DASK</li> <li>aggregating point observations into hurricane tracks using ArcGIS GeoAnalytics server</li> </ol> <p>Part 2</p> <ol> <li>Visualize the locations of hurricane tracks</li> <li>Different basins and the number of hurricanes per basin</li> <li>Number of hurricanes over time</li> <li>Seasonality in occurrence of hurricanes</li> <li>Places where hurricanes make landfall and the people affected</li> </ol> <p>In this notebook you will analyze the aggregated tracks to answer important questions about hurricane severity and how they correlate over time.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#access-aggregated-hurricane-data","title":"Access aggregated hurricane data\u00b6","text":"<p>Below, we access the tracks aggregated using GeoAnalytics in the previous notebook.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#query-hurricane-tracks-into-a-spatially-enabled-dataframe","title":"Query hurricane tracks into a Spatially enabled <code>DataFrame</code>\u00b6","text":""},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#query-landfall-tracks-layer-into-a-spatially-enabled-dataframe","title":"Query landfall tracks layer into a Spatially Enabled <code>DataFrame</code>\u00b6","text":"<p>We query the landfall tracks layer created in the pervious notebook into a DataFrame.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#manage-missing-sesnsor-data","title":"Manage missing sesnsor data\u00b6","text":"<p>Before we can analyze if hurricanes intensify over time, we need to identify and account for missing values in our data. Sensor measurements such as wind speed, atmospheric pressure, eye diameter, generally suffer from missing values and outliers. The reconstruct tracks tool has identified <code>12,362</code> individual hurricanes that occurred during the past <code>169</code> years.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#visualize-missing-records","title":"Visualize missing records\u00b6","text":"<p>An easy way to visualize missing records is to hack the <code>heatmap</code> of <code>seaborn</code> library to display missing records. The snippet below shows missing records in yellow color.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#missing-value-imputation","title":"Missing value imputation\u00b6","text":""},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#does-intensity-of-hurricanes-increase-over-time","title":"Does intensity of hurricanes increase over time?\u00b6","text":"<p>This last part of this study analyzes if there a temporal trend in the intensity of hurricanes. A number of studies have concluded that anthropogenic influences in the form of global climate change make hurricanes worse and dangerous. We analyze if such patterns can be noticed from an empirical standpoint.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#does-the-number-of-hurricanes-increase-over-time","title":"Does the number of hurricanes increase over time?\u00b6","text":""},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#does-hurricane-wind-speed-increase-over-time","title":"Does hurricane wind speed increase over time?\u00b6","text":"<p>To understand if wind speed increases over time, we create a scatter plot of <code>min_season</code> against the <code>max_wind_merged</code> column. The <code>seaborn</code> plotting library can enhance this plot with correlation coefficient and its level of signifance (p value).</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#analyzing-hurricane-wind-speed-over-time-by-basin","title":"Analyzing hurricane wind speed over time by basin\u00b6","text":"<p>Below we plot a grid of scatter plots with linear regression plots overlaid over them. The <code>seaborn</code> library's <code>lmplot()</code> function makes it trivial accomplish this in a single command.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#analyzing-hurricane-category-over-time-by-basin","title":"Analyzing hurricane category over time by basin\u00b6","text":"<p>Hurricanes are classified on a Saffir-Simpson scale of <code>1-5</code> based on their wind speed. Let us compute this column on the dataset and observe if there are teomporal aspects to it.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#does-eye-pressure-decrease-over-time","title":"Does eye pressure decrease over time?\u00b6","text":"<p>Just like a high wind speed, lower atmospheric pressure increases the intensity of hurricanes. To analyze this, we produce a scatter grid of <code>min_pressure_merged</code> column and regress it against <code>min_season</code> column. We split this by basins.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#do-hurricanes-linger-longer-over-time","title":"Do hurricanes linger longer over time?\u00b6","text":"<p>While wind speed and atmospheric pressure measure two types of intensities, a neverending hurricane can also hurt the communities affected as it inundates the coast with rainfall and storm surge for longer periods of time. In this section we correlate the track duration in days against seasons.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#do-hurricanes-travel-longer-inland-over-time","title":"Do hurricanes travel longer inland over time?\u00b6","text":"<p>Along the lines of track duration, it is relevant for us to investigate whether hurricanes travel longer inland over time. Thus, we correlate track length column of hurricanes that made landfall against seasons.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#correlate-observations-over-time","title":"Correlate observations over time\u00b6","text":"<p>Let us collect all trend analysis we performed so far into a single <code>DataFrame</code>.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook used the aggregated hurricane tracks produced in part 1 and the landfall tracks layer produced in part 2. We checked for missing sensor observations and imputed those records via interpolation. We then comprehensively analyzed how hurricane severity indicators such as wind speed, atmospheric pressure, track duration, track length inland and category correlate over time (seasons). We noticed the total number of hurricanes have decreased since <code>1970</code>s globally and across all basins. While the number of hurricanes is less, we noticed their wind speed, track duration and category correlate positively and atmospheric pressure correlates negatively against seasons. This aligns with findings from major studies. Thus, we notice a reduction in the number of category 1 and 2 and an increase in the number of more severe, category 4, 5 hurricanes. We noticed the North Indian basin to be highly correlated in this regard.</p> <p>In this study, through these 3 part notebooks, we saw how to download hurricane data for the past <code>169</code> years from NOAA NEIC site over FTP and how to it initially using Pandas. Since this data is larger than memory for average computers, we learnt how to aggregate the columns using the DASK distributed, delayed processing library. We input the result of DASK to the ArcGIS GeoAnalytics server to aggregate these point observations into hurricane tracks. The \"reconstruct tracks\" tool which performed this aggregation identified <code>12,362</code> individual hurricanes across the globe.</p> <p>In the second notebook, we performed comprehensive visualization and exploratory analysis to understand the geography of hurricanes and the various basins they are categorized into. We observed a strong sinusoidal nature where hurricanes in the northern and southern hemisphere are offset by <code>6</code> months. We noticed certain names (<code>Irma</code>, <code>Florence</code>) are more popular than the rest. We overlaid the tracks over land boundary dataset to compute tracks traveled inland and identify places where landfall occur. Through density analysis of the landfall locations, we were able to identify the places there were most affected from a repeat landfall basis. By geo-enriching these places, we learnt that China, Hongkong and India are home to population that is most affected.</p> <p>Many studies and articles [5] [6] [7] [8] shine light on anthropogenic influences on global Sea Surface Temperature (SST). Higher sea surface temperatures are observed to produce more intense storms; they are attributed to polar ice cap melting and rising sea levels. These combined with increased hurricane intensity can lead to severe strom surges causing increased flooding and damage of the coastal communities. Based on our density and geoenrichment analysis, we noticed these are places along the coast that are densely populated.</p> <p>This study showcases how a comprehensive spatial data science project can be performed using ArcGIS and open source Python libraries on the ArcGIS Notebook server. The notebook medium makes it convenient to document code, narrative and graphics in one place. This helps in making research reproducible and approachable. Thus in this study we were able to empirically validate the conclusions derived by other studies. In future, when more hurricane datasets become available, this study can be repeated just by rerunning these notebooks.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#references","title":"References\u00b6","text":"<ul> <li>1. Recent intense hurricane response to global climate change. Holland, G. &amp; Bruy\u00e8re, C.L. Clim Dyn (2014) 42: 617.</li> <li>2. US Hurricane Clustering: A New Reality?</li> <li>3. Increasing destructiveness of tropical cyclones over the past 30\u2009years</li> <li>4. The Hurricanes, and Climate-Change Questions, Keep Coming. Yes, They\u2019re Linked. </li> <li>5. We have 12 years to limit climate change catastrophe, warns UN </li> <li>6. Four big takeaways from the UN\u2019s alarming climate change report</li> <li>7. Hurricane Florence caused up to 22 billion in damages. Climate change made the storm worse.</li> <li>8. What Climate Change Taught Us About Hurricanes </li> </ul>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/","title":"Getting started with PySpark","text":"<p>PySpark library gives you a Python API to read and work with your RDDs in HDFS through Apache spark. This tutorial explains the caveats in installing and getting started with PySpark.</p>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#installing-pyspark-scala-java-spark","title":"Installing PySpark, Scala, Java, Spark","text":"<p>Follow this tutorial. The overall steps are</p> <ul> <li>get a linux VM ready. It could be an EC2 instance on AWS</li> <li>get SSH ability into this VM</li> <li>install anaconda. Note: Spark 2.0.0 cannot work with Python 3.6 and needs 3.5. So you can get a version of anaconda that installs 3.5 by default or you can get a higher version of Spark.</li> <li>change the default system Python to use Anaconda python</li> <li>install pip and <code>py4j</code> lib that allows you to run java via Python.</li> <li>download and extract Spark. Here if you get <code>spark-2.0.0-bin-hadoop2.7.tgz</code>, then you need Python 3.5 and not higher. I am getting <code>spark-2.2.1-bin-hadoop2.7.tgz</code> and it works well. Spark JIRA issue for reference</li> </ul> <p>To premanently store the SPARK path store this in the <code>.bashrc</code> file on the home dir of the user account</p> <pre><code>export PATH=\"your default path output\"\n\n# anaconda bin dir - this replaces default Python to anaconda python\nexport PATH=\"/home/USERNAME/anaconda3/bin:$PATH\"\n\n# linux uses : for path separators\n# now add Spark home to Path and PythonPath\nexport SPARK_HOME=\"/home/USERNAME/spark-2.2.1-bin-hadoop2.7\"\nexport PATH=$SPARK_HOME:$PATH\nexport PYTHONPATH=$SPARK_HOME/python #this adds pyspark to python path.\n</code></pre>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#scala-vs-python","title":"Scala vs Python","text":"<p>Advantages of using Scala  - concurrency as Scala supports async.  - Type safety during compile time  - User Defined Functions (UDF) are more efficient in Scala  - type safety - Scala is suitable for bigger projects as its hassle free when you are refactoring a large codebase.  - due to absence of type safety, it does not make sense to with spark Datasets in Python. You can only work with RDD and DataFrames.</p> <p>Advantages of using Python  - easy to learn and use  - suitable for ad-hoc and small projects  - SparkMLib for ML.</p> <p>tie points  - Spark streaming is equally good in both  - DataFrames are similar in both</p>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#rdds-dataframes-datasets","title":"RDDs, DataFrames, Datasets","text":"<p>For an overview on RDDs, refer here. RDDs are compile-time type-safe and evaluate lazily. RDDs can slow in non-JVM langs like Python, cannot be optimized by spark. DataFrames are built on top of RDDs and you let Spark figure out how to work with RDDs. Hence DF is optimized. The only downside is compile-time type-safety. To rectify this, Spark built Datasets.</p> <p>In Spark 2.0, DataFrames and Datasets are merged. Conceptually, if you work with untyped data (as in Python, R), you can use a DataFrame where as if you work in typed languages (Scala), you can work with DataSets.</p>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#actions-and-transformations","title":"Actions and transformations","text":"<p>Transformations create a new dataset. Actions return a value (like a summary statistic). All transformations in Spark are lazy. You can also persist RDDs on disk if you expect to read it later.</p> <p>When working with Spark, you use lambda functions a lot. Lambda plays well with Spark's motto of lazy evaluations.</p>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#some-spark-commands","title":"Some Spark commands","text":"<ul> <li><code>rdd.textFile()</code> to read text files, csv files etc.</li> <li><code>rdd.collect()</code> brings entire RDD to a single machine for processing and displays the result. This is mem intensive and can overwhelm the master if you use it on a large dataset.</li> <li><code>rdd.take(n)</code> on the other hand will only collect and return <code>n</code> lines.</li> <li><code>rdd.toDF()</code> to convert RDD to Spark DF</li> <li><code>df.first()</code> and <code>df.top(n)</code> also work like take.</li> <li><code>df.printSchema()</code> to list the columns and their types. </li> <li>You can also use <code>df.describe().show()</code> to get summary stats.</li> <li><code>df.select('column1','column2').show(m)</code> to select a couple of columns and show their first m rows.</li> <li><code>df.withColumn('colname', transformation_expression)</code> is the primary way you to update values in a DataFrame column.</li> </ul>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#resources","title":"Resources","text":"<ul> <li>A tale of 3 Apache Spark APIs - RDDs, DataFrames, Datasets</li> <li>Datacamp - Apache Spark and Python</li> <li>Datacamp - Predict CA housing prices using SparkMLib and PySpark</li> </ul>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/","title":"Predicting CA housing prices using SparkMLib","text":"In\u00a0[19]: Copied! <pre># Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Build the SparkSession\nspark = SparkSession.builder \\\n   .master(\"local\") \\\n   .appName(\"Linear Regression Model\") \\\n   .config(\"spark.executor.memory\", \"1gb\") \\\n   .getOrCreate()\n   \nsc = spark.sparkContext\n</pre> # Import SparkSession from pyspark.sql import SparkSession  # Build the SparkSession spark = SparkSession.builder \\    .master(\"local\") \\    .appName(\"Linear Regression Model\") \\    .config(\"spark.executor.memory\", \"1gb\") \\    .getOrCreate()     sc = spark.sparkContext <p>Number of records: 20640</p> <p>variables: Lat, Long, Median Age, #rooms, #bedrooms, population in block, households, med income, med house value</p> In\u00a0[4]: Copied! <pre>!ls ../datasets/CaliforniaHousing/\n</pre> !ls ../datasets/CaliforniaHousing/ <pre>cal_housing.data  cal_housing.domain\r\n</pre> In\u00a0[2]: Copied! <pre># load data file\nrdd = sc.textFile('../datasets/CaliforniaHousing/cal_housing.data')\n\n# load header\nheader = sc.textFile('../datasets/CaliforniaHousing/cal_housing.domain')\n</pre> # load data file rdd = sc.textFile('../datasets/CaliforniaHousing/cal_housing.data')  # load header header = sc.textFile('../datasets/CaliforniaHousing/cal_housing.domain') In\u00a0[8]: Copied! <pre>len(rdd.collect())\n</pre> len(rdd.collect()) Out[8]: <pre>20640</pre> In\u00a0[10]: Copied! <pre>len(rdd.take(5))\n</pre> len(rdd.take(5)) Out[10]: <pre>5</pre> In\u00a0[11]: Copied! <pre>rdd.take(5)\n</pre> rdd.take(5) Out[11]: <pre>['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000',\n '-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000',\n '-122.240000,37.850000,52.000000,1467.000000,190.000000,496.000000,177.000000,7.257400,352100.000000',\n '-122.250000,37.850000,52.000000,1274.000000,235.000000,558.000000,219.000000,5.643100,341300.000000',\n '-122.250000,37.850000,52.000000,1627.000000,280.000000,565.000000,259.000000,3.846200,342200.000000']</pre> In\u00a0[3]: Copied! <pre># split by comma\nrdd = rdd.map(lambda line : line.split(','))\n\n# get the first two lines\nrdd.first()\n</pre> # split by comma rdd = rdd.map(lambda line : line.split(','))  # get the first two lines rdd.first() Out[3]: <pre>['-122.230000',\n '37.880000',\n '41.000000',\n '880.000000',\n '129.000000',\n '322.000000',\n '126.000000',\n '8.325200',\n '452600.000000']</pre> In\u00a0[4]: Copied! <pre># convert RDD to a dataframe\nfrom pyspark.sql import Row\n\n# Map the RDD to a DF\ndf = rdd.map(lambda line: Row(longitude=line[0], \n                              latitude=line[1], \n                              housingMedianAge=line[2],\n                              totalRooms=line[3],\n                              totalBedRooms=line[4],\n                              population=line[5], \n                              households=line[6],\n                              medianIncome=line[7],\n                              medianHouseValue=line[8])).toDF()\n\n# show the top few DF rows\ndf.show(5)\n</pre> # convert RDD to a dataframe from pyspark.sql import Row  # Map the RDD to a DF df = rdd.map(lambda line: Row(longitude=line[0],                                latitude=line[1],                                housingMedianAge=line[2],                               totalRooms=line[3],                               totalBedRooms=line[4],                               population=line[5],                                households=line[6],                               medianIncome=line[7],                               medianHouseValue=line[8])).toDF()  # show the top few DF rows df.show(5) <pre>+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n| households|housingMedianAge| latitude|  longitude|medianHouseValue|medianIncome| population|totalBedRooms| totalRooms|\n+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n| 126.000000|       41.000000|37.880000|-122.230000|   452600.000000|    8.325200| 322.000000|   129.000000| 880.000000|\n|1138.000000|       21.000000|37.860000|-122.220000|   358500.000000|    8.301400|2401.000000|  1106.000000|7099.000000|\n| 177.000000|       52.000000|37.850000|-122.240000|   352100.000000|    7.257400| 496.000000|   190.000000|1467.000000|\n| 219.000000|       52.000000|37.850000|-122.250000|   341300.000000|    5.643100| 558.000000|   235.000000|1274.000000|\n| 259.000000|       52.000000|37.850000|-122.250000|   342200.000000|    3.846200| 565.000000|   280.000000|1627.000000|\n+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\nonly showing top 5 rows\n\n</pre> In\u00a0[5]: Copied! <pre>df.printSchema()\n</pre> df.printSchema() <pre>root\n |-- households: string (nullable = true)\n |-- housingMedianAge: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n |-- medianHouseValue: string (nullable = true)\n |-- medianIncome: string (nullable = true)\n |-- population: string (nullable = true)\n |-- totalBedRooms: string (nullable = true)\n |-- totalRooms: string (nullable = true)\n\n</pre> In\u00a0[6]: Copied! <pre># convert all strings to float using a User Defined Function\n\nfrom pyspark.sql.types import *\n\ndef cast_columns(df):\n    for column in df.columns:\n        df = df.withColumn(column, df[column].cast(FloatType()))\n    return df\n\nnew_df = cast_columns(df)\n</pre> # convert all strings to float using a User Defined Function  from pyspark.sql.types import *  def cast_columns(df):     for column in df.columns:         df = df.withColumn(column, df[column].cast(FloatType()))     return df  new_df = cast_columns(df) In\u00a0[7]: Copied! <pre>new_df.show(2)\n</pre> new_df.show(2) <pre>+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n|     126.0|            41.0|   37.88|  -122.23|        452600.0|      8.3252|     322.0|        129.0|     880.0|\n|    1138.0|            21.0|   37.86|  -122.22|        358500.0|      8.3014|    2401.0|       1106.0|    7099.0|\n+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\nonly showing top 2 rows\n\n</pre> In\u00a0[8]: Copied! <pre>new_df.printSchema()\n</pre> new_df.printSchema() <pre>root\n |-- households: float (nullable = true)\n |-- housingMedianAge: float (nullable = true)\n |-- latitude: float (nullable = true)\n |-- longitude: float (nullable = true)\n |-- medianHouseValue: float (nullable = true)\n |-- medianIncome: float (nullable = true)\n |-- population: float (nullable = true)\n |-- totalBedRooms: float (nullable = true)\n |-- totalRooms: float (nullable = true)\n\n</pre> In\u00a0[28]: Copied! <pre>new_df.describe().show()\n</pre> new_df.describe().show() <pre>+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n|summary|       households|  housingMedianAge|         latitude|          longitude|  medianHouseValue|      medianIncome|        population|    totalBedRooms|        totalRooms|\n+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n|  count|            20640|             20640|            20640|              20640|             20640|             20640|             20640|            20640|             20640|\n|   mean|499.5396802325581|28.639486434108527|35.63186143109965|-119.56970444871473|206855.81690891474|3.8706710030346416|1425.4767441860465|537.8980135658915|2635.7630813953488|\n| stddev|382.3297528316098| 12.58555761211163|2.135952380602968|  2.003531742932898|115395.61587441359|1.8998217183639696|  1132.46212176534| 421.247905943133|2181.6152515827944|\n|    min|              1.0|               1.0|            32.54|            -124.35|           14999.0|            0.4999|               3.0|              1.0|               2.0|\n|    max|           6082.0|              52.0|            41.95|            -114.31|          500001.0|           15.0001|           35682.0|           6445.0|           39320.0|\n+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>from pyspark.sql.functions import col\n\ndf = df.withColumn('medianHouseValue', col('medianHouseValue')/100000)\n</pre> from pyspark.sql.functions import col  df = df.withColumn('medianHouseValue', col('medianHouseValue')/100000) In\u00a0[10]: Copied! <pre>df.first()\n</pre> df.first() Out[10]: <pre>Row(households='126.000000', housingMedianAge='41.000000', latitude='37.880000', longitude='-122.230000', medianHouseValue=4.526, medianIncome='8.325200', population='322.000000', totalBedRooms='129.000000', totalRooms='880.000000')</pre> In\u00a0[11]: Copied! <pre># add rooms per household\ndf = df.withColumn('roomsPerHousehold', col('totalRooms')/col('households'))\n\n# add population per household (num people in the home)\ndf = df.withColumn('popPerHousehold', col('population')/col('households'))\n\n# add bedrooms per room\ndf = df.withColumn('bedroomsPerRoom', col('totalBedRooms')/col('totalRooms'))\n</pre> # add rooms per household df = df.withColumn('roomsPerHousehold', col('totalRooms')/col('households'))  # add population per household (num people in the home) df = df.withColumn('popPerHousehold', col('population')/col('households'))  # add bedrooms per room df = df.withColumn('bedroomsPerRoom', col('totalBedRooms')/col('totalRooms')) In\u00a0[12]: Copied! <pre>df.first()\n</pre> df.first() Out[12]: <pre>Row(households='126.000000', housingMedianAge='41.000000', latitude='37.880000', longitude='-122.230000', medianHouseValue=4.526, medianIncome='8.325200', population='322.000000', totalBedRooms='129.000000', totalRooms='880.000000', roomsPerHousehold=6.984126984126984, popPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)</pre> In\u00a0[13]: Copied! <pre>df.columns\n</pre> df.columns Out[13]: <pre>['households',\n 'housingMedianAge',\n 'latitude',\n 'longitude',\n 'medianHouseValue',\n 'medianIncome',\n 'population',\n 'totalBedRooms',\n 'totalRooms',\n 'roomsPerHousehold',\n 'popPerHousehold',\n 'bedroomsPerRoom']</pre> In\u00a0[14]: Copied! <pre>df = df.select('medianHouseValue','households',\n 'housingMedianAge',\n 'latitude',\n 'longitude',\n 'medianIncome',\n 'population',\n 'totalBedRooms',\n 'totalRooms',\n 'roomsPerHousehold',\n 'popPerHousehold',\n 'bedroomsPerRoom')\n</pre> df = df.select('medianHouseValue','households',  'housingMedianAge',  'latitude',  'longitude',  'medianIncome',  'population',  'totalBedRooms',  'totalRooms',  'roomsPerHousehold',  'popPerHousehold',  'bedroomsPerRoom') <p>Create a new DataFrame that explicitly labels the columns as labels and features. <code>DenseVector</code> is used to temporarily convert the data into numpy array and regroup into a named column DataFrame</p> In\u00a0[15]: Copied! <pre>from pyspark.ml.linalg import DenseVector\n\n# return a tuple of first column and all other columns\ntemp_data = df.rdd.map(lambda x:(x[0], DenseVector(x[1:])))\n\n#construct back a new DataFrame\ndf2 = spark.createDataFrame(temp_data, ['label','features'])\n</pre> from pyspark.ml.linalg import DenseVector  # return a tuple of first column and all other columns temp_data = df.rdd.map(lambda x:(x[0], DenseVector(x[1:])))  #construct back a new DataFrame df2 = spark.createDataFrame(temp_data, ['label','features']) In\u00a0[16]: Copied! <pre>df2.take(2)\n</pre> df2.take(2) Out[16]: <pre>[Row(label=4.526, features=DenseVector([126.0, 41.0, 37.88, -122.23, 8.3252, 322.0, 129.0, 880.0, 6.9841, 2.5556, 0.1466])),\n Row(label=3.585, features=DenseVector([1138.0, 21.0, 37.86, -122.22, 8.3014, 2401.0, 1106.0, 7099.0, 6.2381, 2.1098, 0.1558]))]</pre> In\u00a0[17]: Copied! <pre># use StandardScaler to scale the features to std normal distribution\nfrom pyspark.ml.feature import StandardScaler\n\ns_scaler_model = StandardScaler(inputCol='features', outputCol='features_scaled')\nscaler_fn = s_scaler_model.fit(df2)\nscaled_df = scaler_fn.transform(df2)\n\nscaled_df.take(2)\n</pre> # use StandardScaler to scale the features to std normal distribution from pyspark.ml.feature import StandardScaler  s_scaler_model = StandardScaler(inputCol='features', outputCol='features_scaled') scaler_fn = s_scaler_model.fit(df2) scaled_df = scaler_fn.transform(df2)  scaled_df.take(2) Out[17]: <pre>[Row(label=4.526, features=DenseVector([126.0, 41.0, 37.88, -122.23, 8.3252, 322.0, 129.0, 880.0, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3296, 3.2577, 17.7345, -61.0073, 4.3821, 0.2843, 0.3062, 0.4034, 2.8228, 0.2461, 2.5264])),\n Row(label=3.585, features=DenseVector([1138.0, 21.0, 37.86, -122.22, 8.3014, 2401.0, 1106.0, 7099.0, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.9765, 1.6686, 17.7251, -61.0023, 4.3696, 2.1202, 2.6255, 3.254, 2.5213, 0.2031, 2.6851]))]</pre> In\u00a0[18]: Copied! <pre>train_data, test_data = scaled_df.randomSplit([.8,.2], seed=101)\n</pre> train_data, test_data = scaled_df.randomSplit([.8,.2], seed=101) In\u00a0[19]: Copied! <pre>type(train_data)\n</pre> type(train_data) Out[19]: <pre>pyspark.sql.dataframe.DataFrame</pre> In\u00a0[21]: Copied! <pre>from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(labelCol='label', maxIter=20)\n\nlinear_model = lr.fit(train_data)\n</pre> from pyspark.ml.regression import LinearRegression  lr = LinearRegression(labelCol='label', maxIter=20)  linear_model = lr.fit(train_data) In\u00a0[22]: Copied! <pre>type(linear_model)\n</pre> type(linear_model) Out[22]: <pre>pyspark.ml.regression.LinearRegressionModel</pre> In\u00a0[23]: Copied! <pre>linear_model.coefficients\n</pre> linear_model.coefficients Out[23]: <pre>DenseVector([0.0011, 0.0109, -0.4173, -0.4236, 0.4188, -0.0005, 0.0001, 0.0, 0.0275, 0.0012, 3.2844])</pre> <p>Print columns and their coefficients</p> In\u00a0[39]: Copied! <pre>list(zip(df.columns[1:], linear_model.coefficients))\n</pre> list(zip(df.columns[1:], linear_model.coefficients)) Out[39]: <pre>[('households', 0.0011435392550412861),\n ('housingMedianAge', 0.010914556934928758),\n ('latitude', -0.41728655702892636),\n ('longitude', -0.42357898833074664),\n ('medianIncome', 0.41879550542755656),\n ('population', -0.00047200983464106163),\n ('totalBedRooms', 0.00011060741530102377),\n ('totalRooms', 4.099208155268924e-05),\n ('roomsPerHousehold', 0.027483252262545631),\n ('popPerHousehold', 0.0011993665224223444),\n ('bedroomsPerRoom', 3.2844476401153044)]</pre> In\u00a0[28]: Copied! <pre>linear_model.intercept\n</pre> linear_model.intercept Out[28]: <pre>-36.56273436779799</pre> In\u00a0[31]: Copied! <pre>linear_model.summary.numInstances\n</pre> linear_model.summary.numInstances Out[31]: <pre>16535</pre> <p>MAE from training data</p> In\u00a0[35]: Copied! <pre>linear_model.summary.meanAbsoluteError * 100000\n</pre> linear_model.summary.meanAbsoluteError * 100000 Out[35]: <pre>49805.60256405839</pre> <p>Thus, MAE on training data is off by $50,000</p> In\u00a0[34]: Copied! <pre>linear_model.summary.meanSquaredError\n</pre> linear_model.summary.meanSquaredError Out[34]: <pre>0.46775402314782377</pre> In\u00a0[45]: Copied! <pre>linear_model.summary.rootMeanSquaredError * 100000\n</pre> linear_model.summary.rootMeanSquaredError * 100000 Out[45]: <pre>68392.54514549255</pre> <p>Thus, RMSE shows fitting on training data is off by $68,392</p> In\u00a0[40]: Copied! <pre>list(zip(df.columns[1:], linear_model.summary.pValues))\n</pre> list(zip(df.columns[1:], linear_model.summary.pValues)) Out[40]: <pre>[('households', 0.0),\n ('housingMedianAge', 0.0),\n ('latitude', 0.0),\n ('longitude', 0.0),\n ('medianIncome', 0.0),\n ('population', 0.0),\n ('totalBedRooms', 0.2242631044109853),\n ('totalRooms', 0.00010585023878628697),\n ('roomsPerHousehold', 0.0),\n ('popPerHousehold', 0.011952235555041435),\n ('bedroomsPerRoom', 0.0)]</pre> In\u00a0[41]: Copied! <pre>predicted = linear_model.transform(test_data)\npredicted.columns\n</pre> predicted = linear_model.transform(test_data) predicted.columns Out[41]: <pre>['label', 'features', 'features_scaled', 'prediction']</pre> In\u00a0[43]: Copied! <pre>type(predicted)\n</pre> type(predicted) Out[43]: <pre>pyspark.sql.dataframe.DataFrame</pre> In\u00a0[47]: Copied! <pre>test_predictions = predicted.select('prediction').rdd.map(lambda x:x[0])\ntest_labels = predicted.select('label').rdd.map(lambda x:x[0])\n\ntest_predictions_labels = test_predictions.zip(test_labels)\ntest_predictions_labels_df = spark.createDataFrame(test_predictions_labels, \n                                                   ['predictions','labels'])\n\ntest_predictions_labels_df.take(2)\n</pre> test_predictions = predicted.select('prediction').rdd.map(lambda x:x[0]) test_labels = predicted.select('label').rdd.map(lambda x:x[0])  test_predictions_labels = test_predictions.zip(test_labels) test_predictions_labels_df = spark.createDataFrame(test_predictions_labels,                                                     ['predictions','labels'])  test_predictions_labels_df.take(2) Out[47]: <pre>[Row(predictions=1.8357791571765532, labels=0.225),\n Row(predictions=-0.9555783395577535, labels=0.225)]</pre> In\u00a0[49]: Copied! <pre>from pyspark.ml.evaluation import RegressionEvaluator\n\nlinear_reg_eval = RegressionEvaluator(predictionCol='predictions', labelCol='labels')\n</pre> from pyspark.ml.evaluation import RegressionEvaluator  linear_reg_eval = RegressionEvaluator(predictionCol='predictions', labelCol='labels') In\u00a0[50]: Copied! <pre>linear_reg_eval.evaluate(test_predictions_labels_df)\n</pre> linear_reg_eval.evaluate(test_predictions_labels_df) Out[50]: <pre>0.6962295496358668</pre> In\u00a0[58]: Copied! <pre># mean absolute error\nprediction_mae = linear_reg_eval.evaluate(test_predictions_labels_df, \n                                          {linear_reg_eval.metricName:'mae'}) * 100000\nprediction_mae\n</pre> # mean absolute error prediction_mae = linear_reg_eval.evaluate(test_predictions_labels_df,                                            {linear_reg_eval.metricName:'mae'}) * 100000 prediction_mae Out[58]: <pre>49690.440586665725</pre> In\u00a0[59]: Copied! <pre># RMSE\nprediction_rmse = linear_reg_eval.evaluate(test_predictions_labels_df, \n                                           {linear_reg_eval.metricName:'rmse'}) * 100000\n\nprediction_rmse\n</pre> # RMSE prediction_rmse = linear_reg_eval.evaluate(test_predictions_labels_df,                                             {linear_reg_eval.metricName:'rmse'}) * 100000  prediction_rmse Out[59]: <pre>69622.95496358669</pre> In\u00a0[60]: Copied! <pre>print('(training error, prediction error)')\nprint((linear_model.summary.rootMeanSquaredError * 100000, prediction_rmse))\nprint((linear_model.summary.meanAbsoluteError * 100000, prediction_mae))\n</pre> print('(training error, prediction error)') print((linear_model.summary.rootMeanSquaredError * 100000, prediction_rmse)) print((linear_model.summary.meanAbsoluteError * 100000, prediction_mae)) <pre>(training error, prediction error)\n(68392.54514549255, 69622.95496358669)\n(49805.60256405839, 49690.440586665725)\n</pre> In\u00a0[61]: Copied! <pre>predicted_pandas_df = predicted.select('features','prediction').toPandas()\npredicted_pandas_df.head()\n</pre> predicted_pandas_df = predicted.select('features','prediction').toPandas() predicted_pandas_df.head() Out[61]: features prediction 0 [63.0, 33.0, 37.93, -122.32, 2.675, 216.0, 73.... 1.835779 1 [1439.0, 8.0, 35.43, -116.57, 2.7138, 6835.0, ... -0.955578 2 [15.0, 17.0, 33.92, -114.67, 1.2656, 29.0, 24.... -0.426930 3 [288.0, 20.0, 38.56, -121.36, 1.8288, 667.0, 3... 0.843599 4 [382.0, 52.0, 37.78, -122.41, 1.8519, 1055.0, ... 2.335877 In\u00a0[62]: Copied! <pre>predicted_pandas_df.columns\n</pre> predicted_pandas_df.columns Out[62]: <pre>Index(['features', 'prediction'], dtype='object')</pre> In\u00a0[65]: Copied! <pre>import pandas as pd\npredicted_pandas_df2 = pd.DataFrame(predicted_pandas_df['features'].values.tolist(), \n                                   columns=df.columns[1:])\n\npredicted_pandas_df2.head()\n</pre> import pandas as pd predicted_pandas_df2 = pd.DataFrame(predicted_pandas_df['features'].values.tolist(),                                     columns=df.columns[1:])  predicted_pandas_df2.head() Out[65]: households housingMedianAge latitude longitude medianIncome population totalBedRooms totalRooms roomsPerHousehold popPerHousehold bedroomsPerRoom 0 63.0 33.0 37.93 -122.32 2.6750 216.0 73.0 296.0 4.698413 3.428571 0.246622 1 1439.0 8.0 35.43 -116.57 2.7138 6835.0 1743.0 9975.0 6.931897 4.749826 0.174737 2 15.0 17.0 33.92 -114.67 1.2656 29.0 24.0 97.0 6.466667 1.933333 0.247423 3 288.0 20.0 38.56 -121.36 1.8288 667.0 332.0 1232.0 4.277778 2.315972 0.269481 4 382.0 52.0 37.78 -122.41 1.8519 1055.0 422.0 1014.0 2.654450 2.761780 0.416174 In\u00a0[66]: Copied! <pre>predicted_pandas_df2['predictedHouseValue'] = predicted_pandas_df['prediction']\n</pre> predicted_pandas_df2['predictedHouseValue'] = predicted_pandas_df['prediction'] In\u00a0[67]: Copied! <pre>predicted_pandas_df2.to_csv('CA_house_prices_predicted.csv')\n</pre> predicted_pandas_df2.to_csv('CA_house_prices_predicted.csv') In\u00a0[68]: Copied! <pre>!ls\n</pre> !ls <pre>CA_house_prices_predicted.csv  spark_sanity.ipynb\r\nensure_machine.ipynb\t       spark-warehouse\r\nhello-world.ipynb\t       Untitled.ipynb\r\nspark-ml-CA-housing.ipynb      using-spark-dataframes.ipynb\r\n</pre> In\u00a0[69]: Copied! <pre>predicted_pandas_df2.shape\n</pre> predicted_pandas_df2.shape Out[69]: <pre>(4105, 12)</pre> In\u00a0[9]: Copied! <pre>import pandas as pd\nfrom arcgis.gis import GIS\ngis = GIS(\"https://www.arcgis.com\",\"arcgis_python\")\n</pre> import pandas as pd from arcgis.gis import GIS gis = GIS(\"https://www.arcgis.com\",\"arcgis_python\") <pre>Enter password: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[3]: Copied! <pre>from arcgis.features import SpatialDataFrame\n</pre> from arcgis.features import SpatialDataFrame In\u00a0[4]: Copied! <pre>sdf = SpatialDataFrame.from_csv('CA_house_prices_predicted.csv')\nsdf.head(5)\n</pre> sdf = SpatialDataFrame.from_csv('CA_house_prices_predicted.csv') sdf.head(5) Out[4]: households housingMedianAge latitude longitude medianIncome population totalBedRooms totalRooms roomsPerHousehold popPerHousehold bedroomsPerRoom predictedHouseValue 0 63.0 33.0 37.93 -122.32 2.6750 216.0 73.0 296.0 4.698413 3.428571 0.246622 1.835779 1 1439.0 8.0 35.43 -116.57 2.7138 6835.0 1743.0 9975.0 6.931897 4.749826 0.174737 -0.955578 2 15.0 17.0 33.92 -114.67 1.2656 29.0 24.0 97.0 6.466667 1.933333 0.247423 -0.426930 3 288.0 20.0 38.56 -121.36 1.8288 667.0 332.0 1232.0 4.277778 2.315972 0.269481 0.843599 4 382.0 52.0 37.78 -122.41 1.8519 1055.0 422.0 1014.0 2.654450 2.761780 0.416174 2.335877 In\u00a0[11]: Copied! <pre>houses_predicted_fc = gis.content.import_data(sdf[:999])\nhouses_predicted_fc\n</pre> houses_predicted_fc = gis.content.import_data(sdf[:999]) houses_predicted_fc Out[11]: <pre>&lt;FeatureCollection&gt;</pre> In\u00a0[14]: Copied! <pre>ca_map = gis.map('California')\nca_map\n</pre> ca_map = gis.map('California') ca_map <p></p> In\u00a0[18]: Copied! <pre>ca_map.add_layer(houses_predicted_fc, {'renderer':'ClassedColorRenderer',\n                                      'field_name':'predictedHouseValue'})\n</pre> ca_map.add_layer(houses_predicted_fc, {'renderer':'ClassedColorRenderer',                                       'field_name':'predictedHouseValue'}) <pre> \n</pre> <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#predicting-ca-housing-prices-using-sparkmlib","title":"Predicting CA housing prices using SparkMLib\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#boiler-plate-initialize-sparksession-context","title":"Boiler plate - initialize SparkSession &amp; Context\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#about-ca-housing-dataset","title":"About CA housing dataset\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#preprocess-data","title":"Preprocess data\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#convert-rdd-to-spark-dataframe","title":"Convert RDD to Spark DataFrame\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#exploratory-data-analysis","title":"Exploratory data analysis\u00b6","text":"<p>Print the summary stats of the table</p>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#feature-engineering","title":"Feature engineering\u00b6","text":"<p>Add more columns such as 'number of bedrooms per room', 'rooms per household'. Also scale the 'medianHouseValue' by 1000 so it falls within range of other numbers.</p>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#re-order-columns-and-split-table-into-label-and-features","title":"Re-order columns and split table into label and features\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#scale-data-by-shifting-mean-to-0-and-making-sd-1","title":"Scale data by shifting mean to 0 and making SD = 1\u00b6","text":"<p>This ensures all columns have similar levels of variability</p>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#split-data-into-training-and-test-sets","title":"Split data into training and test sets\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#perform-multiple-regression","title":"Perform Multiple Regression\u00b6","text":"<p>Train the model</p>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#inspect-model-properties","title":"Inspect model properties\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#perform-predictions","title":"Perform predictions\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#regression-evaluator","title":"Regression evaluator\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#errors-mae-rmse","title":"Errors - MAE, RMSE\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#compare-training-vs-prediction-errors","title":"Compare training vs prediction errors\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#export-data-as-a-pandas-dataframe","title":"Export data as a Pandas DataFrame\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#write-to-disk-as-csv","title":"Write to disk as CSV\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#publish-to-gis","title":"Publish to GIS\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#spark-jobs","title":"Spark jobs\u00b6","text":""},{"location":"projects/dl/","title":"Deep Learning","text":""},{"location":"projects/dl/#prologue","title":"Prologue","text":"<p>GPUs are required to learn DL, this far seems clear. But why you ask.. read this for more. At a high level, GPUs by Nvidia is optimal for DL as most libraries support this hardware. <code>CUDA</code> prog language developed by Nvidia is also the most developed in terms of features.</p>"},{"location":"projects/dl/#dl-libraries-in-the-market","title":"DL Libraries in the market","text":"<p>The libraries for DL come and go; <code>theano</code>, <code>caffe</code> to name a few. <code>fastai</code> library is built on top of <code>pyTorch</code> from Facebook. Why pyTorch you ask, Rachel reasons that \"the speed at which programmers can experiment and iterate is more important than theoretical speedps\". Hence pytorch which supports dynamic computation. Dynamic computation means the program is executed in the order you wrote it. Contrast this with static computation where you define the architecture of neural net and run code on it. Tensorflow famously uses static computation, but announced support for dynamic computation recently.</p>"},{"location":"projects/dl/#uncommon-wisdom","title":"Uncommon wisdom","text":"<ul> <li>You don't need GPU in production. Rachel writes about a number of reasons why this is the case. Even if you need to train your model every day.</li> <li>You don't need to be an expert in Math</li> <li>You don't need to be an expert programmer. Knowing the above two will certainly help.</li> <li>You don't need massive datasets... hold on. With techniques like transfer learning, data augmentation make it easy to take a pre-trained model and apply it to your problem.</li> </ul>"},{"location":"projects/dl/#setup-google-colab-the-free-gpu-notebook-service","title":"Setup - Google Colab, the free GPU notebook service","text":"<p>Google colab at this point lacks an interface to see what notebooks you have and the interface to import notebooks from GitHub and other sources is hidden somewhere deep. </p> <p>However, once you open, you need to run <code>!curl -s https://course.fast.ai/setup/colab | bash</code> to prepare the colab runtime to use Fast.ai and GPU. After this, you can run fastai course materials for free on GPU. THere are some notes about GPU availability, but this is yet to be seen by me.</p>"},{"location":"projects/dl/#setup-local-gpu-machine","title":"Setup - local GPU machine","text":"<p>If you have GPU enabled hardware and want to set it up to learn Fastai, follow these instructions:</p> <ul> <li>Configure GPU on Windows</li> <li>Set up Windows OS with GPU for Fastai v1</li> <li>Set up Windows OS with GPU for Fastai v2</li> </ul>"},{"location":"projects/dl/#detailed-notes","title":"Detailed notes","text":"<ul> <li>Neural network - concepts</li> <li>Neural network - understanding backpropagation</li> <li>Deep learning concepts</li> <li>Getting started with fast.ai</li> <li>Fast.ai course lesson 1 - classifying pets<ul> <li>lesson 1 mind map courtesy of Fast.ai community members.</li> </ul> </li> <li>Fast.ai Vision</li> </ul>"},{"location":"projects/dl/configure-gpu-windows/","title":"Configure GPU on Windows OS","text":"<p>Note: This doc assumes a GPU has been properly attached on your machine. The purpose of this doc is to help you configure it correctly for deep learning.</p> <p>Jeremy Howard says don't spend time setting up your own GPU workstation for deep learning and accordingly, their documentation for the same is slim. Hence this is a doc for how to use your GPU enabled Windows workstation for learning and building on top of Fastai for deep learning.</p>"},{"location":"projects/dl/configure-gpu-windows/#step-1-check-if-you-have-a-gpu-and-the-appropriate-drivers","title":"Step 1 - Check if you have a GPU and the appropriate drivers","text":"<ul> <li> <p>Ensure your machine has GPU and appropriate drivers. As of this wiki, only Nvidia CUDA compatible drivers are usable for deep learning. You can verify this from Windows Device Manager. See pic below:</p> <p></p> </li> <li> <p>If you don't have a GPU, you need to recheck how the hardware is installed.</p> </li> <li> <p>Use the task manager to see if you have a 'GPU' section (similar to the 'CPU' section) as shown below. If you don't see the GPU section, you might be running an older build of Windows 10. We will solve this issue as well. If GPU shows up, ignore step 3.</p> <p></p> </li> <li> <p>Check if drivers for the GPU are installed. Run <code>nvidia-smi</code> from terminal. The output should look like below. If it does, ignore step 4. If you see an error, you either don't have the drivers, or you have drivers, but not added the nvidia tool to <code>%path%</code>. We will do these in the following steps.</p> <p></p> </li> </ul>"},{"location":"projects/dl/configure-gpu-windows/#step-2-optional-install-new-windows-terminal","title":"Step 2 - Optional - Install new Windows terminal","text":"<ul> <li>Unlike OSX and Linux, Windows does not have a proper terminal. The default command prompt has limited features and is cumbersome for most advanced users.</li> <li>Install the new terminal from https://github.com/Microsoft/Terminal. This is still a beta product, however adds some useful features like multiple tabs, configurable prompts, better font, better copy paste experience, resize &amp; reflow to name a few.</li> <li> <p>I was able to add Git bash, Pro Python Command Prompt, Anaconda Navigator as additional prompts for convenience (as shown below:)</p> <p></p> </li> </ul>"},{"location":"projects/dl/configure-gpu-windows/#step-3-update-windows-10-build","title":"Step 3 - Update Windows 10 build.","text":"<ul> <li>You need the the 2017 Fall update or later builds to see the GPU tab in the task manager. Your GPU should also be compatible, but the chances that it is not is slim.</li> <li> <p>To update your OS, open the command center / control panel and search for 'Update'. Then enable Windows to update. Your machine might restart multiple times depending on where you are on the update cycle. My machine was reimaged to have the latest OS and below is the OS version for reference:</p> <p></p> </li> </ul>"},{"location":"projects/dl/configure-gpu-windows/#step-4-install-gpu-drivers","title":"Step 4 - Install GPU drivers","text":"<ul> <li>If you suspect NVIDIA driver is not installed, you need to download it from https://www.nvidia.com/Download/index.aspx?lang=en-us. Enter your GPU type and download the drivers. Install the drivers.</li> <li>Try to rerun the <code>nvidia-smi</code> tool. If you see an error that such a tool is not found, search for it in the Program Files. In my case, it was located at <code>C:\\Program Files\\NVIDIA Corporation\\NVSMI</code>.</li> <li>Add <code>C:\\Program Files\\NVIDIA Corporation\\NVSMI</code> to Windows Path. Here is a help article for this. Then retry the command.</li> </ul>"},{"location":"projects/dl/coursera-neural-nets-concepts/","title":"Neural networks - concepts","text":""},{"location":"projects/dl/coursera-neural-nets-concepts/#why-use-neural-nets","title":"Why use neural nets?","text":"<p>Consider a classification problem where the decision boundary is non-linear as shown below:</p> <p></p> <p>We can represent non-linearity in a linear model by adding higher order features. However, when the original dataset already comes with a large number of features (say <code>100</code>), then feature engineered features increases by \\(\\frac{O(n^{2})}{2}\\) if we want to include quadratic features. Thus, for input data set with <code>100</code> features, the feature engineered features is in the order of <code>5000s</code>. Fitting a model on such a data set is expensive, further, the model will overfit. Furthermore, if we want to represent cubic features, then order increases to \\(O(n^{3})\\).</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#why-not-traditional-ml","title":"Why not traditional ML?","text":"<p>Image classification is also a non-linear problem. This is because the algorithm sees images as matrices. In the graphic below, we build a training set that classifies cars from non-cars.</p> <p></p> <p>Each pixel in the image is now a feature. Thus a <code>50x50</code> grayscale image has <code>2500</code> features! Since the decision boundary is usually non-linear, the number of feature required for a quadratic fit is <code>3 million</code> features. Trying to fit a logistic regression to this dataset is not feasible.</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#why-are-neural-nets-powerful","title":"Why are neural nets powerful?","text":"<p>Neural nets mimic the biological neural nets found in animal brains. In brains, specific regions are responsible for specific functions. However, when scientists have conducted experiments where they would cut the signals from the ear to the sound processing region and rewrite the signals from eyes to it, the sound processing region now learns to process vision and functions just as good as the original vision processing engine. Similarly, they were able to repeat this for touch as well. Animal brain is effective as each region is not a bunch of complex algorithms, instead, most regions are general purpose systems built to infer data / signals.</p> <p>An example of this approach are usecases for differently abled people shown below:</p> <p></p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#neural-net-representation","title":"Neural net representation","text":"<p>The physical neuron in a brain looks like below. It has a set of dendrites which act as inputs, a processing engine and the axon which acts as output.</p> <p></p> <p>ANNs model these 3 parts of the neuron as shown below. A set of inputs, multiplied by their weights are fed to an activation function, which is a logit or sigmoid function.</p> <p></p> <p>A group of neurons working together forms a neural net. The first layer is called the input layer and the last called the output layer. Sometimes, the bias is represented as an explicit node.</p> <p></p> <p>Weights in a neural net: The graphic below shows how weights are applied in a neural net. The hypothesis function for each neuron takes the familiar \\(g(\\theta^{T}X)\\) form. <code>g</code> is the sigmoid function and \\(\\theta_{i,k}^{j}\\) represents the weight for <code>jth</code> layer, hidden node <code>i</code>, input node <code>k</code>. There is always a bias node which is represented with index <code>0</code>.</p> <p></p> <p>Thus, when you have <code>2</code> nodes in layer 1 (input) and <code>3</code> nodes in  layer 2, the dimension of the weight matrix for layer 2 is <code>3 x (2+1)</code>, we add <code>+1</code> to include the bias node in the first layer. Since weights is a matrix, we represent it with capital theta \\(\\Theta\\).</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#vectorized-implementation-of-forward-propagation","title":"Vectorized implementation of forward propagation","text":"<p>The input parameters in the previous slide can be represented as a vector \\(x\\) $$ x = \\begin{bmatrix}     x_{0}\\\\     x_{1}\\\\     x_{2}\\\\     x_{3} \\end{bmatrix} $$ The activation function can be represented as \\(a^{(j)} = g(z^{(j)})\\) where </p> \\[ z^{(2)} = \\begin{bmatrix}     z_{1}^{2}\\\\\\     z_{2}^{2}\\\\\\     z_{3}^{2} \\end{bmatrix} \\] <p>Thus, \\(z^{(2)} = \\Theta^{(1)}x\\) and \\(a^{(2)} = g(z^{(2)})\\). By extension, for the next layer, \\(z^{(3)} = \\Theta^{(2)}a^{(2)}\\) and \\(h_{\\Theta}(x) = a^{(3)} = g(z^{(3)})\\)</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#neural-nets-learn-their-own-features","title":"Neural nets learn their own features","text":"<p>If you look at the second half of the simple neural net presented earlier, it is simply a logistic regression. The inputs are however, not inputs from real world, but activations of the previous layer. Thus, neural net can create its own input features. Because of this, it is capable of representing non-linear and higher order functions, even when the real world input does not have them.</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#logical-operations-with-neurons","title":"Logical operations with neurons","text":"<p>Neurons in neural nets build complex representations using simple condition checks. Below is an example of how logical <code>AND</code>, <code>OR</code> operators are represented:</p> <p></p> <p>Then, by simply changing the weights, the same neuron can be switched to an <code>OR</code> operator:</p> <p></p> <p>Why are these useful? Many layers of such neurons can build to represent more complex decision boundaries such as <code>XOR</code> or <code>XNOR</code> or even non-linear boundaries. Below is an example of how <code>2</code> layers of NN are used to build <code>XNOR</code> gate using <code>OR</code>, <code>AND</code>, <code>NOR</code> gates. <code>XNOR</code> gives <code>1</code> if both <code>x1</code>, <code>x2</code> are <code>0</code> or <code>1</code>.</p> <p></p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#multiclass-classification-with-nn","title":"Multiclass classification with NN","text":"<p>Multiclass classification in NN is essentially a on-vs-all classification. The output layer has as many nodes as the number of classes. Further, the value of the output layer looks like one-hot encoding</p> <p></p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#ocr-on-mnist-digits-database-using-nn","title":"OCR on MNIST digits database using NN","text":"<p>The MNIST database has <code>14</code> million images of handdrawn digits. We work with a subset of <code>5000</code> images. Each image is <code>20x20</code> pixels. When laid out as a column vector (which is how Neural Nets and log reg algorithms will read it), we get a <code>1x400</code> row vector. A sample of 100 images is below:</p> <p></p> <p>When classifying these digits, we work with <code>1</code> image at a time. This is unlike linear or logistic regression where we would represent the whole training set as matrix <code>X</code>. Here, we treat each pixel as a feature. Thus our input layer has <code>400+1</code> nodes (1 added to represent bias). The hidden layer from pre-trained network has <code>25</code> nodes. The output layer should have <code>10</code> nodes to represent the <code>10</code> classes we predict.</p> <p>Thus, input layer is x = \\(a^{(1)}_{401x1}\\). The weight matrix</p> \\[ a^{(1)} = x_{401x1} \\] \\[ z^{(2)} = \\Theta^{(1)}_{25x401} . a^{(1)} \\] \\[ a^{(2)}_{25x1} = sigmoid(z^{(2)}) \\] <p>We will add a bias to \\(a^{(2)}\\) when computing the next layer, making it \\(a^{(2)}_{26x1}\\)</p> \\[ z^{(3)} = \\Theta^{(2)}_{10x26} . a^{(2)} \\] \\[ a^{(3)}_{10x1} = sigmoid(z^{(3)}) \\] \\[ h_{\\Theta}(x) = max(sigmoid(a^{(3)})) \\] <p>The implementation code can be see here.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/","title":"Understanding backpropagation in neural networks","text":"<p>This page talks about the formula, intuition and the mechanics of the backpropagation optimization function. </p> <p>A neural network is composed of several layers of neurons connected to one another. Each neuron is activated when the sum of weights multiplied by the values of the neurons in previous layer is over a threshold when you pass it through a sigmoid / logit function. The threshold is usually <code>0.5</code>. In order for a neural network to get trained, the weights of all the different neurons need to be adjusted to yield the minimal loss. Backpropagation is the process by which the weights are adjusted during the training process. To understand how this works, we need to understand how to calculate the loss/cost function of a neural network.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#cost-function-of-a-neural-network","title":"Cost function of a neural network","text":"<p>The cost function of a NN builds on the cost function of a logistic regression. Recall from Logistic regression page, the cost function of a logistic regression is</p> \\[ J(\\theta) = -\\frac{1}{m}\\left[\\sum_{i=1}^{m}y^{(i)}log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))\\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2} \\] <p>where \\(m\\) is number of training samples and \\(n\\) is number of parameters.</p> <p>A neural network is like a multi-class logistic regression. Thus, we need to sum over \\(K\\) classes. In reality, \\(K\\) refers to the number of nodes in the output layer. Thus, in binary classification, although the number of classes is <code>2</code>, the number of nodes in output layer is just <code>1</code>. Thus <code>K=1</code>. The cost function is given as</p> \\[ J(\\Theta) = \\frac{-1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} \\left[ y_{k}^{(i)}log(h_{\\theta}(x_{k}^{(i)})) + (1-y_{k}^{(i)})log(1-h_{\\theta}(x_{k}^{(i)}))\\right] \\] <p>where  - \\(m\\) is number of training samples. \\(i=1\\;to\\;m\\)  - \\(K\\) is number of nodes in output layer and \\(k=1 \\; to \\; K\\)</p> <p>The above formula is cost without penalty or regularization. It is similar to the cost of a multiclass logistic regression. In practice, for logistic regression, we compute cost for training data as a whole. In NN, we compute cost for each training sample per by summing the loss over each class, each sample and finally dividing by number of samples.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#cost-function-of-neural-network-with-regularization","title":"Cost function of neural network with regularization","text":"<p>We penalize a network for the number of hidden layers and the number of nodes in each layer, simple. The penalty is simply the square of weights for each node, summed up and divided by twice the number of training samples. We finally multiply this result by a factor denoted by \\(\\lambda\\). Thus</p> \\[ J(\\Theta) = \\frac{-1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K}              \\left[                  y_{k}^{(i)}log(h_{\\theta}(x_{k}^{(i)})) +              (1-y_{k}^{(i)})log(1-h_{\\theta}(x_{k}^{(i)}))             \\right] +              \\frac{\\lambda}{2m} \\left[                                       \\sum_{l=1}^{L-1}\\sum_{r=1}^{s_{l}}\\sum_{c=1}^{s_{l+1}} (\\Theta_{r,c}^{l})^{2}                                 \\right] \\] <p>where </p> <ul> <li>\\(m\\) is number of training samples. \\(i=1\\;to\\;m\\)</li> <li>\\(K\\) is number of nodes in output layer and \\(k=1 \\; to \\; K\\)</li> <li>\\(L\\) is number of layers in the network and \\(l=1 \\; to \\; L-1\\) as we exclude the input layer</li> <li>\\(s_{l}\\) is number of nodes in a given layer \\(l\\). </li> <li>In the penalty term we sum from for each row and column of the weight matrix. That is, from \\(r=1 \\; to \\; s_{l}\\) and \\(c=1 \\; to \\; s_{l+1}\\) <code>r</code> and <code>c</code> represent the rows and columns of the weight matrix for each layer. </li> </ul> <p>Intuitively, the number rows in a weight matrix depends on number of nodes in that layer. The number of columns however, depends on number of nodes in the next layer (hence summing up to \\(s_{l+1}\\)).</p> <p>From the course, the cost function is given as below. Notice a slight change in the suffix used for penalty. I changed it for clarity in my formula above.</p> <p></p> <p>The \\(\\Theta\\) (weight) matrix is 2D for each layer. When you stack all layers together, it becomes a 3D matrix.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#backpropagation","title":"Backpropagation","text":"<p>The cost function gives us a single unit of measure on how well the network performs. The value of the cost function itself is not to be interpreted as such (unlike RMSE or \\(R^{2}\\) which give you an interpretable result). However, you can compare the performance of different hyperparameters or weights by comparing the resulting loss reported by the cost function. </p> <p>Thus, the objective of backpropagation is to minimize the cost function described above using partial derivatives. For each training sample, we compute an error matrix, which reflects the difference between predicted and output values. It is straightforward to compute the error for the last layer, which is the difference between expected and predicted outputs. Progressively, we compute the error for each of the previous layers.</p> <p>Let us start by reviewing the steps in forward propagation. The vectorized implementation of it is given in the slide below:</p> <p></p> <p>Next, we start by computing the error in the last layer. </p> \\[ \\delta_{j}^{(4)} = a_{j}^{(4)} - y_{j} $$$$ Where \\; j \\; stands \\; for \\; each \\; node \\; in \\; the \\; layer.\\; In \\; vectorized \\; terms $$$$ \\delta^{(4)} = a^{(4)} - y \\] <p>The error vector is now simply the difference between the expected and predicted vectors. We can compute the delta terms for the earlier layers of the network as follows:</p> \\[ \\delta^{(3)} = (\\Theta^{(3)})^{T}\\delta^{(4)}.*g'(z^{(3)}) \\] <p>Where \\(g'\\) for <code>l=3</code> is</p> \\[ g' = a^{(2)}.*(1-a^{(2)}) \\] <p>and so on..</p> <p>We don't calculate error for layer 1. Thus no \\(\\delta^{(1)}\\). The equations above give the formulae to calculate error for one training sample. When calculating the backprop for many training samples, we first perform forward propagation, compute the weights, then immediately, perform backprop to calculate the error terms and update the weights immediately. Next, we proceed to the next training sample.</p> <p></p> <p>The figure above shows the vectorized implementations of calculating the error term for all samples as a 2D matrix \\(\\Delta_{ij}^{(l)}\\), where \\(i=1 to m\\) represents individual training samples and \\(j\\) and \\(l\\) stand for number of nodes in each layer and layer number respectively. The vectorized implementation is given by</p> \\[ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^{T} \\] <p>The matrixes \\(D^(l)_{ij}\\) represent the gradient matrices.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#backpropagation-intuition","title":"Backpropagation Intuition","text":"<p>Consider the following example of a simple neural net.</p> <p></p> <p>The error term for last layer is calculated as \\(\\delta_{1}^{(4)} = y^{(i)} - a_{1}^{(4)}\\). Then, the error term for previous layers is given by</p> \\[ \\delta_{1}^{(3)} = \\Theta_{1,1}^{(3)}\\delta_{1}^{(4)} $$ $$ \\delta_{2}^{(3)} = \\Theta_{1,2}^{(3)}\\delta_{1}^{(4)} \\] <p>Similarly, for the previous layer, $$ \\delta_{1}^{(2)} = \\Theta_{1,1}^{(2)}\\delta_{1}^{(3)} + \\Theta_{2,1}^{(2)}\\delta_{2}^{(3)} $$</p> <p>Thus, to calculate the errors in each of the nodes, we need to errors in the terms to the right. In other words, we calculate the error terms from the right to the left, in reverse of the network direction, thus the name backpropagation (of errors).</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#backpropagation-implementation","title":"Backpropagation implementation","text":"<p>Unrolling: The input layer of a neural network is usually a 1-D array. Thus, when training on image data (which is at a minimum 2D), the images are flattened to a long 1D array (vector), where each row is spliced one after another. This process is called unrolling. The inverse transformation is called rolling which returns the matrix back to its n-D form.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#gradient-checking","title":"Gradient checking","text":"<p>Gradient checking is an alternate method of getting gradients through numerical approximation techniques. The idea is, consider the cost function is a curve and is a function of your weights \\(\\theta\\). The objective is to compute the gradient of the curve at different locations. This numerical approximation method computes the gradient by treating infinitesimally small segments as a straight line and compute the slope of such segments as an approximation for the gradient.</p> <p>Thus:</p> \\[ gradApprox \\; = \\; \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon} \\]"},{"location":"projects/dl/coursera-understanding-backpropagation/#random-initialization","title":"Random initialization","text":"<p>In logistic regression and linear regression, we initialized the weights to 0 and were able to compute the cost and gradients and finally arrive at the best result. However, this is not possible with neural networks. When weights are all <code>0</code> (or a constant), then each neuron is essentially identical to the rest. Thus, we need to break this symmetry by initializing the weights randomly.</p> <p>In practice, a lower and upper bound (denoted as \\([-\\epsilon_{init}, \\epsilon_{init}]\\)) is chosen and weights are assigned randomly between these values. A rule of thumb for calculating the limits is to base it on the number of units in the network as:</p> \\[ \\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}} \\] <p>where  - \\(L_{in} = s_{l}\\) and \\(L_{out} = s_{l+1}\\) which are the number of units in the current and next layer.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#network-architecture-choices","title":"Network architecture choices","text":"<p>Architecture choices refer to number of hidden layers and number of nodes / neurons in each layer etc. In general, the number of nodes in input layer depend on number of parameters (or pixels in an image) and the number nodes in output layer depend on number of classes. </p> <p>As to the hidden layers, having <code>1</code> hidden layer is pretty common, the next common is if you are having multiple hidden layers, then the number of nodes in each hidden layer is maintained the same. By and large, the more hidden units / nodes in each layer, the better it is.</p> <p>The number of nodes in hidden layers is comparable or (slightly more than) to number of units in input layer.</p>"},{"location":"projects/dl/setup-win-fastai-v1/","title":"Set up Windows OS for Fastai v1","text":""},{"location":"projects/dl/setup-win-fastai-v1/#preparation-set-up-gpu-drivers","title":"Preparation - Set up GPU drivers","text":"<p>Follow the instructions in Configure GPU on windows page first.</p>"},{"location":"projects/dl/setup-win-fastai-v1/#install-fastai-v1","title":"Install Fastai v1","text":"<p>The steps to install v1 of Fastai is made simpler if you use the ArcGIS Deep Learning Essentials conda meta package. The steps are, from your terminal / Anaconda bash run</p> <pre><code>conda create env -n agslearn python=3.8\nconda activate agslearn\n\nconda install -c esri arcgis_learn\n</code></pre> <p>The <code>ags_learn</code> conda meta-package contains a list of all necessary Fastai and related libs for making Deep Learning and Geospatial deep learning possible on Windows and Linux.</p>"},{"location":"projects/dl/setup-win-fastai-v1/#verify-gpu-is-picked-up","title":"Verify GPU is picked up","text":"<p>From terminal or Notebook, run the following</p> <pre><code>import torch\ntorch.cuda.is_available()\n</code></pre> <p>If you get a <code>True</code>, you are good to go. Else revisit the config doc page listed above.</p> <p>Additionally, you can time how long it takes between CPU and GPU to run the same compute operation:</p> <pre><code>import torch\nt_cpu = torch.rand(800,800,800)  # uses CPU\n%timeit t_cpu @ t_cpu\n\n&gt;&gt;&gt; 4.29 s \u00b1 778 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nimport torch\nt_gpu = torch.rand(800,800,800).cuda()  # uses GPU\n%timeit t_gpu @ t_gpu\n\n&gt;&gt;&gt; 56.7 \u00b5s \u00b1 13.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>"},{"location":"projects/dl/setup-win-fastai-v2/","title":"Set up Windows OS for Fastai v2","text":""},{"location":"projects/dl/setup-win-fastai-v2/#preparation-set-up-gpu-drivers","title":"Preparation - Set up GPU drivers","text":"<p>Follow the instructions in Configure GPU on windows page first.</p>"},{"location":"projects/dl/setup-win-fastai-v2/#install-fastai-v2","title":"Install Fastai v2.","text":"<p>What did not work:  - The Fastbook install instructions don't work as the automatic Pip install steps fail. Manually running Pip install also results in conflicts.  - Running <code>conda install -c fastchan fastai</code> will not resolve as conda satsolver takes forever without results.</p> <p>What worked:  - Install Anaconda individual edition  - Create a new env: <code>conda create -n fastaiv2</code> without any packages. This provides conda a fresh start and makes it easy for the solver  - Then run <code>conda install -c fastai -c pytorch fastai</code> to install all fastaiv2 and all of its dependencies.  - If you want to run the notebooks, then run <code>conda install jupyter</code> to install Jupyter  - Optional: Clone the v2 book repo: https://github.com/fastai/fastbook  - Run <code>pip install -U fastbook</code> to install the book's deps and files on disk.</p>"},{"location":"projects/dl/setup-win-fastai-v2/#verify-gpu-is-picked-up","title":"Verify GPU is picked up","text":"<p>From terminal or Notebook, run the following</p> <pre><code>import torch\ntorch.cuda.is_available()\n</code></pre> <p>If you get a <code>True</code>, you are good to go. Else revisit the config doc page listed above.</p>"},{"location":"projects/dl/setup-win-fastai-v2/#verify-fastai-v2-can-be-imported","title":"Verify FastAI v2 can be imported","text":"<p>Run the quickstart example from the book (or copy below):</p> <pre><code>from fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n</code></pre> <p>As you run the above, open the Task Manager and monitor the GPU usage. You should see a spike while the CPU rate seems constant. The training should go much faster. Instead if you notice CPU peaking to 95-100%, then verify if <code>torch.cuda.is_available()</code> returns <code>True</code>. If it does not, then recheck the driver and OS version steps from above.</p> <p>Congrats! You are all set to use fastaiv2 on Windows OS without needing a dual boot for Linux or installing the Windows subsystem for linux options.</p>"},{"location":"projects/dl/fastai/fastai-1/","title":"Getting started with Fast.ai","text":"In\u00a0[\u00a0]: Copied! <pre>import fastai\n</pre> import fastai In\u00a0[6]: Copied! <pre>from fastai.vision import *\nfrom fastai.metrics import error_rate\n</pre> from fastai.vision import * from fastai.metrics import error_rate <p>Inside colab, importing fastai, automatically imports the <code>datasets</code> module</p> In\u00a0[4]: Copied! <pre>fastai.datasets.URLs.PETS\n</pre> fastai.datasets.URLs.PETS Out[4]: <pre>'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet'</pre> In\u00a0[10]: Copied! <pre>help(fastai.untar_data)\n</pre> help(fastai.untar_data) <pre>Help on function untar_data in module fastai.datasets:\n\nuntar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True) -&gt; pathlib.Path\n    Download `url` to `fname` if it doesn't exist, and un-tgz to folder `dest`.\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs\n                                  ).normalize(imagenet_stats)\n</pre> data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs                                   ).normalize(imagenet_stats) <p>Which gives you an Image data bunch object.</p> <p>Just calling out the object will reveal the number of training, test datasets</p> <pre>&gt;&gt;&gt;data\n\nImageDataBunch;\n\nTrain: LabelList (5912 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\nsamoyed,newfoundland,american_bulldog,american_pit_bull_terrier,saint_bernard\nPath: /content/data/oxford-iiit-pet/images;\n\nValid: LabelList (1478 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\nbasset_hound,Persian,wheaten_terrier,shiba_inu,keeshond\nPath: /content/data/oxford-iiit-pet/images;\n\nTest: None\n</pre> <p>You can query just the validation data set as below:</p> <pre>&gt;&gt;&gt; data.valid_ds.x\n\nImageList (1478 items)\nImage (3, 333, 500),Image (3, 333, 500),Image (3, 500, 333),Image (3, 500, 375),Image (3, 375, 500)\nPath: /content/data/oxford-iiit-pet/images\n</pre> <p>To visually see a sample of the training data, use</p> <pre>data.show_batch(rows=3, figsize=(7,6))\n</pre> <p></p> <p>To get the list of data classes present in the training data, use</p> <pre>&gt;&gt;&gt; print(data.classes)\nlen(data.classes),data.c\n\n\n['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']\n\n(37, 37)\n</pre> <p>fast.ai comes with several models. If you do a</p> <pre><code>dir(fastai.vision.models)\n</code></pre> <p>you get</p> <pre><code>['BasicBlock', 'Darknet', 'DynamicUnet', 'ResLayer', 'ResNet', 'SqueezeNet',\n 'UnetBlock', 'WideResNet', 'XResNet',\n 'alexnet', 'darknet', 'densenet121', 'densenet161', 'densenet169',\n 'densenet201', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50',\n 'squeezenet1_0', 'squeezenet1_1', 'unet', 'vgg16_bn', 'vgg19_bn', 'wrn',\n 'wrn_22', 'xception', 'xresnet', 'xresnet101', 'xresnet152', 'xresnet18',\n 'xresnet34', 'xresnet50']\n</code></pre> <p>The <code>learner</code> object created already is validated against a validation set. The <code>ImageDataBunch</code> object already knows which is training and which is validation. Thus the <code>error_rate</code> parameter seeks to minimize test error and thereby avoid overfitting.</p> <p>To start with, use <code>resnet34</code> which is pretty capable for most problems.</p> <p>When it comes to training, it is always recommended to use <code>fit_one_cycle()</code> rather than <code>fit()</code> method. This is to avoid overfitting. Fit one cycle is based on a 2018 paper which changed the approach to image DL. The images are shown only once and the learner is expected to figure out the pattern. Thus:</p> <pre>&gt;&gt;&gt; learn.fit_one_cycle(4)\n</pre> <p>which will run 4 times on the images. Each time it runs, it gets a bit better</p> <pre><code>Total time: 07:24\n\n epoch \ttrain_loss \tvalid_loss \terror_rate \ttime\n    0 \t1.387328 \t0.305607 \t0.084574 \t01:50\n    1 \t0.550968 \t0.220240 \t0.080514 \t01:50\n    2 \t0.353485 \t0.186418 \t0.066306 \t01:52\n    3 \t0.258271 \t0.169682 \t0.060217 \t01:51\n\nCPU times: user 1min 25s, sys: 41.1 s, total: 2min 6s\nWall time: 7min 24s\n</code></pre> <p>Thus at <code>4</code>th time, we get an error rate of <code>6%</code> or <code>94%</code> accuracy. This is phenomenal accuracy in DL speak compared to the most sophisticated approch of 2012 which got around <code>80%</code> accuracy.</p> <p>Then we save the model using</p> <pre><code>learn.save('stage01', return_path=True)\n</code></pre> <p>This stores the model along with the training data used to create it. Note: This fast.ai model is based on the <code>restnet34</code> model which is about <code>84</code>mb in size. The fast.ai model is <code>87</code>mb in size, the thin layer of specialization is about <code>3</code>mb in size now.</p> <p></p> <p>Another option is to plot all misclassifications using a confusion matrix:</p> <pre><code>interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n</code></pre> <p></p> <p>If you notice above, a lot of classes have values <code>1</code>. To view the list of classes most misclassified as a list, use:</p> <pre>&gt;&gt;&gt; learn.most_confused(min_val=2) # display descending order all values other than diagonal. Ignore 1s though.\n\n[('american_pit_bull_terrier', 'staffordshire_bull_terrier', 9),\n ('British_Shorthair', 'Russian_Blue', 6),\n ('Ragdoll', 'Birman', 6),\n ('Egyptian_Mau', 'Bengal', 3),\n ('Siamese', 'Birman', 3),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 3),\n ('yorkshire_terrier', 'havanese', 3),\n ('Bengal', 'Abyssinian', 2),\n ('Bengal', 'Egyptian_Mau', 2),\n ('Egyptian_Mau', 'Abyssinian', 2),\n ('Maine_Coon', 'Ragdoll', 2),\n ('Persian', 'Maine_Coon', 2),\n ('Ragdoll', 'Persian', 2),\n ('american_bulldog', 'american_pit_bull_terrier', 2),\n ('american_pit_bull_terrier', 'american_bulldog', 2),\n ('chihuahua', 'miniature_pinscher', 2),\n ('leonberger', 'newfoundland', 2),\n ('miniature_pinscher', 'chihuahua', 2),\n ('newfoundland', 'english_cocker_spaniel', 2),\n ('staffordshire_bull_terrier', 'american_bulldog', 2)]\n</pre> <pre>&gt;&gt;&gt; learn.lr_find()\n&gt;&gt;&gt; learn.recorder.plot()\n</pre> <p></p> <p>As you see the plot I have is quite different from what Jeremy has in his lecture. Not sure why. But the concept that loss decreases, plateaus then increases can be seen.</p> <p>I will retrian the model by unfreezing it and training with specific learning rate.</p> <pre>&gt;&gt;&gt; learn.unfreeze()\n&gt;&gt;&gt; learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-5))\n</pre> <p>which leads to</p> <pre><code>Total time: 04:17\n\nepoch \ttrain_loss \tvalid_loss \terror_rate \ttime\n0 \t0.233426 \t0.213525 \t0.067659 \t02:08\n1 \t0.214522 \t0.208705 \t0.064953 \t02:09\n</code></pre> <p>This gives me a model with error rate of <code>6.4%</code>, while I already had a <code>6%</code> error rate.</p>"},{"location":"projects/dl/fastai/fastai-1/#getting-started-with-fastai","title":"Getting started with Fast.ai\u00b6","text":"<p>As of 2019, fast.ai supports 4 types of DL applications</p> <ul> <li>computer vision</li> <li>natural language text</li> <li>tabular data</li> <li>collaborative filtering</li> </ul> <p>Fast.ai uses type hinting introduced in Python 3.5 quite heavily. Thus if you type <code>help(fastai.untar_data)</code>, you notice type hints.</p>"},{"location":"projects/dl/fastai/fastai-1/#genearl-notes","title":"Genearl notes\u00b6","text":"<ol> <li>The image dimensions used here is 224. This is a convention.</li> <li>normalizing images means turning them to (mean 0, 1 SD). This is done prior to training</li> <li><code>data.c</code> -&gt; gives number of classes. <code>data.classes</code> -&gt; gives the names of the classes.</li> <li>we use transfer learning. We pick a model that already knows something about images and tune it to our case study.</li> </ol>"},{"location":"projects/dl/fastai/fastai-1/#workflow","title":"Workflow\u00b6","text":"<ol> <li>download data into local directory</li> <li>import data files into a <code>data_bunch</code>. This process automatically creates a validation set.</li> <li>normalize</li> <li>run <code>show_batch</code> to see the classes and labels</li> <li>print the number of classes</li> <li>create a <code>ConvLearner</code> object by passing the data bunch, specifying the model architecture and metrics to use to evaluate training stats</li> <li>Fit the model. You can use <code>fit</code> or <code>fit_one_cycle</code> methods, but recommended is to use latter. Pass the epoch number (also called <code>cycles</code>)</li> <li>look at the results and if good, save by calling <code>learn.save('filename')</code></li> <li>Validation - create an <code>interpreter</code> object using <code>ClassificationInterpretation.from_learner(learn)</code>. The learn object so far knows the data and the model used to train. Now its time to validate</li> <li>Find the biggest losses using <code>interp.plot_top_losses(9, figsize=(15,11))</code>. You can also plot <code>interp.plot_confusion_matrix()</code> to view the CF matrix. Fastai also has <code>interp.most_confused(min_val=2)</code> which will return the top losses.</li> </ol>"},{"location":"projects/dl/fastai/fastai-1/#making-model-better","title":"Making model better\u00b6","text":"<ol> <li>Generally, when you call <code>fit_one_cycle</code> it only trains the last or last few layers. To improve this better, you need to call <code>learn.unfreeze()</code> to unfreeze the model.</li> </ol> <p>Next, you repeat the <code>learn.fit_one_cycle(numepochs)</code>. Sometimes, the error goes up when doing this. This happens because you have a reckless learning rate which makes the model lose it original learning. We need to be more nuanced here.</p> <ol> <li><p>Find the optimal learning rate: Now load the original model using <code>learn.load('stage-1')</code>, then run <code>learn.lr_find()</code> and find the highest learning rate that has the lowest loss.</p> </li> <li><p>With this new information retrain the model. <code>learn.unfreeze(); learn.fit_one_cycle(epochs=2, max_lr=slice(1e-6, 1e-4))</code>. What the slice suggests is, train the initial layers at start value specified and last layer at the end value specified and interpolate for the rest of the layers. It is tradecraft to make the end learning rate about 10 times smaller than rate at which errors start to increase.</p> </li> </ol>"},{"location":"projects/dl/fastai/fastai-1/#import-fastai","title":"Import fast.ai\u00b6","text":""},{"location":"projects/dl/fastai/fastai-1/#image-data-bunches","title":"Image data bunches\u00b6","text":"<p>Fast.ai has a useful called <code>ImageDataBunch</code> under the <code>fastai.vision.data</code> module. THis class helps in creating a structure of training, test data, data images, annotations etc, all into 1 class.</p> <p>To load data into an image data bunch, do</p>"},{"location":"projects/dl/fastai/fastai-1/#training-a-neural-net-in-fastai","title":"Training a neural net in fast.ai\u00b6","text":"<p>There are 2 concepts at a high level:</p> <ul> <li>DataBunch: A general fastai concept for your data, and from there, there are subclasses for particular applications like <code>ImageDataBunch</code></li> <li>Learner: A general concept for things that can learn to fit a model. From that, there are various subclasses to make things easier in particular, there is a convnet learner (something that will create a convolutional neural network for you).</li> </ul> <p>The general syntax to instantiate a learner in fast ai is as below:</p> <pre>learn = cnn_learner(&lt;DataBunch obj&gt;, &lt;models.model&gt;, metrics=error_rate)\n</pre> <p>such as</p> <pre>&gt;&gt;&gt; learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n&gt;&gt;&gt; type(learn)\nfastai.basic_train.Learner\n</pre>"},{"location":"projects/dl/fastai/fastai-1/#transfer-learning","title":"Transfer learning\u00b6","text":"<p>Resnet34 is a CNN that is trained on over a million images of various categories. This already knows to differentiatie between a large number of classes seen in everyday life. Thus, resnet34 is a generalist.</p> <p>Transfer learning is the process of taking a generalist neural net and training it to become a specialist. We train the restnet34 in lesson 1 to classify between <code>37</code> classes of cats and dogs.</p> <p>Transfer learning allows you to train nets with <code>1/100</code>th less time using <code>1/100</code> less data.</p>"},{"location":"projects/dl/fastai/fastai-1/#validation","title":"Validation\u00b6","text":"<p>Since this is a classification problem, we use confusion matrix for accuracy assessment. We create a <code>ClassificationInterpretation</code> object using the <code>Learner</code> object created earlier</p> <pre>&gt;&gt;&gt; interp = ClassificationInterpretation.from_learner(learn)\n&gt;&gt;&gt; type(interp)\nfastai.train.ClassificationInterpretation\n</pre> <p>We can plot the top losses using the <code>plot_top_losses()</code> method off the <code>Learner</code> object. This plots the top 'n' classes where the classifier has least <code>precision</code>.</p> <pre><code>interp.plot_top_losses(9, figsize=(15,11), heatmap=False)\n</code></pre>"},{"location":"projects/dl/fastai/fastai-1/#model-fine-tuning","title":"Model fine-tuning\u00b6","text":"<p>So far, we took a <code>resnet34</code> model, added a few layers to the end and trained. This was very fast. However, to improve this furture, we need to retrain the whole model, meaning, all its layers.</p> <p><code>resnet34</code> has <code>34</code> layers, <code>resnet50</code> has <code>50</code> layers. For instance, these are how the layers in resnet look like</p> <ol> <li>layer 1 - looks for edges</li> <li>layer 2 - activates for two edges, curves. Thus can detect window, table corners, circles such as clocks</li> <li>layer 3 - for patterns of layer 2 - thus can sense geometric shapes, lines of text or barcode</li> <li>layer 4 - dog faces</li> <li>layer 5 - people faces, eyeball of animaps, tires, faces of breeds of dogs</li> </ol> <p>Thus earlier, when we trained on resnet 34, we kept these layers as is and only trained on a few on top of them. To tune the model, we don't really have to change levels 1,2 which are fundamental. There are not many ways to improve levels 1,2. As levels increase, different levels of semantic complexity are handled.</p> <p>However, when we make the resnet learn on all layers, it performs worse! To balance, we unfreeze, then load the saved model we had earlier. Then we ask it to run a learning rate finder. The learning rate is pretty important, it says, \"how quickly am I updating the parameters in my model\". The general pattern of lr rate is, it improves and then degenerates after some point.</p> <p>Thus the general pattern advocated is to use the lr finder to find the shape of learning rate. Then unfreeze and call fit method with appropriate learning rate window. This trains the lower layers at a rate and higher abstraction layers are a different rate.</p>"},{"location":"projects/dl/fastai/fastai-1/#deeper-layers","title":"Deeper layers\u00b6","text":"<p>Another option is to use <code>resnet50</code>. However a typical GPU with <code>16</code>GB RAM is insufficient to handle this deep of layers.</p>"},{"location":"projects/dl/fastai/lesson1-pets/","title":"Image classification with FastAI","text":"In\u00a0[1]: Copied! <pre># install fast.ai course content.\n!curl -s https://course.fast.ai/setup/colab | bash\n</pre> # install fast.ai course content. !curl -s https://course.fast.ai/setup/colab | bash <pre>Updating fastai...\nDone.\n</pre> <p>Welcome to lesson 1! For those of you who are using a Jupyter Notebook for the first time, you can learn about this useful tool in a tutorial we prepared specially for you; click <code>File</code>-&gt;<code>Open</code> now and click <code>00_notebook_tutorial.ipynb</code>.</p> <p>In this lesson we will build our first image classifier from scratch, and see if we can achieve world-class results. Let's dive in!</p> <p>Every notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook.</p> In\u00a0[\u00a0]: Copied! <pre>#@title Default title text\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n</pre> #@title Default title text %reload_ext autoreload %autoreload 2 %matplotlib inline <p>We import all the necessary packages. We are going to work with the fastai V1 library which sits on top of Pytorch 1.0. The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models.</p> In\u00a0[\u00a0]: Copied! <pre>import fastai\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\n</pre> import fastai from fastai.vision import * from fastai.metrics import error_rate <p>If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel-&gt;Restart, uncomment the 2nd line below to use a smaller batch size (you'll learn all about what this means during the course), and try again.</p> In\u00a0[\u00a0]: Copied! <pre>bs = 64\n# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel-&gt;Restart\n</pre> bs = 64 # bs = 16   # uncomment this line if you run out of memory even after clicking Kernel-&gt;Restart <p>We are going to use the Oxford-IIIT Pet Dataset by O. M. Parkhi et al., 2012 which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate \"Image\", \"Head\", and \"Body\" models for the pet photos. Let's see how accurate we can be using deep learning!</p> <p>We are going to use the <code>untar_data</code> function to which we must pass a URL as an argument and which will download and extract the data.</p> In\u00a0[5]: Copied! <pre>help(untar_data)\n</pre> help(untar_data) <pre>Help on function untar_data in module fastai.datasets:\n\nuntar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True, force_download=False) -&gt; pathlib.Path\n    Download `url` to `fname` if it doesn't exist, and un-tgz to folder `dest`.\n\n</pre> In\u00a0[6]: Copied! <pre>URLs.PETS\n</pre> URLs.PETS Out[6]: <pre>'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet'</pre> In\u00a0[4]: Copied! <pre>path = untar_data(URLs.PETS); path\n</pre> path = untar_data(URLs.PETS); path Out[4]: <pre>PosixPath('/root/.fastai/data/oxford-iiit-pet')</pre> In\u00a0[5]: Copied! <pre>path.ls()\n</pre> path.ls() Out[5]: <pre>[PosixPath('/root/.fastai/data/oxford-iiit-pet/annotations'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images')]</pre> In\u00a0[\u00a0]: Copied! <pre>path_anno = path/'annotations'\npath_img = path/'images'\n</pre> path_anno = path/'annotations' path_img = path/'images' In\u00a0[7]: Copied! <pre>(path_anno,path_img)\n</pre> (path_anno,path_img) Out[7]: <pre>(PosixPath('/root/.fastai/data/oxford-iiit-pet/annotations'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images'))</pre> <p>The first thing we do when we approach a problem is to take a look at the data. We always need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.</p> <p>The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, <code>ImageDataBunch.from_name_re</code> gets the labels from the filenames using a regular expression.</p> In\u00a0[8]: Copied! <pre>fnames = get_image_files(path_img)\nfnames[:5]\n</pre> fnames = get_image_files(path_img) fnames[:5] Out[8]: <pre>[PosixPath('/root/.fastai/data/oxford-iiit-pet/images/japanese_chin_78.jpg'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images/leonberger_92.jpg'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images/basset_hound_194.jpg'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_33.jpg'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_136.jpg')]</pre> In\u00a0[12]: Copied! <pre>help(get_image_files)\n</pre> help(get_image_files) <pre>Help on function get_image_files in module fastai.vision.data:\n\nget_image_files(c:Union[pathlib.Path, str], check_ext:bool=True, recurse=False) -&gt; Collection[pathlib.Path]\n    Return list of files in `c` that are images. `check_ext` will filter to `image_extensions`.\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(2)\npat = r'/([^/]+)_\\d+.jpg$'\n</pre> np.random.seed(2) pat = r'/([^/]+)_\\d+.jpg$' In\u00a0[14]: Copied! <pre>help(ImageDataBunch.from_name_re)\n</pre> help(ImageDataBunch.from_name_re) <pre>Help on method from_name_re in module fastai.vision.data:\n\nfrom_name_re(path:Union[pathlib.Path, str], fnames:Collection[pathlib.Path], pat:str, valid_pct:float=0.2, **kwargs) method of builtins.type instance\n    Create from list of `fnames` in `path` with re expression `pat`.\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs\n                                  ).normalize(imagenet_stats)\n</pre> data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs                                   ).normalize(imagenet_stats) In\u00a0[16]: Copied! <pre>type(data)\n</pre> type(data) Out[16]: <pre>fastai.vision.data.ImageDataBunch</pre> In\u00a0[11]: Copied! <pre>data\n</pre> data Out[11]: <pre>ImageDataBunch;\n\nTrain: LabelList (5912 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\njapanese_chin,leonberger,basset_hound,samoyed,samoyed\nPath: /root/.fastai/data/oxford-iiit-pet/images;\n\nValid: LabelList (1478 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\nnewfoundland,keeshond,chihuahua,keeshond,pug\nPath: /root/.fastai/data/oxford-iiit-pet/images;\n\nTest: None</pre> In\u00a0[12]: Copied! <pre>data.valid_ds.x\n</pre> data.valid_ds.x Out[12]: <pre>ImageList (1478 items)\nImage (3, 500, 468),Image (3, 375, 500),Image (3, 500, 332),Image (3, 375, 500),Image (3, 333, 500)\nPath: /root/.fastai/data/oxford-iiit-pet/images</pre> In\u00a0[\u00a0]: Copied! <pre>help(data.show_batch)\n</pre> help(data.show_batch) <pre>Help on method show_batch in module fastai.basic_data:\n\nshow_batch(rows:int=5, ds_type:fastai.basic_data.DatasetType=&lt;DatasetType.Train: 1&gt;, reverse:bool=False, **kwargs) -&gt; None method of fastai.vision.data.ImageDataBunch instance\n    Show a batch of data in `ds_type` on a few `rows`.\n\n</pre> In\u00a0[33]: Copied! <pre>data.show_batch(rows=3, figsize=(7,6))\n</pre> data.show_batch(rows=3, figsize=(7,6)) In\u00a0[13]: Copied! <pre>print(data.classes)\nlen(data.classes),data.c\n</pre> print(data.classes) len(data.classes),data.c <pre>['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']\n</pre> Out[13]: <pre>(37, 37)</pre> <p>Now we will start training our model. We will use a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).</p> <p>We will train for 4 epochs (4 cycles through all our data).</p> In\u00a0[14]: Copied! <pre>from pprint import pprint\npprint(dir(fastai.vision.models), compact=True)\n</pre> from pprint import pprint pprint(dir(fastai.vision.models), compact=True) <pre>['BasicBlock', 'Darknet', 'DynamicUnet', 'ResLayer', 'ResNet', 'SqueezeNet',\n 'UnetBlock', 'WideResNet', 'XResNet', '__builtins__', '__cached__', '__doc__',\n '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__',\n 'alexnet', 'darknet', 'densenet121', 'densenet161', 'densenet169',\n 'densenet201', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50',\n 'squeezenet1_0', 'squeezenet1_1', 'unet', 'vgg16_bn', 'vgg19_bn', 'wrn',\n 'wrn_22', 'xception', 'xresnet', 'xresnet101', 'xresnet152', 'xresnet18',\n 'xresnet34', 'xresnet50']\n</pre> In\u00a0[15]: Copied! <pre>%%time\nlearn = cnn_learner(data, models.resnet34, metrics=error_rate)\n</pre> %%time learn = cnn_learner(data, models.resnet34, metrics=error_rate) <pre>Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.torch/models/resnet34-333f7ec4.pth\n87306240it [00:00, 96012757.25it/s]\n</pre> <pre>CPU times: user 3.29 s, sys: 1.22 s, total: 4.51 s\nWall time: 8.05 s\n</pre> In\u00a0[16]: Copied! <pre>type(learn)\n</pre> type(learn) Out[16]: <pre>fastai.basic_train.Learner</pre> In\u00a0[17]: Copied! <pre>pprint(dir(learn), compact=True)\n</pre> pprint(dir(learn), compact=True) <pre>['TTA', '__annotations__', '__class__', '__dataclass_fields__',\n '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__',\n '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__',\n '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__',\n '__new__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__',\n '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__',\n '_test_writeable_path', 'add_time', 'backward', 'bn_wd', 'callback_fns',\n 'callbacks', 'clip_grad', 'create_opt', 'data', 'destroy', 'dl', 'export',\n 'fit', 'fit_one_cycle', 'freeze', 'freeze_to', 'get_preds', 'init',\n 'interpret', 'layer_groups', 'load', 'loss_func', 'lr_find', 'lr_range',\n 'metrics', 'mixup', 'model', 'model_dir', 'opt', 'opt_func', 'path',\n 'pred_batch', 'predict', 'purge', 'save', 'show_results', 'split', 'summary',\n 'to_fp16', 'to_fp32', 'train_bn', 'true_wd', 'tta_only', 'unfreeze',\n 'validate', 'wd']\n</pre> In\u00a0[18]: Copied! <pre>learn.model\n</pre> learn.model Out[18]: <pre>Sequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): Flatten()\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25)\n    (4): Linear(in_features=1024, out_features=512, bias=True)\n    (5): ReLU(inplace)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5)\n    (8): Linear(in_features=512, out_features=37, bias=True)\n  )\n)</pre> In\u00a0[19]: Copied! <pre>%%time\nlearn.fit_one_cycle(4)\n</pre> %%time learn.fit_one_cycle(4)  Total time: 07:56 <p> epoch train_loss valid_loss error_rate time 0 1.403851 0.349996 0.105548 01:58 1 0.554456 0.267080 0.084574 01:58 2 0.333741 0.225842 0.071719 01:59 3 0.250328 0.214102 0.071042 01:59 </p> <pre>CPU times: user 1min 26s, sys: 41.8 s, total: 2min 7s\nWall time: 7min 56s\n</pre> In\u00a0[62]: Copied! <pre>help(learn.save)\n</pre> help(learn.save) <pre>Help on method save in module fastai.basic_train:\n\nsave(name:Union[pathlib.Path, str], return_path:bool=False, with_opt:bool=True) method of fastai.basic_train.Learner instance\n    Save model and optimizer state (if `with_opt`) with `name` to `self.model_dir`.\n\n</pre> In\u00a0[20]: Copied! <pre>learn.save(name='stage-1', return_path=True)\n</pre> learn.save(name='stage-1', return_path=True) Out[20]: <pre>PosixPath('/root/.fastai/data/oxford-iiit-pet/images/models/stage-1.pth')</pre> <p>Let's see what results we have got.</p> <p>We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly.</p> <p>Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.</p> In\u00a0[\u00a0]: Copied! <pre>interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\n# len(data.valid_ds)==len(losses)==len(idxs)\n</pre> interp = ClassificationInterpretation.from_learner(learn)  losses,idxs = interp.top_losses()  # len(data.valid_ds)==len(losses)==len(idxs) In\u00a0[33]: Copied! <pre>type(interp)\n</pre> type(interp) Out[33]: <pre>fastai.train.ClassificationInterpretation</pre> In\u00a0[22]: Copied! <pre>losses\n</pre> losses Out[22]: <pre>tensor([8.1622e+00, 6.3674e+00, 6.2552e+00,  ..., 3.8147e-06, 1.9073e-06,\n        1.9073e-06])</pre> In\u00a0[23]: Copied! <pre>idxs\n</pre> idxs Out[23]: <pre>tensor([ 987,  591, 1415,  ..., 1276,   73, 1166])</pre> In\u00a0[24]: Copied! <pre>len(data.valid_ds) == len(losses) == len(idxs)\n</pre> len(data.valid_ds) == len(losses) == len(idxs) Out[24]: <pre>True</pre> In\u00a0[28]: Copied! <pre>interp.plot_top_losses(9, figsize=(15,11), heatmap=False)\n</pre> interp.plot_top_losses(9, figsize=(15,11), heatmap=False) In\u00a0[27]: Copied! <pre>help(interp.plot_top_losses)\n</pre> help(interp.plot_top_losses) <pre>Help on method _cl_int_plot_top_losses in module fastai.vision.learner:\n\n_cl_int_plot_top_losses(k, largest=True, figsize=(12, 12), heatmap:bool=True, heatmap_thresh:int=16, return_fig:bool=None) -&gt; Union[matplotlib.figure.Figure, NoneType] method of fastai.train.ClassificationInterpretation instance\n    Show images in `top_losses` along with their prediction, actual, loss, and probability of predicted class.\n\n</pre> In\u00a0[29]: Copied! <pre>interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n</pre> interp.plot_confusion_matrix(figsize=(12,12), dpi=60) In\u00a0[30]: Copied! <pre>interp.most_confused(min_val=2)  # display descending order all values other than diagonal. Ignore 1s though.\n</pre> interp.most_confused(min_val=2)  # display descending order all values other than diagonal. Ignore 1s though. Out[30]: <pre>[('american_pit_bull_terrier', 'staffordshire_bull_terrier', 9),\n ('British_Shorthair', 'Russian_Blue', 6),\n ('Ragdoll', 'Birman', 6),\n ('Egyptian_Mau', 'Bengal', 3),\n ('Siamese', 'Birman', 3),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 3),\n ('yorkshire_terrier', 'havanese', 3),\n ('Bengal', 'Abyssinian', 2),\n ('Bengal', 'Egyptian_Mau', 2),\n ('Egyptian_Mau', 'Abyssinian', 2),\n ('Maine_Coon', 'Ragdoll', 2),\n ('Persian', 'Maine_Coon', 2),\n ('Ragdoll', 'Persian', 2),\n ('american_bulldog', 'american_pit_bull_terrier', 2),\n ('american_pit_bull_terrier', 'american_bulldog', 2),\n ('chihuahua', 'miniature_pinscher', 2),\n ('leonberger', 'newfoundland', 2),\n ('miniature_pinscher', 'chihuahua', 2),\n ('newfoundland', 'english_cocker_spaniel', 2),\n ('staffordshire_bull_terrier', 'american_bulldog', 2)]</pre> In\u00a0[32]: Copied! <pre>help(interp.most_confused)\n</pre> help(interp.most_confused) <pre>Help on method most_confused in module fastai.train:\n\nmost_confused(min_val:int=1, slice_size:int=1) -&gt; Collection[Tuple[str, str, int]] method of fastai.train.ClassificationInterpretation instance\n    Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences.\n\n</pre> <p>Since our model is working as we expect it to, we will unfreeze our model and train some more.</p> In\u00a0[\u00a0]: Copied! <pre>learn.unfreeze()\n</pre> learn.unfreeze() In\u00a0[36]: Copied! <pre>%%time\nlearn.fit_one_cycle(1)\n</pre> %%time learn.fit_one_cycle(1)  Total time: 02:09 <p> epoch train_loss valid_loss error_rate time 0 0.490545 0.324573 0.106225 02:09 </p> <pre>CPU times: user 26.2 s, sys: 13.4 s, total: 39.6 s\nWall time: 2min 9s\n</pre> In\u00a0[\u00a0]: Copied! <pre>learn.summary\n# for i in learn.model.children():\n#   print(i)\n</pre> learn.summary # for i in learn.model.children(): #   print(i) In\u00a0[48]: Copied! <pre>learn.load('stage-1')\n</pre> learn.load('stage-1') Out[48]: <pre>Learner(data=ImageDataBunch;\n\nTrain: LabelList (5912 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\njapanese_chin,leonberger,basset_hound,samoyed,samoyed\nPath: /root/.fastai/data/oxford-iiit-pet/images;\n\nValid: LabelList (1478 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\nnewfoundland,keeshond,chihuahua,keeshond,pug\nPath: /root/.fastai/data/oxford-iiit-pet/images;\n\nTest: None, model=Sequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): Flatten()\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25)\n    (4): Linear(in_features=1024, out_features=512, bias=True)\n    (5): ReLU(inplace)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5)\n    (8): Linear(in_features=512, out_features=37, bias=True)\n  )\n), opt_func=functools.partial(&lt;class 'torch.optim.adam.Adam'&gt;, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[&lt;function error_rate at 0x7f3bc87b4488&gt;], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/oxford-iiit-pet/images'), model_dir='models', callback_fns=[functools.partial(&lt;class 'fastai.basic_train.Recorder'&gt;, add_time=True)], callbacks=[], layer_groups=[Sequential(\n  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace)\n  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (6): ReLU(inplace)\n  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (11): ReLU(inplace)\n  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (16): ReLU(inplace)\n  (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (21): ReLU(inplace)\n  (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (23): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (24): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n  (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (26): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (27): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (28): ReLU(inplace)\n  (29): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (30): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (31): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (33): ReLU(inplace)\n  (34): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (35): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (36): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (38): ReLU(inplace)\n  (39): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n), Sequential(\n  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace)\n  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (9): ReLU(inplace)\n  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (14): ReLU(inplace)\n  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (19): ReLU(inplace)\n  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (24): ReLU(inplace)\n  (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (29): ReLU(inplace)\n  (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (32): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (34): ReLU(inplace)\n  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (36): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (37): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n  (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (39): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (41): ReLU(inplace)\n  (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (43): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (46): ReLU(inplace)\n  (47): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n), Sequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): AdaptiveMaxPool2d(output_size=1)\n  (2): Flatten()\n  (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (4): Dropout(p=0.25)\n  (5): Linear(in_features=1024, out_features=512, bias=True)\n  (6): ReLU(inplace)\n  (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (8): Dropout(p=0.5)\n  (9): Linear(in_features=512, out_features=37, bias=True)\n)], add_time=True)</pre> In\u00a0[49]: Copied! <pre>%%time\nlearn.lr_find()\n</pre> %%time learn.lr_find() <pre>LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\nCPU times: user 15.7 s, sys: 7.82 s, total: 23.5 s\nWall time: 1min 15s\n</pre> In\u00a0[50]: Copied! <pre>learn.recorder.plot()\n</pre> learn.recorder.plot() In\u00a0[51]: Copied! <pre>learn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(1e-6,1e-5))\n</pre> learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-5))  Total time: 04:17 <p> epoch train_loss valid_loss error_rate time 0 0.233426 0.213525 0.067659 02:08 1 0.214522 0.208705 0.064953 02:09 </p> <p>That's a pretty accurate model!</p> <p>Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. It will be explained later in the course and you can learn the details in the resnet paper).</p> <p>Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let's see if we can achieve a higher performance here. To help it along, let's us use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory.</p> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),\n                                   size=299, bs=bs//2).normalize(imagenet_stats)\n</pre> data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),                                    size=299, bs=bs//2).normalize(imagenet_stats) In\u00a0[53]: Copied! <pre>learn = cnn_learner(data, models.resnet50, metrics=error_rate)\n</pre> learn = cnn_learner(data, models.resnet50, metrics=error_rate) <pre>Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.torch/models/resnet50-19c8e357.pth\n102502400it [00:03, 27500065.01it/s]\n</pre> In\u00a0[54]: Copied! <pre>%%time\nlearn.lr_find()\nlearn.recorder.plot()\n</pre> %%time learn.lr_find() learn.recorder.plot() <pre>LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\nCPU times: user 32 s, sys: 28.1 s, total: 1min\nWall time: 1min 41s\n</pre> In\u00a0[\u00a0]: Copied! <pre>learn.fit_one_cycle(8)\n</pre> learn.fit_one_cycle(8)        12.50% [1/8 03:35&lt;25:09]      epoch train_loss valid_loss error_rate time 0 0.753832 0.250755 0.072395 03:35 <p>        93.48% [172/184 02:50&lt;00:11 0.4062]      </p> In\u00a0[\u00a0]: Copied! <pre>learn.save('stage-1-50')\n</pre> learn.save('stage-1-50') <p>It's astonishing that it's possible to recognize pet breeds so accurately! Let's see if full fine-tuning helps:</p> In\u00a0[\u00a0]: Copied! <pre>learn.unfreeze()\nlearn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))\n</pre> learn.unfreeze() learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4)) <pre>Total time: 03:27\nepoch  train_loss  valid_loss  error_rate\n1      0.097319    0.155017    0.048038    (01:10)\n2      0.074885    0.144853    0.044655    (01:08)\n3      0.063509    0.144917    0.043978    (01:08)\n\n</pre> <p>If it doesn't, you can always go back to your previous model.</p> In\u00a0[\u00a0]: Copied! <pre>learn.load('stage-1-50');\n</pre> learn.load('stage-1-50'); In\u00a0[\u00a0]: Copied! <pre>interp = ClassificationInterpretation.from_learner(learn)\n</pre> interp = ClassificationInterpretation.from_learner(learn) In\u00a0[\u00a0]: Copied! <pre>interp.most_confused(min_val=2)\n</pre> interp.most_confused(min_val=2) Out[\u00a0]: <pre>[('american_pit_bull_terrier', 'staffordshire_bull_terrier', 6),\n ('Bengal', 'Egyptian_Mau', 5),\n ('Bengal', 'Abyssinian', 4),\n ('boxer', 'american_bulldog', 4),\n ('Ragdoll', 'Birman', 4),\n ('Egyptian_Mau', 'Bengal', 3)]</pre> In\u00a0[\u00a0]: Copied! <pre>path = untar_data(URLs.MNIST_SAMPLE); path\n</pre> path = untar_data(URLs.MNIST_SAMPLE); path Out[\u00a0]: <pre>PosixPath('/home/ubuntu/course-v3/nbs/dl1/data/mnist_sample')</pre> In\u00a0[\u00a0]: Copied! <pre>tfms = get_transforms(do_flip=False)\ndata = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26)\n</pre> tfms = get_transforms(do_flip=False) data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26) In\u00a0[\u00a0]: Copied! <pre>data.show_batch(rows=3, figsize=(5,5))\n</pre> data.show_batch(rows=3, figsize=(5,5)) In\u00a0[\u00a0]: Copied! <pre>learn = cnn_learner(data, models.resnet18, metrics=accuracy)\nlearn.fit(2)\n</pre> learn = cnn_learner(data, models.resnet18, metrics=accuracy) learn.fit(2) <pre>Total time: 00:23\nepoch  train_loss  valid_loss  accuracy\n1      0.116117    0.029745    0.991168  (00:12)\n2      0.056860    0.015974    0.994603  (00:10)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(path/'labels.csv')\ndf.head()\n</pre> df = pd.read_csv(path/'labels.csv') df.head() Out[\u00a0]: name label 0 train/3/7463.png 0 1 train/3/21102.png 0 2 train/3/31559.png 0 3 train/3/46882.png 0 4 train/3/26209.png 0 In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28)\n</pre> data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28) In\u00a0[\u00a0]: Copied! <pre>data.show_batch(rows=3, figsize=(5,5))\ndata.classes\n</pre> data.show_batch(rows=3, figsize=(5,5)) data.classes Out[\u00a0]: <pre>[0, 1]</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24)\ndata.classes\n</pre> data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24) data.classes Out[\u00a0]: <pre>[0, 1]</pre> In\u00a0[\u00a0]: Copied! <pre>fn_paths = [path/name for name in df['name']]; fn_paths[:2]\n</pre> fn_paths = [path/name for name in df['name']]; fn_paths[:2] Out[\u00a0]: <pre>[PosixPath('/home/ubuntu/course-v3/nbs/dl1/data/mnist_sample/train/3/7463.png'),\n PosixPath('/home/ubuntu/course-v3/nbs/dl1/data/mnist_sample/train/3/21102.png')]</pre> In\u00a0[\u00a0]: Copied! <pre>pat = r\"/(\\d)/\\d+\\.png$\"\ndata = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24)\ndata.classes\n</pre> pat = r\"/(\\d)/\\d+\\.png$\" data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24) data.classes Out[\u00a0]: <pre>['3', '7']</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,\n        label_func = lambda x: '3' if '/3/' in str(x) else '7')\ndata.classes\n</pre> data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,         label_func = lambda x: '3' if '/3/' in str(x) else '7') data.classes Out[\u00a0]: <pre>['3', '7']</pre> In\u00a0[\u00a0]: Copied! <pre>labels = [('3' if '/3/' in str(x) else '7') for x in fn_paths]\nlabels[:5]\n</pre> labels = [('3' if '/3/' in str(x) else '7') for x in fn_paths] labels[:5] Out[\u00a0]: <pre>['3', '3', '3', '3', '3']</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24)\ndata.classes\n</pre> data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24) data.classes Out[\u00a0]: <pre>['3', '7']</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/dl/fastai/lesson1-pets/#image-classification-with-fastai","title":"Image classification with FastAI\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#looking-at-the-data","title":"Looking at the data\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#training-resnet34","title":"Training: resnet34\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#results","title":"Results\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#unfreezing-fine-tuning-and-learning-rates","title":"Unfreezing, fine-tuning, and learning rates\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#training-resnet50","title":"Training: resnet50\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#other-data-formats","title":"Other data formats\u00b6","text":""},{"location":"projects/dl/fastai/vision/","title":"Deep Learning on imagery using `fastai.vision` module","text":""},{"location":"projects/dl/fastai/vision/#list-of-relevant-functions-and-classes","title":"List of relevant functions and classes","text":""},{"location":"projects/dl/fastai/vision/#getting-sample-data","title":"Getting sample data","text":"<ul> <li><code>fastai.datasets</code> contains a set of curated datasets that sits on S3. You can get the list from <code>fastai.datasets.URLs</code></li> <li><code>URLs.PETS</code> for example will return the download URL.</li> <li><code>fastai.datasets.untar_data()</code> will download to a <code>.fastai/data</code> folder under local user data and will return that path as a <code>Pathlib.Path</code> object</li> </ul>"},{"location":"projects/dl/fastai/vision/#loading-image-data","title":"Loading image data","text":"<ul> <li><code>fastai.vision.data.get_image_files()</code> will scan a directory of image files and return a list of <code>Path</code> objects.</li> <li>The next step is to create an <code>ImageDataBunch</code> instance. In FastAI, <code>DataBunch</code> objects form the main way to represent and hold training and test datasets.</li> <li><code>fastai.vision.data.ImageDataBunch.from_name_re()</code> is a static, factory method allows you to construct an <code>ImageDataBunch</code> and while doing that, it can extract labels from file names. It accepts a Python Regular Expression syntax for this. You also feed it with transformations, size to resize and batch size the GPU can handle. See example below:</li> </ul> <pre><code>pat = r'/([^/]+)_\\d+.jpg$'\ndata = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs)\ndata = data.normalize(imagenet_stats)\n</code></pre> <ul> <li><code>fastai.vision.transform.get_transforms()</code> is a utility func that is used to specify and get back a list of transformation that need to applied on the DataBunch object.</li> <li>The <code>from_name_re()</code> will split the data into training and validation sets. These can be accessed via <code>data.valid_ds</code> and <code>data.train_ds</code> where, <code>data</code> is instance of <code>ImageDataBunch</code>.</li> <li><code>data.show_batch()</code> can be used to display training data in a notebook.</li> <li><code>data.classes</code> will return the label classes it has parsed using the regular expression earlier.</li> <li><code>data.batch_size</code> shows the batch size configured</li> </ul>"},{"location":"projects/dl/fastai/vision/#different-ways-of-loading-data-into-databunch-objects","title":"Different ways of loading data into DataBunch objects","text":"<ul> <li><code>data = ImageDataBunch.from_folder(path, ds_tfms, size)</code> can create it from folder, sub-folder structure</li> <li><code>data = ImageDataBunch.from_csv(path, ds_tfms, size)</code> can load it from a CSV containing file names and class values</li> <li><code>data = ImageDataBunch.from_df(path, df, ds_tfms, size)</code> can load data from a df</li> <li><code>data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms, size, label_func= lambda x:'3' if '/3/' in str(x) else '7')</code> to create from an anonymous function</li> <li><code>data = ImageDataBunch.from_lists(path, fn_paths, labels, ds_tfms, size)</code> to create from a list of class values.</li> </ul>"},{"location":"projects/dl/fastai/vision/#training","title":"Training","text":"<ul> <li><code>fastai.vision.models</code> module can list all models that are supported. For instance, <code>[mdl for mdl in dir(fastai.vision.models) if '__' not in mdl]</code> list comp will return <code>40</code> such models as of 2021.</li> </ul> <pre><code>from pprint import pprint\npprint([mdl for mdl in dir(fastai.vision.models) if '__' not in mdl], compact=True)\n\n&gt;&gt;&gt; ['BasicBlock', 'Darknet', 'DynamicUnet', 'ResLayer', 'ResNet', 'SqueezeNet',\n 'UnetBlock', 'WideResNet', 'XResNet', 'alexnet', 'darknet', 'densenet121',\n 'densenet161', 'densenet169', 'densenet201', 'mobilenet_v2', 'resnet101',\n 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'squeezenet1_0',\n 'squeezenet1_1', 'unet', 'vgg11_bn', 'vgg13_bn', 'vgg16_bn', 'vgg19_bn', 'wrn',\n 'wrn_22', 'xception', 'xresnet', 'xresnet101', 'xresnet152', 'xresnet18',\n 'xresnet18_deep', 'xresnet34', 'xresnet34_deep', 'xresnet50',\n 'xresnet50_deep']\n40\n</code></pre> <ul> <li><code>fastai.metrics.error_rate()</code> is a type of loss function, we use in training on images</li> <li><code>fastai.vision.learner.cnn_learner()</code> is a static, factory method that creates a convolutional neural network based on the backbone and loss function specified. For instance, <code>learn = cnn_learner(data, models.resnet34, metrics=error_rate)</code>.<ul> <li>Note, when creating the learner, you pass the whole data bunch - including both training and test data.</li> <li>The <code>error_rate</code> function will help in evaluating the performance on both the training data as well as the validation data.</li> </ul> </li> <li><code>learn.fit_one_cycle(cyc_len=4)</code> is used to train the <code>restnet34</code> model. The cycle length parameter determines how many times to repeat the one cycle learning. The output of this cell shows the following:</li> </ul> <pre><code>epoch   train_loss  valid_loss  error_rate  time\n0       1.361700    0.337071    0.104195    02:24\n1       0.601790    0.297722    0.089310    02:07\n2       0.380089    0.282888    0.079838    02:26\n3       0.271350    0.246164    0.071719    02:07\n\nWall time: 9min 6s\n</code></pre> <ul> <li>The output above shows at end of epoch 4, we have an error rate of <code>0.071</code> which means about <code>92.9%</code> accuracy.</li> <li>Calling <code>learn.summary()</code> returns you a high level info on each of the layers in the DL model along with summary info at the end.</li> <li>Finally, save the model by calling <code>learn.save(file='pets-lesson01-stage1', return_path=True)</code> which will return the path to the model file on disk, such as: <code>PosixPath('/Users/atma6951/.fastai/data/oxford-iiit-pet/images/models/pets-lesson01-stage-1.pth)</code> and weighs about <code>90 MB</code> in size. By default, Fastai tries to keep the models in the same location as the data bunch.</li> </ul>"},{"location":"projects/dl/fastai/vision/#model-accuracy","title":"Model accuracy","text":"<p>Once training is complete, you can use the following tools to evaluate the accuracy.</p> <ul> <li>Use <code>interp = fastai.train.ClassificationInterpretation.from_learner(learn)</code> to create an instance of <code>ClassificationInterpretation</code> class. Running this takes a while as fastai will compute the accuracy of each of the result in the validation dataset.</li> <li><code>interp.top_losses()</code> will return a tuple of losses and indices which correspond to loss value and index of that dataset in the <code>data.valid_ds</code> bunch. Since the function sorts the data by descending loss value, it supplies the index to match with original dataset.</li> <li><code>interp.plot_top_losses(k=9, figsize=(7,7))</code> will plot the top losses in a matrix along with the <code>predicted / actual / loss / probability</code> values as annotations.</li> <li><code>interp.plot_confusion_matrix(figsize=(16,16))</code> will plot the seaborn style confusion matrix with heatmap. For a <code>37</code> class problem like the pets, this matrix gets hard to read. When num classes is high and accuracy is also generally high, use,</li> <li><code>interp.most_confused(min_val=2)</code> will return a list of tuples - containing <code>prediction, actual, num_confusions</code>. The <code>min_val=2</code> tells the API to ignore cases where just <code>1</code> file is misclassified. It is essentially, the descending order of non-diagonal cells in the confusion matrix.</li> </ul>"},{"location":"projects/dl/fastai/vision/#model-fine-tuning","title":"Model fine-tuning","text":"<p>So far, the <code>fit_one_cycle()</code> method was used on <code>4</code> epochs and the training went fairly quickly (<code>10</code> mins). This is because, the <code>cnn_learner()</code> produced a model that is based on <code>resnet32</code> and added a few layers to the end. The <code>fit_one_cycle()</code> trained only those last few layers and left most of the earlier ones intact.</p> <p>This (transfer learning) principle works fairly well and gets us about <code>92%</code> accuracy. To improve this further, we need to <code>unfreeze</code> all the layers in the models and adjust their weights during the training.</p> <ul> <li><code>learn.unfreeze()</code> will unfreeze the whole model - backbone and the additional layers</li> <li><code>learn.fit_one_cycle(1)</code> thereafter will try to teach / change weights on the whole model. Often this results in lower accuracy because the initial layers of the model need not be changed as much since they often do preliminary work compared to the later layers which do the actual classification.</li> </ul> <p>To resolve the lower accuracy issue, we need to introduce learning rate and modify the weights of the initial layers much less frequently than those of the later layers. Learning rate is usually specified as a list of floats to match each layer in the model. In Fastai, we use the <code>slice(low, high)</code> Python function to evenly distribute LR between the first and last layers of the model.</p> <ul> <li>Run <code>learn.lr_find()</code> where the API will evaluate various learning rates and find the loss for each.</li> <li>Run <code>learn.recorder.plot(suggestion=True, show_grid=True)</code> to view the learning rate plot. It looks like below:</li> </ul> <p></p> <p>In general, we are not looking for learning at lowest loss, but for rate at the steepest segment of the loss curve. Using this suggestion, you can run</p> <pre><code>%%time\nlearn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(1e-6,1e-5))  # LR chosen from the suggestion in the image\n\n&gt;&gt;&gt;\nepoch   train_loss  valid_loss  error_rate  time\n    0   0.230512    0.231129    0.067659    02:13\n    1   0.246171    0.228836    0.064276    02:15\n\nWall time: 4min 28s\n</code></pre> <p>With an additional 4.5 minutes, we improved the accuracy to <code>93%</code>. You can then save that model as <code>learn.save('pets-lesson01-stage-2', return_path=True)</code> for later.</p>"},{"location":"projects/dl/fastai/vision/#predict-on-real-world-data","title":"Predict on real world data","text":"<p>To predict on any given image, use the <code>fastai.vision.image.open_image()</code> function to load an image. You get back an <code>fastai.vision.image.Image</code> object that can be passed to <code>learn.predict()</code> function.</p> <p>The prediction is a <code>tuple</code> of (<code>Category</code>, <code>category index</code>, <code>probabilities for each class</code>).</p>"},{"location":"projects/dl/fastai/vision/#hyper-parameter-tuning","title":"Hyper-parameter tuning","text":""},{"location":"projects/dl/fastai/vision/#learning-rate-too-high","title":"Learning rate too high","text":"<p>When the rate it too high, the validation loss gets obscenely high - like an impossible number. The default <code>max_lr</code> is <code>0.003</code>.</p>"},{"location":"projects/dl/fastai/vision/#learning-rate-too-low","title":"Learning rate too low","text":"<p>When the rate is too small, the model's validation drops, but very very slowly. The command <code>learn.recorder.plot_losses()</code> will plot the validation and training loss for visual interpretation. You can bump the rate by a factor of <code>10</code> or <code>100</code> and try again.</p>"},{"location":"projects/dl/fastai/vision/#training-loss-validation-loss","title":"Training loss &gt; Validation loss","text":"<p>When a model is properly trained, the training loss is typically lower than validation loss. If the training loss is greater, it means the model is not trained enough - try increasing number of epochs or increase the learning rate.</p>"},{"location":"projects/dl/fastai/vision/#too-few-epochs","title":"Too few epochs","text":"<p>Too few epochs and too low LR look alike. For instance when you train for just 1 epoch, the training loss might be greater than validation loss. Or, the difference between training and validation might be too high. Try increasing epochs or the LR.</p>"},{"location":"projects/dl/fastai/vision/#too-many-epochs","title":"Too many epochs","text":"<p>Too many epochs is too much training and can lead to <code>overfitting</code>. However, it is quite difficult to overfit in deep learning in practice. A sign of overfitting is when the model error starts increasing after a few epochs.</p>"},{"location":"projects/fun/","title":"Fun projects","text":"<p>Some uncategorized projects:</p> <ul> <li>Model complexity vs accuracy - an empirical analysis</li> <li>Verifying central limit theorem</li> <li>Verifying central limit theorem in regression</li> </ul>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/","title":"Model complexity vs accuracy - empirical anlaysis","text":"In\u00a0[101]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error <p>Create a linear stream of <code>10</code>million points between <code>-50</code> and <code>50</code>.</p> In\u00a0[102]: Copied! <pre>x = np.arange(-50,50,0.00001)\nx.shape\n</pre> x = np.arange(-50,50,0.00001) x.shape Out[102]: <pre>(10000000,)</pre> <p>Create random noise of same dimension</p> In\u00a0[103]: Copied! <pre>bias = np.random.standard_normal(x.shape)\n</pre> bias = np.random.standard_normal(x.shape) In\u00a0[104]: Copied! <pre>y2 = np.cos(x)**3 * (x**2/max(x)) + bias*5\n</pre> y2 = np.cos(x)**3 * (x**2/max(x)) + bias*5 In\u00a0[105]: Copied! <pre>x_train, x_test, y_train, y_test = train_test_split(x,y2, test_size=0.3)\n</pre> x_train, x_test, y_train, y_test = train_test_split(x,y2, test_size=0.3) In\u00a0[106]: Copied! <pre>x_train.shape\n</pre> x_train.shape Out[106]: <pre>(7000000,)</pre> <p>Plotting algorithms cannot work with millions of points, so you downsample just for plotting</p> In\u00a0[107]: Copied! <pre>stepper = int(x_train.shape[0]/1000)\nstepper\n</pre> stepper = int(x_train.shape[0]/1000) stepper Out[107]: <pre>7000</pre> In\u00a0[108]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(13,8))\nax.scatter(x[::stepper],y2[::stepper], marker='d')\nax.set_title('Distribution of training points')\n</pre> fig, ax = plt.subplots(1,1, figsize=(13,8)) ax.scatter(x[::stepper],y2[::stepper], marker='d') ax.set_title('Distribution of training points') Out[108]: <pre>Text(0.5,1,'Distribution of training points')</pre> In\u00a0[109]: Copied! <pre>def greedy_fitter(x_train, y_train, x_test, y_test, max_order=25):\n    \"\"\"Fitter will try to find the best order of \n    polynomial curve fit for the given synthetic data\"\"\"\n    import time\n    train_predictions=[]\n    train_rmse=[]\n        \n    test_predictions=[]\n    test_rmse=[]\n    \n    for order in range(1,max_order+1):\n        t1 = time.time()\n        coeff = np.polyfit(x_train, y_train, deg=order)\n        n_order = order\n        count = 0\n        y_predict = np.zeros(x_train.shape)\n        while n_order &gt;=0:\n            y_predict += coeff[count]*x_train**n_order\n            count+=1\n            n_order = n_order-1\n        \n        # append to predictions\n        train_predictions.append(y_predict)\n        \n        # find training errors\n        current_train_rmse =np.sqrt(mean_squared_error(y_train, y_predict))\n        train_rmse.append(current_train_rmse)\n        \n        # predict and find test errors\n        n_order = order\n        count = 0\n        y_predict_test = np.zeros(x_test.shape)\n        while n_order &gt;=0:\n            y_predict_test += coeff[count]*x_test**n_order\n            count+=1\n            n_order = n_order-1\n        \n        # append test predictions\n        test_predictions.append(y_predict_test)\n        # find test errors\n        current_test_rmse =np.sqrt(mean_squared_error(y_test, y_predict_test))\n        test_rmse.append(current_test_rmse)\n        \n        t2 = time.time()\n        elapsed = round(t2-t1, 3)\n        print(\"Elapsed: \" + str(elapsed) + \\\n              \"s Order: \" + str(order) + \\\n              \" Train RMSE: \" + str(round(current_train_rmse, 4)) + \\\n              \" Test RMSE: \" + str(round(current_test_rmse, 4)))\n    \n    return (train_predictions, train_rmse, test_predictions, test_rmse)\n</pre> def greedy_fitter(x_train, y_train, x_test, y_test, max_order=25):     \"\"\"Fitter will try to find the best order of      polynomial curve fit for the given synthetic data\"\"\"     import time     train_predictions=[]     train_rmse=[]              test_predictions=[]     test_rmse=[]          for order in range(1,max_order+1):         t1 = time.time()         coeff = np.polyfit(x_train, y_train, deg=order)         n_order = order         count = 0         y_predict = np.zeros(x_train.shape)         while n_order &gt;=0:             y_predict += coeff[count]*x_train**n_order             count+=1             n_order = n_order-1                  # append to predictions         train_predictions.append(y_predict)                  # find training errors         current_train_rmse =np.sqrt(mean_squared_error(y_train, y_predict))         train_rmse.append(current_train_rmse)                  # predict and find test errors         n_order = order         count = 0         y_predict_test = np.zeros(x_test.shape)         while n_order &gt;=0:             y_predict_test += coeff[count]*x_test**n_order             count+=1             n_order = n_order-1                  # append test predictions         test_predictions.append(y_predict_test)         # find test errors         current_test_rmse =np.sqrt(mean_squared_error(y_test, y_predict_test))         test_rmse.append(current_test_rmse)                  t2 = time.time()         elapsed = round(t2-t1, 3)         print(\"Elapsed: \" + str(elapsed) + \\               \"s Order: \" + str(order) + \\               \" Train RMSE: \" + str(round(current_train_rmse, 4)) + \\               \" Test RMSE: \" + str(round(current_test_rmse, 4)))          return (train_predictions, train_rmse, test_predictions, test_rmse) <p>Run the model. Change the <code>max_order</code> to higher or lower if you wish</p> In\u00a0[110]: Copied! <pre>%%time\ncomplexity=50\ntrain_predictions, train_rmse, test_predictions, test_rmse = greedy_fitter(\n    x_train, y_train, x_test, y_test, max_order=complexity)\n</pre> %%time complexity=50 train_predictions, train_rmse, test_predictions, test_rmse = greedy_fitter(     x_train, y_train, x_test, y_test, max_order=complexity) <pre>Elapsed: 0.826s Order: 1 Train RMSE: 13.1708 Test RMSE: 13.1646\nElapsed: 1.264s Order: 2 Train RMSE: 13.1646 Test RMSE: 13.1582\nElapsed: 2.061s Order: 3 Train RMSE: 13.1646 Test RMSE: 13.1582\nElapsed: 2.727s Order: 4 Train RMSE: 13.1627 Test RMSE: 13.1564\nElapsed: 3.4s Order: 5 Train RMSE: 13.1627 Test RMSE: 13.1564\nElapsed: 4.144s Order: 6 Train RMSE: 13.1585 Test RMSE: 13.1519\nElapsed: 5.01s Order: 7 Train RMSE: 13.1585 Test RMSE: 13.1519\nElapsed: 5.749s Order: 8 Train RMSE: 13.0983 Test RMSE: 13.0891\nElapsed: 6.43s Order: 9 Train RMSE: 13.0983 Test RMSE: 13.0891\nElapsed: 7.193s Order: 10 Train RMSE: 12.876 Test RMSE: 12.865\nElapsed: 7.955s Order: 11 Train RMSE: 12.876 Test RMSE: 12.865\nElapsed: 8.777s Order: 12 Train RMSE: 12.4236 Test RMSE: 12.4185\nElapsed: 9.727s Order: 13 Train RMSE: 12.4236 Test RMSE: 12.4185\nElapsed: 10.495s Order: 14 Train RMSE: 11.9035 Test RMSE: 11.9015\nElapsed: 11.452s Order: 15 Train RMSE: 11.9035 Test RMSE: 11.9014\nElapsed: 11.929s Order: 16 Train RMSE: 11.6687 Test RMSE: 11.6657\nElapsed: 12.827s Order: 17 Train RMSE: 11.6687 Test RMSE: 11.6657\nElapsed: 13.863s Order: 18 Train RMSE: 11.6666 Test RMSE: 11.6638\nElapsed: 15.234s Order: 19 Train RMSE: 11.6666 Test RMSE: 11.6638\nElapsed: 15.793s Order: 20 Train RMSE: 11.2828 Test RMSE: 11.2825\nElapsed: 16.477s Order: 21 Train RMSE: 11.2828 Test RMSE: 11.2825\nElapsed: 18.752s Order: 22 Train RMSE: 10.6544 Test RMSE: 10.6509\nElapsed: 19.699s Order: 23 Train RMSE: 10.6544 Test RMSE: 10.6509\nElapsed: 20.26s Order: 24 Train RMSE: 10.6051 Test RMSE: 10.601\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 20.433s Order: 25 Train RMSE: 10.6051 Test RMSE: 10.601\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 20.777s Order: 26 Train RMSE: 10.6168 Test RMSE: 10.613\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 20.747s Order: 27 Train RMSE: 10.6168 Test RMSE: 10.613\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 22.231s Order: 28 Train RMSE: 9.7878 Test RMSE: 9.7872\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 23.836s Order: 29 Train RMSE: 9.7878 Test RMSE: 9.7872\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 25.725s Order: 30 Train RMSE: 9.5223 Test RMSE: 9.5227\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 25.587s Order: 31 Train RMSE: 9.5223 Test RMSE: 9.5227\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 25.041s Order: 32 Train RMSE: 9.3192 Test RMSE: 9.3201\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 26.645s Order: 33 Train RMSE: 9.3192 Test RMSE: 9.3201\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 27.387s Order: 34 Train RMSE: 9.2033 Test RMSE: 9.2045\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 28.049s Order: 35 Train RMSE: 9.2033 Test RMSE: 9.2045\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 29.866s Order: 36 Train RMSE: 9.1679 Test RMSE: 9.1692\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 31.415s Order: 37 Train RMSE: 9.1679 Test RMSE: 9.1692\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 33.605s Order: 38 Train RMSE: 9.1874 Test RMSE: 9.1887\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 33.52s Order: 39 Train RMSE: 9.1874 Test RMSE: 9.1886\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 33.863s Order: 40 Train RMSE: 9.1526 Test RMSE: 9.1539\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 34.658s Order: 41 Train RMSE: 9.1526 Test RMSE: 9.1539\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 35.006s Order: 42 Train RMSE: 9.0739 Test RMSE: 9.0755\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 35.865s Order: 43 Train RMSE: 9.0739 Test RMSE: 9.0755\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 36.595s Order: 44 Train RMSE: 8.3806 Test RMSE: 8.3852\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 39.269s Order: 45 Train RMSE: 8.3806 Test RMSE: 8.3852\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 38.545s Order: 46 Train RMSE: 8.4328 Test RMSE: 8.4372\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 42.502s Order: 47 Train RMSE: 8.4328 Test RMSE: 8.4372\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 41.427s Order: 48 Train RMSE: 8.5054 Test RMSE: 8.5096\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 43.643s Order: 49 Train RMSE: 8.5054 Test RMSE: 8.5096\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 43.055s Order: 50 Train RMSE: 8.5792 Test RMSE: 8.5831\nCPU times: user 41min 15s, sys: 4min 3s, total: 45min 19s\nWall time: 17min 31s\n</pre> In\u00a0[111]: Copied! <pre>%%time\nfig, axes = plt.subplots(1,1, figsize=(15,15))\naxes.scatter(x_train[::stepper], y_train[::stepper], \n             label='Original data', color='gray', marker='x')\norder=1\nfor p, r in zip(train_predictions, train_rmse):\n    axes.scatter(x_train[:stepper], p[:stepper], \n                 label='O: ' + str(order) + \" RMSE: \" + str(round(r,2)),\n                 marker='.')\n    order+=1\naxes.legend(loc=0)\naxes.set_title('Performance against training data')\n</pre> %%time fig, axes = plt.subplots(1,1, figsize=(15,15)) axes.scatter(x_train[::stepper], y_train[::stepper],               label='Original data', color='gray', marker='x') order=1 for p, r in zip(train_predictions, train_rmse):     axes.scatter(x_train[:stepper], p[:stepper],                   label='O: ' + str(order) + \" RMSE: \" + str(round(r,2)),                  marker='.')     order+=1 axes.legend(loc=0) axes.set_title('Performance against training data') <pre>CPU times: user 1.1 s, sys: 39.6 ms, total: 1.14 s\nWall time: 918 ms\n</pre> In\u00a0[112]: Copied! <pre>%%time\nfig, axes = plt.subplots(1,1, figsize=(15,15))\naxes.scatter(x_test[::stepper], y_test[::stepper], \n             label='Test data', color='gray', marker='x')\norder=1\nfor p, r in zip(test_predictions, test_rmse):\n    axes.scatter(x_test[:stepper], p[:stepper], \n                 label='O: ' + str(order) + \" RMSE: \" + str(round(r,2)),\n                 marker='.')\n    order+=1\naxes.legend(loc=0)\naxes.set_title('Performance against test data')\n</pre> %%time fig, axes = plt.subplots(1,1, figsize=(15,15)) axes.scatter(x_test[::stepper], y_test[::stepper],               label='Test data', color='gray', marker='x') order=1 for p, r in zip(test_predictions, test_rmse):     axes.scatter(x_test[:stepper], p[:stepper],                   label='O: ' + str(order) + \" RMSE: \" + str(round(r,2)),                  marker='.')     order+=1 axes.legend(loc=0) axes.set_title('Performance against test data') <pre>CPU times: user 893 ms, sys: 25.9 ms, total: 919 ms\nWall time: 901 ms\n</pre> In\u00a0[120]: Copied! <pre>ax = plt.plot(np.arange(1,complexity+1),test_rmse)\nplt.title('Bias vs Complexity'); plt.xlabel('Order of polynomial'); plt.ylabel('Test RMSE')\nax[0].axes.get_yaxis().get_major_formatter().set_useOffset(False)\nplt.savefig('Model efficiency.png')\n</pre> ax = plt.plot(np.arange(1,complexity+1),test_rmse) plt.title('Bias vs Complexity'); plt.xlabel('Order of polynomial'); plt.ylabel('Test RMSE') ax[0].axes.get_yaxis().get_major_formatter().set_useOffset(False) plt.savefig('Model efficiency.png') <p></p>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#model-complexity-vs-accuracy-empirical-anlaysis","title":"Model complexity vs accuracy - empirical anlaysis\u00b6","text":"<p>This notebook is intended as a moderate stress test for the DSX infrastructure. Notebook generates a known function, adds random noise to it and runs an ML algorithm on a wild goose chase asking it to fit and predict based on this data.</p> <p>Right now I am running this against 10 Million points. To increase the complexity, you can do two things</p> <ul> <li>Increase the number of points (direct hit)</li> <li>Increase the complexity of the function (indirect)</li> </ul>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#define-the-function","title":"Define the function\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#train-test-split","title":"Train test split\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#curve-fitting","title":"Curve fitting\u00b6","text":"<p>Let us define a function that will try to fit against the training data. It starts with lower order and sequentially increases the complexity of the model. The hope is, somewhere here is the sweet spot of low bias and variance. We will find it empirically</p>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#plot-results","title":"Plot results\u00b6","text":"<p>How well did the models fit against training data?</p>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#training-results","title":"Training results\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#test-results","title":"Test results\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#bias-vs-variance","title":"Bias vs Variance\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#cpu-usage-during-curve-fitting","title":"CPU usage during curve fitting\u00b6","text":""},{"location":"projects/fun/verifying_central_limit_theorem/","title":"Verifying Central Limit Theorem","text":"<p>The Central Limit Theorem states that the sampling distribution of the sampling means approaches a normal distribution as the sample size gets larger \u2014 no matter what the shape of the population distribution. This fact holds especially true for sample sizes over 30. All this is saying is that as you take more samples, especially large ones, your graph of the sample means will look more like a normal distribution.</p> In\u00a0[11]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline In\u00a0[2]: Copied! <pre>rand_1k = np.random.randint(0,100,1000)\n</pre> rand_1k = np.random.randint(0,100,1000) In\u00a0[3]: Copied! <pre>rand_1k.size\n</pre> rand_1k.size Out[3]: <pre>1000</pre> In\u00a0[12]: Copied! <pre>sns.distplot(rand_1k)\n</pre> sns.distplot(rand_1k) <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a19f2c048&gt;</pre> <p>Thus the population follows a <code>uniform</code> distribution, not a <code>normal</code> distribution. Still, we will see the distribution of our means will follow a <code>normal</code> distribution.</p> In\u00a0[4]: Copied! <pre>np.mean(rand_1k)\n</pre> np.mean(rand_1k) Out[4]: <pre>48.826</pre> In\u00a0[5]: Copied! <pre>subset_100 = np.random.choice(rand_1k, size=100, replace=False)\nsubset_100.size\n</pre> subset_100 = np.random.choice(rand_1k, size=100, replace=False) subset_100.size Out[5]: <pre>100</pre> In\u00a0[6]: Copied! <pre>np.mean(subset_100)\n</pre> np.mean(subset_100) Out[6]: <pre>43.2</pre> <p>The mean of this subset of <code>100</code> integers is <code>43.2</code>. Not close enough.</p> In\u00a0[7]: Copied! <pre># generate 50 random samples of size 100 each\nsubset_means = []\nfor i in range(0,50):\n    current_subset = np.random.choice(rand_1k, size=100, replace=False)\n    subset_means.append(np.mean(current_subset))\n</pre> # generate 50 random samples of size 100 each subset_means = [] for i in range(0,50):     current_subset = np.random.choice(rand_1k, size=100, replace=False)     subset_means.append(np.mean(current_subset)) <p>Calculate the mean of means (its meta :))</p> In\u00a0[33]: Copied! <pre>clt_mean = np.mean(subset_means)\nclt_mean\n</pre> clt_mean = np.mean(subset_means) clt_mean Out[33]: <pre>48.9768</pre> <p>Calculate the SD of the means</p> In\u00a0[34]: Copied! <pre>subset_sd = np.std(subset_means)\nsubset_sd\n</pre> subset_sd = np.std(subset_means) subset_sd Out[34]: <pre>2.657234983963594</pre> In\u00a0[37]: Copied! <pre>ax = sns.distplot(subset_means, bins=10)\n# draw mean in black\nax.axvline(clt_mean, color='black', linestyle='dashed')\n\n# draw mean +- 1 SD\nax.axvline(clt_mean + subset_sd, color='red', linestyle='dotted')\nax.axvline(clt_mean - subset_sd, color='red', linestyle='dotted')\n</pre> ax = sns.distplot(subset_means, bins=10) # draw mean in black ax.axvline(clt_mean, color='black', linestyle='dashed')  # draw mean +- 1 SD ax.axvline(clt_mean + subset_sd, color='red', linestyle='dotted') ax.axvline(clt_mean - subset_sd, color='red', linestyle='dotted') <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> Out[37]: <pre>&lt;matplotlib.lines.Line2D at 0x1a1ac5f908&gt;</pre> <p>Difference between mean of means and the population mean</p> In\u00a0[38]: Copied! <pre>np.mean(rand_1k) - clt_mean\n</pre> np.mean(rand_1k) - clt_mean Out[38]: <pre>-0.15079999999999671</pre>"},{"location":"projects/fun/verifying_central_limit_theorem/#verifying-central-limit-theorem","title":"Verifying Central Limit Theorem\u00b6","text":""},{"location":"projects/fun/verifying_central_limit_theorem/#generate-1k-random-integers","title":"Generate 1k random integers\u00b6","text":"<p>Let us use NumPy to generate <code>1000</code> random integers between the range <code>0-100</code>. Our objective is to calculate the population mean and verify if the mean obtained using CLT comes close to population mean.</p>"},{"location":"projects/fun/verifying_central_limit_theorem/#calculate-population-mean","title":"Calculate population mean\u00b6","text":""},{"location":"projects/fun/verifying_central_limit_theorem/#try-out-creating-a-subset-and-finding-its-mean","title":"Try out creating a subset and finding its mean\u00b6","text":""},{"location":"projects/fun/verifying_central_limit_theorem/#apply-clt","title":"Apply CLT.\u00b6","text":"<p>We will generate <code>50</code> samples with <code>100</code> items each and find their means.</p>"},{"location":"projects/fun/verifying_clt_in_regression/","title":"Verifying Central Limit Theorem in regression","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings('ignore') In\u00a0[80]: Copied! <pre>rand_1kx = np.random.randint(0,100,1000)\nx_mean = np.mean(rand_1kx)\nx_sd = np.std(rand_1kx)\nx_mean\n</pre> rand_1kx = np.random.randint(0,100,1000) x_mean = np.mean(rand_1kx) x_sd = np.std(rand_1kx) x_mean Out[80]: <pre>49.954</pre> In\u00a0[81]: Copied! <pre>pop_intercept = 30\npop_slope = 1.8\nerror_boost = 10\npop_error = np.random.standard_normal(size = rand_1kx.size) * error_boost\n# I added an error booster since without it, the correlation was too high.\n\ny = pop_intercept + pop_slope*rand_1kx + pop_error\ny_mean = np.mean(y)\ny_sd = np.std(y)\ny_mean\n</pre> pop_intercept = 30 pop_slope = 1.8 error_boost = 10 pop_error = np.random.standard_normal(size = rand_1kx.size) * error_boost # I added an error booster since without it, the correlation was too high.  y = pop_intercept + pop_slope*rand_1kx + pop_error y_mean = np.mean(y) y_sd = np.std(y) y_mean Out[81]: <pre>119.4183378140413</pre> <p>Make a scatter plot of <code>X</code> and <code>y</code> variables.</p> In\u00a0[82]: Copied! <pre>sns.jointplot(rand_1kx, y)\n</pre> sns.jointplot(rand_1kx, y) Out[82]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a1700b160&gt;</pre> <p><code>X</code> and <code>y</code> follow <code>uniform</code> distribution, but the error $\\epsilon$ is generated from <code>standard normal distribution</code> with a boosting factor. Let us plot its histogram to verify the distribution</p> In\u00a0[83]: Copied! <pre>sns.distplot(pop_error)\n</pre> sns.distplot(pop_error) Out[83]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a171de8d0&gt;</pre> In\u00a0[84]: Copied! <pre>from sklearn.linear_model import LinearRegression\nX_train_full = rand_1kx.reshape(-1,1)\ny_train_full = y.reshape(-1,1)\n</pre> from sklearn.linear_model import LinearRegression X_train_full = rand_1kx.reshape(-1,1) y_train_full = y.reshape(-1,1) In\u00a0[85]: Copied! <pre>y_train_full.shape\n</pre> y_train_full.shape Out[85]: <pre>(1000, 1)</pre> In\u00a0[86]: Copied! <pre>lm.fit(X_train, y_train)\n\n#print the linear model built\npredicted_pop_slope = lm.coef_[0][0]\npredicted_pop_intercept = lm.intercept_[0]\n\nprint(\"y = \" + str(predicted_pop_slope) + \"*X\" + \" + \" + str(predicted_pop_intercept))\n</pre> lm.fit(X_train, y_train)  #print the linear model built predicted_pop_slope = lm.coef_[0][0] predicted_pop_intercept = lm.intercept_[0]  print(\"y = \" + str(predicted_pop_slope) + \"*X\" + \" + \" + str(predicted_pop_intercept)) <pre>y = 1.795560991921382*X + 30.718916711669976\n</pre> In\u00a0[87]: Copied! <pre>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(rand_1kx, y, test_size=0.33)\nprint(X_train.size)\n\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n</pre> from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(rand_1kx, y, test_size=0.33) print(X_train.size)  from sklearn.linear_model import LinearRegression lm = LinearRegression() <pre>670\n</pre> In\u00a0[88]: Copied! <pre>X_train = X_train.reshape(-1,1)\nX_test = X_test.reshape(-1,1)\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n</pre> X_train = X_train.reshape(-1,1) X_test = X_test.reshape(-1,1) y_train = y_train.reshape(-1,1) y_test = y_test.reshape(-1,1) In\u00a0[89]: Copied! <pre>y_train.shape\n</pre> y_train.shape Out[89]: <pre>(670, 1)</pre> In\u00a0[90]: Copied! <pre>lm.fit(X_train, y_train)\n\n#print the linear model built\npredicted_subset_slope = lm.coef_[0][0]\npredicted_subset_intercept = lm.intercept_[0]\n\nprint(\"y = \" + str(predicted_subset_slope) + \"*X\" \n      + \" + \" + str(predicted_subset_intercept))\n</pre> lm.fit(X_train, y_train)  #print the linear model built predicted_subset_slope = lm.coef_[0][0] predicted_subset_intercept = lm.intercept_[0]  print(\"y = \" + str(predicted_subset_slope) + \"*X\"        + \" + \" + str(predicted_subset_intercept)) <pre>y = 1.794887898705644*X + 29.857924099881075\n</pre> In\u00a0[95]: Copied! <pre>y_predicted = lm.predict(X_test)\nresiduals = y_test - y_predicted\n</pre> y_predicted = lm.predict(X_test) residuals = y_test - y_predicted <p>Fitted vs Actual scatter</p> In\u00a0[96]: Copied! <pre>jax = sns.jointplot(y_test, y_predicted)\njax.set_axis_labels(xlabel='Y', ylabel='Predicted Y')\n</pre> jax = sns.jointplot(y_test, y_predicted) jax.set_axis_labels(xlabel='Y', ylabel='Predicted Y') Out[96]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a179c9048&gt;</pre> In\u00a0[98]: Copied! <pre>dax = sns.distplot(residuals)\ndax.set_title('Distribution of residuals')\n</pre> dax = sns.distplot(residuals) dax.set_title('Distribution of residuals') Out[98]: <pre>Text(0.5,1,'Distribution of residuals')</pre> In\u00a0[99]: Copied! <pre>jax = sns.jointplot(y_predicted, residuals)\njax.set_axis_labels(xlabel='Predicted Y', ylabel='Residuals')\n</pre> jax = sns.jointplot(y_predicted, residuals) jax.set_axis_labels(xlabel='Predicted Y', ylabel='Residuals') Out[99]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a1629f8d0&gt;</pre> In\u00a0[100]: Copied! <pre>jax = sns.jointplot(y_test, residuals)\njax.set_axis_labels(xlabel='Y', ylabel='Residuals')\n</pre> jax = sns.jointplot(y_test, residuals) jax.set_axis_labels(xlabel='Y', ylabel='Residuals') Out[100]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a16437eb8&gt;</pre> In\u00a0[101]: Copied! <pre>pop_df = pd.DataFrame(data={'x':rand_1kx, 'y':y})\npop_df.head()\n</pre> pop_df = pd.DataFrame(data={'x':rand_1kx, 'y':y}) pop_df.head() Out[101]: x y 0 38 85.149359 1 58 130.858406 2 15 67.280103 3 56 125.509595 4 19 55.793980 In\u00a0[102]: Copied! <pre>pop_df.shape\n</pre> pop_df.shape Out[102]: <pre>(1000, 2)</pre> In\u00a0[103]: Copied! <pre>sample_slopes = []\nsample_intercepts = []\n\nfor i in range(0,50):\n    # perform a choice on dataframe index\n    sample_index = np.random.choice(pop_df.index, size=50)\n    \n    # select the subset using that index\n    sample_df = pop_df.iloc[sample_index]\n    \n    # convert to numpy and reshape the matrix for lm.fit\n    sample_x = np.array(sample_df['x']).reshape(-1,1)\n    sample_y = np.array(sample_df['y']).reshape(-1,1)\n    \n    lm.fit(X=sample_x, y=sample_y)\n    \n    sample_slopes.append(lm.coef_[0][0])\n    sample_intercepts.append(lm.intercept_[0])\n</pre> sample_slopes = [] sample_intercepts = []  for i in range(0,50):     # perform a choice on dataframe index     sample_index = np.random.choice(pop_df.index, size=50)          # select the subset using that index     sample_df = pop_df.iloc[sample_index]          # convert to numpy and reshape the matrix for lm.fit     sample_x = np.array(sample_df['x']).reshape(-1,1)     sample_y = np.array(sample_df['y']).reshape(-1,1)          lm.fit(X=sample_x, y=sample_y)          sample_slopes.append(lm.coef_[0][0])     sample_intercepts.append(lm.intercept_[0]) <p>Plot the distribution of sample slopes and intercepts</p> In\u00a0[104]: Copied! <pre>mean_sample_slope = np.mean(sample_slopes)\nmean_sample_intercept = np.mean(sample_intercepts)\n\nfig, ax = plt.subplots(1,2, figsize=(15,6))\n\n# plot sample slopes\nsns.distplot(sample_slopes, ax=ax[0])\nax[0].set_title('Distribution of sample slopes. Mean: ' \n                + str(round(mean_sample_slope, 2)))\nax[0].axvline(mean_sample_slope, color='black')\n\n# plot sample slopes\nsns.distplot(sample_intercepts, ax=ax[1])\nax[1].set_title('Distribution of sample intercepts. Mean: ' \n                + str(round(mean_sample_intercept,2)))\nax[1].axvline(mean_sample_intercept, color='black')\n</pre> mean_sample_slope = np.mean(sample_slopes) mean_sample_intercept = np.mean(sample_intercepts)  fig, ax = plt.subplots(1,2, figsize=(15,6))  # plot sample slopes sns.distplot(sample_slopes, ax=ax[0]) ax[0].set_title('Distribution of sample slopes. Mean: '                  + str(round(mean_sample_slope, 2))) ax[0].axvline(mean_sample_slope, color='black')  # plot sample slopes sns.distplot(sample_intercepts, ax=ax[1]) ax[1].set_title('Distribution of sample intercepts. Mean: '                  + str(round(mean_sample_intercept,2))) ax[1].axvline(mean_sample_intercept, color='black') Out[104]: <pre>&lt;matplotlib.lines.Line2D at 0x1a17ed97b8&gt;</pre> In\u00a0[114]: Copied! <pre>print(\"Predicting using population\")\nprint(\"----------------------------\")\nprint(\"Error in intercept: {}\".format(pop_intercept - predicted_pop_intercept))\nprint(\"Error in slope: {}\".format(pop_slope - predicted_pop_slope))\n\nprint(\"\\n\\nPredicting using subset\")\nprint(\"----------------------------\")\nprint(\"Error in intercept: {}\".format(pop_intercept - predicted_subset_intercept))\nprint(\"Error in slope: {}\".format(pop_slope - predicted_subset_slope))\n\nprint(\"\\n\\nPredicting using a number of smaller samples\")\nprint(\"------------------------------------------------\")\nprint(\"Error in intercept: {}\".format(pop_intercept - mean_sample_intercept))\nprint(\"Error in slope: {}\".format(pop_slope - mean_sample_slope))\n</pre> print(\"Predicting using population\") print(\"----------------------------\") print(\"Error in intercept: {}\".format(pop_intercept - predicted_pop_intercept)) print(\"Error in slope: {}\".format(pop_slope - predicted_pop_slope))  print(\"\\n\\nPredicting using subset\") print(\"----------------------------\") print(\"Error in intercept: {}\".format(pop_intercept - predicted_subset_intercept)) print(\"Error in slope: {}\".format(pop_slope - predicted_subset_slope))  print(\"\\n\\nPredicting using a number of smaller samples\") print(\"------------------------------------------------\") print(\"Error in intercept: {}\".format(pop_intercept - mean_sample_intercept)) print(\"Error in slope: {}\".format(pop_slope - mean_sample_slope)) <pre>Predicting using population\n----------------------------\nError in intercept: -0.7189167116699764\nError in slope: 0.0044390080786180786\n\n\nPredicting using subset\n----------------------------\nError in intercept: 0.14207590011892535\nError in slope: 0.0051121012943560196\n\n\nPredicting using a number of smaller samples\n------------------------------------------------\nError in intercept: 0.4823977050074646\nError in slope: 0.002971759530004725\n</pre> <p>As we can see, error in quite small in all 3 cases, especially for <code>slope</code>. Prediction by averaging a number of smaller samples gives us much closer slope to population.</p> <p>For intercept, the least error was with prediction using subset, which is still interesting as prediction using the whole population yielded poorer intercept!</p> <p>In general, for really large datasets, that cannot be held in system memory, we can apply Central Limit Theorem for estimating slope and intercept by averaging over a number of smaller samples.</p>"},{"location":"projects/fun/verifying_clt_in_regression/#verifying-central-limit-theorem-in-regression","title":"Verifying Central Limit Theorem in regression\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#synthesize-the-dataset","title":"Synthesize the dataset\u00b6","text":"<p>Create <code>1000</code> random integers between <code>0</code>, <code>100</code> for <code>X</code> and create <code>y</code> such that $$ y = \\beta_{0} + \\beta_{1}X + \\epsilon $$ where $$ \\beta_{0} = 30 \\ and \\ \\beta_{1} = 1.8 \\ and \\ \\epsilon \\ = \\ standard \\ normal \\ error $$</p>"},{"location":"projects/fun/verifying_clt_in_regression/#predict-using-population","title":"Predict using population\u00b6","text":"<p>Let us predict the coefficients and intercept when using the whole dataset. We will compare this approach with CLT approach of breaking into multiple subsets and averaging the coefficients and intercepts</p>"},{"location":"projects/fun/verifying_clt_in_regression/#using-whole-population","title":"Using whole population\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#prediction-with-66-of-data","title":"Prediction with 66% of data\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#perform-predictions-and-plot-the-charts","title":"Perform predictions and plot the charts\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#predict-using-multiple-samples","title":"Predict using multiple samples\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#select-50-samples-of-size-200-and-perform-regression","title":"Select 50 samples of size 200 and perform regression\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#conclusion","title":"Conclusion\u00b6","text":"<p>Here we compare the coefficients and intercepts obtained by different methods to see how CLT adds up.</p>"},{"location":"projects/math/","title":"Learn Mathematics with Python","text":"<p>One of my goals is to explain math using programming. Much has been said and done explaining programming using math, while that has led us to technological advancements, I strongly feel, math could be made more fun and elegant if we could use some high level languages to teach the beauty of it. Below are some topics needed to understand machine learning better.</p> <ul> <li>Set theory, probability, statistics</li> </ul>"},{"location":"projects/math/math-practical-odyssey-1/","title":"Mathematics - a practical odyssey","text":"<p>Complement of a set is the set of all elements not in that set, but in Universal set. It is represented as $$ A' = \\{x\\mid x \\in U ~and~ x \\not\\in A \\}$$ $$ n(A') = n(U) - n(A)$$</p> In\u00a0[2]: Copied! <pre># use itertools to calculate permutations in Python\nfrom itertools import permutations\n\n# create list of items in pool\nn = list(range(1,11))\n\nnpr_choices = list(permutations(n,3)) # since you get back a iterable\nprint(len(npr_choices))\n</pre> # use itertools to calculate permutations in Python from itertools import permutations  # create list of items in pool n = list(range(1,11))  npr_choices = list(permutations(n,3)) # since you get back a iterable print(len(npr_choices)) <pre>720\n</pre> <p>Combinations Selection without replacement and order is not important. If you change the order, that choice is not counted as a new choice. Thus, there are generally fewer combinations than permutations possible in a given scenario. Exceptions are narrow choices like choose 1 or 0 items from a pool of items. In this case, number of permutations and combinaions are the same.</p> <p>$$ _{n}C_{r} = \\frac{n!}{r! (n-r)!}$$</p> <p>Thus, the various combinations of choosing 3 balls from a bag of 10 balls are: $$ _{10}C_{3} = \\frac{10!}{3! \\cdot 7!} = 120$$</p> In\u00a0[3]: Copied! <pre># use combinations method from itertools module\nfrom itertools import combinations\n\nn = list(range(1,11))\n\nncr_choices = list(combinations(n,3))\nprint(len(ncr_choices))\n</pre> # use combinations method from itertools module from itertools import combinations  n = list(range(1,11))  ncr_choices = list(combinations(n,3)) print(len(ncr_choices)) <pre>120\n</pre> <p>Probability is the measure of likelihood of an event happening. In general, it is the ratio of number of outcomes in the event <code>E</code> to the total number of possible outcomes. Thus</p> <p>$$ p(E) = \\frac{n(E)}{n(S)}$$</p> <p>Probability is a computed value. If you are conducting an empirical study, you can measure probability of an event by measuring the relative frequency of that event. The law of large numbers states that</p> <pre><code>If an experiment is repeated a large number of times, the relative frequency of an outcome will tend to be close to the probability of the outcome.</code></pre> <p>Probability of being dealt 4 Aces in poker First find the sample space. Sample space includes all possible ways of selecting 5 cards from a pack of 52.</p> <p>Thus, n(S) = $_{52}C_{5} = 2,598,960$.</p> <p>Now, n(E) = number of ways of selecting 4 aces + 1 any other card.</p> <p>n(E) = $_{4}C_{4} \\cdot _{48}C_{1}$</p> <p>p(E) = n(E) / n(S) = (4*48)/2,598,960 = 0.00001847</p> <p>Probability of being dealt 4 of a kind.</p> <p>The, n(S) remains same. But n(E) is expanded 13 times as there are 13 different ways to select a 4 of a kind in a single pack of cards.</p> <p>p(E) = 13* (probability of 4 aces)</p> <p>Probability of getting 5 hearts.</p> <p>The n(S) remains same as $_{52}C_{5}$. Now the n(E) is written as $_{13}C_{5}$ as there are 13C5 number of ways to select a set of 5 hearts.</p> <p>Thus, p(E) = 13C5 / 52C5 = 0.0004951</p> <p>Conditional probability of event A given event B is written as $$p(A|B) = \\frac{n(A\\cap B)}{n(B)} $$</p> <p>dividing both numerator and demoninator of right side by n(S), we can rewrite this as</p>  $$p(A|B) = \\frac{p(A \\cap B)}{p(B)} $$  and, more commonly  <p>$$p(A \\cap B) = p(A|B) \\cdot p(B) $$</p> <p>The probability of selecting 1st heart is 13/52. Hence:</p> <p>$$p(B) = \\frac{13}{52}$$</p> <p>Since the 1st card is heart, you are left with 12 hearts and 51 total cards. So the probability of selecting another heart is: $$ p(A|B) = \\frac{12}{51} = 0.2352$$</p> <p>Thus:</p> <p>$$p(A \\cap B) = \\frac{12}{51} \\cdot \\frac{13}{52} = 0.0588$$</p> <p>We can also calculate probability of both hearts without conditional probability using combinatronics as below:</p> <p>$$p(A \\cap B) = \\frac{_{13}C_{2}}{_{52}C_{2}} $$</p> <p>$$p(A \\cap B) = \\frac{13}{52} \\cdot \\frac{12}{51} $$</p> <p>An example of independent events is, \"find the probability of getting a 6 when tossing a pair of die, given the first also yielded a 6\". Here the event of getting a 6 in one or first try does not affect getting 6 another time. Hence, they are simply independent events.</p> <p>Using cardinal number of sets, in set theory,</p>  $$ p(A \\cup B) = p(A) + p(B) - p(A \\cap B)$$  <p>for mutually exclusive A and B events, the $p(A \\cap B) = 0$ thus, $$ p(A \\cup B) = p(A) + p(B)$$</p> <p>Using De Morgan's laws,</p>  $$p(A' \\cap B') = p(A \\cup B)' $$  <p>and</p> <p>$$p(A' \\cup B') = p(A \\cap B)' $$</p> <p>The Area under the curve = 1. Here $\\mu$ is population mean and $\\sigma$ is population standard deviation. Thus, 68% values vall within $\\mu \\pm 1 \\sigma$ and 95% with $\\mu \\pm 2 \\sigma$ and 99.74% within $\\mu \\pm 3 \\sigma$. We use the Z table to find the area / probability for any other values of x.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/math/math-practical-odyssey-1/#mathematics-a-practical-odyssey","title":"Mathematics - a practical odyssey\u00b6","text":"<p>ToC</p> <ul> <li>set theory<ul> <li>notations</li> <li>De Morgans's laws</li> <li>Permutations and combinations</li> </ul> </li> <li>Probability<ul> <li>Combinatronics and probability</li> <li>Conditional probability</li> </ul> </li> <li>Statistics<ul> <li>Measures of central tendency</li> </ul> </li> </ul>"},{"location":"projects/math/math-practical-odyssey-1/#set-theory","title":"Set theory\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#notations","title":"Notations\u00b6","text":"<p>Set builder notation for representing a set: $$ G = \\{ x\\mid x &lt; 0 ~and~ x \\in R \\} $$ which is read as 'the set of all x, such that x is less than 0 and x is a real number'.</p> <p>Cardinal number of a set is the number of elements in it. It is denoted as <code>n(A)</code>. An empty set has 0 elements. It is represented as $\\phi$ or as <code>{}</code>. Universal set is always represented as <code>U</code>.</p> <p>A is a proper subset of B, if all elements of A are in B and B has more elements than A. It is represented as $A \\subset B$. A is an improper subset of B if A and B have the same elements. A not a subset is represented a $A  \\not\\subset B$. Thus $$ A \\cup B = \\{ x \\mid x \\in A ~or~ x\\in B\\}$$ $$ A \\cap B = \\{ x \\mid x \\in A ~and~ x\\in B\\}$$</p> <p>Intersection of sets represents the common elements and is written as $ A \\cap B$. Union of sets represents all elements, without repetition and is written as $A \\cup B$.</p> <p>Two sets are Mutually exclusive, if there are no common elements. Written as $ A \\cap B = \\phi $.</p>"},{"location":"projects/math/math-practical-odyssey-1/#cardinal-number-formula-for-union-of-two-sets","title":"Cardinal number formula for union of two sets\u00b6","text":"<p>$$n(A \\cup B) = n(A) + n(b) - n(A\\cap B)$$</p>"},{"location":"projects/math/math-practical-odyssey-1/#de-morgans-laws","title":"De Morgan's laws\u00b6","text":"<p>For any two sets A and B (no need to be independent or mutually exclusive) $$ (A\\cup B)' = A' \\cap B'$$ $$ (A\\cap B)' = A' \\cup B'$$</p>"},{"location":"projects/math/math-practical-odyssey-1/#permutations-and-combinations","title":"Permutations and combinations\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#fundamental-principle-of-counting","title":"Fundamental principle of counting\u00b6","text":"<p>The number of possible ways in which you can choose a 4 digit PIN number: Here, you can repeat numbers (or, your choices are replaced once you chose them so they can be chosen another time). Here, you simply raise the choices by number of selections.</p> <p>Number of ways to select 4 digit PIN = <code>10 x 10 x 10 x 10</code> = <code>10,000</code>.</p>"},{"location":"projects/math/math-practical-odyssey-1/#permutations","title":"Permutations\u00b6","text":"<p>Selection without replacement (no repetition) and order of selection is important, meaning, if you change the order, you can count that selection as a whole new choice. The formula is</p> <p>$$ _{n}P_{r} = \\frac{n!}{(n-r)!}$$ where <code>n</code> is number of items in the pool and <code>r</code> is the number of choices to be made.</p> <p>The number of ways to choose the first 3 places from a pool of 10 contestants: $$_{10}P_{3} = \\frac{10!}{7!} = 720$$</p>"},{"location":"projects/math/math-practical-odyssey-1/#probability","title":"Probability\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#probability-rules","title":"Probability rules\u00b6","text":"<p>$$  p(\\phi) = 0 \\\\ p(S) = 1 \\\\ 0 \\leq p \\leq 1 $$</p>"},{"location":"projects/math/math-practical-odyssey-1/#combinatronics-and-probability","title":"Combinatronics and probability\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#conditional-probability","title":"Conditional probability\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#understanding-the-difference-between-a-given-b-and-a-and-b","title":"Understanding the difference between 'A given B' and 'A and B'\u00b6","text":"<p>Two cards are dealt from a pack of cards, find the probability that</p> <ul> <li>both cards are hearts. This is $p(A \\cap B)$</li> <li>second card is heart, given first is a heart. This is looking for the odds of the 2nd card. It is $p(A|B)$ where A is second card and B is first card being heart.</li> </ul>"},{"location":"projects/math/math-practical-odyssey-1/#dependent-and-independent-events","title":"Dependent and Independent events\u00b6","text":"<p>If A and B are independent, then knowing B occurred does not affect the probability of A. Thus:</p>  $$p(A|B) = p(A)$$  <p>Conversely, if $p(A|B) \\neq p(A)$ then A and B are dependent.</p>  Thus, for A and B **independent** events, $$p(A \\cap B) = p(A) \\cdot p(B) $$"},{"location":"projects/math/math-practical-odyssey-1/#statistics","title":"Statistics\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#measures-of-central-tendency","title":"Measures of central tendency\u00b6","text":"<p>Population mean $\\mu$ and median cannot be calculated. We generally calculate the sample mean and median.</p> <p>$$Sample ~ mean: \\bar x = \\frac{\\sum x}{n} $$</p> <p>$$Sample ~ median: ~ L = \\frac{n+1}{2} $$</p>"},{"location":"projects/math/math-practical-odyssey-1/#measures-of-dispersion","title":"Measures of dispersion\u00b6","text":"<p>Variance is the squared deviation from the mean.</p> <p>$$Sample ~ variance: s^{2} = \\frac{\\sum{(x - \\bar x)^{2}}}{n-1}$$</p> <p>$$Sample ~ standard ~ deviation: s = \\sqrt{\\frac{\\sum{(x - \\bar x)^{2}}}{n-1}}$$</p>"},{"location":"projects/math/math-practical-odyssey-1/#normal-distribution","title":"Normal distribution\u00b6","text":"<p>If you plot the outcomes of a continuous random variable (of a process occurring in nature, like rainfall), it takes the shape of a bell curve, with values close to mean occurring more often than those farther from it. Since the curve represents the the probabilities of various outcomes, it is also a probability distribution.</p> <p></p>"},{"location":"projects/math/math-practical-odyssey-1/#transformation-to-standard-normal","title":"Transformation to standard normal\u00b6","text":"<p>The Z table gives probabilities for standard normal dist ($\\mu = 0$ and $\\sigma=1$). To transfrom a normal dist with any other mean and SD, use the formula</p> <p>$$ z= \\frac{x - \\mu}{\\sigma}$$</p> <p>where x is the value in the given distribution and z is the value in std. normal dist.</p>"},{"location":"projects/math/math-practical-odyssey-1/#central-limit-theorem","title":"Central limit theorem\u00b6","text":"<p>Since it is hard to find the population mean, SD, we use the CLT. According to CLT, means of a large number of samples is normally distributed around the population mean. Same applies for the SD and other measures.</p>"},{"location":"projects/math/math-practical-odyssey-1/#level-of-confidence","title":"Level of confidence\u00b6","text":"<p>\"We might say we are 95% confident the maximum error of a poll is plus or minus 3%\". This means, if 100 samples are analyzed, 95 of them would differ from population by under 0.03 of that measure, and 5 would be greater than 0.03.</p>"},{"location":"projects/ml/","title":"Machine Learning Projects","text":"<p>Today, we teach machines to learn. Tomorrow, we hope they'd return the favor ;-)</p>"},{"location":"projects/ml/#getting-started","title":"Getting started","text":"<ul> <li>Foundational ML concepts</li> <li>Understanding Scikit-Learn syntax</li> <li>Understanding Gradient Descent</li> <li>A primer on linear algebra</li> <li>Naive Bayes classification with <code>sklearn</code> - a work in progress</li> </ul>"},{"location":"projects/ml/#generalized-linear-models","title":"Generalized linear models","text":"<p>Theory</p> <ul> <li>Linear regression - stat concepts</li> <li>Solving multivariate linear regression using Gradient Descent</li> <li>Analytical vs Gradient Descent methods of solving linear regression</li> <li>Logistic regression, concepts</li> <li>Model regularization</li> </ul> <p>Applications</p> <ul> <li>Implementing linear regression using Gradient descent in Python</li> <li>Linear regression with <code>sklearn</code> and <code>statmodels</code></li> <li>Implementing logistic regression using gradient descent</li> <li>MNIST digits classification using Logistic regression in Scikit-Learn</li> </ul>"},{"location":"projects/ml/#ml-at-scale-with-pyspark","title":"ML at scale with PySpark","text":"<ul> <li>Getting started with PySpark</li> <li>CA housing price prediction with PySpark</li> </ul>"},{"location":"projects/ml/#house-hunting-the-data-scientist-way","title":"House hunting the data scientist way","text":"<ul> <li>Recording of this talk and the slide deck</li> <li>Technical write up</li> <li>Notebooks: Get my notebooks from: arcgis-python-api/talks/GeoDevPDX2018<ul> <li>Cleaning data</li> <li>Exploratory data analysis</li> <li>Feature engineering - neighboring facilities</li> <li>Feature engineering - batch</li> <li>Ranking properties</li> <li>Building a recommendation engine</li> </ul> </li> </ul>"},{"location":"projects/ml/#analyzing-over-a-century-of-global-hurricane-data","title":"Analyzing over a century of global hurricane data","text":"<p>This study showcases applying spatial data science techniques to analyze weather data and impacts of climate change on natural disasters. It is featured as a technology spotlight in the book GIS for Science. To get a high level overview of this study and its results, read the StoryMap webapp. For detailed analysis, read the analysis notebooks below:</p> <ul> <li>Part 1: Preparing larger-than-memory hurricane data using Dask and GeoAnalytics</li> <li>Part 2: EDA on hurricane tracks</li> <li>Part 3: Does intensity of hurricanes increase over time?</li> </ul>"},{"location":"projects/ml/gradient-descent-in-python/","title":"Implementing Gradient Descent for Linear Regression","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport plotly_express as px\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom mpl_toolkits import mplot3d\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode\nfrom plotly.offline import plot, iplot\n</pre> import pandas as pd import plotly_express as px import numpy as np import matplotlib.pyplot as plt import os from mpl_toolkits import mplot3d  from plotly.offline import download_plotlyjs, init_notebook_mode from plotly.offline import plot, iplot In\u00a0[13]: Copied! <pre>#set notebook mode\ninit_notebook_mode(connected=True)\n</pre> #set notebook mode init_notebook_mode(connected=True) In\u00a0[2]: Copied! <pre>path='/Users/atma6951/Documents/code/pychakras/pyChakras/ml/coursera-ml-matlabonline/machine-learning-ex/ex1'\ndata_df = pd.read_csv(os.path.join(path, 'ex1data1.txt'), header=None, names=['X','y'])\ndata_df.head()\n</pre> path='/Users/atma6951/Documents/code/pychakras/pyChakras/ml/coursera-ml-matlabonline/machine-learning-ex/ex1' data_df = pd.read_csv(os.path.join(path, 'ex1data1.txt'), header=None, names=['X','y']) data_df.head() Out[2]: X y 0 6.1101 17.5920 1 5.5277 9.1302 2 8.5186 13.6620 3 7.0032 11.8540 4 5.8598 6.8233 In\u00a0[3]: Copied! <pre>data_df.shape\n</pre> data_df.shape Out[3]: <pre>(97, 2)</pre> In\u00a0[4]: Copied! <pre>n_rows = data_df.shape[0]\n</pre> n_rows = data_df.shape[0] In\u00a0[5]: Copied! <pre>X=data_df['X'].to_numpy().reshape(n_rows,1)\n# Represent x_0 as a vector of 1s for vector computation\nones = np.ones((n_rows,1))\nX = np.concatenate((ones, X), axis=1)\ny=data_df['y'].to_numpy().reshape(n_rows,1)\n</pre> X=data_df['X'].to_numpy().reshape(n_rows,1) # Represent x_0 as a vector of 1s for vector computation ones = np.ones((n_rows,1)) X = np.concatenate((ones, X), axis=1) y=data_df['y'].to_numpy().reshape(n_rows,1) In\u00a0[6]: Copied! <pre>X.shape, y.shape\n</pre> X.shape, y.shape Out[6]: <pre>((97, 2), (97, 1))</pre> In\u00a0[7]: Copied! <pre>plt.scatter(x=data_df['X'], y=data_df['y'])\nplt.xlabel('X'); plt.ylabel('y');\nplt.title('Input dataset');\n</pre> plt.scatter(x=data_df['X'], y=data_df['y']) plt.xlabel('X'); plt.ylabel('y'); plt.title('Input dataset'); In\u00a0[7]: Copied! <pre>def compute_cost(X, y, theta=np.array([[0],[0]])):\n    \"\"\"Given covariate matrix X, the prediction results y and coefficients theta\n    compute the loss\"\"\"\n    \n    m = len(y)\n    J=0 # initialize loss to zero\n    \n    # reshape theta\n    theta=theta.reshape(2,1)\n    \n    # calculate the hypothesis - y_hat\n    h_x = np.dot(X,theta)\n    \n    # subtract y from y_hat, square and sum\n    error_term = sum((h_x - y)**2)\n    \n    # divide by twice the number of samples - standard practice.\n    loss = error_term/(2*m)\n    \n    return loss\n</pre> def compute_cost(X, y, theta=np.array([[0],[0]])):     \"\"\"Given covariate matrix X, the prediction results y and coefficients theta     compute the loss\"\"\"          m = len(y)     J=0 # initialize loss to zero          # reshape theta     theta=theta.reshape(2,1)          # calculate the hypothesis - y_hat     h_x = np.dot(X,theta)          # subtract y from y_hat, square and sum     error_term = sum((h_x - y)**2)          # divide by twice the number of samples - standard practice.     loss = error_term/(2*m)          return loss In\u00a0[8]: Copied! <pre>compute_cost(X,y)\n</pre> compute_cost(X,y) Out[8]: <pre>array([32.07273388])</pre> In\u00a0[9]: Copied! <pre>def gradient_descent(X, y, theta=np.array([[0],[0]]),\n                    alpha=0.01, num_iterations=1500):\n    \"\"\"\n    Solve for theta using Gradient Descent optimiztion technique. \n    Alpha is the learning rate\n    \"\"\"\n    m = len(y)\n    J_history = []\n    theta0_history = []\n    theta1_history = []\n    theta = theta.reshape(2,1)\n    \n    for i in range(num_iterations):\n        error = (np.dot(X, theta) - y)\n        \n        term0 = (alpha/m) * sum(error* X[:,0].reshape(m,1))\n        term1 = (alpha/m) * sum(error* X[:,1].reshape(m,1))\n        \n        # update theta\n        term_vector = np.array([[term0],[term1]])\n#         print(term_vector)\n        theta = theta - term_vector.reshape(2,1)\n        \n        # store history values\n        theta0_history.append(theta[0].tolist()[0])\n        theta1_history.append(theta[1].tolist()[0])\n        J_history.append(compute_cost(X,y,theta).tolist()[0])\n        \n    return (theta, J_history, theta0_history, theta1_history)\n</pre> def gradient_descent(X, y, theta=np.array([[0],[0]]),                     alpha=0.01, num_iterations=1500):     \"\"\"     Solve for theta using Gradient Descent optimiztion technique.      Alpha is the learning rate     \"\"\"     m = len(y)     J_history = []     theta0_history = []     theta1_history = []     theta = theta.reshape(2,1)          for i in range(num_iterations):         error = (np.dot(X, theta) - y)                  term0 = (alpha/m) * sum(error* X[:,0].reshape(m,1))         term1 = (alpha/m) * sum(error* X[:,1].reshape(m,1))                  # update theta         term_vector = np.array([[term0],[term1]]) #         print(term_vector)         theta = theta - term_vector.reshape(2,1)                  # store history values         theta0_history.append(theta[0].tolist()[0])         theta1_history.append(theta[1].tolist()[0])         J_history.append(compute_cost(X,y,theta).tolist()[0])              return (theta, J_history, theta0_history, theta1_history) In\u00a0[10]: Copied! <pre>%%time\nnum_iterations=1500\ntheta_init=np.array([[1],[1]])\nalpha=0.01\ntheta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,\n                                                                   alpha, num_iterations)\n</pre> %%time num_iterations=1500 theta_init=np.array([[1],[1]]) alpha=0.01 theta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,                                                                    alpha, num_iterations) <pre>CPU times: user 247 ms, sys: 2.52 ms, total: 250 ms\nWall time: 248 ms\n</pre> In\u00a0[11]: Copied! <pre>theta\n</pre> theta Out[11]: <pre>array([[-3.57081935],\n       [ 1.16038773]])</pre> In\u00a0[61]: Copied! <pre>fig, ax1 = plt.subplots()\n\n# plot thetas over time\ncolor='tab:blue'\nax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color)\nax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color)\n# ax1.legend()\nax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color);\nax1.tick_params(axis='y', labelcolor=color)\n\n# plot loss function over time\ncolor='tab:red'\nax2 = ax1.twinx()\nax2.plot(J_history, label='Loss function', color=color)\nax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations')\nax2.set_ylabel('Loss: $J(\\\\theta)$', color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\n# ax2.legend();\nfig.legend();\n</pre> fig, ax1 = plt.subplots()  # plot thetas over time color='tab:blue' ax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color) ax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color) # ax1.legend() ax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color); ax1.tick_params(axis='y', labelcolor=color)  # plot loss function over time color='tab:red' ax2 = ax1.twinx() ax2.plot(J_history, label='Loss function', color=color) ax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations') ax2.set_ylabel('Loss: $J(\\\\theta)$', color=color) ax1.tick_params(axis='y', labelcolor=color)  # ax2.legend(); fig.legend(); In\u00a0[12]: Copied! <pre>%%time\n# theta range\ntheta0_vals = np.linspace(-10,0,100)\ntheta1_vals = np.linspace(-1,4,100)\nJ_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n\n# compute cost for each combination of theta\nc1=0; c2=0\nfor i in theta0_vals:\n    for j in theta1_vals:\n        t = np.array([i, j])\n        J_vals[c1][c2] = compute_cost(X, y, t.transpose()).tolist()[0]\n        c2=c2+1\n    c1=c1+1\n    c2=0 # reinitialize to 0\n</pre> %%time # theta range theta0_vals = np.linspace(-10,0,100) theta1_vals = np.linspace(-1,4,100) J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))  # compute cost for each combination of theta c1=0; c2=0 for i in theta0_vals:     for j in theta1_vals:         t = np.array([i, j])         J_vals[c1][c2] = compute_cost(X, y, t.transpose()).tolist()[0]         c2=c2+1     c1=c1+1     c2=0 # reinitialize to 0 <pre>CPU times: user 566 ms, sys: 32.6 ms, total: 598 ms\nWall time: 569 ms\n</pre> In\u00a0[15]: Copied! <pre>import plotly.graph_objects as go\nfig = go.Figure(data=[go.Surface(x=theta0_vals, y=theta1_vals, z=J_vals)])\nfig.update_layout(title='Loss function for different thetas', autosize=True,\n                  width=600, height=600, xaxis_title='theta0', \n                 yaxis_title='theta1')\nfig.show()\n</pre> import plotly.graph_objects as go fig = go.Figure(data=[go.Surface(x=theta0_vals, y=theta1_vals, z=J_vals)]) fig.update_layout(title='Loss function for different thetas', autosize=True,                   width=600, height=600, xaxis_title='theta0',                   yaxis_title='theta1') fig.show() In\u00a0[93]: Copied! <pre>num_iterations=1500\ntheta_init=np.array([[-5],[4]])\nalpha=0.01\ntheta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,\n                                                                   alpha, num_iterations)\n</pre> num_iterations=1500 theta_init=np.array([[-5],[4]]) alpha=0.01 theta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,                                                                    alpha, num_iterations) In\u00a0[94]: Copied! <pre>plt.contour(theta0_vals, theta1_vals, J_vals, levels = np.logspace(-2,3,100))\nplt.xlabel('$\\\\theta_{0}$'); plt.ylabel(\"$\\\\theta_{1}$\")\nplt.title(\"Contour plot of loss function for different values of $\\\\theta$s\");\nplt.plot(theta0_history, theta1_history, 'r+');\n</pre> plt.contour(theta0_vals, theta1_vals, J_vals, levels = np.logspace(-2,3,100)) plt.xlabel('$\\\\theta_{0}$'); plt.ylabel(\"$\\\\theta_{1}$\") plt.title(\"Contour plot of loss function for different values of $\\\\theta$s\"); plt.plot(theta0_history, theta1_history, 'r+'); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/ml/gradient-descent-in-python/#implementing-gradient-descent-for-linear-regression","title":"Implementing Gradient Descent for Linear Regression\u00b6","text":"<p>For a theoretical understanding of Gradient Descent visit here. This page walks you through implementing gradient descent for a simple linear regression. Later, we also simulate a number of parameters, solve using GD and visualize the results in a 3D mesh to understand this process better.</p>"},{"location":"projects/ml/gradient-descent-in-python/#load-the-data","title":"Load the data\u00b6","text":""},{"location":"projects/ml/gradient-descent-in-python/#plot-the-dataset","title":"Plot the dataset\u00b6","text":""},{"location":"projects/ml/gradient-descent-in-python/#create-a-cost-function","title":"Create a cost function\u00b6","text":"<p>Here we will compute the cost function and code that into a Python function. Cost function is given by</p> <p>$$ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i})^2 $$</p> <p>where $h_{\\theta}(x_{i}) = \\theta^{T}X$</p>"},{"location":"projects/ml/gradient-descent-in-python/#solve-using-gradient-descent","title":"Solve using Gradient Descent\u00b6","text":"<p>Using GD, we simultaneously solve for theta0 and theta1 using the formula: $$ repeat \\; until \\; convergence $$ $$ \\theta_{0} := \\theta_{0} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} [(h_{\\theta}(x_{i}) - y_{i})x^{(0)}_{i}] $$ $$ \\theta_{1} := \\theta_{1} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} [(h_{theta}(x_{i}) - y_{i})x^{(1)}_{i}] $$</p>"},{"location":"projects/ml/gradient-descent-in-python/#plot-gradient-descent","title":"Plot Gradient Descent\u00b6","text":"<p>We visualize the process of reducing the loss function using gradient descent in this graph</p>"},{"location":"projects/ml/gradient-descent-in-python/#compute-cost-surface-for-an-array-of-input-thetas","title":"Compute cost surface for an array of input thetas\u00b6","text":"<p>Let us synthesize a range of theta values and compute the cost surface as a mesh. We will then overlay the path our GD algorithm took to reach the optima</p>"},{"location":"projects/ml/gradient-descent-in-python/#visualize-loss-function-as-contours","title":"Visualize loss function as contours\u00b6","text":""},{"location":"projects/ml/gradient-descent-in-python/#and-overlay-the-path-took-by-gd-to-seek-optima","title":"And overlay the path took by GD to seek optima\u00b6","text":""},{"location":"projects/ml/gradient-descent/","title":"Understanding Gradient Descent","text":""},{"location":"projects/ml/gradient-descent/#linear-regression","title":"Linear regression","text":""},{"location":"projects/ml/gradient-descent/#cost-functions","title":"Cost functions","text":"<p>The linear regression estimation function (hypothesis function) can be written as \\(h_{\\theta} (x) = \\theta_{0} + \\theta_{1}x\\). The cost function for this equation can be written as</p> \\[ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i})^2 \\] <p>The core of the cost function is the squared difference between prediction and truth in \\(y\\). In other words, this can be written as</p> \\[ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat y_{i} - y_{i})^2 \\] <p>We square the error as that is a common way of measuring loss. We sum the loss for each value of \\(y\\), get the average and then half the average. The squared error cost function is pretty common and works well for linear regressions. We halve the error function for convenience later when we take the derivative of the function and the <code>2</code> gets cancelled out. However, the objective of the estimation function is choosing values of \\(\\theta_{0} and \\theta_{1}\\) such that they minimize the cost function.</p>"},{"location":"projects/ml/gradient-descent/#minimizing-cost-functions","title":"Minimizing cost functions","text":"<p>To understand how to minimize the cost functions, let us simplify the above regression to a state where intercept is <code>0</code>. Thus the estimation / hypothesis function changes to</p> \\[ J(\\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m}(h_{\\theta}(x_{i}) - y_{i})^2 $$ which reduces to $$ J(\\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m}(\\theta_{1}x_{i} - y_{i})^2 \\] <p>Next, we solve for the cost function for different values of \\(\\theta_{1}\\) and plot them in a graph as shown below:</p> <p></p> <p>The cost function takes shape of a parabola, with a clear minima.</p>"},{"location":"projects/ml/gradient-descent/#minimizing-multidimensional-cost-functions","title":"Minimizing multidimensional cost functions","text":"<p>In the previous example, we assumed \\(\\theta_{0}=0\\). If that was not the case, then we need to minimize the residuals / cost function while changing values of both the variables. This leads to a 3D plot as shown below: </p> <p>Another way to represent the cost function is via contour plots as shown below:</p> <p></p> <p>Points along same contour have same values of error/loss for different values of \\(\\theta_{0}\\) and \\(\\theta_{1}\\). The objective is to find the lowest point in the contours - which has the lowest error/loss and find its parameters.</p> <p>In a multiple regression problem, there are several predictor variables. Thus the loss function is hard to visualize as there now multiple dimensions, one for coefficient of predictor variable + intercept term. An algorithmic way of minimizing the cost function is called gradient descent.</p>"},{"location":"projects/ml/gradient-descent/#gradient-descent","title":"Gradient descent","text":"<p>Gradient descent is an algorithm that is used to minimize the loss function. It is also used widely in many machine learning problems. The idea is, to start with arbitrary values for \\(\\theta_{0}\\) and \\(\\theta_{1}\\), keep changing them little by little until we reach minimal values for the loss function \\(J(\\theta_{0}, \\theta_{1})\\).</p> <p>The following graphic shows the distribution of the loss function. The GD algorithm starts at an arbitrary point for \\(\\theta_{0}\\) and \\(\\theta_{1}\\), takes small steps, at each step determining the direction of travel and stride length, and arrives as the local minima. The direction is determined by getting the slope of the tangent (derivative) at each point.</p> <p></p> <p>An interesting feature of GD is, if you started at a different point, you might end up at a different local minima. The definition of gradient descent for any arbitrary equation is:</p> <p>$$ \\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0}, \\theta_{1}) \\ (for \\; j=0 \\;and\\; j=1)  $$ We repeat the above equation until convergence. The \\(:=\\) is assignment operator, \\(\\partial\\) is partial differential operator, \\(\\alpha\\) is the learning rate. \\(\\alpha\\) controls the stride length in the descent graphic.</p> <p>Thus, when you have two coefficients, you would compute  $$ temp0 := \\theta_{0} - \\alpha\\frac{\\partial}{\\partial\\theta_{0}}J(\\theta_{0}, \\theta_{1}) $$ $$ temp1 := \\theta_{1} - \\alpha\\frac{\\partial}{\\partial\\theta_{1}}J(\\theta_{0}, \\theta_{1}) $$ $$ \\theta_{0} := temp0 $$ $$ \\theta_{1}:= temp1 $$ Note: It is important to compute \\(\\theta_{0}, \\theta_{1}\\) simultaneously (in parallel). You should not compute 1 and substitute its value when computing the next parameter. Intuitively, you are changing both \\(\\theta_{0}, \\theta_{1}\\) instead of changing just \\(\\theta_{0}\\), then the other. This is the principle behind partial differential equations -&gt; you are differentiating multiple parameters at the same time, as opposed to ordinary differential equations where you differentiate just one variable.</p>"},{"location":"projects/ml/gradient-descent/#gradient-descent-intuition","title":"Gradient descent intuition","text":"<p>For simplicity, let us simplify our solver to minimize over just one coefficient \\(J(\\theta_{1})\\). Now strictly speaking, this is a ordinary differential equation, not partial. The equation now becomes</p> \\[ repeat \\; until \\; convergence \\\\ \\theta_{1} := \\theta_{1} - \\alpha \\frac{d}{d\\theta_{1}} J(\\theta_{1}) \\] <p>The shape of \\(J(\\theta_{1})\\) looks like below:</p> <p></p> <p>The \\(\\frac{d}{d\\theta_{1}} J(\\theta_{1})\\) term is the derivative and gives the slope of the tangent at each point. The direction of the slope changes depending on the position on the curve and will lead the iteration to local minima.</p> <p>The learning rate \\(\\alpha\\) is multiplied by the slope / derivative term. A larger \\(\\alpha\\) will lead to an aggressive iteration which may overshoot the minima or even lead to run away divergence. A very small or conservative \\(\\alpha\\) will slow down the convergence or might settle for minor minimums as local minima.</p> <p></p> <p>Further, as we approach the local minima, the slope decreases. Thus even for a fixed \\(\\alpha\\), the rate of change will slow down in general, leading to a safe landing at minima. The slope at local minima is <code>0</code>. Thus, once reached, the second term of the equation (right of the minus sign) turns to <code>0</code> and the iteration as converged.</p>"},{"location":"projects/ml/gradient-descent/#gradient-descent-for-linear-regression","title":"Gradient descent for linear regression","text":"<p>The loss function \\(J(\\theta_{0}, \\theta_{1})\\) can be expanded out with actual loss function equation from earlier. Thus now the differential equations become:</p> \\[ repeat \\; until \\; convergence $$ $$ \\theta_{0} := \\theta_{0} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i}) $$ $$ \\theta_{1} := \\theta_{1} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} [(h_{\\theta}(x_{i}) - y_{i})x_{i}] \\] <p>Here, m is number of training samples, \\(\\theta_{0}\\) is intercept and \\(\\theta_{1}\\) is the coefficient 1, \\(x_{i}\\) and \\(y_{i}\\) are values of the training data.</p> <p>This process of using the full training sample for calculating the GD is called batch gradient descent. In the case of linear regression, the shape of the loss function is a convex function which looks like a bowl. There is only a global minima and no local minimas.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/","title":"Implementing Gradient Descent for Logistic Regression","text":"In\u00a0[\u00a0]: Copied! In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import pandas as pd import numpy as np import math import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[2]: Copied! <pre>vals = np.arange(-10,10,0.2)\ngz= 1/(1+np.e**(0-vals))\nplt.plot(vals, gz)\nplt.title('Sigmoid function');\n</pre> vals = np.arange(-10,10,0.2) gz= 1/(1+np.e**(0-vals)) plt.plot(vals, gz) plt.title('Sigmoid function'); In\u00a0[3]: Copied! <pre>xvals = np.arange(0,1,0.1)\ny1vals = 0-np.log(xvals)\ny0vals = 0-np.log(1-xvals)\nplt.plot(xvals, y1vals, 'b', label='y=1')\nplt.plot(xvals, y0vals, 'g', label='y=0')\nplt.title('Loss functions of logistic regression')\nplt.legend()\nplt.xlabel('Hypothesis: $h\\\\theta(x)$')\nplt.ylabel('Loss');\n</pre> xvals = np.arange(0,1,0.1) y1vals = 0-np.log(xvals) y0vals = 0-np.log(1-xvals) plt.plot(xvals, y1vals, 'b', label='y=1') plt.plot(xvals, y0vals, 'g', label='y=0') plt.title('Loss functions of logistic regression') plt.legend() plt.xlabel('Hypothesis: $h\\\\theta(x)$') plt.ylabel('Loss'); <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n  \n</pre> In\u00a0[4]: Copied! <pre>from numpy import loadtxt, where\n\n#load the dataset\ndata_path = '/Users/atma6951/Documents/code/pychakras/pyChakras/ml/coursera-ml-matlabonline/machine-learning-ex/ex2'\ndata = loadtxt(f'{data_path}/ex2data1.txt', delimiter=',')\ndata.shape\n</pre> from numpy import loadtxt, where  #load the dataset data_path = '/Users/atma6951/Documents/code/pychakras/pyChakras/ml/coursera-ml-matlabonline/machine-learning-ex/ex2' data = loadtxt(f'{data_path}/ex2data1.txt', delimiter=',') data.shape Out[4]: <pre>(100, 3)</pre> In\u00a0[224]: Copied! <pre># set dependent and independent variables.\nX = data[:, 0:2]\ny = data[:, 2]\ny = y.reshape(y.shape[0],1)\n\n# find positive and negative cases\npos = where(y == 1)\nneg = where(y == 0)\n\n# plot the data\nplt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b')\nplt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r')\nplt.xlabel('Exam 1 score')\nplt.ylabel('Exam 2 score')\nplt.legend(['Admitted', 'Not Admitted']);\n</pre> # set dependent and independent variables. X = data[:, 0:2] y = data[:, 2] y = y.reshape(y.shape[0],1)  # find positive and negative cases pos = where(y == 1) neg = where(y == 0)  # plot the data plt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b') plt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r') plt.xlabel('Exam 1 score') plt.ylabel('Exam 2 score') plt.legend(['Admitted', 'Not Admitted']); In\u00a0[225]: Copied! <pre>X.shape, y.shape\n</pre> X.shape, y.shape Out[225]: <pre>((100, 2), (100, 1))</pre> In\u00a0[6]: Copied! <pre>def sigmoid(z):\n    denom = 1+np.e**(0-z)\n    return 1/denom\n</pre> def sigmoid(z):     denom = 1+np.e**(0-z)     return 1/denom In\u00a0[28]: Copied! <pre># test sigmoid function for a range of values\n[sigmoid(-5), sigmoid(0), sigmoid(5)]\n</pre> # test sigmoid function for a range of values [sigmoid(-5), sigmoid(0), sigmoid(5)] Out[28]: <pre>[0.006692850924284857, 0.5, 0.9933071490757153]</pre> In\u00a0[33]: Copied! <pre>def cost_function(theta, X, y):\n    m=np.size(y)  # number of training samples\n    \n    h_theta_x = sigmoid(np.dot(X,theta))\n    term1 = (0-y)*np.log(h_theta_x)\n    term2 = (1-y)*np.log(1-h_theta_x)\n    J = (np.sum(term1-term2))/m\n    \n    grad = np.dot(np.transpose(h_theta_x - y),X)\n    grad = grad/m\n    \n    return (J, grad)\n</pre> def cost_function(theta, X, y):     m=np.size(y)  # number of training samples          h_theta_x = sigmoid(np.dot(X,theta))     term1 = (0-y)*np.log(h_theta_x)     term2 = (1-y)*np.log(1-h_theta_x)     J = (np.sum(term1-term2))/m          grad = np.dot(np.transpose(h_theta_x - y),X)     grad = grad/m          return (J, grad) In\u00a0[20]: Copied! <pre># Create an array for X_0\nx_0 = np.ones(X.shape[0]).reshape(X.shape[0],1)\nx_0.shape\n</pre> # Create an array for X_0 x_0 = np.ones(X.shape[0]).reshape(X.shape[0],1) x_0.shape Out[20]: <pre>(100, 1)</pre> In\u00a0[21]: Copied! <pre># Splice this with existing array X\nX2 = np.concatenate((x_0, X), axis=1)\nX2.shape\n</pre> # Splice this with existing array X X2 = np.concatenate((x_0, X), axis=1) X2.shape Out[21]: <pre>(100, 3)</pre> In\u00a0[201]: Copied! <pre># initialize the coefficients to 0\ninitial_theta = np.zeros(X.shape[1]+1).reshape(X.shape[1]+1,1)\ninitial_theta\n</pre> # initialize the coefficients to 0 initial_theta = np.zeros(X.shape[1]+1).reshape(X.shape[1]+1,1) initial_theta Out[201]: <pre>array([[0.],\n       [0.],\n       [0.]])</pre> In\u00a0[202]: Copied! <pre># compute cost and gradient\n(J, grad) = cost_function(initial_theta, X2, y)\nprint(f'Cost at initial theta: {J}')\nprint(f'Gradient at inital theta: {grad.flatten()}')\n</pre> # compute cost and gradient (J, grad) = cost_function(initial_theta, X2, y) print(f'Cost at initial theta: {J}') print(f'Gradient at inital theta: {grad.flatten()}') <pre>Cost at initial theta: 0.6931471805599453\nGradient at inital theta: [ -0.1        -12.00921659 -11.26284221]\n</pre> In\u00a0[109]: Copied! <pre>def gradient_descent(X, y, theta=initial_theta,\n                    alpha=0.01, num_iterations=1500):\n    \"\"\"\n    Solve for theta using Gradient Descent optimiztion technique. \n    Alpha is the learning rate\n    \"\"\"\n    m = len(y)\n    J_history = []\n    theta0_history = []\n    theta1_history = []\n    theta2_history = []\n    theta = theta.reshape(3,1)\n    \n    for i in range(num_iterations):\n        error = (np.dot(X, theta) - y)\n        \n        term0 = (alpha/m) * np.sum(error* X[:,0].reshape(m,1))\n        term1 = (alpha/m) * np.sum(error* X[:,1].reshape(m,1))\n        term2 = (alpha/m) * np.sum(error* X[:,2].reshape(m,1))\n        \n        # update theta\n        term_vector = np.array([[term0],[term1], [term2]])\n#         print(term_vector)\n        theta = theta - term_vector.reshape(3,1)\n        \n        # store history values\n        theta0_history.append(theta[0].tolist()[0])\n        theta1_history.append(theta[1].tolist()[0])\n        theta2_history.append(theta[2].tolist()[0])\n        J_history.append(cost_function(theta,X,y)[0])\n        \n    return (theta, J_history, theta0_history, theta1_history, theta2_history)\n</pre> def gradient_descent(X, y, theta=initial_theta,                     alpha=0.01, num_iterations=1500):     \"\"\"     Solve for theta using Gradient Descent optimiztion technique.      Alpha is the learning rate     \"\"\"     m = len(y)     J_history = []     theta0_history = []     theta1_history = []     theta2_history = []     theta = theta.reshape(3,1)          for i in range(num_iterations):         error = (np.dot(X, theta) - y)                  term0 = (alpha/m) * np.sum(error* X[:,0].reshape(m,1))         term1 = (alpha/m) * np.sum(error* X[:,1].reshape(m,1))         term2 = (alpha/m) * np.sum(error* X[:,2].reshape(m,1))                  # update theta         term_vector = np.array([[term0],[term1], [term2]]) #         print(term_vector)         theta = theta - term_vector.reshape(3,1)                  # store history values         theta0_history.append(theta[0].tolist()[0])         theta1_history.append(theta[1].tolist()[0])         theta2_history.append(theta[2].tolist()[0])         J_history.append(cost_function(theta,X,y)[0])              return (theta, J_history, theta0_history, theta1_history, theta2_history) In\u00a0[216]: Copied! <pre>%%time\nnum_iterations=150\ninitial_theta2 = (np.ones(3)-0.5).reshape(3,1)\nalpha=0.0002\ntheta, J_history, theta0_history, \\\ntheta1_history, theta2_history = gradient_descent(X2,y,initial_theta,\n                                                  alpha,num_iterations)\n</pre> %%time num_iterations=150 initial_theta2 = (np.ones(3)-0.5).reshape(3,1) alpha=0.0002 theta, J_history, theta0_history, \\ theta1_history, theta2_history = gradient_descent(X2,y,initial_theta,                                                   alpha,num_iterations) <pre>CPU times: user 13.4 ms, sys: 2.6 ms, total: 16 ms\nWall time: 13.6 ms\n</pre> In\u00a0[217]: Copied! <pre>theta.flatten()\n</pre> theta.flatten() Out[217]: <pre>array([-0.00142996,  0.0058575 ,  0.00403084])</pre> In\u00a0[218]: Copied! <pre>fig, ax1 = plt.subplots(figsize=(8,5))\n\n# plot thetas over time\ncolor='tab:blue'\nax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color)\nax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color)\nax1.plot(theta2_history, label='$\\\\theta_{2}$', linestyle='-.', color=color)\n# ax1.legend()\nax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color);\nax1.tick_params(axis='y', labelcolor=color)\n\n# plot loss function over time\ncolor='tab:red'\nax2 = ax1.twinx()\nax2.plot(J_history, label='Loss function', color=color)\nax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations')\nax2.set_ylabel('Loss: $J(\\\\theta)$', color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\n# ax2.legend();\nfig.legend();\n</pre> fig, ax1 = plt.subplots(figsize=(8,5))  # plot thetas over time color='tab:blue' ax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color) ax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color) ax1.plot(theta2_history, label='$\\\\theta_{2}$', linestyle='-.', color=color) # ax1.legend() ax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color); ax1.tick_params(axis='y', labelcolor=color)  # plot loss function over time color='tab:red' ax2 = ax1.twinx() ax2.plot(J_history, label='Loss function', color=color) ax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations') ax2.set_ylabel('Loss: $J(\\\\theta)$', color=color) ax1.tick_params(axis='y', labelcolor=color)  # ax2.legend(); fig.legend(); In\u00a0[220]: Copied! <pre># when x1 = 30, find x2. x2= (-theta0 - theta1*30)/theta2\nt = theta.flatten()\nx2 = (0-t[0] - t[1]*40)/t[2]\nprint(x2)\n\n# when x1 = 100, find x2. x2= (-theta0 - theta1*100)/theta2\nt = theta.flatten()\nx2 = (0-t[0] - t[1]*100)/t[2]\nprint(x2)\n</pre> # when x1 = 30, find x2. x2= (-theta0 - theta1*30)/theta2 t = theta.flatten() x2 = (0-t[0] - t[1]*40)/t[2] print(x2)  # when x1 = 100, find x2. x2= (-theta0 - theta1*100)/theta2 t = theta.flatten() x2 = (0-t[0] - t[1]*100)/t[2] print(x2) <pre>-57.77211067097919\n-144.96240929648445\n</pre> <p>Note: At this point, I realize my gradient descent is not really optimizing well. The equation of the decision boundary line is way off. Hence I approach to solve this problem using Scikit-Learn and see what its parameters are.</p> In\u00a0[243]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X, y)\n</pre> from sklearn.linear_model import LogisticRegression clf = LogisticRegression(random_state=0).fit(X, y) <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/Users/atma6951/anaconda3/envs/pychakras/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n</pre> In\u00a0[244]: Copied! <pre>clf.coef_ # theta1, theta2\n</pre> clf.coef_ # theta1, theta2 Out[244]: <pre>array([[0.03844482, 0.03101855]])</pre> In\u00a0[246]: Copied! <pre># mean test accuracy\nclf.score(X, y)\n</pre> # mean test accuracy clf.score(X, y) Out[246]: <pre>0.87</pre> In\u00a0[247]: Copied! <pre>clf.predict(X)\n</pre> clf.predict(X) Out[247]: <pre>array([0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n       1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n       0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n       1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])</pre> In\u00a0[257]: Copied! <pre># plot function copied from https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nh = .02  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(8, 6))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\n# plt.scatter(X[:, 0], X[:, 1], edgecolors='k', cmap=plt.cm.Paired)\nplt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b')\nplt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r')\nplt.xlabel('Test 1 scores')\nplt.ylabel('Test 2 scores')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title('Logistic regression decision boundary');\n</pre> # plot function copied from https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5  h = .02  # step size in the mesh xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])  # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure(1, figsize=(8, 6)) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)  # Plot also the training points # plt.scatter(X[:, 0], X[:, 1], edgecolors='k', cmap=plt.cm.Paired) plt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b') plt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r') plt.xlabel('Test 1 scores') plt.ylabel('Test 2 scores')  plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title('Logistic regression decision boundary');"},{"location":"projects/ml/implementing-logistic-regression-in-python/#implementing-gradient-descent-for-logistic-regression","title":"Implementing Gradient Descent for Logistic Regression\u00b6","text":"<p>This notebook follows the topics discussed in logistic regression course notes. Please refer to that page for context. This notebook tries to implement the concepts in Python, instead of MatLab/Octave. I have borrowed some inspiration and code from this blog.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/#plot-sigmoid-function","title":"Plot sigmoid function\u00b6","text":"<p>To bound our probability predictions between <code>0-1</code>, we use a sigmoid function. Its definition is below.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/#plot-loss-function-for-logistic-regression","title":"Plot loss function for logistic regression\u00b6","text":""},{"location":"projects/ml/implementing-logistic-regression-in-python/#load-and-visualize-training-data","title":"Load and visualize training data\u00b6","text":""},{"location":"projects/ml/implementing-logistic-regression-in-python/#define-sigmoid-and-cost-functions","title":"Define sigmoid and cost functions\u00b6","text":""},{"location":"projects/ml/implementing-logistic-regression-in-python/#apply","title":"Apply\u00b6","text":""},{"location":"projects/ml/implementing-logistic-regression-in-python/#minimize-the-cost-function-using-gradient-descent","title":"Minimize the cost function using gradient descent\u00b6","text":"<p>Note: The implementation of gradient descent for logistic regression is the same as that for linear regression, as seen here.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/#prediction-and-plot-decision-boundary","title":"Prediction and plot decision boundary\u00b6","text":"<p>Using gradient descent, we found, the values of theta. The decision boundary exists where $h_{\\theta}(x) = 0$. Thus, we write the equation as</p> <p>$$ \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} = 0 \\\\ -0.04904473x_{0} + 0.00618754x_{1} + 0.00439495x_{2} = 0 \\\\ 0.00618754x_{1} + 0.00439495x_{2} = 0.04904473 $$</p> <p>substituting x1=0 and find x2, then vice versa. Thus, we get points <code>(0,11.15933),(7.92636,0)</code>. But these are out of bounds to plot. Instead, we calculate values within the range of <code>30-100</code> as these are exam scores.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/#logistic-regression-using-scikit-learn","title":"Logistic regression using Scikit-Learn\u00b6","text":"<p>Using the logistic regression from SKlearn, we fit the same data and explore what the parameters are.</p>"},{"location":"projects/ml/linear-algebra/","title":"A Primer on Linear Algebra","text":"<p>A mxn matrix is given by </p> \\[ R^{mn} = \\begin{bmatrix}     a&amp;b&amp;c \\\\\\     f&amp;n&amp;i \\\\\\     c&amp;b&amp;w \\end{bmatrix}_{3\\times3} \\] <p>where \\(m\\) is number of rows and \\(n\\) is number of columns.</p> <p>A vector is a single column matrix and is given by </p> \\[ v = \\begin{bmatrix}     a\\\\\\     b\\\\\\     d\\\\\\     h\\\\\\     j \\end{bmatrix} \\] <p>with \\(5\\) rows.</p>"},{"location":"projects/ml/linear-algebra/#notations","title":"Notations","text":"<ol> <li>\\(A_{ij}\\) refers to element in \\(i\\)th row, \\(j\\)th column.</li> <li>In general matrixes are <code>1</code> indexed - both in math and in Matlab</li> <li>\\(v_{i}\\) refers to element in \\(i\\)th row of a vector</li> <li>A vector with <code>n</code> rows is considered an n-dimensional vector</li> <li>Matrices are denoted in uppercase and vectors and scalars in lower case.</li> </ol>"},{"location":"projects/ml/linear-algebra/#matrix-operations","title":"Matrix operations","text":""},{"location":"projects/ml/linear-algebra/#matrix-addition-and-subtraction","title":"Matrix addition and subtraction","text":"<p>You cannot add a scalar to a matrix. You can however add two matrices, they need to be of same dimensions. You add each element at corresponding positions.</p> \\[ \\begin{bmatrix}     3&amp;5\\\\\\     7&amp;8\\\\\\     -9&amp;0 \\end{bmatrix} + \\begin{bmatrix}     8&amp;0\\\\\\     5&amp;-2\\\\\\     2&amp;1 \\end{bmatrix} =  \\begin{bmatrix}     3+8&amp;5+0\\\\\\     7+5&amp;8-2\\\\\\     -9+2&amp;0+1 \\end{bmatrix} \\] <p>The same applies for subtraction.</p>"},{"location":"projects/ml/linear-algebra/#matrix-and-scalar-multiplication-and-division","title":"Matrix and scalar multiplication and division","text":"<p>You can multiply a scalar with a matrix, there are not restrictions with respect to dimensions. You multiply or divide each element with the same scalar.</p> \\[ 3 \\times \\begin{bmatrix}     3&amp;5\\\\\\     7&amp;8\\\\\\     9&amp;0 \\end{bmatrix} = \\begin{bmatrix}     9&amp;15\\\\\\     21&amp;24\\\\\\     27&amp;0 \\end{bmatrix} \\] <p>Division is similar.</p>"},{"location":"projects/ml/linear-algebra/#matrix-and-vector-multiplication","title":"Matrix and vector multiplication","text":"\\[ \\begin{bmatrix}     a&amp;b&amp;c\\\\\\     f&amp;n&amp;i\\\\\\     c&amp;b&amp;w \\end{bmatrix}^{\\rightarrow}_{3\\times3} \\times \\begin{bmatrix}     3\\\\\\     7\\\\\\     9 \\end{bmatrix}\\downarrow = \\begin{bmatrix}     3a + 7b + 9c\\\\\\     3f + 7n + 9i\\\\\\     3c + 7b + 9w \\end{bmatrix} \\]"},{"location":"projects/ml/linear-algebra/#solving-linear-equations-as-matrix-operations","title":"Solving linear equations as matrix operations","text":"<p>For optimization, you can represent linear equations as matrix operations. For instance, consider the hypothesis function \\(h_{\\theta}x = -40 + 0.45x_{i}\\). To compute the hypothesis for \\(n\\) different values of \\(x_{i}\\) (34,56,21,11,10), you can represent the calculation as a matrix operation:</p> <p>$$ \\begin{bmatrix}     1&amp; 34\\\\     1&amp; 56\\\\     1&amp; 21\\\\     1&amp; 11\\\\     1&amp; 10 \\end{bmatrix} \\times \\begin{bmatrix}     -40\\\\     0.45 \\end{bmatrix} = \\begin{bmatrix}     -24.7\\\\     -14.8\\\\     -30.55\\\\     -35.5\\\\     -35.5 \\end{bmatrix} $$ Such matrix computation is way faster than a loop. This is applicable for most language including java, c++, octave, python.</p>"},{"location":"projects/ml/linear-algebra/#matrix-x-matrix-multiplication","title":"Matrix x matrix multiplication","text":"<p>To multiply two matrices, the number of columns of first should match number of row of second =&gt; (mxn x nxp = mxp matrix).</p> \\[ \\begin{bmatrix}     a&amp;b&amp;c\\\\\\     f&amp;n&amp;i\\\\\\     c&amp;b&amp;w \\end{bmatrix} \\times \\begin{bmatrix}     3&amp;1\\\\\\     7&amp;2\\\\\\     9&amp;3 \\end{bmatrix} = \\begin{bmatrix}     (3a + 7b + 9c)&amp;(1a  + 2b+3c)\\\\\\     (3f + 7n + 9i)&amp;(1f+2n+3i)\\\\\\     (3c + 7b + 9w)&amp;(1c+2b+3w) \\end{bmatrix} \\] <p>Extending the former example, suppose you want to calculate the prediction for 3 different hypothesis functions, you can represent that problem as a matrix x matrix multiplication:</p> <p></p> <p>Representing these as matrix operations allows programming languages to compute them in parallel, allowing for great speedups.</p>"},{"location":"projects/ml/linear-algebra/#properties-of-matrix-multiplications","title":"Properties of matrix multiplications","text":"<ul> <li>Matrices are not commutative: \\(A\\times B \\ne B\\times A\\) </li> <li>Matrices are associative: \\((A \\times B) \\times C = A \\times (B \\times C)\\)</li> <li>Identity matrix is a matrix made of ones for diagonals of same dimension such that \\(A \\times I = I \\times A = A\\)</li> <li>Identity matrix is always a square matrix.</li> </ul>"},{"location":"projects/ml/linear-algebra/#matrix-inverse","title":"Matrix inverse","text":"<p>A matrix is said to be the inverse of another matrix, if you multiply that with the matrix, you get an identity matrix. \\(A \\times A^{-1} = I\\).</p> <p>Only certain square matrices have inverses. We typically compute inverse using software.</p>"},{"location":"projects/ml/linear-algebra/#matrix-transpose","title":"Matrix transpose","text":"<p>Transpose of a matrix can be created by flipping the rows and columns. For a matrix \\(A\\), matrix \\(B\\) is said to be its transpose \\(A^{T} = B\\) if \\(B_{ij} = A_{ji}\\). In other words, \\(A_{ij} = A^T_{ji}\\).</p>"},{"location":"projects/ml/linear-regression-analytical-solution/","title":"Analytical vs Gradient Descent methods of solving linear regression","text":"<p>The Gradient Descent offers an iterative method to solve linear models. However, there is a traditional and direct way of solving it called as normal equations. In normal equations, you build a matrix where each record of observation becomes a row (<code>m</code> rows) and each feature becomes a column. You prefix an additional column to represent the constant (<code>n+1</code> columns). This matrix, represented as <code>X</code> is of dimension <code>m x (n+1)</code>. You represent the response variable as a vector <code>y</code> of dimension <code>m x 1</code>.</p> <p></p> <p>The formula to calculate the optimal coefficients is given by \\(\\theta = (X^{T}X)^{-1}X^{T}y\\). Where \\(\\theta\\) is a vector of shape <code>n+1</code> containing \\([\\theta_{0}, \\theta_{1} ... \\theta_{n}]\\).</p>"},{"location":"projects/ml/linear-regression-analytical-solution/#caveats-when-applying-analytical-technique","title":"Caveats when applying analytical technique","text":"<ul> <li>In the analytical, normal equation method, there is no iteration to arrive at optimal \\(\\theta\\). You simply calculate it.</li> <li>You do not have to scale features. It is ok to have them in their native dimensions.</li> </ul>"},{"location":"projects/ml/linear-regression-analytical-solution/#guidelines-for-choosing-between-gd-and-normal-equation","title":"Guidelines for choosing between GD and Normal equation","text":"<ul> <li>GD needs you to play with \\(\\alpha\\) (learning rate), while normal equation does not.</li> <li>GD is an iterative process, while normal eq is not.</li> <li>GD shines well when you have a large number of attributes / features / independent variables. The order of GD is given by \\(O(kn^{2})\\) for <code>n</code> features.</li> <li>Normal equation needs to invert a matrix which is an expensive operation. Its time complexity is given by \\(O(n^{3})\\).</li> <li>If you have <code>&gt;10,000</code> independent variables, or if the number of observations / rows is less than number of independent variables (<code>m &lt; (n+1)</code>), then normal equation not produce a matrix that is invertible. You are better off with Gradient Descent regression.</li> <li>If you have highly correlated features (multi-collinearity) or when you have more features than observations, you might end up with a non-invertible matrix for the normal equation. In these cases, you can choose GD or you can delete some features or regularization techniques if you want to continue with normal equation.</li> <li>GD is an approximation technique, while normal equation is a deterministic approach. GD might settle in a local minima and not global minima. Although, for linear regressions, the shape of the loss function is such that there is no local but only a global minima.</li> </ul>"},{"location":"projects/ml/logistic-reg-concepts/","title":"Understanding logistic regression","text":"<p>A logistic regression is a binary classification algorithm. Its values are <code>0</code> for false cases and <code>1</code> for true cases. Before talking about logistic reg, let us consider why not to use linear regression for classification. We could hypothetically use linear reg to predict class probabilities and could theoretically set a threshold, (usually <code>0.5</code>) above which we group to Class 1 and below which we group to Class 2. However this system has flaws. Consider the graphic below:</p> <p></p> <p>The system works well with the hypothesis function in red, until a new valid but outlier data enters the training. Now the regression function is changed to blue and that changes how it classifies borderline cases. Further, regression does not lend well for multi-class problems. Further, regression is likely to predict <code>&gt;1</code> and <code>&lt;0</code> class probabilities which aren't true.</p>"},{"location":"projects/ml/logistic-reg-concepts/#logistic-regression-model","title":"Logistic regression model","text":"<p>We represent the hypothesis function of logistic regression as \\(0 \\le h_{\\theta}(x) \\le 1\\). The hypothesis function can be represented as:</p> \\[ h_{\\theta}(x) = g(\\theta^{T}x) $$ $$ where \\; g(z) = \\frac{1}{1+e^{-z}} \\] <p>\\(g(z)\\) is a <code>sigmoid</code> function, also called a <code>logistic</code> function, from which the regression gets its name. The hypothesis function looks similar to that of a linear regression, except for the product with the sigmoid function. The shape of the sigmoid function is given by:</p> <pre><code>vals = np.arange(-20,20,0.1)\ngz= 1/(1+np.e**(0-vals))\nplt.plot(vals, gz)\nplt.title('Sigmoid function');\n</code></pre> <p></p> <p>As <code>z</code> reaches \\(\\infty\\), \\(g(z)\\) asymptotes to 1. The values always range betwen <code>0 &lt; g(z) &lt; 1</code>. Thus, the hypothesis can be rewritten as </p> \\[ h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^{T}x}} \\] <p>\\(h_{\\theta}(x)\\) gives us the probability that output = 1. Thus, if \\(h_{\\theta}{x} = 0.7\\), that means a <code>70%</code> probability the output is <code>1</code> or a true case. Mathematically, this is represented as </p> <p>$$ h_{\\theta}(x) = P(y=1 | x;\\theta) = 0.7 $$ which is read as \"probability that y=1, given x, parametertized by \\(\\theta\\)\". Since probability adds to <code>1</code>, we can say, the inverse, probability of <code>y=0</code> is <code>0.3</code>: </p> \\[ h_{\\theta}(x) = P(y=0 | x;\\theta) = 0.3 \\]"},{"location":"projects/ml/logistic-reg-concepts/#decision-boundary-of-a-logistic-regression-model","title":"Decision boundary of a logistic regression model","text":"<p>The shape of the logistic function is such that, <code>g(z) &gt; 0.5</code> when <code>z &gt; 0</code> and <code>g(z) &lt; 0.5</code> when <code>z &lt; 0</code>, and <code>g(z) = 0.5</code> when <code>z=0</code>. The <code>z</code> can be expanded as \\(\\theta^{T}x\\). Additionally, we can simplify that <code>y=1</code> when \\(g(z) \\ge 0.5\\) and <code>y=0</code> when \\(g(z) &lt; 0.5\\).</p> <p>Now, consider this example dataset. The red cross show <code>y=1</code> case and blue circles show <code>y=0</code> case.:</p> <p></p> <p>We can represent this as \\(h_{\\theta}(x) = g(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2})\\). Solving for theta, say, we find that it is <code>[-3;1;1]</code>. Thus, from previous derivation, we know that <code>y=1</code> when \\(-3 + x_{1} + x_{2} \\ge 0\\) as <code>g(z) has to be &gt;= 0</code>. We can simplify it as shown in picture and derive the equation of the decision boundary which is sown in magenta on the pic. The following challenge will explain this better:</p> <p></p>"},{"location":"projects/ml/logistic-reg-concepts/#non-linear-decision-boundaries","title":"Non linear decision boundaries","text":"<p>We can represent non linearity in a linear model by adding additional parameters which are higher order representations of existing parameters, such as \\(h_{\\theta}(x) = g(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{1}^{2} + \\theta_{4}x_{2}^{2})\\) as shown here:</p> <p></p> <p>To get the decision boundary, we need to solve for <code>g(z)=0.5</code> case which happens when <code>z=0</code>. Thus, in pic we solve for \\(x_{1}^{2} + x_{2}^{2} = 1\\) which is the equation of a <code>circle</code>.</p> <p>Using this theory, decision boundaries that take complex shapes can be represented using a linear model and can be solved using logistic regression.</p>"},{"location":"projects/ml/logistic-reg-concepts/#cost-function-for-logistic-regression","title":"Cost function for logistic regression","text":"<p>The same cost function that we had for linear regression would apply for logistic regression, however, it leads to a non-convex loss function. The GD algorithm would fail to arrive at the global minima. Thus, we derive a new cost function as:</p> \\[ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}cost(h_{\\theta}(x),y) $$ where, $$ cost(h_{\\theta}(x), y) = \\begin{cases}                             -log(h_{\\theta}(x)) &amp; y=1 \\\\\\                             -log(1-h_{\\theta}(x)) &amp; y=0                         \\end{cases} \\] <p>Note, <code>log(0) = inf</code>, <code>log(-1) = nan</code>, <code>log(1) = 0</code>. Thus, we can plot this cost term as follows:</p> <pre><code>xvals = np.arange(0,1,0.1)\ny1vals = 0-np.log(xvals)\ny0vals = 0-np.log(1-xvals)\nplt.plot(xvals, y1vals, 'b', label='y=1')\nplt.plot(xvals, y0vals, 'g', label='y=0')\nplt.title('Loss functions of logistic regression')\nplt.legend(); plt.xlabel('Hypothesis: $h\\\\theta(x)$'); plt.ylabel('Loss');\n</code></pre> <p></p> <p>From the graph, when <code>h(x)=1</code> for <code>y=1</code>, the cost term is <code>0</code> (blue line). As predicted class reduces and approaches <code>0</code>, the cost raises to <code>inf</code>. Likewise, when <code>h(x)=0</code> for <code>y=0</code>, the cost is also <code>0</code>. However as the predicted value increases, the cost also increases penalizing the wrong prediction.</p> <p>Instead of having two equations, the cost term can be simplified into the following:</p> \\[ cost(h_{\\theta}x,y) = -ylog(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x)) \\] <p>plugging the cost term in the cost function, we get:</p> \\[ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y_{i}log(h_{\\theta}(x_{i})) + (1-y_{i})log(1-h_{\\theta}(x_{i})) \\]"},{"location":"projects/ml/logistic-reg-concepts/#gradient-descent-optimization-for-logistic-regression","title":"Gradient descent optimization for logistic regression","text":"<p>To find the values of \\(\\theta\\) at the global optima, we need to differentiate the cost function written earlier. This turns out to be</p> <p>repeat until convergence: $$ \\theta_{j} := \\theta_{j} - \\alpha\\sum_{i=1}^{m}(h_{\\theta}(x_{i}) - y_{i})x_{i,j} $$</p> <p>The above update rule, is just the same we had for linear regression. Thus, GD for linear and logistic regression is the same. What has changed is the hypothesis and the cost functions. A vectorized implementation of GD is</p> \\[ \\theta := \\theta - \\frac{\\alpha}{m}X^{T}(g(X\\theta) - \\vec{y}) \\]"},{"location":"projects/ml/logistic-reg-concepts/#using-logistic-regression-for-multiple-classes","title":"Using logistic regression for multiple classes","text":"<p>While the most use cases of logistic reg is to predict boolean classes, we can extend it to a multi-class problem using a technique called one-vs-all. </p> <p></p> <p>We essentially build multiple models, equalling the number of classes. Each model predicts the probability that given value will fall within the class it is trained to predict. Finally we find the class with max probability and assign it to the prediction.</p>"},{"location":"projects/ml/logistic-reg-concepts/#ocr-on-mnist-digits-database-using-multi-class-logistic-regression","title":"OCR on MNIST digits database using multi-class logistic regression","text":"<p>The MNIST database has <code>14</code> million images of handdrawn digits. We work with a subset of <code>5000</code> images. Each image is <code>20x20</code> pixels. When laid out as a column vector (which is how Neural Nets and log reg algorithms will read it), we get a <code>1x400</code> row vector. A sample of 100 images is below:</p> <p></p> <p>Here, we build a multi-class regularized logistic regression model to solve this classification problem. We train this multi-class logistic regression model by iterating a simple log reg model for each class. Each iteration will produce a set of \\(\\theta\\) values which predict the probability for that class. Finally we will assemble all theta into a 2D matrix.</p> <p>The weights / theta is a 2D matrix (unlike a vector for simple logistic reg), where each row is the weight vector for a particular class. Since each pixel is considered an input feature and since we have <code>10</code> classes to predict, we get theta as \\(\\Theta_{10x401}\\) matrix. <code>401</code> because we add <code>1</code> bias feature to <code>400</code> features which is obtained by flattening the <code>20x20</code> image into a <code>400x1</code> vector.</p> <p>During the training process, we need to represent the output class not as digits, but as one-hot encoded vector since that is what log reg understands. Finally during the prediction phase, we will translate the one-hot encoded prediction into the actual class label, which is the predicted digit.</p> <p>The full MATLAB/OCTAVE implementation can be found here.</p>"},{"location":"projects/ml/ml-concepts/","title":"Machine Learning Concepts","text":"<p>Machine Learning (ML) is the art and science of teaching machines with large amounts of data to perform a given task. Unlike typical programming, where the developer defines the methods, we leave it to the machines / algorithms to figure out the patterns from the data itself.</p>"},{"location":"projects/ml/ml-concepts/#classifications-of-ml-systems","title":"Classifications of ML systems","text":"<p>ML algorithms can be classified using the following categories:</p> <ul> <li> <p>Level of supervision - supervised, unsupervised, reinforcement learning</p> <ul> <li> <p>Supervised Learning - training data that is labeled is fed to the algorithm. Examples: KNN, Linear Regression, Logistic Regression, SVM, Decision Trees, RF, Neural Networks</p> </li> <li> <p>Unsupervised learning - unlabeled data is fed to the algorithm and it figures out natural groupings in the data. Some examples include - </p> <ul> <li>Clustering: K-means, Hierarchical cluster analysis (HCA), Expectation maximization. Unlike K-means, HCA can have subdivide each cluster into sub-clusters allowing for better grouping of data.</li> <li>Visualization &amp; Dimensionality reduction: PCA, Kernel PCA, Locally-Linear Embedding (LLE), t-distributed Stochastic Neighbor Embedding (t-SNE)</li> <li>Association rule learning: Apriori, Eclat. Here the goal is to dig into large amounts of data to discover relationships between attributes.</li> </ul> </li> <li>Reinforcement learning: RL is a type of learning where instead of providing labeled data, the algorithm is given either a reward or penalty for its output. The learner (called agent) will attempt multiple attempts (called policies) until it maximizes the reward. RL is used train machines to perform some highly complex tasks such as walking. AlphaGo is an example.</li> </ul> </li> <li> <p>Incremental learning - online vs batch learning</p> <ul> <li>Batch or offline learning: The system is trained using all available data and then put into production for inference. Here it does not learn any more. To retrain, it needs to be taken offline and fed the new data.</li> <li>Online learning: Data is fed incrementally in mini-batches and the model progressively gets refined. The concept of mini-batches allows th model to learn on data that is larger than what can be fit in memory. This is also called out-of-core learning. An important concept in online learning is Learning Rate - which controls how much should the model adapt to new data. A high LR will cause it to forget old data, a low LR will add inertia causing it to not fit new data well.</li> </ul> </li> <li> <p>Model generalization: Instance based vs Model based.</p> <ul> <li>Instance based learning: The model learns to find similarities in input data and uses a similarity score to predict on new data. Example: KNN.</li> <li>Model based learning: Here the algorithm generates a model (a math function) that minimizes the loss and uses that model to perform predictions. Example: Linear regression.</li> </ul> </li> </ul>"},{"location":"projects/ml/ml-concepts/#challenges-in-ml","title":"Challenges in ML","text":"<p>Below are come common challenges in machine learning.</p> <ul> <li>Poor quality data: Training data might be non-representative - with under or oversampled for certain classes. Certain times this is unavoidable, other times it could be due to sampling bias.</li> <li>Overfitting training data: Overfitting happens when the model is too complex relative to the amount and noise in training data. When a model overfits, it starts to use noise (such as coincidences) in data as predictors. A classic sign of overfitting is very low training error, but high test error.<ul> <li>Overfitting can be avoided by either increasing the size of training data (to include diverse data which averages out the noise) or by reducing the complexity of the model through regularization.</li> <li>The amount of regularization is controlled by a hyperparameter. A hyperparameter is a parameter which is set prior to training and remains constant for that training run. Unlike model weights, it is not updated as a result of the training itself.</li> <li>A high regularization value will oversimplify the model, leading to reduced Variance. But can result in a high error of Bias.</li> <li>A low regularization value can lead to a model that overfits - leading to high variability, but low training bias and high test bias.</li> <li>This relationship is called the bias-variance trade-off in machine learning.</li> </ul> </li> </ul>"},{"location":"projects/ml/ml-concepts/#a-machine-learning-checklist","title":"A Machine Learning checklist","text":"<p>Adapted from Hands-on ML from Scikit-Learn and TensorFlow.</p> <ol> <li>Frame the problem &amp; look at the big picture<ul> <li>Define the problem in business terms.</li> <li>Define it in ML terms (which type of ML will be used, what are the metrics)</li> <li>How should performance be measured, what's minimum performance</li> <li>Look for similar problems</li> <li>If you were to solve without ML, how would the solution be?</li> </ul> </li> <li>Get the data<ul> <li>Plan how much of the data is needed</li> <li>Plan where data will be stored and how it will be accessed (data workspace)</li> <li>Verify access privileges and permissions / licenses</li> <li>Remove PII</li> <li>Make test set and put it aside. Do everything else including EDA on remaining train set.</li> <li>Make a back-up copy that will not be touched at anytime. If you have to recover from back-up, make another back-up copy.</li> <li>Automate this process so it can be repeated if new data comes along.</li> </ul> </li> <li>EDA<ul> <li>Use an interface like Jupyter Notebooks for this process.</li> <li>Study each attribute and its characteristics (distribution, type, missing values, noise)</li> <li>Visualize using plots and maps</li> <li>Study correlations b/w attributes</li> <li>Identify useful transformations that can be applied</li> <li>Identify how data can be enriched (bringing in ancillary data sources)</li> </ul> </li> <li>Prepare the data<ul> <li>Write all transformations as functions that can be repeated on any data</li> <li>data cleaning - fix outliers, missing data</li> <li>feature selection - make new features, drop irrelevant features</li> <li>feature engineering - discretize continuous features, decompose complex features (datetime, categories etc.), apply transformations</li> <li>feature scaling - normalize / standardize features</li> </ul> </li> <li>Short-list promising models<ul> <li>Take a quick &amp; dirty approach and train several models on a subset</li> <li>Measure and compare the performance - for each model, use N-fold cross-validation and collect performance by computing mean and SD of performance metric</li> <li>Collect most significant variables for each model</li> <li>Iterate to feature selection / engineering</li> <li>Retrain</li> <li>Short-list top 3 models</li> </ul> </li> <li>Fine-tune the system<ul> <li>Bring back the full train set for this step</li> <li>Fine tune hyperparameters using cross-validation</li> <li>try ensemble methods</li> <li>measure performance on test set (generalization error). Don't tune the model beyond this step (if you tune to perform on test set, you end up with an overfit model)</li> </ul> </li> <li>Present the solution<ul> <li>Document and clean up the code</li> <li>Make a business presentation - explaining how business objective is being met / exceeded.</li> <li>Highlight interesting / hidden info that was discovered. Use as viz lavishly.</li> </ul> </li> </ol>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/","title":"MNIST digits classification using Logistic regression in Scikit-Learn","text":"In\u00a0[1]: Copied! <pre>from sklearn.datasets import load_digits\ndigits = load_digits()\n</pre> from sklearn.datasets import load_digits digits = load_digits() In\u00a0[2]: Copied! <pre>type(digits.data)\n</pre> type(digits.data) Out[2]: <pre>numpy.ndarray</pre> In\u00a0[3]: Copied! <pre>(digits.data.shape, digits.target.shape, digits.images.shape)\n</pre> (digits.data.shape, digits.target.shape, digits.images.shape) Out[3]: <pre>((1797, 64), (1797,), (1797, 8, 8))</pre> <p><code>1797</code> images, each <code>8x8</code> in dimension and <code>1797</code> labels.</p> In\u00a0[4]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[5]: Copied! <pre>plt.figure(figsize=(20,4))\nfor index, (image, label) in enumerate(zip(digits.data[0:5], \n                                           digits.target[0:5])):\n    plt.subplot(1, 5, index + 1)\n    plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n    plt.title('Training: %i\\n' % label, fontsize = 20);\n</pre> plt.figure(figsize=(20,4)) for index, (image, label) in enumerate(zip(digits.data[0:5],                                             digits.target[0:5])):     plt.subplot(1, 5, index + 1)     plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)     plt.title('Training: %i\\n' % label, fontsize = 20); In\u00a0[5]: Copied! <pre>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data, \n                                                    digits.target,\n                                                   test_size=0.25,\n                                                   random_state=0)\n</pre> from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(digits.data,                                                      digits.target,                                                    test_size=0.25,                                                    random_state=0) In\u00a0[7]: Copied! <pre>X_train.shape, X_test.shape\n</pre> X_train.shape, X_test.shape Out[7]: <pre>((1347, 64), (450, 64))</pre> In\u00a0[6]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(fit_intercept=True,\n                        multi_class='auto',\n                        penalty='l2', #ridge regression\n                        solver='saga',\n                        max_iter=10000,\n                        C=50)\nclf\n</pre> from sklearn.linear_model import LogisticRegression clf = LogisticRegression(fit_intercept=True,                         multi_class='auto',                         penalty='l2', #ridge regression                         solver='saga',                         max_iter=10000,                         C=50) clf Out[6]: <pre>LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n                   warm_start=False)</pre> In\u00a0[9]: Copied! <pre>%%time\nclf.fit(X_train, y_train)\n</pre> %%time clf.fit(X_train, y_train) <pre>CPU times: user 6.81 s, sys: 9.52 ms, total: 6.81 s\nWall time: 6.82 s\n</pre> Out[9]: <pre>LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n                   warm_start=False)</pre> <p>Let us see what the classifier has learned</p> In\u00a0[10]: Copied! <pre>clf.classes_\n</pre> clf.classes_ Out[10]: <pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[11]: Copied! <pre>clf.coef_.shape\n</pre> clf.coef_.shape Out[11]: <pre>(10, 64)</pre> In\u00a0[12]: Copied! <pre>clf.coef_[0].round(2) # prints weights for 8x8 image for class 0\n</pre> clf.coef_[0].round(2) # prints weights for 8x8 image for class 0 Out[12]: <pre>array([ 0.  , -0.  , -0.04,  0.1 ,  0.06, -0.14, -0.16, -0.02, -0.  ,\n       -0.03, -0.04,  0.2 ,  0.09,  0.08, -0.05, -0.01, -0.  ,  0.06,\n        0.15, -0.03, -0.39,  0.25,  0.09, -0.  , -0.  ,  0.13,  0.16,\n       -0.18, -0.57,  0.02,  0.12, -0.  ,  0.  ,  0.16,  0.11, -0.16,\n       -0.41,  0.05,  0.08,  0.  , -0.  , -0.06,  0.27, -0.11, -0.2 ,\n        0.15,  0.04, -0.  , -0.  , -0.12,  0.08, -0.05,  0.2 ,  0.1 ,\n       -0.04, -0.01, -0.  , -0.01, -0.09,  0.21, -0.04, -0.06, -0.1 ,\n       -0.05])</pre> In\u00a0[13]: Copied! <pre>clf.intercept_ # for 10 classes - this is a One-vs-All classification\n</pre> clf.intercept_ # for 10 classes - this is a One-vs-All classification Out[13]: <pre>array([ 0.0010181 , -0.07236521,  0.00379207,  0.00459855,  0.04585855,\n        0.00014299, -0.00442972,  0.01179654,  0.04413398, -0.03454583])</pre> In\u00a0[14]: Copied! <pre>clf.n_iter_[0] # num of iterations before tolerance was reached\n</pre> clf.n_iter_[0] # num of iterations before tolerance was reached Out[14]: <pre>1876</pre> In\u00a0[16]: Copied! <pre>coef = clf.coef_.copy()\nplt.imshow(coef[0].reshape(8,8).round(2));  # proof of concept\n</pre> coef = clf.coef_.copy() plt.imshow(coef[0].reshape(8,8).round(2));  # proof of concept In\u00a0[17]: Copied! <pre>coef = clf.coef_.copy()\nscale = np.abs(coef).max()\nplt.figure(figsize=(10,5))\n\nfor i in range(10): # 0-9\n    coef_plot = plt.subplot(2, 5, i + 1) # 2x5 plot\n\n    coef_plot.imshow(coef[i].reshape(8,8), \n                     cmap=plt.cm.RdBu,\n                     vmin=-scale, vmax=scale,\n                    interpolation='bilinear')\n    \n    coef_plot.set_xticks(()); coef_plot.set_yticks(()) # remove ticks\n    coef_plot.set_xlabel(f'Class {i}')\n\nplt.suptitle('Coefficients for various classes');\n</pre> coef = clf.coef_.copy() scale = np.abs(coef).max() plt.figure(figsize=(10,5))  for i in range(10): # 0-9     coef_plot = plt.subplot(2, 5, i + 1) # 2x5 plot      coef_plot.imshow(coef[i].reshape(8,8),                       cmap=plt.cm.RdBu,                      vmin=-scale, vmax=scale,                     interpolation='bilinear')          coef_plot.set_xticks(()); coef_plot.set_yticks(()) # remove ticks     coef_plot.set_xlabel(f'Class {i}')  plt.suptitle('Coefficients for various classes'); <p>Now predict on unknown dataset and compare with ground truth</p> In\u00a0[18]: Copied! <pre>print(clf.predict(X_test[0:9]))\nprint(y_test[0:9])\n</pre> print(clf.predict(X_test[0:9])) print(y_test[0:9]) <pre>[2 8 2 6 6 7 1 9 8]\n[2 8 2 6 6 7 1 9 8]\n</pre> <p>Score against training and test data</p> In\u00a0[19]: Copied! <pre>clf.score(X_train, y_train) # training score\n</pre> clf.score(X_train, y_train) # training score Out[19]: <pre>1.0</pre> In\u00a0[20]: Copied! <pre>score = clf.score(X_test, y_test) # test score\nscore\n</pre> score = clf.score(X_test, y_test) # test score score Out[20]: <pre>0.9555555555555556</pre> <p>Test score: <code>0.9555</code></p> In\u00a0[21]: Copied! <pre>from sklearn import metrics\n</pre> from sklearn import metrics In\u00a0[22]: Copied! <pre>predictions = clf.predict(X_test)\n\ncm = metrics.confusion_matrix(y_true=y_test, \n                         y_pred = predictions, \n                        labels = clf.classes_)\ncm\n</pre> predictions = clf.predict(X_test)  cm = metrics.confusion_matrix(y_true=y_test,                           y_pred = predictions,                          labels = clf.classes_) cm Out[22]: <pre>array([[37,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0, 40,  0,  0,  0,  0,  0,  0,  2,  1],\n       [ 0,  0, 42,  2,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0, 43,  0,  0,  0,  0,  1,  1],\n       [ 0,  0,  0,  0, 37,  0,  0,  1,  0,  0],\n       [ 0,  0,  0,  0,  0, 46,  0,  0,  0,  2],\n       [ 0,  1,  0,  0,  0,  0, 51,  0,  0,  0],\n       [ 0,  0,  0,  1,  1,  0,  0, 46,  0,  0],\n       [ 0,  3,  1,  0,  0,  0,  0,  0, 43,  1],\n       [ 0,  0,  0,  0,  0,  1,  0,  0,  1, 45]])</pre> <p>Visualize confusion matrix as a heatmap</p> In\u00a0[23]: Copied! <pre>import seaborn as sns\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot=True, \n            linewidths=.5, square = True, cmap = 'Blues_r');\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nall_sample_title = 'Accuracy Score: {0}'.format(score)\nplt.title(all_sample_title);\n</pre> import seaborn as sns  plt.figure(figsize=(10,10)) sns.heatmap(cm, annot=True,              linewidths=.5, square = True, cmap = 'Blues_r');  plt.ylabel('Actual label') plt.xlabel('Predicted label') all_sample_title = 'Accuracy Score: {0}'.format(score) plt.title(all_sample_title); In\u00a0[24]: Copied! <pre>index = 0\nmisclassified_images = []\nfor label, predict in zip(y_test, predictions):\n    if label != predict: \n        misclassified_images.append(index)\n    index +=1\n</pre> index = 0 misclassified_images = [] for label, predict in zip(y_test, predictions):     if label != predict:          misclassified_images.append(index)     index +=1 In\u00a0[25]: Copied! <pre>print(misclassified_images)\n</pre> print(misclassified_images) <pre>[56, 94, 118, 124, 130, 169, 181, 196, 213, 251, 315, 325, 331, 335, 378, 398, 425, 429, 430, 440]\n</pre> In\u00a0[26]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.suptitle('Misclassifications');\n\nfor plot_index, bad_index in enumerate(misclassified_images[0:20]):\n    p = plt.subplot(4,5, plot_index+1) # 4x5 plot\n    \n    p.imshow(X_test[bad_index].reshape(8,8), cmap=plt.cm.gray,\n            interpolation='bilinear')\n    p.set_xticks(()); p.set_yticks(()) # remove ticks\n    \n    p.set_title(f'Pred: {predictions[bad_index]}, Actual: {y_test[bad_index]}');\n</pre> plt.figure(figsize=(10,10)) plt.suptitle('Misclassifications');  for plot_index, bad_index in enumerate(misclassified_images[0:20]):     p = plt.subplot(4,5, plot_index+1) # 4x5 plot          p.imshow(X_test[bad_index].reshape(8,8), cmap=plt.cm.gray,             interpolation='bilinear')     p.set_xticks(()); p.set_yticks(()) # remove ticks          p.set_title(f'Pred: {predictions[bad_index]}, Actual: {y_test[bad_index]}'); In\u00a0[7]: Copied! <pre>%%time\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml(data_id=554) # https://www.openml.org/d/554\n</pre> %%time from sklearn.datasets import fetch_openml mnist = fetch_openml(data_id=554) # https://www.openml.org/d/554 <pre>CPU times: user 15.3 s, sys: 348 ms, total: 15.6 s\nWall time: 15.6 s\n</pre> In\u00a0[8]: Copied! <pre>type(mnist)\n</pre> type(mnist) Out[8]: <pre>sklearn.utils.Bunch</pre> In\u00a0[9]: Copied! <pre>type(mnist.data), type(mnist.categories), type(mnist.feature_names), type(mnist.target)\n</pre> type(mnist.data), type(mnist.categories), type(mnist.feature_names), type(mnist.target) Out[9]: <pre>(numpy.ndarray, dict, list, numpy.ndarray)</pre> In\u00a0[10]: Copied! <pre>mnist.data.shape, mnist.target.shape\n</pre> mnist.data.shape, mnist.target.shape Out[10]: <pre>((70000, 784), (70000,))</pre> <p>There are <code>70,000</code> images, each of dimension <code>28x28</code> pixels.</p> In\u00a0[11]: Copied! <pre>mnist.target[0]\n</pre> mnist.target[0] Out[11]: <pre>'5'</pre> In\u00a0[12]: Copied! <pre>plt.figure(figsize=(20,4))\nfor index, (image, label) in enumerate(zip(mnist.data[0:5], \n                                           mnist.target[0:5])):\n    plt.subplot(1, 5, index + 1)\n    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n    plt.title('Training: ' + label, fontsize = 20);\n</pre> plt.figure(figsize=(20,4)) for index, (image, label) in enumerate(zip(mnist.data[0:5],                                             mnist.target[0:5])):     plt.subplot(1, 5, index + 1)     plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)     plt.title('Training: ' + label, fontsize = 20); In\u00a0[13]: Copied! <pre>mnist.target.astype('int')\n</pre> mnist.target.astype('int') Out[13]: <pre>array([5, 0, 4, ..., 4, 5, 6])</pre> In\u00a0[14]: Copied! <pre>from sklearn.model_selection import train_test_split\nX2_train, X2_test, y2_train, y2_test = train_test_split(mnist.data, \n                                                    mnist.target.astype('int'), #targets str to int convert\n                                                   test_size=1/7.0,\n                                                   random_state=0)\n</pre> from sklearn.model_selection import train_test_split X2_train, X2_test, y2_train, y2_test = train_test_split(mnist.data,                                                      mnist.target.astype('int'), #targets str to int convert                                                    test_size=1/7.0,                                                    random_state=0) In\u00a0[15]: Copied! <pre>X2_train.shape, X2_test.shape\n</pre> X2_train.shape, X2_test.shape Out[15]: <pre>((60000, 784), (10000, 784))</pre> <p>Are the different classes evenly distributed? We can find this by plotting a histogram of the labels in both test and training datasets.</p> In\u00a0[77]: Copied! <pre>plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.hist(y2_train);\nplt.title('Frequency of different classes - Training data');\n\nplt.subplot(1,2,2)\nplt.hist(y2_test);\nplt.title('Frequency of different classes - Test data');\n</pre> plt.figure(figsize=(10,5)) plt.subplot(1,2,1) plt.hist(y2_train); plt.title('Frequency of different classes - Training data');  plt.subplot(1,2,2) plt.hist(y2_test); plt.title('Frequency of different classes - Test data'); In\u00a0[57]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nclf2 = LogisticRegression(fit_intercept=True,\n                        multi_class='auto',\n                        penalty='l1', #lasso regression\n                        solver='saga',\n                        max_iter=1000,\n                        C=50,\n                        verbose=2, # output progress\n                        n_jobs=5, # parallelize over 5 processes\n                        tol=0.01\n                         )\nclf2\n</pre> from sklearn.linear_model import LogisticRegression clf2 = LogisticRegression(fit_intercept=True,                         multi_class='auto',                         penalty='l1', #lasso regression                         solver='saga',                         max_iter=1000,                         C=50,                         verbose=2, # output progress                         n_jobs=5, # parallelize over 5 processes                         tol=0.01                          ) clf2 Out[57]: <pre>LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=5, penalty='l1',\n                   random_state=None, solver='saga', tol=0.01, verbose=2,\n                   warm_start=False)</pre> <p>Since there are <code>10</code> classes and <code>12</code> available cores, we will try to run the learning step in <code>5</code> jobs. Earlier, when I did not parallelize, the job did not finish within 1 hour, when I had to put the machine to sleep for a meeting.</p> In\u00a0[58]: Copied! <pre>%%time\nclf2.fit(X2_train, y2_train)\n</pre> %%time clf2.fit(X2_train, y2_train) <pre>[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n</pre> <pre>convergence after 47 epochs took 143 seconds\nCPU times: user 9min 30s, sys: 469 ms, total: 9min 30s\nWall time: 2min 22s\n</pre> <pre>[Parallel(n_jobs=5)]: Done   1 out of   1 | elapsed:  2.4min finished\n</pre> Out[58]: <pre>LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=5, penalty='l1',\n                   random_state=None, solver='saga', tol=0.01, verbose=2,\n                   warm_start=False)</pre> <p>Note: Since the verbosity is set <code>&gt;0</code>, the messages were printed, but they got printed on the terminal, not in the notebook.</p> <p>Let us see what the classifier has learned</p> In\u00a0[29]: Copied! <pre>clf2.classes_\n</pre> clf2.classes_ Out[29]: <pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[30]: Copied! <pre>clf2.coef_.shape\n</pre> clf2.coef_.shape Out[30]: <pre>(10, 784)</pre> <p>Get the coefficients for a single class, <code>1</code> in this case:</p> In\u00a0[59]: Copied! <pre>clf2.coef_[1].round(3) # prints weights for 8x8 image for class 0\n</pre> clf2.coef_[1].round(3) # prints weights for 8x8 image for class 0 Out[59]: <pre>array([ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   , -0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   ,\n       -0.001, -0.001, -0.001,  0.   ,  0.002,  0.004,  0.001,  0.002,\n        0.002,  0.001, -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n       -0.   , -0.   , -0.   , -0.001, -0.001, -0.002, -0.002, -0.003,\n        0.001,  0.002, -0.001,  0.001,  0.002,  0.   , -0.002,  0.   ,\n       -0.001, -0.001, -0.001, -0.001, -0.001, -0.   , -0.   ,  0.   ,\n        0.   ,  0.   ,  0.   , -0.   ,  0.   ,  0.001,  0.   , -0.   ,\n       -0.   , -0.   ,  0.   , -0.   ,  0.   ,  0.001,  0.   ,  0.   ,\n       -0.   ,  0.001, -0.001,  0.001,  0.   , -0.   , -0.001, -0.001,\n       -0.003, -0.002, -0.   , -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.003,  0.001, -0.002, -0.003, -0.002, -0.003, -0.003,\n       -0.002,  0.   , -0.001,  0.001, -0.001, -0.001,  0.   , -0.001,\n        0.   ,  0.   ,  0.002,  0.002, -0.001, -0.002, -0.   , -0.   ,\n        0.   ,  0.   , -0.   , -0.   ,  0.   ,  0.001,  0.   , -0.003,\n       -0.002, -0.001,  0.   ,  0.   , -0.002, -0.002, -0.001, -0.002,\n       -0.001, -0.003,  0.   , -0.001, -0.   , -0.   , -0.   ,  0.   ,\n       -0.001, -0.002, -0.   , -0.   ,  0.   , -0.   , -0.   , -0.   ,\n        0.   , -0.   , -0.002, -0.001, -0.003,  0.   , -0.   , -0.002,\n        0.001, -0.002,  0.001, -0.003,  0.   , -0.001, -0.002, -0.   ,\n        0.001,  0.   , -0.002, -0.001, -0.003, -0.002, -0.   , -0.   ,\n        0.   , -0.   , -0.   , -0.   ,  0.001, -0.001, -0.002,  0.001,\n       -0.002, -0.003, -0.001, -0.002, -0.   ,  0.001, -0.001,  0.   ,\n       -0.003,  0.001, -0.001,  0.001, -0.002, -0.001, -0.001, -0.003,\n       -0.004, -0.002, -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   , -0.002, -0.001,  0.001,  0.   , -0.002, -0.001, -0.001,\n       -0.001, -0.   , -0.   ,  0.003,  0.001, -0.001,  0.   , -0.002,\n       -0.002, -0.001, -0.001, -0.003, -0.002, -0.002, -0.   , -0.   ,\n       -0.   , -0.   , -0.   , -0.001, -0.001, -0.003, -0.002, -0.001,\n        0.   , -0.001, -0.001,  0.001, -0.   ,  0.002,  0.003,  0.003,\n        0.002,  0.001, -0.001, -0.   , -0.002,  0.   , -0.001, -0.001,\n       -0.002, -0.001, -0.   ,  0.   ,  0.   ,  0.   , -0.   , -0.   ,\n       -0.001, -0.002, -0.004, -0.003, -0.002, -0.001, -0.001, -0.001,\n       -0.001,  0.001,  0.003,  0.003,  0.   , -0.001,  0.   , -0.001,\n       -0.   , -0.002, -0.001, -0.001, -0.001, -0.   , -0.   ,  0.   ,\n        0.   , -0.   , -0.   , -0.   ,  0.   , -0.001, -0.001, -0.001,\n       -0.001,  0.   , -0.001, -0.002, -0.   ,  0.002,  0.005,  0.003,\n        0.   , -0.   , -0.001, -0.001, -0.001, -0.   , -0.   , -0.   ,\n       -0.001, -0.   , -0.   ,  0.   ,  0.   , -0.   , -0.   ,  0.   ,\n        0.001,  0.   ,  0.   , -0.001,  0.   ,  0.001, -0.004, -0.002,\n        0.   ,  0.001,  0.002,  0.002,  0.001,  0.003, -0.003, -0.002,\n        0.   ,  0.   , -0.001, -0.001, -0.001, -0.   , -0.   , -0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.001,  0.   , -0.001, -0.001,\n        0.001, -0.001, -0.003, -0.002, -0.   ,  0.001,  0.004,  0.002,\n        0.001, -0.   , -0.003, -0.003, -0.   , -0.001, -0.001, -0.001,\n       -0.   , -0.   , -0.   , -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n       -0.   , -0.001, -0.001, -0.   ,  0.001, -0.002, -0.003, -0.   ,\n        0.001,  0.002,  0.004, -0.001,  0.003, -0.001, -0.002, -0.005,\n       -0.002, -0.001, -0.001, -0.001, -0.   , -0.   , -0.   , -0.   ,\n        0.   ,  0.   ,  0.   , -0.   , -0.   , -0.001, -0.001, -0.001,\n       -0.001,  0.001, -0.   , -0.   , -0.001,  0.001,  0.003, -0.002,\n        0.001, -0.005, -0.003, -0.003, -0.001, -0.   , -0.001, -0.001,\n        0.001, -0.001, -0.   , -0.   ,  0.   ,  0.   ,  0.   , -0.   ,\n       -0.   , -0.001, -0.002, -0.002, -0.001, -0.001,  0.   , -0.001,\n        0.001,  0.003,  0.002,  0.001, -0.001, -0.005, -0.001, -0.   ,\n       -0.   ,  0.   ,  0.   , -0.   , -0.   , -0.001,  0.   , -0.   ,\n       -0.   ,  0.   , -0.   , -0.   , -0.   , -0.003, -0.005, -0.003,\n        0.   ,  0.   , -0.002, -0.001,  0.   , -0.   ,  0.002, -0.001,\n       -0.003,  0.   ,  0.002,  0.   ,  0.002,  0.   , -0.001, -0.   ,\n        0.001,  0.002,  0.001,  0.   ,  0.   ,  0.   , -0.   , -0.   ,\n       -0.001, -0.005, -0.002,  0.001, -0.001,  0.001, -0.001, -0.001,\n       -0.   ,  0.002, -0.002, -0.001,  0.002, -0.001, -0.001,  0.001,\n        0.001, -0.001, -0.001,  0.   ,  0.002,  0.001,  0.001,  0.   ,\n        0.   , -0.   , -0.   , -0.   , -0.   , -0.005, -0.   ,  0.   ,\n        0.   ,  0.002,  0.003,  0.002,  0.001, -0.001,  0.001, -0.   ,\n        0.001,  0.002,  0.003,  0.001,  0.   , -0.001, -0.   ,  0.001,\n        0.001,  0.001,  0.001,  0.   ,  0.   ,  0.   , -0.   ,  0.001,\n        0.002,  0.003,  0.003,  0.002,  0.002, -0.001,  0.001, -0.001,\n       -0.   ,  0.001,  0.002,  0.001,  0.001,  0.001,  0.003,  0.001,\n        0.003,  0.003, -0.001,  0.   ,  0.003,  0.001,  0.   ,  0.   ,\n        0.   ,  0.   , -0.   ,  0.   ,  0.001,  0.007,  0.003,  0.   ,\n       -0.   ,  0.001, -0.002, -0.001, -0.002, -0.004, -0.001, -0.   ,\n       -0.   ,  0.   ,  0.002,  0.001,  0.003,  0.002, -0.   , -0.   ,\n        0.001,  0.001,  0.   ,  0.   ,  0.   ,  0.   , -0.   , -0.001,\n       -0.002,  0.002,  0.001,  0.001,  0.   ,  0.001,  0.   ,  0.   ,\n        0.001,  0.001, -0.001,  0.002,  0.001,  0.002, -0.   ,  0.   ,\n       -0.001, -0.001, -0.001, -0.001, -0.   , -0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   , -0.001, -0.001, -0.001, -0.001, -0.   ,\n       -0.001,  0.001, -0.001, -0.001, -0.001, -0.001, -0.001, -0.   ,\n       -0.004,  0.001,  0.001,  0.   , -0.001, -0.001, -0.001, -0.   ,\n       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   , -0.   ,\n       -0.   , -0.001, -0.001, -0.002, -0.001, -0.003, -0.006, -0.004,\n       -0.001, -0.002, -0.002, -0.003, -0.004, -0.003, -0.002, -0.002,\n       -0.001, -0.   , -0.   , -0.   , -0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   , -0.   , -0.001, -0.001, -0.002, -0.002, -0.001, -0.001,\n       -0.   , -0.   , -0.001, -0.001, -0.   , -0.   , -0.   , -0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ])</pre> <pre>convergence after 591 epochs took 1805 seconds\n</pre> In\u00a0[60]: Copied! <pre>clf2.intercept_ # for 10 classes - this is a One-vs-All classification\n</pre> clf2.intercept_ # for 10 classes - this is a One-vs-All classification Out[60]: <pre>array([-1.11398188e-04,  1.38709472e-04,  1.16909054e-04, -2.37842193e-04,\n        6.62466316e-05,  8.48133979e-04, -4.22181499e-05,  2.66499796e-04,\n       -8.62715013e-04, -1.82325388e-04])</pre> In\u00a0[78]: Copied! <pre>clf2.n_iter_[0] # num of iterations before tolerance was reached\n</pre> clf2.n_iter_[0] # num of iterations before tolerance was reached Out[78]: <pre>47</pre> In\u00a0[62]: Copied! <pre>coef = clf2.coef_.copy()\nscale = np.abs(coef).max()\nplt.figure(figsize=(13,7))\n\nfor i in range(10): # 0-9\n    coef_plot = plt.subplot(2, 5, i + 1) # 2x5 plot\n\n    coef_plot.imshow(coef[i].reshape(28,28), \n                     cmap=plt.cm.RdBu,\n                     vmin=-scale, vmax=scale,\n                    interpolation='bilinear')\n    \n    coef_plot.set_xticks(()); coef_plot.set_yticks(()) # remove ticks\n    coef_plot.set_xlabel(f'Class {i}')\n\nplt.suptitle('Coefficients for various classes');\n</pre> coef = clf2.coef_.copy() scale = np.abs(coef).max() plt.figure(figsize=(13,7))  for i in range(10): # 0-9     coef_plot = plt.subplot(2, 5, i + 1) # 2x5 plot      coef_plot.imshow(coef[i].reshape(28,28),                       cmap=plt.cm.RdBu,                      vmin=-scale, vmax=scale,                     interpolation='bilinear')          coef_plot.set_xticks(()); coef_plot.set_yticks(()) # remove ticks     coef_plot.set_xlabel(f'Class {i}')  plt.suptitle('Coefficients for various classes'); <p>Now predict on unknown dataset and compare with ground truth</p> In\u00a0[63]: Copied! <pre>print(clf2.predict(X2_test[0:9]))\nprint(y2_test[0:9])\n</pre> print(clf2.predict(X2_test[0:9])) print(y2_test[0:9]) <pre>[0 4 1 2 4 7 7 1 1]\n[0 4 1 2 7 9 7 1 1]\n</pre> <p>Score against training and test data</p> In\u00a0[64]: Copied! <pre>clf2.score(X2_train, y2_train) # training score\n</pre> clf2.score(X2_train, y2_train) # training score Out[64]: <pre>0.9374333333333333</pre> In\u00a0[65]: Copied! <pre>score2 = clf2.score(X2_test, y2_test) # test score\nscore2\n</pre> score2 = clf2.score(X2_test, y2_test) # test score score2 Out[65]: <pre>0.9191</pre> <p>Test Score: <code>0.9191</code> or 91%</p> In\u00a0[46]: Copied! <pre>from sklearn import metrics\n</pre> from sklearn import metrics In\u00a0[66]: Copied! <pre>predictions2 = clf2.predict(X2_test)\n\ncm = metrics.confusion_matrix(y_true=y2_test, \n                         y_pred = predictions2, \n                        labels = clf2.classes_)\ncm\n</pre> predictions2 = clf2.predict(X2_test)  cm = metrics.confusion_matrix(y_true=y2_test,                           y_pred = predictions2,                          labels = clf2.classes_) cm Out[66]: <pre>array([[ 967,    0,    1,    2,    1,    9,    9,    0,    7,    0],\n       [   0, 1114,    5,    3,    1,    5,    0,    4,    7,    2],\n       [   3,   13,  931,   18,   11,    1,   15,   10,   34,    4],\n       [   1,    5,   33,  894,    0,   26,    2,   12,   27,   13],\n       [   1,    2,    5,    1,  897,    1,   11,    9,    7,   28],\n       [  10,    2,    6,   30,    9,  747,   16,    6,   30,    7],\n       [   7,    3,    6,    0,   11,   18,  938,    1,    5,    0],\n       [   2,    5,   13,    2,   11,    2,    1,  982,    4,   42],\n       [   4,   18,    8,   18,    6,   25,    9,    2,  861,   12],\n       [   3,    5,    6,   10,   35,    7,    2,   32,    9,  860]])</pre> In\u00a0[71]: Copied! <pre>import seaborn as sns\n\nplt.figure(figsize=(12,12))\nsns.heatmap(cm, annot=True, \n            linewidths=.5, square = True, cmap = 'Blues_r', fmt='0.4g');\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nall_sample_title = 'Accuracy Score: {0}'.format(score2)\nplt.title(all_sample_title);\n</pre> import seaborn as sns  plt.figure(figsize=(12,12)) sns.heatmap(cm, annot=True,              linewidths=.5, square = True, cmap = 'Blues_r', fmt='0.4g');  plt.ylabel('Actual label') plt.xlabel('Predicted label') all_sample_title = 'Accuracy Score: {0}'.format(score2) plt.title(all_sample_title); <pre>convergence after 5470 epochs took 10856 seconds\n</pre>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#mnist-digits-classification-using-logistic-regression-in-scikit-learn","title":"MNIST digits classification using Logistic regression in Scikit-Learn\u00b6","text":"<p>This notebook is broadly adopted from this blog and this scikit-learn example</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#logistic-regression-on-smaller-built-in-subset","title":"Logistic regression on smaller built-in subset\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#load-the-dataset","title":"Load the dataset\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#display-sample-data","title":"Display sample data\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#split-into-training-and-test","title":"Split into training and test\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#learning","title":"Learning\u00b6","text":"<p>Refer to the Logistic reg API ref for these parameters and the guide for equations, particularly how penalties are applied.</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#viewing-coefficients-as-an-image","title":"Viewing coefficients as an image\u00b6","text":"<p>Since there is a coefficient for each pixel in the <code>8x8</code> image, we can view them as an image itself. The code below is similar to the original viz code, but runs on coeff.</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#prediction-and-scoring","title":"Prediction and scoring\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#confusion-matrix","title":"Confusion matrix\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#inspecting-misclassified-images","title":"Inspecting misclassified images\u00b6","text":"<p>We compare predictions with labels to find which images are wrongly classified, then display them.</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#predicting-on-full-mnist-database","title":"Predicting on full MNIST database\u00b6","text":"<p>In the previous section, we worked with as tiny subset. In this section, we will download and play with the full MNIST dataset. Downloading for the first time from open ml db takes me about half a minute. Since this dataset is cached locally, subsequent runs should not take as much.</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#preview-some-images","title":"Preview some images\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#split-into-training-and-test","title":"Split into training and test\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#learning","title":"Learning\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#visualize-coefficients-as-an-image","title":"Visualize coefficients as an image\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#prediction-and-scoring","title":"Prediction and scoring\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#confusion-matrix","title":"Confusion matrix\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#conclusion","title":"Conclusion\u00b6","text":"<p>This notebook shows performing multi-class classification using logistic regression using one-vs-all technique. When run on MNIST DB, the best accuracy is still just 91%. There is still scope for improvement.</p>"},{"location":"projects/ml/model-regularization/","title":"Reducing overfitting using regularization","text":""},{"location":"projects/ml/model-regularization/#examples-of-overfitting","title":"Examples of overfitting","text":"<p>A model is set to be over fitting, when it performs exceedingly well on training data but poorly on validation/test data. Such a model typically is of higher order and having a high variance.</p> <p></p> <p>The image above shows 3 models, the one of right is over fitting and the one on left is under fitting (has a high bias). Under fitting can happen when the model is too simple or uses too few features to model the complexity. A overfitting model has high variance because if you change or shuffle the input training set slightly, the model changes dramatically. In other words, it has high variability depending on the input set.</p> <p></p> <p>The graphic above shows the different levels of fit for logistic regression.</p>"},{"location":"projects/ml/model-regularization/#addressing-overfitting","title":"Addressing overfitting","text":"<p>Some options include</p> <ol> <li>reducing number of features. However this leads to reducing useful information available.</li> <li>Regularization. Here, we will keep all the features, but limit or constrain the magnitude their coefficients \\(\\theta_{j}\\). This works well even when you have a lot of features.</li> </ol>"},{"location":"projects/ml/model-regularization/#regularization-for-linear-regression","title":"Regularization for linear regression","text":"<p>Regularization is the process of applying penalty to coefficients of higher order variables. The higher the coefficients are for those variables, the higher the cost/loss is. Thus, the optimization process will move toward fits where such coefficients are smaller, close to <code>0</code>. Intuitively, this leads to a simpler model, that is less prone to overfitting. In practice, we may not know which variables are higher order polynomials. Thus, we add penalties to all coefficients. Thus, the new cost function looks like</p> <p>$$ J(\\theta) = \\frac{1}{2m}[\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\lambda \\sum_{j=1}^{n}\\theta_{j}^{2}] $$ where \\(\\lambda\\) is the regularization parameter. The first part of the loss function fights to get the best fit while the second fights to keep the model simple and coefficients smaller. If \\(\\lambda\\) is too high, then model results in underfitting which has a high bias. If \\(\\lambda\\) is too low, then it results in overfitting which has a high variance.</p>"},{"location":"projects/ml/model-regularization/#l1-l2-lasso-ridge-regularizations","title":"L1, L2 (Lasso, Ridge) regularizations","text":"<p>In the equation above, the \\(\\theta\\) was squared in the regularization function. This is L2 regularization, aka. Ridge regression. Remember this as L2 squares the coefficients (\\(\\theta\\)) attached to \\(\\lambda\\). </p> <p>Whereas, in the case of L1, the absolute value of \\(\\theta\\) is used. L1 is also called Lasso regression which stands for Least Absolute Shrinkage and Selection Operator. The same cost function for lasso would look like the below:</p> \\[ J(\\theta) = \\frac{1}{2m}[\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\lambda \\sum_{j=1}^{n}|\\theta_{j}|] \\] <p>Mathematically, we don't penalize \\(\\theta_{0}\\). However in practice, it makes little difference if you penalize all coefficients or if you ignore the intercept.</p>"},{"location":"projects/ml/model-regularization/#computing-gradient-descent-with-regularization","title":"Computing gradient descent with regularization","text":"<p>Computing the gradient descent for this new loss function, we get:</p> <p>$$ \\theta_{0} := \\theta_{0} - \\alpha\\frac{1}{m} \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})x_{0i} $$ $$ \\theta_{j} := \\theta_{j} - \\alpha[\\frac{1}{m} \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})x_{ji} - \\frac{\\lambda}{m}\\theta_{j}] $$ which is rewritten as $$ \\theta_{j} := \\theta_{j}(1-\\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})x_{ji} $$ The term \\((1-\\alpha\\frac{\\lambda}{m})\\) is always <code>&lt;1</code>, which has a shrinking effect on \\(\\theta\\). Thus, for each iteration, it strives to keep it small.</p>"},{"location":"projects/ml/model-regularization/#regularization-for-logistic-regression","title":"Regularization for logistic regression","text":"<p>The cost function of a non-regularized logistic function looks like</p> <p>$$ J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^{m}[y_{i}log(h_{\\theta}(x_{i})) + (1-y_{i})log(1-h_{\\theta}(x_{i}))] $$ to this cost function, we add the regularization parameter to get: $$ J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^{m}[y_{i}log(h_{\\theta}(x_{i})) + (1-y_{i})log(1-h_{\\theta}(x_{i}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2} $$ The regularization term avoids penalty for bias term \\(\\theta_{0}\\).</p> <p>The gradient descent for logistic regression with regularization is identical to that of the linear regression explained earlier, except that, the hypothesis function is a sigmoid.</p>"},{"location":"projects/ml/multivariate-linear-regression/","title":"Solving multivariate linear regression using Gradient Descent","text":"<p>Note: This is a continuation of Gradient Descent topic. The context and equations used here derive from that article.</p> <p>When we regress for <code>y</code> using multiple predictors of <code>x</code>, the hypothesis function becomes:</p> \\[ h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + ... + \\theta_{n}x_{n} \\] <p>If we consider \\(x_{0} = 1\\), then the above can be represented as matrix multiplication using linear algebra.</p> \\[ x = \\begin{bmatrix}     x_{0}\\\\\\     x_{1}\\\\\\     \\vdots\\\\\\     x_{n} \\end{bmatrix} \\ and \\ \\theta=\\begin{bmatrix}     \\theta_{0}\\\\\\     \\theta_{1}\\\\\\     \\vdots\\\\\\     \\theta_{n} \\end{bmatrix} \\] <p>Thus,</p> \\[ h_{\\theta}(x) = \\theta^{T}x \\] <p>Here the dimensions of \\(\\theta\\) and <code>x</code> is <code>n+1</code> as this goes from <code>0</code> to <code>n</code>.</p>"},{"location":"projects/ml/multivariate-linear-regression/#loss-function-of-multivariate-linear-regression","title":"Loss function of multivariate linear regression","text":"<p>The loss function is given by</p> \\[ J(\\theta_{0}, \\theta_{1},...,\\theta_{n}) = \\frac{1}{2m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]^{2} \\] <p>which you can simplify to </p> \\[ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]^{2} \\] <p>The gradient descent of the loss function is now</p> <p>$$ \\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta) $$ Note: Here <code>j</code> represents the <code>n+1</code> features (attributes) and <code>i</code> goes from <code>1 -&gt; m</code> representing the <code>m</code> records.</p> <p>Simplifying the partial differential equation, we get the <code>n+1</code> update rules as follows</p> \\[ \\theta_{0} := \\theta_{0} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]x_{0}^{{(i)}} $$ $$ \\theta_{1} := \\theta_{1} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]x_{1}^{{(i)}} $$ $$ \\vdots $$ $$ \\theta_{n} := \\theta_{n} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]x_{n}^{{(i)}} \\] <p>The equations above are very similar to ones from simple linear equations.</p>"},{"location":"projects/ml/multivariate-linear-regression/#impact-of-scaling-on-gradient-descent","title":"Impact of scaling on Gradient Descent","text":"<p>When the data ranges of features varies quite a bit from each other, the surface of GD is highly skewed as shown below:</p> <p></p> <p>This is because \\(\\theta\\) (which is our weights) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. When scaled, the surface takes a healthier shape, allowing the algorithm to converge faster. Ideally scale values so they fall within <code>-1 to 1</code>.</p>"},{"location":"projects/ml/multivariate-linear-regression/#scaling-methods","title":"Scaling methods","text":"<p>Feature Scaling is simply dividing values by range. Normalization is when you transform them to have a <code>mean = 0</code>.</p> <p>Mean normalization  $$ scaled \\ x_{j} = \\frac{(x_{j} - \\mu_{j})}{s_{j}} $$ where \\(\\mu\\) is mean and <code>s</code> is range.</p> <p>Standard normalization is similar to above, except, <code>s</code> is standard deviation.</p> <p>The exact range of normalization is less important than having all features follow a particular range.</p>"},{"location":"projects/ml/multivariate-linear-regression/#debugging-gradient-descent","title":"Debugging Gradient Descent","text":"<p>The general premise is, as number of iterations increase, the loss should reduce. You can also declare a threshold and if the loss reduces below that for <code>n</code> number of iterations, then you can declare convergence. However, Andrew Ng suggests against this and suggests visualizing the loss on a chart to pick LR.</p> <p>When LR is too high: If you have a diverging graph - loss increases steadily or if the loss is oscillating (pic below), it is likely the the rate is too high. In case of oscillation, the weights sporadically hit the local minima but continue to overshoot.</p> <p></p> <p>Iterating through a number of LRs: Andrew suggests picking a range of LRs <code>0.001, 0.01, 0.1, 1, ...</code> and iterating through them. He typically bumps rates by a factor of <code>10</code>. For convenience, he picks <code>..0.001, 0.003, 0.01, 0.03, 0.1, 0.3..</code> where he bumps by <code>~3</code> which is also effective.</p>"},{"location":"projects/ml/multivariate-linear-regression/#non-linear-functions-vs-non-linear-models","title":"Non-linear functions vs non-linear models","text":"<p>A linear function is one which produces a straight line. It is typically of the form \\(y = \\theta_{0} + \\theta_{1}x_{1}\\). A non-linear function is something that produces a curve. It is typically of the from \\(y = \\theta_{0} + \\theta_{1}x^{k}\\). A linear model is when the model parameters are additive, even though individual parameters are non-linear. It takes form \\(y = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{n}x_{n}\\). A non-linear model is when the model parameters are multiplicative even though they are of order <code>1</code>. It typically takes form \\(y = \\theta_{0}x_{1}theta_{1}x_{2}^{k}x_{3}\\) etc.</p>"},{"location":"projects/ml/multivariate-linear-regression/#representing-non-linearity-using-polynomial-regression","title":"Representing non-linearity using Polynomial Regression","text":"<p>Sometimes, when you plot the response variable with one of the predictors, it may not take a linear form. You might want an order <code>2</code> or <code>3</code> curve. You can still represent them using linear models. Consider the case where square footage is one of the parameters in predicting house price and you notice a non-linear relationship. From the graphic below, you might try a quadratic model as \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2}^{2}\\). But this model will eventually taper off. Instead, you may try a cubic model as \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2}^{2} + \\theta_{3}x_{3}^{3}\\).</p> <p></p> <p>The way to represent non-linearity is to sequentially raise the power / order of the parameter, represent them as additional features. This is a step in feature engineering. This method is called polynomial regression. When you raise the power, the range of that parameter also increases exponentially. Thus you model might become highly skewed. It is vital to scale features in a polynomial regression.</p> <p>Another option here is, instead of raising power, you take square roots or nth roots, such as: \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}\\sqrt{x_{2}} + \\theta_{3}\\sqrt[3]{x_{3}}\\)</p>"},{"location":"projects/ml/sklearn-1/","title":"Scikit Learn syntax","text":"In\u00a0[\u00a0]: Copied!"},{"location":"projects/ml/sklearn-1/#scikit-learn-syntax","title":"Scikit Learn syntax\u00b6","text":""},{"location":"projects/ml/sklearn-1/#library-constructs","title":"Library constructs\u00b6","text":""},{"location":"projects/ml/sklearn-1/#estimator","title":"Estimator\u00b6","text":"<p>Every algorithm is exposed via an <code>Estimator</code> which can be imported as</p> <pre>from sklearn.&lt;family&gt; import &lt;model&gt;\n</pre> <p>for linear regression</p> <pre>from sklearn.linear_model import LinearRegression\nlm_model = LinearRegression(&lt;estimator parameters&gt;)\n</pre> <p>Estimator parameters are provided as arguments when you instantiate an Estimator. Sklearn provides good defaults.</p> <p>In Scikit-learn, Estimators are designed such that</p> <ul> <li>consistency: all estimators share a common interface</li> <li>inspection: the hyperparameters you set when you instantiate an estimator is available for inspection as properties of that object</li> <li>limited hierarchy: only the algorithms are represented as Python objects. Training data, results, parameter names follow standard Python or Numpy / Pandas types</li> <li>composition: many workflows can be achieved as a series of more fundamental algorithms</li> <li>sensible defaults: you guessed it.</li> </ul>"},{"location":"projects/ml/sklearn-1/#general-steps-when-using-scikit-learn","title":"General steps when using Scikit-learn\u00b6","text":"<ul> <li>choose a class of model</li> <li>instantiate a model from the class by specifying hyperparameters to its constructor</li> <li>arrange data into <code>X</code> and <code>y</code> and split them for training and testing</li> <li>fit / learn the model on training data by calling <code>fit()</code> method</li> <li>predict new values by calling the <code>predict()</code> method</li> <li>evaluate results</li> </ul>"},{"location":"projects/ml/sklearn-1/#train-test-split","title":"Train-test split\u00b6","text":"<p>To split the input data into train and validation sets, use</p> <pre>from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n</pre> <p>to split it at 70% train and 30% test sets. This method splits both the dependent and independent attributes so as to validate the prediction.</p>"},{"location":"projects/ml/sklearn-1/#training","title":"Training\u00b6","text":"<p>The general syntax is <code>model.fit(independent_train, dependent_train)</code>. Thus</p> <pre>lm_model.fit(x_train, y_train)\n</pre> <p>In case of unsupervised models you only have a training data, no test data. Hence</p> <pre>model.fit(x_train)\n</pre> <p>In Scikit-Learn, by convention all model parameters that were learned during the <code>fit()</code> process have trailing underscores; for example in this linear model, we have <code>model.coef_</code>, <code>model.intercept_</code></p>"},{"location":"projects/ml/sklearn-1/#training-score","title":"Training score\u00b6","text":"<p>A <code>model.score()</code> method returns the a value <code>0-1</code> illustrating how well the model fitted the training data. Note this is useful to understand the influence of underfitting and overfitting of training data.</p>"},{"location":"projects/ml/sklearn-1/#prediction","title":"Prediction\u00b6","text":"<p>Use <code>model.predict(&lt;independent_test data&gt;)</code>. Thus for linear reg,</p> <pre>y_predicted = lm_model.predict(x_test)\n</pre>"},{"location":"projects/ml/sklearn-1/#prediction-probabilities","title":"Prediction probabilities\u00b6","text":"<p>In case of classification problems, you also get a <code>model.predict_proba()</code> method which will return the probabilities for each class. The <code>model.predict()</code> will return the class with highest probability.</p>"},{"location":"projects/ml/sklearn-1/#transformation","title":"Transformation\u00b6","text":"<p>Relevant in unsupervised models, <code>model.transform()</code> is used to transform input data to a new basis. Some models combine the fitting and transformation in one step using the <code>model.fit_transform()</code> method.</p>"},{"location":"projects/ml/sklearn-1/#validation","title":"Validation\u00b6","text":"<p>You can obtain the MAE (Mean Absolute Error), MSE (Mean Squared Error) and RMSE (Root Mean Squared Error) from the <code>metrics</code> module.</p> <pre>from sklearn import metrics\nimport numpy.np\n\nmetrics.mean_absolute_error(y_test, y_predicted)\nmetrics.mean_squared_error(y_test, y_predicted)\nnp.sqrt(metrics.mean_squared_error(y_test, y_predicted)\n</pre>"},{"location":"projects/ml/sklearn_naive_bayes_classifier/","title":"Naive Bayes classification - Sklearn","text":"<p>In machine learning, Naive Bayes is used to compute conditional probability of predicted class $y$ occuring given all the predictor variables $x$. In other words, Bayes theorem relates P(outcome/evidence) (what we want to predict) to P(evidence / outcome) (training set).</p> <p>The algorithm builds the conditional probability and applies prediction. The algorithm is called naive because it assumes independence between predictor variables, while in reality this may not be true in all cases.</p> <p>$$ P(A_i | B_j) = \\frac{P(B_j | A_i)P(A_i)}{P(B_j | A_1)P(A_1) + P(B_j | A_2)P(A_2) + ... + P(B_j | A_k)P(A_k)} $$</p> <p>Consider $A_1 ,... A_k$ as $k$ predictor variables in machine learning. The Naive Bayes classifier will build the conditional probabilities of $p(B_j|A_k)$ to later predict what would $p(A_i | B_j)$ be.</p> In\u00a0[1]: Copied! <pre>import seaborn as sns\niris = sns.load_dataset('iris')\n</pre> import seaborn as sns iris = sns.load_dataset('iris') In\u00a0[2]: Copied! <pre>iris.head()\n</pre> iris.head() Out[2]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa In\u00a0[3]: Copied! <pre>iris.shape\n</pre> iris.shape Out[3]: <pre>(150, 5)</pre> In\u00a0[9]: Copied! <pre>iris['species'].value_counts().plot(kind='bar')\n</pre> iris['species'].value_counts().plot(kind='bar') Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a0d0e03c8&gt;</pre> In\u00a0[13]: Copied! <pre>%matplotlib inline\nsns.pairplot(iris, hue='species')\n</pre> %matplotlib inline sns.pairplot(iris, hue='species') Out[13]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x1a17a970b8&gt;</pre> In\u00a0[2]: Copied! <pre>X_iris = iris.drop('species', axis=1)\nX_iris.shape\n</pre> X_iris = iris.drop('species', axis=1) X_iris.shape Out[2]: <pre>(150, 4)</pre> In\u00a0[3]: Copied! <pre>y_iris = iris['species']\ny_iris.shape\n</pre> y_iris = iris['species'] y_iris.shape Out[3]: <pre>(150,)</pre> <p><code>X_iris</code> is in caps as <code>X</code> is a vector of multiple features for each record, whereas <code>y_iris</code> is in small case as it is a scalar for each record.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/ml/sklearn_naive_bayes_classifier/#naive-bayes-classification-sklearn","title":"Naive Bayes classification - Sklearn\u00b6","text":"<p>Naive Bayes classifier builds directly on conditional probability and this</p> <p>$$ p(y|x) = \\frac{p(y \\cap x)}{p(x)} $$ from the above formula, $p(y \\cap x)$ can be written as</p> <p>$$ p(y \\cap x) = p(x | y).p(y) $$</p> <p>thus</p> <p>$$ p(y|x) = \\frac{p(x|y).p(y)}{p(x)} $$</p>"},{"location":"projects/ml/sklearn_naive_bayes_classifier/#naive-bayes-on-iris-dataset","title":"Naive Bayes on Iris dataset\u00b6","text":""},{"location":"projects/ml/sklearn_naive_bayes_classifier/#eda","title":"EDA\u00b6","text":""},{"location":"projects/ml/sklearn_naive_bayes_classifier/#create-features-and-labels","title":"Create features and labels\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/","title":"Applying Linear Regression with scikit-learn and statmodels","text":"<p><code>Scikit-learn</code> is one of the science kits for <code>SciPy</code> stack. Scikit has a collection of prediction and learning algorithms, grouped into</p> <ul> <li>classification</li> <li>clustering</li> <li>regression</li> <li>dimensionality reduction</li> </ul> <p>Each algorithm follows a typical pattern with a <code>fit</code>, <code>predict</code> method. In addition you get a set of utility methods that help with splitting datasets into train-test sets and for validating the outputs.</p> In\u00a0[3]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n</pre> import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns In\u00a0[4]: Copied! <pre>usa_house = pd.read_csv('../udemy_ml_bootcamp/Machine Learning Sections/Linear-Regression/USA_housing.csv')\nusa_house.head(5)\n</pre> usa_house = pd.read_csv('../udemy_ml_bootcamp/Machine Learning Sections/Linear-Regression/USA_housing.csv') usa_house.head(5) Out[4]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price Address 0 79545.45857 5.682861 7.009188 4.09 23086.80050 1.059034e+06 208 Michael Ferry Apt. 674\\nLaurabury, NE 3701... 1 79248.64245 6.002900 6.730821 3.09 40173.07217 1.505891e+06 188 Johnson Views Suite 079\\nLake Kathleen, CA... 2 61287.06718 5.865890 8.512727 5.13 36882.15940 1.058988e+06 9127 Elizabeth Stravenue\\nDanieltown, WI 06482... 3 63345.24005 7.188236 5.586729 3.26 34310.24283 1.260617e+06 USS Barnett\\nFPO AP 44820 4 59982.19723 5.040555 7.839388 4.23 26354.10947 6.309435e+05 USNS Raymond\\nFPO AE 09386 In\u00a0[4]: Copied! <pre>usa_house.info()\n</pre> usa_house.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 7 columns):\nAvg Income                 5000 non-null float64\nAvg House Age              5000 non-null float64\nAvg. Number of Rooms       5000 non-null float64\nAvg. Number of Bedrooms    5000 non-null float64\nArea Population            5000 non-null float64\nPrice                      5000 non-null float64\nAddress                    5000 non-null object\ndtypes: float64(6), object(1)\nmemory usage: 273.5+ KB\n</pre> In\u00a0[5]: Copied! <pre>usa_house.describe()\n</pre> usa_house.describe() Out[5]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price count 5000.000000 5000.000000 5000.000000 5000.000000 5000.000000 5.000000e+03 mean 68583.108984 5.977222 6.987792 3.981330 36163.516039 1.232073e+06 std 10657.991214 0.991456 1.005833 1.234137 9925.650114 3.531176e+05 min 17796.631190 2.644304 3.236194 2.000000 172.610686 1.593866e+04 25% 61480.562390 5.322283 6.299250 3.140000 29403.928700 9.975771e+05 50% 68804.286405 5.970429 7.002902 4.050000 36199.406690 1.232669e+06 75% 75783.338665 6.650808 7.665871 4.490000 42861.290770 1.471210e+06 max 107701.748400 9.519088 10.759588 6.500000 69621.713380 2.469066e+06 <p>Find the correlation between each of the numerical columns to the house price</p> In\u00a0[6]: Copied! <pre>sns.pairplot(usa_house)\n</pre> sns.pairplot(usa_house) Out[6]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x108258ef0&gt;</pre> <p>From this chart, we now,</p> <ul> <li>distribution of house price is normal (last chart)</li> <li>some scatters show a higher correlation, while some other show no correlation.</li> </ul> In\u00a0[7]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize=[15,5])\nsns.distplot(usa_house['Price'], ax=axs[0])\nsns.heatmap(usa_house.corr(), ax=axs[1], annot=True)\nfig.tight_layout()\n</pre> fig, axs = plt.subplots(1,2, figsize=[15,5]) sns.distplot(usa_house['Price'], ax=axs[0]) sns.heatmap(usa_house.corr(), ax=axs[1], annot=True) fig.tight_layout() <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> In\u00a0[8]: Copied! <pre>usa_house.columns\n</pre> usa_house.columns Out[8]: <pre>Index(['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population', 'Price', 'Address'],\n      dtype='object')</pre> In\u00a0[9]: Copied! <pre>X = usa_house[['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population']]\n\ny = usa_house[['Price']]\n</pre> X = usa_house[['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',        'Avg. Number of Bedrooms', 'Area Population']]  y = usa_house[['Price']] In\u00a0[10]: Copied! <pre>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n</pre> from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33) In\u00a0[11]: Copied! <pre>len(X_train)\n</pre> len(X_train) Out[11]: <pre>3350</pre> In\u00a0[12]: Copied! <pre>len(X_test)\n</pre> len(X_test) Out[12]: <pre>1650</pre> In\u00a0[13]: Copied! <pre>X_test.head()\n</pre> X_test.head() Out[13]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population 1066 64461.39215 7.949614 6.675121 2.04 34210.93608 4104 61687.39442 5.507913 6.995603 3.34 45279.16397 662 69333.68219 5.924392 6.542682 2.00 17187.11819 2960 74095.71281 5.908765 6.847362 3.00 32774.02197 1604 53066.37227 6.754571 8.062652 3.23 19103.12711 In\u00a0[14]: Copied! <pre>from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n</pre> from sklearn.linear_model import LinearRegression lm = LinearRegression() In\u00a0[15]: Copied! <pre>lm.fit(X_train, y_train) #no need to capture the return. All is stored in lm\n</pre> lm.fit(X_train, y_train) #no need to capture the return. All is stored in lm Out[15]: <pre>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</pre> <p>Create a table showing the coefficient (influence) of each of the columns</p> In\u00a0[16]: Copied! <pre>cdf = pd.DataFrame(lm.coef_[0], index=X_train.columns, columns=['coefficients'])\ncdf\n</pre> cdf = pd.DataFrame(lm.coef_[0], index=X_train.columns, columns=['coefficients']) cdf Out[16]: coefficients Avg Income 21.639515 Avg House Age 166094.788683 Avg. Number of Rooms 119855.858430 Avg. Number of Bedrooms 3037.782793 Area Population 15.215241 <p>Note, the coefficients for house age, number of rooms is pretty large. However that does not really mean they are more influential compared to income. It is simply because our dataset has not been normalized and the data range for each of these columns vary widely.</p> In\u00a0[17]: Copied! <pre>y_predicted = lm.predict(X_test)\nlen(y_predicted)\n</pre> y_predicted = lm.predict(X_test) len(y_predicted) Out[17]: <pre>1650</pre> In\u00a0[18]: Copied! <pre>plt.scatter(y_test, y_predicted) #actual vs predicted\n</pre> plt.scatter(y_test, y_predicted) #actual vs predicted Out[18]: <pre>&lt;matplotlib.collections.PathCollection at 0x1a1b4f4668&gt;</pre> In\u00a0[19]: Copied! <pre>sns.distplot((y_test - y_predicted))\n</pre> sns.distplot((y_test - y_predicted)) <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> Out[19]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1b519c88&gt;</pre> In\u00a0[20]: Copied! <pre>from sklearn import metrics\nmetrics.mean_absolute_error(y_test, y_predicted)\n</pre> from sklearn import metrics metrics.mean_absolute_error(y_test, y_predicted) Out[20]: <pre>80502.80373530561</pre> <p>RMSE</p> In\u00a0[21]: Copied! <pre>import numpy\nnumpy.sqrt(metrics.mean_squared_error(y_test, y_predicted))\n</pre> import numpy numpy.sqrt(metrics.mean_squared_error(y_test, y_predicted)) Out[21]: <pre>99779.47354600814</pre> In\u00a0[22]: Copied! <pre>X_test['predicted_price'] = y_predicted\n</pre> X_test['predicted_price'] = y_predicted <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"Entry point for launching an IPython kernel.\n</pre> In\u00a0[23]: Copied! <pre>X_test.head()\n</pre> X_test.head() Out[23]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population predicted_price 1066 64461.39215 7.949614 6.675121 2.04 34210.93608 1.396405e+06 4104 61687.39442 5.507913 6.995603 3.34 45279.16397 1.141590e+06 662 69333.68219 5.924392 6.542682 2.00 17187.11819 8.904433e+05 2960 74095.71281 5.908765 6.847362 3.00 32774.02197 1.267610e+06 1604 53066.37227 6.754571 8.062652 3.23 19103.12711 8.913813e+05 In\u00a0[24]: Copied! <pre>from sklearn.preprocessing import StandardScaler\ns_scaler = StandardScaler()\n</pre> from sklearn.preprocessing import StandardScaler s_scaler = StandardScaler() In\u00a0[25]: Copied! <pre># get all columns except 'Address' which is non numeric\nusa_house.columns[:-1]\n</pre> # get all columns except 'Address' which is non numeric usa_house.columns[:-1] Out[25]: <pre>Index(['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population', 'Price'],\n      dtype='object')</pre> In\u00a0[26]: Copied! <pre>usa_house_scaled = s_scaler.fit_transform(usa_house[usa_house.columns[:-1]])\nusa_house_scaled = pd.DataFrame(usa_house_scaled, columns=usa_house.columns[:-1])\nusa_house_scaled.head()\n</pre> usa_house_scaled = s_scaler.fit_transform(usa_house[usa_house.columns[:-1]]) usa_house_scaled = pd.DataFrame(usa_house_scaled, columns=usa_house.columns[:-1]) usa_house_scaled.head() Out[26]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price 0 1.028660 -0.296927 0.021274 0.088062 -1.317599 -0.490081 1 1.000808 0.025902 -0.255506 -0.722301 0.403999 0.775508 2 -0.684629 -0.112303 1.516243 0.930840 0.072410 -0.490211 3 -0.491499 1.221572 -1.393077 -0.584540 -0.186734 0.080843 4 -0.807073 -0.944834 0.846742 0.201513 -0.988387 -1.702518 In\u00a0[37]: Copied! <pre>usa_house_scaled.describe().round(3) # round the numbers for dispaly\n</pre> usa_house_scaled.describe().round(3) # round the numbers for dispaly Out[37]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price count 5000.000 5000.000 5000.000 5000.000 5000.000 5000.000 mean 0.000 -0.000 -0.000 -0.000 0.000 0.000 std 1.000 1.000 1.000 1.000 1.000 1.000 min -4.766 -3.362 -3.730 -1.606 -3.626 -3.444 25% -0.666 -0.661 -0.685 -0.682 -0.681 -0.664 50% 0.021 -0.007 0.015 0.056 0.004 0.002 75% 0.676 0.679 0.674 0.412 0.675 0.677 max 3.671 3.573 3.750 2.041 3.371 3.503 In\u00a0[27]: Copied! <pre>X_scaled = usa_house_scaled[['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population']]\n\ny_scaled = usa_house_scaled[['Price']]\n</pre> X_scaled = usa_house_scaled[['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',        'Avg. Number of Bedrooms', 'Area Population']]  y_scaled = usa_house_scaled[['Price']] In\u00a0[28]: Copied! <pre>Xscaled_train, Xscaled_test, yscaled_train, yscaled_test = \\\ntrain_test_split(X_scaled, y_scaled, test_size=0.33)\n</pre> Xscaled_train, Xscaled_test, yscaled_train, yscaled_test = \\ train_test_split(X_scaled, y_scaled, test_size=0.33) In\u00a0[29]: Copied! <pre>lm_scaled = LinearRegression()\nlm_scaled.fit(Xscaled_train, yscaled_train)\n</pre> lm_scaled = LinearRegression() lm_scaled.fit(Xscaled_train, yscaled_train) Out[29]: <pre>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</pre> In\u00a0[30]: Copied! <pre>cdf_scaled = pd.DataFrame(lm_scaled.coef_[0], \n                          index=Xscaled_train.columns, columns=['coefficients'])\ncdf_scaled\n</pre> cdf_scaled = pd.DataFrame(lm_scaled.coef_[0],                            index=Xscaled_train.columns, columns=['coefficients']) cdf_scaled Out[30]: coefficients Avg Income 0.653151 Avg House Age 0.462883 Avg. Number of Rooms 0.341197 Avg. Number of Bedrooms 0.007156 Area Population 0.424653 In\u00a0[59]: Copied! <pre>lm_scaled.intercept_\n</pre> lm_scaled.intercept_ Out[59]: <pre>array([0.00215375])</pre> <p>From the table above, we notice <code>Avg Income</code> has more influence on the <code>Price</code> than other variables. This was not apparent before scaling the data. Further this corroborates the <code>correlation</code> matrix produced during exploratory data analysis.</p> In\u00a0[33]: Copied! <pre>import statsmodels.api as sm\nimport statsmodels\n</pre> import statsmodels.api as sm import statsmodels In\u00a0[32]: Copied! <pre>from statsmodels.regression import linear_model\n</pre> from statsmodels.regression import linear_model In\u00a0[34]: Copied! <pre>yscaled_train.shape\n</pre> yscaled_train.shape Out[34]: <pre>(3350, 1)</pre> In\u00a0[35]: Copied! <pre>Xscaled_train = sm.add_constant(Xscaled_train)\nsm_ols = linear_model.OLS(yscaled_train, Xscaled_train) # i know, the param order is inverse\nsm_model = sm_ols.fit()\n</pre> Xscaled_train = sm.add_constant(Xscaled_train) sm_ols = linear_model.OLS(yscaled_train, Xscaled_train) # i know, the param order is inverse sm_model = sm_ols.fit() In\u00a0[36]: Copied! <pre>sm_model.summary()\n</pre> sm_model.summary() Out[36]: OLS Regression Results Dep. Variable: Price   R-squared:             0.918 Model: OLS   Adj. R-squared:        0.917 Method: Least Squares   F-statistic:           7446. Date: Thu, 23 Aug 2018   Prob (F-statistic):   0.00 Time: 16:25:05   Log-Likelihood:      -552.85 No. Observations:   3350   AIC:                   1118. Df Residuals:   3344   BIC:                   1154. Df Model:      5 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const    -0.0015     0.005    -0.311  0.756    -0.011     0.008 Avg Income     0.6532     0.005   129.977  0.000     0.643     0.663 Avg House Age     0.4629     0.005    93.799  0.000     0.453     0.473 Avg. Number of Rooms     0.3412     0.006    61.197  0.000     0.330     0.352 Avg. Number of Bedrooms     0.0072     0.006     1.277  0.202    -0.004     0.018 Area Population     0.4247     0.005    86.506  0.000     0.415     0.434 Omnibus: 11.426   Durbin-Watson:         1.995 Prob(Omnibus):  0.003   Jarque-Bera (JB):      9.047 Skew:  0.023   Prob(JB):             0.0109 Kurtosis:  2.750   Cond. No.               1.66 <p>The regression coefficients are identical between <code>sklearn</code> and <code>statsmodels</code> libraries. The $R^{2}$ of <code>0.919</code> is as high as it gets. This indicates the predicted (train) <code>Price</code> varies similar to actual. Another measure of health is the <code>S</code> (std. error) and <code>p-value</code> of coefficients. The <code>S</code> of <code>Avg. Number of Bedrooms</code> is as low as other predictors, however it has a high <code>p-value</code> indicating a low confidence in predicting its coefficient.</p> <p>Similar is the <code>p-value</code> of the intercept.</p> In\u00a0[37]: Copied! <pre>yscaled_predicted = lm_scaled.predict(Xscaled_test)\nresiduals_scaled = yscaled_test - yscaled_predicted\n</pre> yscaled_predicted = lm_scaled.predict(Xscaled_test) residuals_scaled = yscaled_test - yscaled_predicted In\u00a0[42]: Copied! <pre>fig, axs = plt.subplots(2,2, figsize=(10,10))\n# plt.tight_layout()\n\nplt1 = axs[0][0].scatter(x=yscaled_test, y=yscaled_predicted)\naxs[0][0].set_title('Fitted vs Predicted')\naxs[0][0].set_xlabel('Price - test')\naxs[0][0].set_ylabel('Price - predicted')\n\nplt2 = axs[0][1].scatter(x=yscaled_test, y=residuals_scaled)\naxs[0][1].hlines(0, xmin=-3, xmax=3)\naxs[0][1].set_title('Fitted vs Residuals')\naxs[0][1].set_xlabel('Price - test (fitted)')\naxs[0][1].set_ylabel('Residuals')\n\nfrom numpy import random\naxs[1][0].scatter(x=sorted(random.randn(len(residuals_scaled))), \n                  y=sorted(residuals_scaled['Price']))\naxs[1][0].set_title('QQ plot of Residuals')\naxs[1][0].set_xlabel('Std. normal z scores')\naxs[1][0].set_ylabel('Residuals')\n\nsns.distplot(residuals_scaled, ax=axs[1][1])\naxs[1][1].set_title('Histogram of residuals')\nplt.tight_layout()\n</pre> fig, axs = plt.subplots(2,2, figsize=(10,10)) # plt.tight_layout()  plt1 = axs[0][0].scatter(x=yscaled_test, y=yscaled_predicted) axs[0][0].set_title('Fitted vs Predicted') axs[0][0].set_xlabel('Price - test') axs[0][0].set_ylabel('Price - predicted')  plt2 = axs[0][1].scatter(x=yscaled_test, y=residuals_scaled) axs[0][1].hlines(0, xmin=-3, xmax=3) axs[0][1].set_title('Fitted vs Residuals') axs[0][1].set_xlabel('Price - test (fitted)') axs[0][1].set_ylabel('Residuals')  from numpy import random axs[1][0].scatter(x=sorted(random.randn(len(residuals_scaled))),                    y=sorted(residuals_scaled['Price'])) axs[1][0].set_title('QQ plot of Residuals') axs[1][0].set_xlabel('Std. normal z scores') axs[1][0].set_ylabel('Residuals')  sns.distplot(residuals_scaled, ax=axs[1][1]) axs[1][1].set_title('Histogram of residuals') plt.tight_layout() <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> <p>From the charts above,</p> <ul> <li>Fitted vs predicted chart shows a strong correlation between the predictions and actual values</li> <li>Fitted vs Residuals chart shows an even distribution around the <code>0</code> mean line. There are not patterns evident, which means our model does not leak any systematic phenomena into the residuals (errors)</li> <li>Quantile-Quantile plot of residuals vs std. normal and the histogram of residual plots show a sufficiently normal distribution of residuals.</li> </ul> <p>Thus all assumptions hold good.</p> In\u00a0[45]: Copied! <pre>Xscaled_train.columns\n</pre> Xscaled_train.columns Out[45]: <pre>Index(['const', 'Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population'],\n      dtype='object')</pre> In\u00a0[55]: Copied! <pre>usa_house_fitted = Xscaled_test[Xscaled_test.columns[0:]]\nusa_house_fitted['Price'] = yscaled_test\nusa_house_fitted.head()\n</pre> usa_house_fitted = Xscaled_test[Xscaled_test.columns[0:]] usa_house_fitted['Price'] = yscaled_test usa_house_fitted.head() Out[55]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price 116 -0.572234 0.523958 0.333009 0.930840 -0.935132 -0.807544 1766 0.235111 -1.461858 -0.639184 -0.787131 -0.746915 -1.139124 318 -0.401314 -1.180810 0.886801 0.242031 -0.544216 -0.423747 1376 0.190042 -1.552636 0.291719 -0.503503 -1.186530 -1.552110 4699 -2.186060 -0.389114 -0.437924 -1.362489 0.903237 -1.453495 In\u00a0[59]: Copied! <pre>usa_house_fitted_inv = s_scaler.inverse_transform(usa_house_fitted)\nusa_house_fitted_inv = pd.DataFrame(usa_house_fitted_inv, \n                                    columns=usa_house_fitted.columns)\nusa_house_fitted_inv.head().round(3)\n</pre> usa_house_fitted_inv = s_scaler.inverse_transform(usa_house_fitted) usa_house_fitted_inv = pd.DataFrame(usa_house_fitted_inv,                                      columns=usa_house_fitted.columns) usa_house_fitted_inv.head().round(3) Out[59]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price 0 62484.855 6.497 7.323 5.13 26882.652 946943.036 1 71088.669 4.528 6.345 3.01 28750.641 829868.230 2 64306.339 4.807 7.880 4.28 30762.360 1082455.018 3 70608.372 4.438 7.281 3.36 24387.608 684049.919 4 45286.426 5.591 6.547 2.30 45127.832 718869.401 In\u00a0[65]: Copied! <pre>yinv_predicted = (yscaled_predicted * s_scaler.scale_[-1]) + s_scaler.mean_[-1]\n</pre> yinv_predicted = (yscaled_predicted * s_scaler.scale_[-1]) + s_scaler.mean_[-1] In\u00a0[66]: Copied! <pre>yinv_predicted.shape\n</pre> yinv_predicted.shape Out[66]: <pre>(1650, 1)</pre> In\u00a0[67]: Copied! <pre>usa_house_fitted_inv['Price predicted'] = yinv_predicted\nusa_house_fitted_inv.head().round(3)\n</pre> usa_house_fitted_inv['Price predicted'] = yinv_predicted usa_house_fitted_inv.head().round(3) Out[67]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price Price predicted 0 62484.855 6.497 7.323 5.13 26882.652 946943.036 1087455.848 1 71088.669 4.528 6.345 3.01 28750.641 829868.230 855848.404 2 64306.339 4.807 7.880 4.28 30762.360 1082455.018 971840.851 3 70608.372 4.438 7.281 3.36 24387.608 684049.919 877566.569 4 45286.426 5.591 6.547 2.30 45127.832 718869.401 743023.886 In\u00a0[68]: Copied! <pre>mse_scaled = metrics.mean_squared_error(usa_house_fitted_inv['Price'], \n                           usa_house_fitted_inv['Price predicted'])\nnumpy.sqrt(mse_scaled)\n</pre> mse_scaled = metrics.mean_squared_error(usa_house_fitted_inv['Price'],                             usa_house_fitted_inv['Price predicted']) numpy.sqrt(mse_scaled) Out[68]: <pre>101796.27442079457</pre> In\u00a0[69]: Copied! <pre>mae_scaled = metrics.mean_absolute_error(usa_house_fitted_inv['Price'], \n                           usa_house_fitted_inv['Price predicted'])\nmae_scaled\n</pre> mae_scaled = metrics.mean_absolute_error(usa_house_fitted_inv['Price'],                             usa_house_fitted_inv['Price predicted']) mae_scaled Out[69]: <pre>80765.04773907141</pre>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#applying-linear-regression-with-scikit-learn-and-statmodels","title":"Applying Linear Regression with <code>scikit-learn</code> and <code>statmodels</code>\u00b6","text":"<p>This notebook demonstrates how to conduct a valid regression analysis using a combination of Sklearn and statmodels libraries. While sklearn is popular and powerful from an operational point of view, it does not provide the detailed metrics required to statistically analyze your model, evaluate the importance of predictors, build or simplify your model.</p> <p>We use other libraries like <code>statmodels</code> or <code>scipy.stats</code> to bridge this gap.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#scikit-learn","title":"Scikit-learn\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#predicting-housing-prices-without-data-normalization","title":"Predicting housing prices without data normalization\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#exploratory-data-anslysis-eda","title":"Exploratory data anslysis (EDA)\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#data-cleaning","title":"Data cleaning\u00b6","text":"<p>Throw out the text column and split the data into predictor and predicted variables</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#train-test-split","title":"Train test split\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#multiple-regression","title":"Multiple regression\u00b6","text":"<p>We use a number of numerical columns to regress the house price. Each column's influence will vary, just like in real life, the number of bedrooms might not influence as much as population density. We can determine the influence from the correlation shown in the heatmap above</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#fit","title":"Fit\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#predict","title":"Predict\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#accuracy-assessment-model-validation","title":"Accuracy assessment  / Model validation\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#distribution-of-residuals","title":"Distribution of residuals\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#quantifying-errors","title":"Quantifying errors\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#combine-the-predicted-values-with-input","title":"Combine the predicted values with input\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#predicting-housing-prices-with-data-normalization-and-statmodels","title":"Predicting housing prices with data normalization and statmodels\u00b6","text":"<p>As seen earlier, even though sklearn will perform regression, it is hard to compare which of the predictor variables are influential in determining the house price. To answer this better, let us standardize our data to Std. Normal distribution using sklearn preprocessing.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#scale-the-housing-data-to-std-normal-distribution","title":"Scale the housing data to Std. Normal distribution\u00b6","text":"<p>We use <code>StandardScaler</code> from <code>sklearn.preprocessing</code> to normalize each predictor to mean <code>0</code> and unit variance. What we end up with is <code>z-score</code> for each record.</p> <p>$$ z-score = \\frac{x_{i} - \\mu}{\\sigma}   $$</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#train-test-split","title":"Train test split\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#train-the-model","title":"Train the model\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#evaluate-model-parameters-using-statsmodels","title":"Evaluate model parameters using statsmodels\u00b6","text":"<p><code>statmodels</code> is a different Python library built for and by statisticians. Thus it provides a lot more information on your model than <code>sklearn</code>. We use it here to refit against the data and evaluate the strength of fit.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#predict-for-unkown-values","title":"Predict for unkown values\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#evaluate-model-using-charts","title":"Evaluate model using charts\u00b6","text":"<p>In addition to the numerical metrics used above, we need to look at the distribution of residuals to evaluate if the model.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#inverse-transform-the-scaled-data-and-calculate-rmse","title":"Inverse Transform the scaled data and calculate RMSE\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#calculate-rmse","title":"Calculate RMSE\u00b6","text":"<p>RMSE root mean squared error. This is useful as it tell you in terms of the dependent variable, what the mean error in prediction is.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#conclusion","title":"Conclusion\u00b6","text":"<p>In this sample we observed two methods of predicting housing prices. The first involved applying linear regression on the dataset directly. The second involved scaling the features to standard normal distribution and applying a linear model using both <code>sklearn</code> and <code>statsmodels</code> packages. We thoroughly inspected the model parameters, vetted that assumptions hold good.</p> <p>In the end, the accuracy of the models did not increase much after scaling. However, we were able to better determine which predictor variables were influential in a truest sense, not being biased by the scale of the units.</p> <p>The allure of linear models is the explainability. They may not be the best when it comes to accurate predictions, however they help us answer basic questions better, such as \"which characteristics influence the cost of my home, is it # of bedrooms or average income of the residents\"?. The answer is the latter in this example.</p>"},{"location":"projects/mwrs/","title":"Microwave Remote Sensing","text":"<p>coming soon...</p>"},{"location":"projects/spatial/","title":"Spatial analysis","text":""},{"location":"projects/spatial/#getting-started","title":"Getting started","text":"<ul> <li>A guide to spatial analysis -1 - spatial data structures and understanding density</li> <li>A guide to spatial analysis -2 - spatial statistics</li> </ul>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/","title":"Guide to spatial analysis - Introduction","text":"<p>A set of notes explaining the core of spatial analysis. Most of the content in this page are excerpts from Esri Guide to Spatial Analysis volume 1</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/#basics-of-spatial-data-structures","title":"Basics of spatial data structures","text":""},{"location":"projects/spatial/guide-to-spatial-analysis-1/#2-fundamental-representations","title":"2 fundamental representations","text":"<ul> <li>vector - each entity /feature is a row in a table. Each feature can be queried, have one or more attributes. Features are generally discrete entities. Vectors can be represented using</li> <li>points</li> <li>lines</li> <li> <p>polygons</p> </li> <li> <p>raster - custinuous image surface made up of a matrix of cell values in continuous space. Each layer in general has just 1 attribute that it defines.</p> </li> </ul>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/#geographic-attributes","title":"Geographic Attributes","text":"<p>Attributes of entities / features can be classified into following categories  - Categorical - (political affiliation, age group, income group)  - ranks - (severity of poverty, risk prediction, severity of assault)  - counts and amounts - (number of employees at a business, number of people in a house)  - ratios    - proportions - generally the ratio of a measured value to another measured value. Eg: ratio of number of people in age 10-25 to total population    - density - a ratio where the denominator is a spatial measurement. Eg: ratio of number of people to area</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/#visualization","title":"Visualization","text":"<p>You can visualize continuous and discrete values by classes. You can create the classes manually, or you can use one of the following statistical techniques to determine the class intervals.</p> <ul> <li>Natural breaks (Jenks): breaks are chosen along breaking changes in frequency (on a histogram). Thus, each block group on the map is more likely to have similar values.</li> <li>Quantile: Each class has the same number of features. Thus higher frequency areas on the histogram have more classes assigned to them. Thus, the values on the ends of the histogram (which typically have low frequency) get clumbed into the same class.</li> <li>Equal interval: Equal interval is easy to comprehend when you look at the legend. It splits the histogram with a set number of classes that are equally spaced from one another. On the map, the most features fall under a few number of classes</li> <li>Standard Deviation: Breaks are calculated by how many standard deviations they are from the mean. On the map, each group shows how many std each is from the mean.</li> </ul> <p></p> <p>Most people can distinguish up to <code>7</code> classes on a map. <code>4</code> or <code>5</code> are great number of classes to reveal patterns.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/#mapping-density","title":"Mapping density","text":"<p>Density is a ratio of number of features to a fixed areal extent. Thus, mapping number of people per sq. mile is a measure of population density. Mapping the density of features allows you to see patterns of where things are concentrated. Density surfaces are useful when mapping areas such as census tracts or counties which vary greatly in size.</p> <p>There are two ways to map density - a. mapping for defined areas (zip codes, census tracts, etc.) b. creating a density surface. When mapping for a defined area, a dot density renderer is a good visualization choice. You do this by summing the number of features within each polygon. Then you visualize each polygon with a set of dots, where each dot represents a chosen number of features (1 dot = 1000 features). The location of dots does not represent actual features though.</p> <p></p> <p>Density surface is usually calculated as a <code>raster</code> layer. Each <code>cell</code> in the layer gets a value based on the number of features that fall within a given radius around the cell.</p> <p></p> <p>Calculation of the density surface depends on 1. cell size, 2. search radius, 3. calculation method and 4. units. Cell size will determine how coarse or smooth the patterns will appear. The smaller the cell size, the smoother the surface, the longer the computation. A larger cell size will make the surface coarser, but the calculation will be quicker.</p> <p></p> <p>Search radius determines whether you pick up local or global patterns. A larger search radius will generalize the patterns more than a smaller radius, which will show more local variations.</p> <p></p> <p>There are two types of Density calculation methods - a simple method which counts all features within a search radius vs a weighted method that uses a math function to give more importance to features closer to the cell center. The closer the features, higher their weight. The weight drops off for features at the edge and beyond the search radius.</p> <p>Density of defined areas (zip codes / some boundary) allows you to compare known boundaries. However, the location of actual features may be distributed unevenly within the area. This is exaggerated when areas of the boundaries vary greatly (such as counties). A good practice in such cases is to apply dot density renderer for smaller geographies (smaller than what you intend to measure), but overlay that on larger geographies that you are visualizing.</p> <p>The density surface can be visualized as contours or using graduated colors or using both. When using colors, avoid using more than 15 classes. A density surface will smooth out extreme high and low values since it is an interpolation algorithm. As with the dot density renderer, the density value of a cell and the actual number of features within the cell may be different. Thus, you need to overlay the actual features on the surface to ensure the patterns are correctly interpreted.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/","title":"Guide to spatial analysis - spatial statistics","text":""},{"location":"projects/spatial/guide-to-spatial-analysis-2/#1-why-study-spatial-statistics","title":"1. Why study spatial statistics?","text":"<p>Studying spatial statistics (or statistical geography) helps answer questions such as: \"How sure am I that the pattern I am seeing is not simply due to random occurrence? To what extent does the value of a feature depend on its neighbors?\". In addition, spatial stats can help answer questions such as</p> <ol> <li>How are the features distributed? (Spatial distribution)</li> <li>What is the pattern created by these features - are the locations spatially clustered?</li> <li>If clustered, where are the clusters?</li> <li>What are the relationships between sets of features or values? Once you establish the strength of relationship between two layers, then you can use one to predict the other.</li> </ol>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#11-geographic-analysis-with-statistics","title":"1.1 Geographic analysis with statistics","text":"<p>Framing the question: In descriptive statistics, the question usually takes the form: Where is the center of crimes? What is the overall direction of storm tracks?.</p> <p>In inferential statistics, the analysis is stated as a hypothesis and that takes the form: Burglaries are more clustered than auto thefts. To ensure impartiality, statisticians often frame the inverse of what they intend to prove as the hypothesis. Thus the same analysis would be framed as Burglaries are not more clustered than auto thefts (called <code>null hypothesis</code>) and they determine whether or not to reject this null hypothesis.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#12-understanding-geographic-data","title":"1.2 Understanding geographic data","text":"<p>Spatial data types: Geographic features are spread in space. Their location can be analyzed based on their location or using location influenced by an attribute value. Thus, geographic features are either discrete or spatially continuous. Discrete features can be locations of stores, weather stations etc. and are represented as <code>Points</code>. <code>Lines</code> can be disjunct (animal migration routes) or connected, as in a network (streets). Spatially continuous features such as temperature, precipitation are measured anywhere and everywhere. Such <code>continuous field</code> data are represented as a <code>Polygon</code> or as a surface.</p> <p>Now let's talk about types of attribute values: Attribute values can be</p> <ul> <li><code>nominal</code> (categorical) - ex: land cover types, </li> <li><code>ordinal</code> (ordered) - ex: soil suitability, landslide risk zone. In ordinal scale, you only know under which class a feature falls, but don't know by how much is it better or worse than features in other classes.</li> <li><code>interval</code> (quantities) - ex: house prices, population. Here, each class gets an upper and lower limit allowing you to understand how wide a class range is and compare different classes.</li> <li><code>ratio</code> types (proportions) - ex: population density, infection rate. </li> </ul>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#2-data-distributions","title":"2. Data distributions","text":"<p>Below are some of the common distributions seen in geography:</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#21-normal-distribution","title":"2.1 Normal distribution","text":"<p>This kind of distribution occurs for phenomena where values are similar, but some are higher and some are lower. The frequency curve of normal distribution takes shape of the classic symmetrical bell curve. Given enough readings over time, most values will cluster toward the mean.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#22-poisson-distribution","title":"2.2 Poisson distribution","text":"<p>This distribution occurs when extreme events like large magnitude earthquakes occur in time and space. When events are random, there will be a few periods when many events occur, followed by several periods where no or very few events occur. For this distribution, mean number of events is often \\(\\mu &lt; 1\\) and probability of no events occurring is higher.</p> <p></p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#3-spatial-distributions","title":"3. Spatial distributions","text":"<p>The notes above describe distributions of attribute values. But, how do we measure spatial distributions? Most often, your objective is to determine if features are evenly distributed (uniform distribution) or not, meaning there is a spatial phenomenon taking place. There are <code>3</code> ways of finding if features are non-randomly distributed across space:</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#311-method-1-overlay-grid-and-count","title":"3.1.1 Method 1 - overlay grid and count","text":"<p>You can overlay an imaginary grid over your features, count the number of points / features in each cell. Then you compare this metric against a hypothetical even distribution of same features and test for significance.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#312-method-2-frequency-distribution-of-distances","title":"3.1.2 Method 2 - frequency distribution of distances","text":"<p>You can measure the distances between each feature and its neighbors (after defining a neighborhood threshold) and plot the frequency distribution of the distances.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#313-method-3-weighting-attribute-values-by-distance-to-neighbors","title":"3.1.3 Method 3 - weighting attribute values by distance to neighbors","text":"<p>To analyze the spatial distribution of attribute values (not features itself, but certain attribute values), you can divide (weigh) the values by distance to neighbors and create a frequency distribution. You can then compare that against a uniform or random distribution.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#32-measuring-centrality-of-geographic-distributions","title":"3.2 Measuring centrality of geographic distributions","text":"<p>In traditional statistics, you define centrality using mean, median and mode. Similarly, you can define mean center, median center and central feature in a geographic distribution.</p> <p>Mean center is obtained by averaging the X and Y values. Median center is calculated by identifying the X and Y coordinate that has the shortest distance to all features. Central Feature is the feature that has the shortest distance to all other features. Note, mean and median center may or may not fall on the coordinates of an existing feature, but central feature will.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#321-measuring-centrality-weighted-by-some-attribute-value","title":"3.2.1 Measuring centrality weighted by some attribute value","text":"<p>In practice, you generally want to weight the features when measuring the centrality. This becomes particularly important with continuous area features. Since they are continuous, you are not as interested in centrality, but on centrality determined by some attribute value (weight).</p> <p></p> <p>Thus, the social service agency looking for a central location to service population over age 65 could calculate it by weighting the distances with the population over age 65.</p> <p></p> <p>Weighted mean center is calculated as a weighted average: \\(\\bar X = \\frac{\\sum_{i} (w_{i} X_{i})}{\\sum_{i} w_{i}}\\) and \\(\\bar Y = \\frac{\\sum_{i} (w_i Y_{i})}{\\sum_{i}w_{i}}\\)</p> <p>In case of lines and polygon features, the centers and centroids are used and the mean center is calculated against those point values.</p> <p>Median center does not have an exact formula. Instead, the GIS calculates the mean center first, then adjusts it iteratively with slight changes until it finds the median center. It performs gradient descent to find the appropriate median center. To find the weighted median center, it uses the same technique, except, it multiplies the distances with the weight of the corresponding features.</p> <p>Central feature is calculated by iterating over each feature, calculating the distance to each other feature and summing it up. The feature with least total sum is the central feature. Weighted central feature is calculated similarly, by multiplying the distance with weight.</p> <p>Reiterating the fact that weighted centers are better for area features, see the case below: When polygons are of varying sizes, the central measure is automatically pulled toward the direction with multiple smaller polygons. This is because, polygons are converted to centroids when calculating distances. To offset this, calculated weighted center</p> <p></p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#33-measuring-compactness-of-geographic-distributions","title":"3.3 Measuring compactness of geographic distributions","text":"<p>For non-spatial datasets, the dispersion around the center is measured by variance and standard deviation. These values gives you the ability to compare compactness of distributions. Similarly, for spatial datasets, the standard distance, which is the average distance by which features vary from mean center gives a measure of compactness. It is also called the standard deviation distance.</p> <p></p> <p>When calculating the standard distance, you could calculate it just by distance to features or by multiplying the distance with a weight. When calculating the compactness of polygons, we use weighted distance of their centroids.</p> <p></p> <p>The formula to calculate standard distance with just distances of features to mean center is below:</p> <p></p> <p>Note that it looks very similar to calculating SD, except it sums the variance along <code>X</code> and <code>Y</code> and then takes the square root of their sum.</p> <p>When weights come into picture, you do a weighted average of the deviation along X and Y as shown below:</p> <p></p> <p>Weights are typically some numerical attribute (such as population if features are polygons, number of entities if features are lines or points). Further, the compactness measure works well if there no directionality in the dataset.</p>"},{"location":"projects/stats/","title":"Learn Statistics with Python","text":"<p>In these pages, I use Python and its rich stat analysis packages to teach the basics of probability and statistics. All those formulas which one could only apply on a limited number of problems (when calculating by hand) can now be applied across the entire dataset. You can literally verify if the central limit theorem holds good, without having to spend a week with your calculator. Enough said, checkout the pages below</p>"},{"location":"projects/stats/#introduction-to-statistics-with-python","title":"Introduction to statistics with Python","text":"<ul> <li>Collecting data</li> <li>Describing data</li> <li> <p>Conditional probability</p> </li> <li> <p>Probability distributions</p> <ul> <li>Normal distribution</li> <li>Binomial and Poisson distributions</li> </ul> </li> <li> <p>Hypothesis testing</p> </li> </ul>"},{"location":"projects/stats/#theory-of-statistical-learning","title":"Theory of Statistical learning","text":"<ul> <li>Statistical learning</li> <li>Linear regression concepts</li> </ul>"},{"location":"projects/stats/01_data_collection/","title":"Data collection methodologies","text":"<p>In scientific studies, data collection can fall under two broad categories</p> <ul> <li>observational study - where data is recorded without interfering with the process under study. These can be of 3 basic types</li> <li>sample survey - provides info about a population at a particular point in time</li> <li>prospective study - describes the population at present using sample survey and proceeds to follow forward in time to record specific outcomes</li> <li>retrospective study - describes the population at present using sample survey and records specific occurrence that have already taken place</li> <li>experimental study - where explanatory variables are actively manipulated and the effects on response variables are studied.</li> </ul>"},{"location":"projects/stats/01_data_collection/#data-collection-methodologies","title":"Data collection methodologies\u00b6","text":""},{"location":"projects/stats/01_data_collection/#sampling-designs-for-surveys-observational-study","title":"Sampling designs for surveys - observational study\u00b6","text":"<p>The various ways in which data collection experiments can be designed:</p> <ul> <li><p>Simple random sampling - Selecting a group of n units in such a way that each sample of size n has the equal probability of being selected.</p> </li> <li><p>Stratified random sampling - grouping the target population into strata based on a known auxiliary variable and then performing simple random sampling on each of the strata</p> </li> <li><p>cluster sampling - this involves selecting groups (based on convenience of study, such as buildings or city blocks) based on simple random sampling and then surveying or measuring all units within the groups.</p> </li> <li><p>systematic sampling - this involves selecting every nth value from a finite list as the sample. This technique is economical but can he heavily biased.</p> </li> </ul>"},{"location":"projects/stats/01_data_collection/#design-of-experiments-experimental-study","title":"Design of experiments - experimental study\u00b6","text":"<p>There are 2 types of variables in an experimental study - controlled variables (factors) and measured variables (response variables) which are under study</p> <ul> <li><p>factorial treatment - each of the various factors (m,n,o..) affecting the phenomena are combined with one-another. The resulting combinations (mno..) are studied individually</p> </li> <li><p>fractional factorial treatment - as the number of factors increase, factorial treatment becomes impossible to complete. Then, only a few selected combinations are picked for the study forming a fractional factorial treatment.</p> </li> </ul>"},{"location":"projects/stats/02_data_description/","title":"Describing Data Statistically","text":"In\u00a0[8]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsales_data = pd.read_csv('datasets_csv/CH03/ex3-14.txt')\nsales_data.columns = ['Year', 'Month', 'Sales'] #strip out the double quotes\nsales_data.head()\n</pre> import pandas as pd import matplotlib.pyplot as plt %matplotlib inline  sales_data = pd.read_csv('datasets_csv/CH03/ex3-14.txt') sales_data.columns = ['Year', 'Month', 'Sales'] #strip out the double quotes sales_data.head() Out[8]: Year Month Sales 0 1 1 101.9 1 2 1 109.0 2 3 1 115.5 3 4 1 122.0 4 5 1 128.1 In\u00a0[14]: Copied! <pre>print('Mean ' + str(sales_data.Sales.mean()))\nprint('Median ' + str(sales_data.Sales.median()))\nprint('Mode ' + str(sales_data.Sales.mode()))\n</pre> print('Mean ' + str(sales_data.Sales.mean())) print('Median ' + str(sales_data.Sales.median())) print('Mode ' + str(sales_data.Sales.mode())) <pre>Mean 118.7\nMedian 116.95\nMode 0    117.5\ndtype: float64\n</pre> In\u00a0[17]: Copied! <pre>quartiles = sales_data.Sales.quantile(q=[0.25, 0.5, 0.75])\nquartiles\n</pre> quartiles = sales_data.Sales.quantile(q=[0.25, 0.5, 0.75]) quartiles Out[17]: <pre>0.25    108.275\n0.50    116.950\n0.75    128.175\nName: Sales, dtype: float64</pre> In\u00a0[21]: Copied! <pre>IQR = quartiles[0.75] - quartiles[0.25]\nIQR\n</pre> IQR = quartiles[0.75] - quartiles[0.25] IQR Out[21]: <pre>19.899999999999991</pre> In\u00a0[88]: Copied! <pre>#create a subplot to show box plot and histogram next to each other\nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8,10))\n\n#plot a histogram using Pandas.\nhax = sales_data.Sales.hist(ax = axs[0], bins=15, grid=False)\nhax.set_title('Histogram of Sales data')\n\n#get the axis bounds\nhax_bounds = hax.axis()\n\n#plot the mean in black\nmean = sales_data.Sales.mean()\nhax.vlines(mean, hax_bounds[2], hax_bounds[3], label='Mean = ' + str(mean))\n\n#plot the median in yellow\nmedian= sales_data.Sales.median()\nhax.vlines(median, hax_bounds[2], hax_bounds[3], label='Median = ' + str(median), colors='yellow')\n\n#plot the mode in red\nmode= sales_data.Sales.mode()[0]\nhax.vlines(mode, hax_bounds[2], hax_bounds[3], label='Mode = ' + str(mode), colors='red')\n\n#Get the standard deviation\nsd = sales_data.Sales.std()\n\n#get mean +- 1SD lines\nm1sd = mean + 1*sd\nm1negsd = mean - 1*sd\nhax.vlines(m1sd, hax_bounds[2], hax_bounds[3], label='Mean + 1SD = ' + str(m1sd), colors='cyan')\nhax.vlines(m1negsd, hax_bounds[2], hax_bounds[3], label='SD = ' + str(sd), colors='cyan')\n\nhax.legend()\n\n\n############## plot 2\n#now plot the box plot\nbax = sales_data.Sales.plot(kind='box', ax = axs[1], title = 'Boxplot of Sales data', vert=False)\n#vert False to make it horizontal\n\n#Get the quartiles\nquartiles = sales_data.Sales.quantile([0.25, 0.5, 0.75])\nbax.text(quartiles[0.25], 0.75, r'$Q_{0.25}= ' + str(quartiles[0.25])+'$')\nbax.text(quartiles[0.75], 0.75, r'$Q_{0.75}= ' + str(quartiles[0.75])+'$')\n\n#Calculate the IQR\niqr = quartiles[0.75] - quartiles[0.25]\nbax.text(x=150, y=1.25, s='IQR = ' + str(iqr))\n\n#Get the Left inner quartile\nliq = quartiles[0.25] - 1.5*iqr\nbax.text(x=liq, y=0.85, s=str(liq))\n\n#Get the right inner quartile\nriq = quartiles[0.75] + 1.5*iqr\nbax.text(x=riq, y=0.85, s=str(riq))\n</pre> #create a subplot to show box plot and histogram next to each other fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8,10))  #plot a histogram using Pandas. hax = sales_data.Sales.hist(ax = axs[0], bins=15, grid=False) hax.set_title('Histogram of Sales data')  #get the axis bounds hax_bounds = hax.axis()  #plot the mean in black mean = sales_data.Sales.mean() hax.vlines(mean, hax_bounds[2], hax_bounds[3], label='Mean = ' + str(mean))  #plot the median in yellow median= sales_data.Sales.median() hax.vlines(median, hax_bounds[2], hax_bounds[3], label='Median = ' + str(median), colors='yellow')  #plot the mode in red mode= sales_data.Sales.mode()[0] hax.vlines(mode, hax_bounds[2], hax_bounds[3], label='Mode = ' + str(mode), colors='red')  #Get the standard deviation sd = sales_data.Sales.std()  #get mean +- 1SD lines m1sd = mean + 1*sd m1negsd = mean - 1*sd hax.vlines(m1sd, hax_bounds[2], hax_bounds[3], label='Mean + 1SD = ' + str(m1sd), colors='cyan') hax.vlines(m1negsd, hax_bounds[2], hax_bounds[3], label='SD = ' + str(sd), colors='cyan')  hax.legend()   ############## plot 2 #now plot the box plot bax = sales_data.Sales.plot(kind='box', ax = axs[1], title = 'Boxplot of Sales data', vert=False) #vert False to make it horizontal  #Get the quartiles quartiles = sales_data.Sales.quantile([0.25, 0.5, 0.75]) bax.text(quartiles[0.25], 0.75, r'$Q_{0.25}= ' + str(quartiles[0.25])+'$') bax.text(quartiles[0.75], 0.75, r'$Q_{0.75}= ' + str(quartiles[0.75])+'$')  #Calculate the IQR iqr = quartiles[0.75] - quartiles[0.25] bax.text(x=150, y=1.25, s='IQR = ' + str(iqr))  #Get the Left inner quartile liq = quartiles[0.25] - 1.5*iqr bax.text(x=liq, y=0.85, s=str(liq))  #Get the right inner quartile riq = quartiles[0.75] + 1.5*iqr bax.text(x=riq, y=0.85, s=str(riq))   Out[88]: <pre>&lt;matplotlib.text.Text at 0x120d504a8&gt;</pre> <p><code>Variance</code> is the mean squared deviation from mean. Thus $\\sigma^2$ is <code>variance of population</code> and $s^2$ is <code>variance of sample</code></p> <p>$$ s^2 = \\frac{\\sum_{i}^{n}(y_i - \\bar y)^2}{(n-1)} $$</p> <p><code>s</code> = <code>standard deviation</code> is square root of variance. $$ s = \\sqrt{\\frac{\\sum_{i}^{n}(y_i - \\bar y)^2}{(n-1)}} $$</p> <p>We divide the sum of squared deviation by <code>(n-1)</code> because we don't want the sample SD to be underestimated. This is because, we can estimate the SD of population by averaging the SDs of multiple samples. During this process, if we divided by <code>n</code> instead of <code>n-1</code>, then the SD of population is underestimated.</p> <p>For a <code>normal</code>ly distributed population, empirically, $$ \\bar y \\pm 1s = 68\\% \\verb ! of data! \\\\ \\bar y \\pm 2s = 95\\% \\verb ! of data! \\\\ \\bar y \\pm 3s = 97.7\\% \\verb ! of data! $$</p> <p>Thus range is max - min value. Hence $range = 6s$</p> <p>where $s_x$ is standard deviation of $x$ and so on. $r$ is called Pearson's r</p> <p><code>r</code> ranges from <code>-1 to 1</code>. A value of <code>-1</code> indicates strong negative relationship and vice versa. A value close to <code>0</code> might represent no relationship or presence of a non-linear relationship.</p> <p>The numerator, is called the <code>covariance</code> of <code>x</code> and <code>y</code>, which is the combined deviation from their corresponding means.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/stats/02_data_description/#describing-data-statistically","title":"Describing Data Statistically\u00b6","text":""},{"location":"projects/stats/02_data_description/#measures-of-central-tendancy","title":"Measures of central tendancy\u00b6","text":""},{"location":"projects/stats/02_data_description/#measuring-dispersion-inter-quartile-ranges","title":"Measuring dispersion - Inter Quartile Ranges\u00b6","text":""},{"location":"projects/stats/02_data_description/#histogram-of-sales-data","title":"Histogram of sales data\u00b6","text":""},{"location":"projects/stats/02_data_description/#measures-of-dispersion-standard-deviation","title":"Measures of dispersion - Standard Deviation\u00b6","text":"<p>Range (max val - min val), IQR - inter quartile range are a few measures to find the spread of data and outliers. Another quantitative way is variance and standard deviation.</p> <p>$$ \\mu = \\verb !population mean (cannot be calculated in most cases)! \\\\ \\bar y = \\verb !(y bar) sample mean! \\\\ \\bar y = \\frac{\\sum_{i}^{n}y_i}{n} $$</p>"},{"location":"projects/stats/02_data_description/#coefficient-of-variability","title":"Coefficient of variability\u00b6","text":"<p>To compare the dispersion of two different variables (of different ranges and units), we can normalize them to a common unitless measure called <code>CV</code>.</p> <p>$$ CV = \\frac{s}{|\\bar y|} $$</p>"},{"location":"projects/stats/02_data_description/#dispersion-based-on-percentiles","title":"Dispersion based on percentiles\u00b6","text":"<p>A <code>percentile</code> : <code>p</code>th percentile is the value such that <code>p</code>% of values are less than this and <code>100-p</code>% are higher than that value. Thus <code>80</code>th percentile means, <code>79.9%</code> of data points are less than this and <code>19.9%</code> values are higher than this.</p> <p>Quantile is the same as percentile but expressed in decimals. Thus <code>80</code>th percentile = <code>0.8</code> quantile.</p> <p>Quartile is quantile at every quarter. Thus <code>0.25</code>, <code>0.5</code>, <code>0.75</code> quantiles are quartiles. Note: <code>0.5</code> quantile is same as Median.</p> <p><code>IQR</code> InterQuartile Range is the difference between <code>75</code>th and <code>25</code>th percentiles. (0.75 and 0.25 quartiles).</p>"},{"location":"projects/stats/02_data_description/#correlation-coefficient","title":"Correlation coefficient\u00b6","text":"<p>To determine the linear relationship between two variables, we determine how each of the measurement pairs deviate from their corresponding means.</p> <p>$$ r = \\frac{\\sum_{i}^{n}(x_i - \\bar x)(y_i - \\bar y)}{(n-1)s_x s_y} $$</p>"},{"location":"projects/stats/02_data_description/#when-is-big-data-needed","title":"When is big data needed?\u00b6","text":"<p>Are all problems a big data problem? Can problems be solved with sampling and handling a subset of data? Yes, most often, quality and representiveness of data is more important than quantity.</p> <p>The cases where big-data is needed is when data is sparse, when thousands of predictors is required over millions of data points, where values of most predictors is 0. This is a very sparse data set. For such cases, big data improves the accuracy and random sampling cannot just produce a representative sample.</p>"},{"location":"projects/stats/04_conditional_probability/","title":"Conditional Probability","text":"<p>Probability ranges from 0 to 1. The sum of P(A) and the opposite of A occuring is 1. For mutually exclusive events A and B, the Probability of either A or B ocurring is sum of their probabilities.</p> <p>Mutually exclusive: Two events are considered mutually exclusive, if when an event is performed once, the occurrence of one of the events excludes the possibility of another.</p> <p>For two independent events, probabilities of their union and intersection can be represented as</p> <p>$$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) $$</p> <p>If A and B are mutually exclusive, then $P(A \\cap B) = 0$</p> <p>The reason we negate <code>P(A intersection B)</code> can be seen from the venn diagram below. Probabilities of A and B are (0.5 and 0.2). The probability of both A and B ocurring is 0.05. Thus to not double count the intersection, we negate it.</p> In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib_venn import venn2\nvenn2(subsets = (0.45, 0.15, 0.05), set_labels = ('A', 'B'))\n</pre> import matplotlib.pyplot as plt %matplotlib inline from matplotlib_venn import venn2 venn2(subsets = (0.45, 0.15, 0.05), set_labels = ('A', 'B')) Out[4]: <pre>&lt;matplotlib_venn._common.VennDiagram at 0x11a436ef0&gt;</pre> <p>The $P(A/B)$ probability of A given that B occurs, is the probability of A and B occurring $P(A \\cap B)$ to the probability of B occurring $P(B)$. Thus if <code>A</code> and <code>B</code> are <code>mutually exclusive</code>, then there is no conditional probability.</p> <p>Example Consider the case of insurance fraud. In table below, you are given insurance type and what rate of them are fraud claims.</p> In\u00a0[2]: Copied! <pre>import pandas as pd\ndf = pd.DataFrame([[6,1,3,'Fradulent'],[14,29,47,'Not Fradulent']], \n                  columns=['Fire', 'Auto','Other','Status'])\n</pre> import pandas as pd df = pd.DataFrame([[6,1,3,'Fradulent'],[14,29,47,'Not Fradulent']],                    columns=['Fire', 'Auto','Other','Status']) In\u00a0[3]: Copied! <pre>df\n</pre> df Out[3]: Fire Auto Other Status 0 6 1 3 Fradulent 1 14 29 47 Not Fradulent <p>The total number of claims: 100, number of fraud claims: 10. Thus 10% of all claims are fraud. However, with additional information about type of claims, we can fine grain whether a given claim is fraud, if we knew the type of claim (predictor variable).</p> <p>To answer the question, what is the probability that a claim is fraud, given that it is a Fire claim?:</p> <p>$$ p(Fraud \\ |\\ fire \\ policy) = \\frac{p(fire \\cap fraud)}{p(fire \\ policy)} $$ $$ p(Fraud \\ | \\ fire \\ policy) = \\frac{0.06}{0.20} = .30 $$ or 30% of claims are fraud, given that they are of type fire.</p> <p>Here, <code>30%</code> is called the <code>conditional probability</code> and the general <code>10%</code> is called the <code>unconditional</code> or <code>marginal</code> probability. Clearly, knowing the conditional probability is of much higher value than knowing the unconditional probability.</p> <p>In reality, we expand the $\\bar A$ case. Thus if $A_1 ,... A_k$ are mutually exclusive <code>states of nature</code> and if $B_1 .. B_m$ are <code>m</code> possible mutually exclusive observable events, then,</p> <p>$$ P(A_i | B_j) = \\frac{P(B_j | A_i)P(A_i)}{P(B_j | A_1)P(A_1) + P(B_j | A_2)P(A_2) + ... + P(B_j | A_k)P(A_k)} $$</p> <p>Consider $A_1 ,... A_k$ as $k$ predictor variables in machine learning. The Naive Bayes classifier will build the conditional probabilities of $p(B_j|A_k)$ to later predict what would $p(A_i | B_j)$ be.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/stats/04_conditional_probability/#conditional-probability","title":"Conditional Probability\u00b6","text":""},{"location":"projects/stats/04_conditional_probability/#axioms-of-probability","title":"Axioms of probability\u00b6","text":"<p>$$ 0 \\leq P(A) \\leq 1 \\\\ P(A) + P(\\bar A) = 1 \\\\ P(A \\verb ! or ! B) = P(A) + P(B) $$</p>"},{"location":"projects/stats/04_conditional_probability/#conditional-probability","title":"Conditional Probability\u00b6","text":"<p>Generally, conditional probability is more helpful in explaining a situtation than general probabilities.</p> <p>Given two events <code>A</code> and <code>B</code> with non zero probabilities, then the probability of A occurring, given that B has occurs is</p> <p>$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$</p> <p>and $$ P(B|A) = \\frac{P(A \\cap B)}{P(A)} $$</p>"},{"location":"projects/stats/04_conditional_probability/#bayesian-conditional-probability","title":"Bayesian conditional probability\u00b6","text":"<p>The Bayesian theorem builds on conditional probability, specifically on prior and posterior probabilities. It states that, if <code>A</code> and <code>B</code> are any events whose probabilities are not 0 or 1, then:</p> <p>$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\\bar A)P(\\bar A)} $$</p>"},{"location":"projects/stats/04_normal_distribution/","title":"Normal distribution","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[10]: Copied! <pre>vals = np.random.standard_normal(100000)\nlen(vals)\n</pre> vals = np.random.standard_normal(100000) len(vals) Out[10]: <pre>100000</pre> In\u00a0[19]: Copied! <pre>fig, ax = plt.subplots(1,1)\nhist_vals = ax.hist(vals, bins=200, color='red', density=True)\n</pre> fig, ax = plt.subplots(1,1) hist_vals = ax.hist(vals, bins=200, color='red', density=True) <p>The above is the standard normal distribution. Its mean is 0 and SD is 1. About <code>95%</code> values fall within $\\mu \\pm 2 SD$ and <code>98%</code> within $\\mu \\pm 3 SD$</p> <p>The area under this curve is <code>1</code> which gives the probability of values falling within the range of standard normal.</p> <p>A common use is to find the probability of a value falling at a particular range. For instance, find $p(-2 \\le z \\le 2)$ which is the probability of a value falling within $\\mu \\pm 2SD$. This calculated by summing the area under the curve between these bounds.</p> <p>$$p(-2 \\le z \\le 2) = 0.9544$$ which is <code>95.44%</code> probability. Its <code>z</code> score is <code>0.9544</code>.</p> <p>Similarly $$p(z \\ge 5.1) = 0.00000029$$</p> <p>Example Let X be age of US presidents at inaugration. $X \\in N(\\mu = 54.8, \\sigma=6.2)$. What is the probability of choosing a president at random that is less than <code>44</code> years of age.</p> <p>We need to find $p(x&lt;44)$. First we need to transform to standard normal.</p> <p>$$p(z&lt; \\frac{44-54.8}{6.2})$$ $$p(z&lt;-1.741) = 0.0409 \\approx 4\\%$$</p> In\u00a0[2]: Copied! <pre>import scipy.stats as st\n\n# compute the p value for a z score\nst.norm.cdf(-1.741)\n</pre> import scipy.stats as st  # compute the p value for a z score st.norm.cdf(-1.741) Out[2]: <pre>0.04084178926110883</pre> <p>Let us try for some common <code>z scores</code>:</p> In\u00a0[4]: Copied! <pre>[st.norm.cdf(-3), st.norm.cdf(-1), st.norm.cdf(0), st.norm.cdf(1), st.norm.cdf(2)]\n</pre> [st.norm.cdf(-3), st.norm.cdf(-1), st.norm.cdf(0), st.norm.cdf(1), st.norm.cdf(2)] Out[4]: <pre>[0.0013498980316300933,\n 0.15865525393145707,\n 0.5,\n 0.8413447460685429,\n 0.9772498680518208]</pre> <p>As you noticed, the <code>norm.cdf()</code> function gives the cumulative probability (left tail) from <code>-3</code> to <code>3</code> approx. If you need right tailed distribution, you simply subtract this value from <code>1</code>.</p> In\u00a0[7]: Copied! <pre># Find Z score for a probability of 0.97 (2sd)\nst.norm.ppf(0.97)\n</pre> # Find Z score for a probability of 0.97 (2sd) st.norm.ppf(0.97) Out[7]: <pre>1.8807936081512509</pre> In\u00a0[8]: Copied! <pre>[st.norm.ppf(0.95), st.norm.ppf(0.97), st.norm.ppf(0.98), st.norm.ppf(0.99)]\n</pre> [st.norm.ppf(0.95), st.norm.ppf(0.97), st.norm.ppf(0.98), st.norm.ppf(0.99)] Out[8]: <pre>[1.6448536269514722,\n 1.8807936081512509,\n 2.0537489106318225,\n 2.3263478740408408]</pre> <p>As is the <code>ppf()</code> function gives only positive <code>z</code> scores, you need to apply $\\pm$ to it.</p> In\u00a0[3]: Copied! <pre>demo_dist = 55 + np.random.randn(200) * 3.4\nstd_normal = np.random.randn(200)\n</pre> demo_dist = 55 + np.random.randn(200) * 3.4 std_normal = np.random.randn(200) In\u00a0[6]: Copied! <pre>[demo_dist.mean(), demo_dist.std(), demo_dist.min(), demo_dist.max()]\n</pre> [demo_dist.mean(), demo_dist.std(), demo_dist.min(), demo_dist.max()] Out[6]: <pre>[55.11294611274521, 3.29635155084324, 46.56508960658229, 65.69499942017563]</pre> In\u00a0[7]: Copied! <pre>[std_normal.mean(), std_normal.std(), std_normal.min(), std_normal.max()]\n</pre> [std_normal.mean(), std_normal.std(), std_normal.min(), std_normal.max()] Out[7]: <pre>[0.08344701747941835,\n 0.9782741158088577,\n -1.9692101150067682,\n 2.94001634796817]</pre> <p>Now let us use scikit-learn to easily transform this dataset</p> In\u00a0[8]: Copied! <pre>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n</pre> from sklearn.preprocessing import StandardScaler scaler = StandardScaler() In\u00a0[22]: Copied! <pre>demo_dist = demo_dist.reshape(200,1)\ndemo_dist_scaled = scaler.fit_transform(demo_dist)\n</pre> demo_dist = demo_dist.reshape(200,1) demo_dist_scaled = scaler.fit_transform(demo_dist) In\u00a0[23]: Copied! <pre>[round(demo_dist_scaled.mean(),3), demo_dist_scaled.std(), demo_dist_scaled.min(), demo_dist_scaled.max()]\n</pre> [round(demo_dist_scaled.mean(),3), demo_dist_scaled.std(), demo_dist_scaled.min(), demo_dist_scaled.max()] Out[23]: <pre>[0.0, 1.0, -2.5931264837260137, 3.2102320229538095]</pre> In\u00a0[49]: Copied! <pre>fig, axs = plt.subplots(2,2, figsize=(15,8))\np1 = axs[0][0].scatter(sorted(demo_dist), sorted(std_normal))\naxs[0][0].set_title(\"Scatter of original dataset against standard normal\")\n\np1 = axs[0][1].scatter(sorted(demo_dist_scaled), sorted(std_normal))\naxs[0][1].set_title(\"Scatter of scaled dataset against standard normal\")\n\np2 = axs[1][0].hist(demo_dist, bins=50)\naxs[1][0].set_title(\"Histogram of original dataset against standard normal\")\n\np3 = axs[1][1].hist(demo_dist_scaled, bins=50)\naxs[1][1].set_title(\"Histogram of scaled dataset against standard normal\")\n</pre> fig, axs = plt.subplots(2,2, figsize=(15,8)) p1 = axs[0][0].scatter(sorted(demo_dist), sorted(std_normal)) axs[0][0].set_title(\"Scatter of original dataset against standard normal\")  p1 = axs[0][1].scatter(sorted(demo_dist_scaled), sorted(std_normal)) axs[0][1].set_title(\"Scatter of scaled dataset against standard normal\")  p2 = axs[1][0].hist(demo_dist, bins=50) axs[1][0].set_title(\"Histogram of original dataset against standard normal\")  p3 = axs[1][1].hist(demo_dist_scaled, bins=50) axs[1][1].set_title(\"Histogram of scaled dataset against standard normal\") Out[49]: <pre>Text(0.5,1,'Histogram of scaled dataset against standard normal')</pre> <p>As you see above, the shape of distribution is the same, just the values are scaled.</p> In\u00a0[20]: Copied! <pre>demo_dist = 55 + np.random.randn(200) * 3.4\nstd_normal = np.random.randn(200)\n</pre> demo_dist = 55 + np.random.randn(200) * 3.4 std_normal = np.random.randn(200) In\u00a0[21]: Copied! <pre>demo_dist = sorted(demo_dist)\nstd_normal = sorted(std_normal)\n</pre> demo_dist = sorted(demo_dist) std_normal = sorted(std_normal) In\u00a0[22]: Copied! <pre>plt.scatter(demo_dist, std_normal)\n</pre> plt.scatter(demo_dist, std_normal) Out[22]: <pre>&lt;matplotlib.collections.PathCollection at 0x111e56dd8&gt;</pre> <p>For the most part, the values fall on a straight line, except in the fringes. Thus, the demo distribution is fairly normal.</p> <p>The <code>z</code> table and normal distribution are used to derive confidence intervals. Popular intervals and their corresponding <code>z</code> scores are</p> interval z-value 99% $\\pm 2.576$ 98% $\\pm 2.326$ 95% $\\pm 1.96$ 90% $\\pm 1.645$ <p>As you imagine, these are the values of <code>z</code> on X axis of the standard normal distribution and the area they cover.</p> <p>For a normal distribution, confidence intervals for an estimate (such as mean) can be given as $$CI = \\bar x \\pm z\\frac{s}{\\sqrt{n}}$$ where $s$ is sample SD that is substituted in place of population SD, if sample size is larger than 30.</p> <p>Example The average TV viewing times of <code>40</code> adults sampled in Iowa is <code>7.75</code> hours per week. The SD of this sample is <code>12.5</code>. Find the <code>95%</code> CI population's average TV viewing times.</p> <p>$\\bar x = 40$, $s=12.5$, $n=40$, $Z=1.96$ for 95% CI. Thus $$95\\%CI = 7.75 \\pm 1.96\\frac{12.5}{\\sqrt{40}}$$ $$95\\%CI = (3.877 | 11.623)$$</p> <p>Thus the <code>95</code>% CI is pretty wide. Intuitively, if SD of sample is smaller, then so is the CI.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/stats/04_normal_distribution/#normal-distribution","title":"Normal distribution\u00b6","text":"<p>Standard normal distribution takes a bell curve. It is also called as gaussian distribution. Values in nature are believed to take a normal distribution. The equation for normal distribution is</p> <p>$$y = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ where $\\mu$ is mean</p> <p>$\\sigma$ is standard deviation</p> <p>$\\pi$ = 3.14159..</p> <p>$e$ = 2.71828.. (natural log)</p>"},{"location":"projects/stats/04_normal_distribution/#finding-z-score-and-p-values-using-scipy","title":"Finding z score and p values using SciPy\u00b6","text":"<p>The standard normal is useful as a z table to look up the probability of a z score (x axis). You can use Scipy to accomplish this.</p>"},{"location":"projects/stats/04_normal_distribution/#levels-of-significance","title":"Levels of significance\u00b6","text":"<p>By rule of thumb, a <code>z</code> score greater than <code>0.005</code> is considered significant as such a value has a very low probability of occuring. Thus, there is less chance of it occurring randomly and hence, there is probably a force acting on it (significant force, not random chance).</p>"},{"location":"projects/stats/04_normal_distribution/#transformation-to-standard-normal","title":"Transformation to standard normal\u00b6","text":"<p>If the distribution of a phenomena follows normal dist, then you can transform it to standard normal, so you can measure the <code>z</code> scores. To do so, $$std normal value = \\frac{observed - \\mu}{\\sigma}$$ You subtract the mean and divide by SD of the distribution.</p>"},{"location":"projects/stats/04_normal_distribution/#finding-z-score-and-p-values-using-scipy","title":"Finding z score and p values using SciPy\u00b6","text":"<p>The standard normal is useful as a z table to look up the probability of a z score (x axis). You can use Scipy to accomplish this.</p>"},{"location":"projects/stats/04_normal_distribution/#finding-z-score-from-a-p-value","title":"Finding z score from a p value\u00b6","text":"<p>Sometimes, you have the probability (p value), but want to find the <code>z score</code> or how many SD does this value fall from mean. You can do this inverse using <code>ppf()</code>.</p>"},{"location":"projects/stats/04_normal_distribution/#transformation-to-standard-normal-and-machine-learning","title":"Transformation to standard normal and machine learning\u00b6","text":"<p>Transforming features to standard normal has applications in machine learning. As each feature has a different unit, their range, standard deviation vary. Hence we scale them all to standard normal distribution with mean=0 and SD=1. This way a learner finds those variables that are truly influencial and not simply because it has a larger range.</p> <p>To accomplish this easily, we use <code>scikit-learn</code>'s <code>StandardScaler</code> object as shown below:</p>"},{"location":"projects/stats/04_normal_distribution/#assessing-normality-of-a-distribution","title":"Assessing normality of a distribution\u00b6","text":"<p>To assess how normal a distribution of values is, we sort the values, then plot them against sorted values of standard normal distribution. If the values fall on a straight line, then they are normally distributed, else they exhibit skewness and or kurtosis.</p>"},{"location":"projects/stats/04_normal_distribution/#standard-error","title":"Standard error\u00b6","text":"<p>As can be seen, we use statistics to estimate population mean $\\mu$ from sample mean $\\bar x$. Standard error of $\\bar x$ represents on average, how far will it be from $\\mu$.</p> <p>As you suspect, the quality of $\\bar x$, or its standard error will depend on the sample size. In addition, it also depends on population standard deviation. Thus for a tighter population, it is much easy to estimate mean from a small sample as there are fewer outliers.</p> <p>Nonetheless, $$SE(\\bar x) = \\frac{\\sigma}{\\sqrt{n}}$$ where $\\sigma$ is population SD and $n$ is sample size.</p> <p>Empirically, $SE(\\bar x)$ is same as the SD of a distribution of sample means. If you were to collect a number of samples, find their means to form a distribution, the SD of this distribution represents the standard error of that estimate (mean in this case).</p>"},{"location":"projects/stats/04_normal_distribution/#confidence-intervals","title":"Confidence intervals\u00b6","text":"<p>From a population, many samples of size &gt; <code>30</code> is drawn and their means are computed and plotted, then with $\\bar x$ or $\\bar y$ -&gt; mean of a sample and $n$ -&gt; size of 1 sample, $\\sigma_{\\bar x}$ or $\\sigma_{\\bar y}$ is SD of distribution of samples, you can observe that</p> <ul> <li>$\\mu_{\\bar x} = \\mu$ (mean of distribution of sample means equals population mean)</li> <li>$\\frac{\\sigma}{\\sqrt n} = \\sigma_{\\bar x}$ (SD of population over sqrt of sample size equals SD of sampling distribution)</li> <li>relationship between population mean and mean of a single sample is</li> <li>$$ \\mu = \\bar y \\pm z(\\frac{\\sigma}{\\sqrt n})$$</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/","title":"Binomial and Poisson distributions","text":"<p>The probability of observing <code>y</code> success in <code>n</code> trials of a binomial experiment is $$ P(y) = \\frac{n!}{y!(n-y)!}\\pi^y (1-\\pi)^{n-y} $$</p> <p>where</p> <ul> <li>n = number of trials</li> <li>$\\pi$ = probability of success in a single trial</li> <li>$1-\\pi$ = probability of failure in a single tiral</li> <li><code>y</code> = number of successes in <code>n</code> trials</li> <li>$n!$ (n factorial) = $n(n-1)(n-2)..(n-(n-1))$</li> </ul> <p>We can build a simple Python function to calculate the binomial probability as shown below:</p> In\u00a0[1]: Copied! <pre>import math\n\ndef bin_prob(n,y,pi):\n    a = math.factorial(n)/(math.factorial(y)*math.factorial(n-y))\n    b = math.pow(pi, y) * math.pow((1-pi), (n-y))\n    p_y = a*b\n    return p_y\n</pre> import math  def bin_prob(n,y,pi):     a = math.factorial(n)/(math.factorial(y)*math.factorial(n-y))     b = math.pow(pi, y) * math.pow((1-pi), (n-y))     p_y = a*b     return p_y In\u00a0[5]: Copied! <pre>utmost_80 = bin_prob(100,80,0.85)\nprint(\"utmost 80: \" + str(utmost_80))\n\nutmost_50 = bin_prob(100,50,0.85)\nprint(\"utmost 50: \" + str(utmost_50))\n\nutmost_10 = bin_prob(100,10,0.85)\nprint(\"utmost 10: \" + str(utmost_10))\n\nutmost_95 = bin_prob(100, 95, 0.85)\nprint(\"utmost 95: \" + str(utmost_95))\n</pre> utmost_80 = bin_prob(100,80,0.85) print(\"utmost 80: \" + str(utmost_80))  utmost_50 = bin_prob(100,50,0.85) print(\"utmost 50: \" + str(utmost_50))  utmost_10 = bin_prob(100,10,0.85) print(\"utmost 10: \" + str(utmost_10))  utmost_95 = bin_prob(100, 95, 0.85) print(\"utmost 95: \" + str(utmost_95)) <pre>utmost 80: 0.04022449066141771\nutmost 50: 1.9026685879668748e-16\nutmost 10: 2.4027434608795305e-62\nutmost 95: 0.0011271383580980794\n</pre> <p>We could calculate the probability for all possible values of the discrete random varibale in a loop and plot the probabilities as shown below:</p> In\u00a0[20]: Copied! <pre>x =[]\ny =[]\ncum_prob = []\nfor i in range(1,101):\n    x.append(i)\n    p_y = bin_prob(100,i,0.85)\n#     print(str(i) + \"  \" + str(p_y))\n    y.append(p_y)\n    \n    if i==1:\n        cum_prob.append(p_y)\n    else:\n        cum_prob.append(cum_prob[i-2] + p_y)\n</pre> x =[] y =[] cum_prob = [] for i in range(1,101):     x.append(i)     p_y = bin_prob(100,i,0.85) #     print(str(i) + \"  \" + str(p_y))     y.append(p_y)          if i==1:         cum_prob.append(p_y)     else:         cum_prob.append(cum_prob[i-2] + p_y) In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n\nfig,ax = plt.subplots(1,2, figsize=(13,5))\nax[0].plot(x,y)\nax[0].set_title('Probability of y successes')\nax[0].set_xlabel('num of successes in 100 trials')\nax[0].set_ylabel('probability of successes')\n\nax[1].plot(x,cum_prob)\nax[1].set_title('Cumulative Probability of y successes')\nax[1].set_xlabel('num of successes in 100 trials')\nax[1].set_ylabel('cumulative probability of successes')\n</pre> import matplotlib.pyplot as plt %matplotlib inline  fig,ax = plt.subplots(1,2, figsize=(13,5)) ax[0].plot(x,y) ax[0].set_title('Probability of y successes') ax[0].set_xlabel('num of successes in 100 trials') ax[0].set_ylabel('probability of successes')  ax[1].plot(x,cum_prob) ax[1].set_title('Cumulative Probability of y successes') ax[1].set_xlabel('num of successes in 100 trials') ax[1].set_ylabel('cumulative probability of successes')  Out[7]: <pre>&lt;matplotlib.text.Text at 0x1126d2b00&gt;</pre> <p>As we can see in the graph above, the probability that <code>x</code> number of seeds will germinate peaks around <code>85</code>, matching the germination rate of <code>0.85</code>.</p> In\u00a0[24]: Copied! <pre>#find x corresponding to the max probability value\ny.index(max(y)) + 1\n</pre> #find x corresponding to the max probability value y.index(max(y)) + 1 Out[24]: <pre>85</pre> <p>The probability falls steeply before and after 85. Using the <code>cumulative probability</code>, we can answer the question of <code>atleast</code>. Find the probability that</p> <ul> <li>atleast 20 seeds will germinate = prob(that 21 + 22 + 23 ... 100) will germinate</li> </ul> In\u00a0[30]: Copied! <pre>atleast_20 = cum_prob[99] - cum_prob[19]\nprint(\"atleast 20 = \" + str(atleast_20))\n\natleast_85 = cum_prob[99] - cum_prob[84]\nprint(\"atleast 85 = \" + str(atleast_85))\n\natleast_95 = cum_prob[99] - cum_prob[94]\nprint(\"atleast 95 = \" + str(atleast_95))\n</pre> atleast_20 = cum_prob[99] - cum_prob[19] print(\"atleast 20 = \" + str(atleast_20))  atleast_85 = cum_prob[99] - cum_prob[84] print(\"atleast 85 = \" + str(atleast_85))  atleast_95 = cum_prob[99] - cum_prob[94] print(\"atleast 95 = \" + str(atleast_95)) <pre>atleast 20 = 1.0\natleast 85 = 0.45722420577595013\natleast 95 = 0.00042551381703914704\n</pre> <p>We can repeat the experiment with a sample size of <code>20</code> and plot the results</p> In\u00a0[31]: Copied! <pre>x =[]\ny =[]\ncum_prob = []\nfor i in range(1,21):\n    x.append(i)\n    p_y = bin_prob(20,i,0.85)\n#     print(str(i) + \"  \" + str(p_y))\n    y.append(p_y)\n    \n    if i==1:\n        cum_prob.append(p_y)\n    else:\n        cum_prob.append(cum_prob[i-2] + p_y)\n</pre> x =[] y =[] cum_prob = [] for i in range(1,21):     x.append(i)     p_y = bin_prob(20,i,0.85) #     print(str(i) + \"  \" + str(p_y))     y.append(p_y)          if i==1:         cum_prob.append(p_y)     else:         cum_prob.append(cum_prob[i-2] + p_y) In\u00a0[32]: Copied! <pre>#find x corresponding to the max probability value\ny.index(max(y)) + 1\n</pre> #find x corresponding to the max probability value y.index(max(y)) + 1 Out[32]: <pre>17</pre> In\u00a0[33]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n\nfig,ax = plt.subplots(1,2, figsize=(13,5))\nax[0].plot(x,y)\nax[0].set_title('Probability of y successes')\nax[0].set_xlabel('num of successes in 20 trials')\nax[0].set_ylabel('probability of successes')\n\nax[1].plot(x,cum_prob)\nax[1].set_title('Cumulative Probability of y successes')\nax[1].set_xlabel('num of successes in 20 trials')\nax[1].set_ylabel('cumulative probability of successes')\n</pre> import matplotlib.pyplot as plt %matplotlib inline  fig,ax = plt.subplots(1,2, figsize=(13,5)) ax[0].plot(x,y) ax[0].set_title('Probability of y successes') ax[0].set_xlabel('num of successes in 20 trials') ax[0].set_ylabel('probability of successes')  ax[1].plot(x,cum_prob) ax[1].set_title('Cumulative Probability of y successes') ax[1].set_xlabel('num of successes in 20 trials') ax[1].set_ylabel('cumulative probability of successes')  Out[33]: <pre>&lt;matplotlib.text.Text at 0x112aaa630&gt;</pre> <p>Example Let y denote number of field mice captured in a trap in 24 hour period. The average value of y is <code>2.3</code>. What is the probability of capturing exactly <code>4</code> mice in a randomly selected trap?</p> <p>Ans: $$ \\mu=2.3 $$ $$ P(y=4)=? $$</p> In\u00a0[1]: Copied! <pre>import math\ndef poisson_prob(y,mu):\n    e = 2.71828\n    numerator = math.pow(mu, y) * math.pow(e, 0-mu)\n    denomenator = math.factorial(y)\n    \n    return numerator/denomenator\n</pre> import math def poisson_prob(y,mu):     e = 2.71828     numerator = math.pow(mu, y) * math.pow(e, 0-mu)     denomenator = math.factorial(y)          return numerator/denomenator In\u00a0[2]: Copied! <pre>#calculate p(4)\np_4 = poisson_prob(4, 2.3)\np_4\n</pre> #calculate p(4) p_4 = poisson_prob(4, 2.3) p_4 Out[2]: <pre>0.1169024103856968</pre> <p>Lets plot the distribution of y for values 0 to 10</p> In\u00a0[11]: Copied! <pre>y=list(range(0,11))\np_y = []\ncum_y = []\nmu = 2.3\n\nfor yi in y:\n    prob = poisson_prob(yi, mu)\n    p_y.append(prob)\n\n    if yi==0:\n        cum_y.append(prob)\n    else:\n        cum_y.append(cum_y[yi-1] + prob)\n</pre> y=list(range(0,11)) p_y = [] cum_y = [] mu = 2.3  for yi in y:     prob = poisson_prob(yi, mu)     p_y.append(prob)      if yi==0:         cum_y.append(prob)     else:         cum_y.append(cum_y[yi-1] + prob) In\u00a0[13]: Copied! <pre>#plot this\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig,ax = plt.subplots(1,2, figsize=(13,5))\nax[0].plot(y, p_y)\nax[0].set_title('Probability of finding y mice in 24 hours')\nax[0].set_xlabel('Probability of finding exactly y mice in 24 hours')\nax[0].set_ylabel('Probability')\n\nax[1].plot(y,cum_y)\nax[1].set_title('Cumulative Probability of y successes')\nax[1].set_xlabel('Probability of finding atleast y mice in 24 hours')\nax[1].set_ylabel('Cumulative probability')\n</pre> #plot this import matplotlib.pyplot as plt %matplotlib inline  fig,ax = plt.subplots(1,2, figsize=(13,5)) ax[0].plot(y, p_y) ax[0].set_title('Probability of finding y mice in 24 hours') ax[0].set_xlabel('Probability of finding exactly y mice in 24 hours') ax[0].set_ylabel('Probability')  ax[1].plot(y,cum_y) ax[1].set_title('Cumulative Probability of y successes') ax[1].set_xlabel('Probability of finding atleast y mice in 24 hours') ax[1].set_ylabel('Cumulative probability') Out[13]: <pre>&lt;matplotlib.text.Text at 0x115375eb8&gt;</pre>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#random-variables","title":"Random variables\u00b6","text":"<p>When the objective is to predict the category (qualitative, such as predicting political party affiliation), we term the it as predicting a <code>qualitative random variable</code>. On the other hand, if we are predicting a quantitative value (number of cars sold), we term it a <code>quantitative random variable</code>.</p> <p>When the observations of a <code>quantitative random variable</code> can assume values in a continuous interval (such as predicting temperature), it is called a <code>continuous random variable</code>.</p>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#properties-of-discrete-random-variable","title":"Properties of discrete random variable\u00b6","text":"<p>Say, we are predicting the probability of getting heads in two coin tosses P(y). Then</p> <ul> <li>probability of y ranges from 0 and 1</li> <li>sum of probabilities of all values of y = 1</li> <li>probabilities of outcomes of discrete random variable is additive. Thus probability of y = 1 or 2 is P(1) + P(2)</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#binomial-and-poisson-discrete-random-variables","title":"Binomial and Poisson discrete random variables\u00b6","text":""},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#binomial-probability-distribution","title":"Binomial probability distribution\u00b6","text":"<p>A binomial experiment is one in which the outcome is one of two possible outcomes. Coin tosses, accept / reject, pass / fail, infected / uninfected, these are the kinds of studies that involve a binomial experiment. Thus an experiment is of binomial in nature if</p> <ul> <li>experiment has <code>n</code> identical trials</li> <li>each trial results in 1 of 2 outcomes ( success and failure )</li> <li>probability of one of the outcome, say success remains the same for all trials</li> <li>trials are independent of each other</li> <li>the random variable <code>y</code> is the number of successes observed in <code>n</code> trials.</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#mean-and-standard-deviation-of-binomial-probability-distribution","title":"Mean and Standard Deviation of Binomial probability distribution\u00b6","text":"<p>$$ \\mu = n\\pi $$ $$ \\sigma = \\sqrt{n\\pi(1-\\pi)} $$</p> <p>where</p> <ul> <li>$\\mu$ is mean</li> <li>$\\sigma$ is standard deviation</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#binomial-probability-of-germination","title":"Binomial probability of germination\u00b6","text":"<p>Let us consider a problem where 100 seeds are drawn at random. The germination rate of each seed is <code>85%</code>. Or in other words, the probability that a seed will germinate is <code>0.85</code>, derived from experiment that <code>85</code> out of <code>100</code> seeds would germinate in a nursery. Now we want to calculate what is the probability</p> <ul> <li>that utmost only 80 seeds will germinate</li> <li>that utmost only 50 seeds will germinate</li> <li>that utmost only 10 seeds will germinate</li> <li>that utmost only 95 seeds will germinate</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#poisson-probability-distribution","title":"Poisson probability distribution\u00b6","text":"<p>Poisson is used for modeling the events of a particular time over a period of time or region of space. An example is the number of vehicles passing through a security checkpoint in a 5 min interval.</p> <p>Conditions</p> <p>The probability distribution of a discrete random variable y is Poisson, if:</p> <ul> <li>Events occur one at a time. Two or more events do not occur precisely at the same time or space</li> <li>Events are independent - occurrence of an event at a time is independent of any other event in during a non-overlapping period of time or space</li> <li>The expected number of events during one period or region $\\mu$ is the same as the expected number of events in any other period or region</li> </ul> <p>Thus the probability of observing y events in a unit of time or space is given by</p> <p>$$ P(y) = \\frac{\\mu^{y}e^{-\\mu}}{y!} $$</p> <p>where</p> <ul> <li>$\\mu$ is average value of y</li> <li>e is naturally occurring constant. <code>e = 2.71828</code></li> </ul>"},{"location":"projects/stats/05_hypothesis_testing/","title":"Hypothesis testing","text":"<p>Example A sample of <code>49</code> batteries are tested for their limetimes. The SD is <code>15.0</code>, mean longivity is <code>1006.2</code>. Is it possible to claim the batteries last longer than <code>1000</code> hours on average?</p> <p>$\\bar x = 1006.2$, $n=49$, $s=15$, $\\alpha = 0.01$ assumed. $$H_{0} =&gt; \\mu \\le 1000$$ $$H_{A} =&gt; \\mu &gt; 1000$$</p> <p>Find Test Statistic $$TS = \\frac{1006.2-1000}{15/\\sqrt{49}}$$ $$TS=2.89$$ This is a right tailed hypothesis as we test if test statistic is &gt; z score for the said alpha.</p> <p>z score for $\\alpha=0.01$ = 2.576 (for 99% CI) The TS is &gt; z score. Hence reject $H_{0}$. Thus mean battery life &gt; 1000 hours by significance.</p> In\u00a0[\u00a0]: Copied!"},{"location":"projects/stats/05_hypothesis_testing/#hypothesis-testing","title":"Hypothesis testing\u00b6","text":"<p>The goal of hypothesis testing is to answer a simple yes / no question about a population parameter. There are two types of hypothesis, $H_{0}$ the null hypothesis and $H_{A}$ the Alternate hypothesis.</p> <p>The steps followed are:</p> <ul> <li>set up the hypothesis (null, alternate)</li> <li>choose $\\alpha$ level (confidence interval)</li> <li>determine rejection region (on the z curve)</li> <li>compute the test statistic (p value based on z score)</li> <li>make a decision</li> </ul> <p>Rules in hypothesis testing</p> <ol> <li>No equal sign in $H_{A}$. Only $\\ne, &lt;, &gt;$ signs</li> <li>Put what you want to test in $H_{A}$, unless you violate rule 1, then you put that in $H_{0}$</li> <li>Believe $H_{0}$ unless the dataset shows otherwise</li> <li>when we make our decision, we either reject $H_{0}$ or fail to reject it.</li> </ol>"},{"location":"projects/stats/05_hypothesis_testing/#examples-of-formulating-hypothesis","title":"Examples of formulating hypothesis\u00b6","text":"<ol> <li>Nitrate levels are unsafe if &gt; 10ppm. Test if out water is unsafe on average.</li> </ol> <ul> <li>$H_{0} =&gt; \\mu \\le 10ppm$</li> <li>$H_{A} =&gt; \\mu &gt; 10 ppm$</li> </ul> <ol> <li>Test if a coin is fair.</li> </ol> <ul> <li>$H_{0} =&gt; p(h)=0.5$</li> <li>$H_{A} =&gt; p(h)\\ne 0.5$ This is because alt hypothesis should not have equal sign.</li> </ul>"},{"location":"projects/stats/05_hypothesis_testing/#type-1-2-errors","title":"Type 1, 2 errors\u00b6","text":"<p>For a jury trial, our motto is innocent until proven guilty. Hence</p> <ul> <li><p>$H_{0} =&gt; innocent$ as we reject H0 or fail to do so</p> </li> <li><p>$H_{A} =&gt; guilty$</p> </li> <li><p>Type 1 error: False positive</p> <ul> <li>we reject $H_{0}$ when it is still true</li> <li>$\\alpha$ = p(type 1 error) = p(rejecting $H_{0}$ when it is still true)</li> </ul> </li> <li><p>Type 2 error: False negative</p> <ul> <li>$\\beta$ = p(type 2 error) = p(failing to reject $H_{0}$ when $H_{A}$ is true)</li> </ul> </li> </ul> <p>In practice, we fix $\\alpha = 0.5$ and calculate $\\beta' = (1-\\beta)$</p>"},{"location":"projects/stats/05_hypothesis_testing/#testing-your-hypothesis","title":"Testing your hypothesis\u00b6","text":"<p>You calculate the test statistic as $$TS = \\frac{\\bar x - \\mu}{\\frac{s}{\\sqrt n}}$$ You either reject or approve the $H_{0}$ based on the value of the TS compared against the p value for the said $\\alpha$ value (confidence interval)</p>"},{"location":"projects/stats/islr/02_stat_learning/","title":"Statistical Learning","text":"<p>Notes</p> <ul> <li>parametric methods are an approximation of the true functional form of f.</li> <li>simpler (lower order, less flexible) models may lead to poorer estimates of f</li> <li>more flexible (higher order, complex) models may lead to overfitting.</li> <li>Since the model is trained on a subset of values, it might be very different from true nature of f. Hence the model developed is only valid for the range of data it was trained on.</li> </ul> <p>When K=1, the decision boundary is of highest flexibility and overfits with low bias and high variance. When K is very large, it underfits with low flexibility. It has high bias and low variance. As in regression, with classification, increasing flexibility reduces the training error, but does not affect test error rate.</p>"},{"location":"projects/stats/islr/02_stat_learning/#statistical-learning","title":"Statistical Learning\u00b6","text":""},{"location":"projects/stats/islr/02_stat_learning/#prediction","title":"Prediction\u00b6","text":"<p>In a prediction / regression problem, the inputs (denoted by <code>X</code>) are called as <code>predictors</code>, <code>independent variables</code>, <code>features</code> and the predicted variable is called as <code>response</code>, <code>dependent variable</code> and is denoted by <code>Y</code>.</p> <p>The relationship betwen input and predicted is represented as</p> <p>$$ Y = f(X) + \\epsilon $$</p> <p>where $f$ is some fixed, unknown function that is to be determined. $\\epsilon$ is random error term that is independent of <code>X</code> and has zero mean.</p> <p>In reality, $f$ may depend on more than 1 input variable $X$, for instance 2. In this case, $f$ is a <code>2D</code> surface that is fit. In general, the process of estimating $f$ is statistical learning.</p>"},{"location":"projects/stats/islr/02_stat_learning/#reducible-and-irreducible-errors","title":"Reducible and Irreducible errors\u00b6","text":"<p>Since $f$ and $Y$ cannot be calculated, the best we can get is to estimate them. Thus, the estimates are called $\\hat f$ and $\\hat Y$</p> <p>$$ \\hat Y = \\hat f(X) $$</p> <p>The accuracy of $\\hat Y$ depends on reducible and irreducible errors. The error in prediction of $\\hat f$ is reduible and can be improved wth more data and better models. However, $\\hat Y$ is also a function of $\\epsilon$ which is irreducible. Thus, the best our predictions can get is</p> <p>$$ \\hat Y = f(X) $$</p> <p>Focus of Statistical learning is to estimating $f$ as $\\hat f$ with least reducible error. However, the accuracy of $\\hat Y$ will always be controlled by irreducible and unknown error $\\epsilon$.</p> <p>In prediction problems, $\\hat f$ can be treated as blackbox as we are only interested in predicting $Y$.</p>"},{"location":"projects/stats/islr/02_stat_learning/#inference","title":"Inference\u00b6","text":"<p>We are interested in understanding how each of the different $X_{1}... X_{p}$ affect the dependent variable $Y$, hence the name inference. Here, $\\hat f$ cannot be treated as blackbox and we need to know its exact form. Some questions that are sought to be answered through inference:</p> <ul> <li>which predictor variables are associated with the response?</li> <li>what is the relationship b/w response and each predictor?</li> <li>is the relationship linear or is more complicated?</li> </ul>"},{"location":"projects/stats/islr/02_stat_learning/#parametric-and-unparametric-methods-for-estimating-f","title":"Parametric and Unparametric methods for Estimating f\u00b6","text":"<p>The observations for X and Y can be written as ${(x_{1}, y_{1}),(x_{2}, y_{2}),...,(x_{n}, y_{n})}$ where each x has many predictor variables that can be written as $x_{i} = (x_{i1},x_{i2},..,x_{ip})^{T}$. The goal is to find $\\hat f$ such that $Y \\approx \\hat f (X)$</p>"},{"location":"projects/stats/islr/02_stat_learning/#parametric-methods-for-estimating-f","title":"Parametric methods for estimating f\u00b6","text":"<p>Parametric methods take a model based approach (deterministic). We make an assumption about the functional form of f (whether it is linear, non linear, higher order, logistic etc). For instance, if we assume that f is linear, then</p> <p>$$ Y \\approx f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + .. + \\beta_{p}X_{p} $$ we only need to find $p+1$ coefficients. Through training or fitting (using methods like ordinary least squares), we can estimate the coefficients.</p>"},{"location":"projects/stats/islr/02_stat_learning/#non-parametric-methods-for-estimating-f","title":"Non parametric methods for estimating f\u00b6","text":"<p>Non parametric methods avoid assuming the functional form of f. However, these methods require a very large number of observations since they do not try to reduce the phenomenon to a model.</p>"},{"location":"projects/stats/islr/02_stat_learning/#general-concepts","title":"General concepts\u00b6","text":"<p>Model interpretability and complexity: The more complex a model is (higher order more flexible models, decision trees..), the less interpretable it is.</p> <p>Supervised vs Unsupervised algorithms: Supervised methods are used when both the <code>predictor</code> and <code>response</code> variables can be measured and data is available. Unsupervised methods are used when little is known about the data and only <code>predictor</code> variables are available. Unsupervised are best when put to classification / clustering problems.</p> <p>Regression vs Classification: When the <code>response</code> variable is <code>quantitative</code> and continuous, the problem is considered a regression. When the <code>response</code> is <code>qualitative</code> and falls within categories, then the problem is a classification problem. Howerver, this distinction is not really solid as many algorithms can be used for both.</p>"},{"location":"projects/stats/islr/02_stat_learning/#model-accuracy-regression-problems","title":"Model accuracy - Regression problems\u00b6","text":""},{"location":"projects/stats/islr/02_stat_learning/#measuring-quality-of-fit","title":"Measuring quality of fit\u00b6","text":"<p>Here the deviation between <code>predicted</code> and actual values is measured. In regression, <code>Mean Squared Error (MSE)</code> is commonly used.</p> <p>$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_{i} - \\hat f(x_{i}))^2 $$</p> <p>The MSE obtained is called training MSE. When used against unseen test data, we get test MSE. Our objective is to choose the method with lowest test MSE. There is no guarantee that a low training MSE will yield a low test MSE.</p> <p>A fundamental property in statistical learning is as model flexibility increases, training MSE might decrease, but test MSE might not. When a given learning method yields a small training MSE but a large test MSE, we are overfitting the data. This is because, our data might have noise from irreducible error and the model is trying to fit it.</p>"},{"location":"projects/stats/islr/02_stat_learning/#bias-variance-trade-off","title":"Bias variance trade-off\u00b6","text":"<p>If you plot the test MSE against model flexibility, it follows a U shaped curve. Thus it first reduces then increases. The expected test MSE for observation $x_{0}$ $E(y_{0} - \\hat f(x_{0}))$ can be decomposed to <code>3</code> fundamental quantities: (a) the variance of $\\hat f(x_{0})$, (b) the squared bias of $\\hat f(x_{0})$ and (c) variance of irreducible error $\\epsilon$. Thus:</p> <p>$$ E(y_{0} - \\hat f(x_{0}))^{2} = Var(\\hat f(x_{0})) + [Bias(\\hat f(x_{0}))]^{2} + Var(\\epsilon) $$</p> <p>Thus, to reduce the expected test MSE, we need to reduce both the variance of $\\hat f$ and bias of $\\hat f$.</p> <p>Variance refers to the amount by which $\\hat f$ would change if we estimated it using a different training dataset. Ideally, $\\hat f$ should not change much if a slightly different data set is used. A statistical learning method with high variance would yield a very different $\\hat f$ for different training data sets. Higher the model flexibility, the higher is its variance as the model closely fits the training data.</p> <p>Bias refers to the error introduced by approximating a real-life problem. Generally, higher model flexibility, the lower is the bias. Thus as we use more flexible methods, the variance will increase and bias would decrease.</p> <p>As model flexibility increases, the bias reduces faster than the rate at which variance increases. Thus, the test MSE drops initially before increasing (<code>U shape</code>). This relationship is called the bias variance trade-off and the objective is to pick the model flexibility that has the least of both.</p>"},{"location":"projects/stats/islr/02_stat_learning/#model-accuracy-classification-problems","title":"Model accuracy - Classification problems\u00b6","text":""},{"location":"projects/stats/islr/02_stat_learning/#measuring-quality-of-classification","title":"Measuring quality of classification\u00b6","text":"<p><code>error rate</code> is used to quantify the errors in classification. It is the ratio of <code>sum of misclassifications</code> to <code>number of observations</code>.</p> <p>$$ error rate = \\frac{1}{n}\\sum_{i=1}^{n}I(y_{i} \\ne \\hat y_{i}) $$</p> <p>where $\\hat y_{i}$ is predicted class label for ith observation using $\\hat f$. When computed against training data, this yields training error rate. When computed for test data, this yields test error rate.</p>"},{"location":"projects/stats/islr/02_stat_learning/#bayes-classifier","title":"Bayes Classifier\u00b6","text":"<p>Bayes classifier is a simple but idealistic classifier. It assigns each observation to the most likely class given its predictor class. This can be written using conditional probability as below:</p> <p>$$ P(Y=j \\ | \\ X=x_{0}) $$</p> <p>Thus, the error rate with Bayes classifier becomes the average of (1 - max probability for different classes). The Bayes error rate is analogous to **irreducible error*.</p>"},{"location":"projects/stats/islr/02_stat_learning/#knn-classifier","title":"KNN classifier\u00b6","text":"<p>In reality, Bayes classifier is not possible as the conditional probability is unknown. Instead, algorithms attempt to derive the conditional probability. One such is KNN.</p> <p>The KNN classifier identifies K points in training data that are closest to test observation $x_{0}$, represented at $N_{0}$. It then estimates conditional proabability for class $j$ as the fraction of points in $N_{0}$ whose classes equal $j$. This can be written as:</p> <p>$$ P(Y = j \\ | \\ X = x_{0}) = \\frac{1}{K} \\sum_{i \\in N_{0}} I(y_{i} = j) $$</p>"},{"location":"projects/stats/islr/03_linear_regression/","title":"Linear regression concepts","text":"<p>Mathematically, a linear relationship between <code>X</code> and <code>Y</code> can be written as $$Y \\approx \\beta_{0} + \\beta_{1}X$$</p> <p>$\\beta_{0}$ and $\\beta_{1}$ represent the intercept and slope. They are the model coefficients or parameters. Through regression we estimate these parameters. The estimates are represented as $\\hat\\beta_{0}$ and $\\hat\\beta_{1}$. Thus,</p> <p>$$\\hat y = \\hat\\beta_{0} + \\hat\\beta_{1}x$$</p> <p>We estimate $\\hat\\beta_{0}$ and $\\hat\\beta_{1}$ using <code>least squared regression</code>. This technique forms a line that minimizes average squared error for all data points. Each point is weighed equally. If</p> <p>$$\\hat y_{i} = \\hat\\beta_{0} + \\hat\\beta_{1}x_{i}$$ is the prediction for <code>i</code>th value pair of x, y, then the error is calculated as $$e_{i} = y_{i} - \\hat y_{i}$$. This error is also called a <code>residual</code>. This the residual sum of squares (RSS) is calculated as $$RSS = e_{1}^{2} + e_{2}^{2}... + e_{i}^{2}$$</p> <p>Thus if the relationship between $X$ and $Y$ is approximately linear, then we can write: $$Y = \\beta_{0} + \\beta_{1}X + \\epsilon$$</p> <p>where $\\epsilon$ is the catch-all error that is introduced in forcing a linear fit for the model. The above equation is the population regression line. In reality, this is not known (unless you synthesize data using this model). In practice, you estimate the population regression with a smaller subset of datasets.</p> <p>Using Central Limit Theorem, we know the average of a number of sample regression coefficients, predict the population coefficients pretty closely. Proof is availble here.</p> <p>As you see, quadratic produces 1 curve while cubic produces 2 curves.</p> In\u00a0[\u00a0]: Copied!"},{"location":"projects/stats/islr/03_linear_regression/#linear-regression-concepts","title":"Linear regression concepts\u00b6","text":""},{"location":"projects/stats/islr/03_linear_regression/#standard-error","title":"Standard error\u00b6","text":"<p>By averaging a number of estimations of $\\hat\\beta_{0}$ and $\\hat\\beta_{1}$, we are able to estimate the population coefficients in an unbiased manner. Averaging will greatly reduce any systematic over or under estimations when choosing a small sample.</p> <p>Now, how far will a single estimate of $\\hat\\beta_{0}$ be from the actual $\\beta_{0}$? We can calculate it using standard error.</p> <p>To understand standard error let us consider the simple case of estimating population mean using a number of smaller samples. The standard error in a statistic (mean in this case) can be written as:</p> <p>$$ SE(\\hat\\mu) = \\frac{\\sigma}{\\sqrt{n}}$$ where $\\hat\\mu$ is the estimate for which we calculate the standard error for (sample mean in this case), $\\sigma$ is the standard deviation of the population and $n$ is the size of the sample you draw each time.</p> <p>The SE of $\\hat\\mu$ is the same as the Standard Deviation of the sampling distribution of a number of sample means. Thus, the above equation gives the relationship between sample mean and population mean and sample size and how far will the sample mean be off. Thus:</p> <p>$$ SD(\\hat\\mu) = SE(\\hat\\mu) = \\frac{\\sigma}{\\sqrt n}$$</p> <p>Note, SE is likely to be smaller if you have a large sample. The value of SE is the average amount your $\\hat\\mu$ deviates from $\\mu$.</p> <p>In reality, you don't have $\\sigma$ or $\\mu$. Thus, using the SE formula, we can calculate the SD of population as:</p> <p>$$\\sigma = \\sigma_{\\hat\\mu}*\\sqrt n $$</p>"},{"location":"projects/stats/islr/03_linear_regression/#applications-of-standard-error","title":"Applications of Standard Error\u00b6","text":"<p>Confidence Intervals: SE is used to compute <code>CI</code>. A <code>95</code>% CI is defined as the range of values such that with <code>95</code>% probability, the range will contain the true unknown value of the parameter. Similar is <code>99</code>% CI. Thus, <code>95</code>% CI for $\\beta_{1}$ is written as</p> <p>$$ \\hat \\beta_{1} \\pm 2 * SE(\\hat \\beta_{1}) $$ Same for $\\beta_{0}$.</p> <p>Hypothesis tests: SE is also used to perform hypothesis tests on coefficients. Commonly,</p> <ul> <li>null hypothesis: $H_{0}$: there is no relationship between X and Y - meaning $\\beta_{1} \\ = \\ 0$</li> <li>alternate hypothesis: $H_{a}$: there is some significant relationship</li> </ul> <p>To disprove $H_{0}$, we need</p> <ul> <li>$\\hat \\beta_{1}$ to be sufficiently large: (either positive or negative), or,</li> <li>$SE(\\hat \\beta_{1})$ to be small, then even relatively small values of $\\hat \\beta_{1}$ would be statistically significant. Else, slope has to be really large.</li> </ul> <p>We compute <code>t-statistic</code> to evaluate the significance of $\\beta$, which is similar to computing <code>z scores</code>. $$ t \\ = \\ \\frac{\\hat \\beta_{1} - 0}{SE(\\hat \\beta_{1})} $$ We get scores for <code>t</code> distribution. t follows standard normal for <code>n&gt;30</code>. The <code>p-value</code> we get is used to evaluate the significance of the estimate. A small <code>p-value</code> indicates the relationship between predictor and response is unlikely to be by chance.</p> <p>To reject the null hypothesis, we need a <code>p-value &lt; 0.005</code> and <code>t-statistic &lt; 2</code></p>"},{"location":"projects/stats/islr/03_linear_regression/#multiple-linear-regression","title":"Multiple Linear Regression\u00b6","text":"<p>To fit the effects of multiple predictors, we extend the simple linear model by providing a coefficient for each predictor. Thus:</p> <p>$$ Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{p}X_{p} + \\epsilon $$</p> <p>we interpret $\\beta_{p}$ as the average effect on $Y$ that predictor variable has when holding all other variables constant.</p>"},{"location":"projects/stats/islr/03_linear_regression/#what-to-look-for-in-multiple-linear-regression","title":"What to look for in multiple linear regression\u00b6","text":""},{"location":"projects/stats/islr/03_linear_regression/#1-is-there-a-relationship-between-response-and-predictor-variables","title":"1. Is there a relationship between response and predictor variables\u00b6","text":"<p>We run hypothesis tests, just that, $H_{0}$ checks for all coefficients to be <code>0</code> and $H_{a}$ checks for at least one of $\\beta_{p}$ is non zero.</p> <p>$$ H_{0} = \\beta_{1} = \\beta_{2} = \\beta_{p} = 0 $$</p> <p>In simple linear regression, the hypothesis tests were conducted against a t distribution, where as in multiple linear regression, we compute a F distribution. The <code>p-value</code>s are against this <code>F</code> distribution.</p>"},{"location":"projects/stats/islr/03_linear_regression/#hallmarks-of-a-valid-regression-analysis","title":"Hallmarks of a valid regression analysis\u00b6","text":"<p>Source: Regression tutorial from Minitab</p> <p>Applications of regression analysis:</p> <ul> <li>multiple predictor variables</li> <li>continuous and categorical variables</li> <li>higher-order terms to model curvature</li> <li>interaction terms to see if the effect of one predictor depends upon the value of another</li> </ul>"},{"location":"projects/stats/islr/03_linear_regression/#what-to-put-in-a-regression-output","title":"What to put in a regression output\u00b6","text":"<ul> <li>Coefficients of each predictors. The coefficients indicate how influential a predictor is.</li> <li>Std. Error $S$ of coefficients</li> <li><code>p-value</code> of each coefficient. A low <code>p-value</code>, (lower than <code>0.05</code>) indicates the confidence in predicting the coefficient.</li> <li><code>95%</code> CI of each coefficient prediction</li> <li>residual plots<ul> <li>Fitted vs Residual plot to ensure the residuals don't follow any pattern. Residuals should be random around the prediction line, with <code>mean=0</code>). If residuals follow a pattern, then the model is missing or leaking some phenomena into the error term. You may be missing an important predictor variable.</li> <li>QQ plot of residuals against standard normal to assess normalcy of residuals</li> <li>histograms of residuals can also be used to assess normalcy.</li> </ul> </li> </ul> <p>Example from Minitab </p>"},{"location":"projects/stats/islr/03_linear_regression/#r-squared","title":"R-squared\u00b6","text":"<p>$R^2$ is calculated as the ratio of variance in predicted <code>Y</code> to actual <code>Y</code>. In other words, it compares the distance between actual values to mean vs predicted values to mean. $$ R^2 = \\frac{(\\hat y - \\bar y)^2}{(y-\\bar y)^2} $$</p> <p>$R^2$ measures the strength of the relationship between predictors and response. It ranges from <code>0-100%</code> or <code>0-1</code>.</p> <p>$R^2$ is limited, it cannot quite tell you if the model is systematically under or over predicting values. Since it compares deviation from mean, if values fall farther from regression line, yet keep the same variation from mean as actual, then $R^2$ is high. However, this does not mean the model is a good fit.</p> <p>Since it is inherently biased, some researchers don't use this at all.</p>"},{"location":"projects/stats/islr/03_linear_regression/#s-standard-error-in-regression","title":"<code>S</code> - Standard error in Regression\u00b6","text":"<p>A better estimate of regression is Standard Error (also called RSE - residual standard error) which measures average distance each actual value falls from regression line. It is calculated as below:</p> <p>$$ S = \\sqrt{\\frac{\\sum (\\hat y -y)^2}{n-2}} $$</p> <p><code>S</code> represents, in the units of predicted variable, on average how far the actual values fall from prediction. </p>"},{"location":"projects/stats/islr/03_linear_regression/#confidence-intervals","title":"Confidence intervals\u00b6","text":"<p>Approximately <code>95%</code> of predictions fall within $\\pm 2 standard error$ of regression from regression line.</p>"},{"location":"projects/stats/islr/03_linear_regression/#problems","title":"Problems\u00b6","text":"<p>Multicollinearity is when predictors are correlated. In ideal case, each predictor should be independent which helps us properly assess their individual influence on the phenomena.</p>"},{"location":"projects/stats/islr/03_linear_regression/#how-to-interpret-a-regression-analysis","title":"How to interpret a regression analysis\u00b6","text":""},{"location":"projects/stats/islr/03_linear_regression/#p-values","title":"P-values\u00b6","text":"<p>The <code>p-value</code> of a coefficient is the probability of</p> <ul> <li>$H_{0} \\ = \\ 0$ : Null hypothesis that coefficient is 0. That is, it has no effect on the model / phenomena</li> <li>$\\alpha$ is the threshold you set to determine what is statistically significant. By convention it is <code>0.95</code> for <code>p-values</code>. Thus, those coefficents whose <code>p-value</code>s are less than $\\alpha$ have a significant coefficient. The size of the coefficient determines their influence on the model itself, not the <code>p-value</code>.</li> </ul> <p>When you simplify your model (step-wise forward or hierarchical), you start by removing those predictors whose <code>p-value</code> is greater than $\\alpha$.</p>"},{"location":"projects/stats/islr/03_linear_regression/#coefficients","title":"Coefficients\u00b6","text":"<p>Regression coefficients represent the mean value by which the predicted variable will change for <code>1</code> unit change in that particular predictor variable while holding all other variables constant. Thus, regression provides a high level of explainability associated with each variable and its influence on the phenomena.</p>"},{"location":"projects/stats/islr/03_linear_regression/#r-squared","title":"R-squared\u00b6","text":"<p>As said before, $R^{2}$ measures the variability of predicted data vs actual data with mean. It can be interpreted as proportion of variability in Y that can be explained using X. A regression with high <code>p-value</code> and low $R^{2}$ is problematic. What this means is the trend (slope) is significant where as the data has a lot of inherent variability that the model does not explain well.</p> <p>In the case of simple linear regression, $R^{2}$ equals the correlation coefficient $r$. However, in multiple linear regression, this relationship does not extend.</p>"},{"location":"projects/stats/islr/03_linear_regression/#s","title":"S\u00b6","text":"<p>The Std. Error in regression (also called as <code>RSE</code> (residual standard error)) quantifies how far the points are spread from the regression line. This also helps build the prediction interval and understand what the error would be on average when you predict unknown dataset.</p> <p><code>RSE</code> takes the unit of predicted variable. Determining whether or not an <code>RSE</code> is good depends on the problem context.</p>"},{"location":"projects/stats/islr/03_linear_regression/#explaining-a-regression-analysis","title":"Explaining a regression analysis\u00b6","text":"<p>Resource Follow the axioms below when you try to explain a regression analysis</p> <ul> <li>do not use <code>p-value</code> to explain influence of a predictor. <code>p-value</code> only suggests the significance of the coefficient not being <code>0</code></li> <li>do not compare the importance of two predictors based on their coefficients (slopes). The variables might be in different units and hence may not be a fair comparison</li> <li>To compare predictors, standardize them (subtract mean and divide by SD -&gt; transform to std. normal Z scores)</li> <li>Now compare the coefficients for influence.</li> <li>When you add more variables to the model, the $R^{2}$ increases, but this does not mean the model improves. In this case, calculate the adj.$R^{2}$ which accounts for this.</li> <li>Another approach is, when you build the model step-wise, find the variable that accounts for greatest increase in $R^{2}$ value. That is the most influential predictor.</li> <li>Another adavantage of standardization is it reduces multicollinearity</li> </ul>"},{"location":"projects/stats/islr/03_linear_regression/#types-of-regression-analysis","title":"Types of regression analysis\u00b6","text":"<p>Resource - minitab blog</p>"},{"location":"projects/stats/islr/03_linear_regression/#predicting-categorical-response-variable","title":"Predicting categorical response variable:\u00b6","text":"<ul> <li>binary logistic regression: response falls into one of two categories (Yes or No)</li> <li>ordinal logistic regression: response is in categories (discrete) and can be ordered from least to greatest</li> <li>nominal logistic regression: response is in categories (discrete) and not follow any order.</li> </ul>"},{"location":"projects/stats/islr/03_linear_regression/#regressing-a-continuous-response","title":"Regressing a continuous response:\u00b6","text":"<p>The response is on a continuous scale.</p> <ul> <li>OLS - ordinary least squares regression: Linear and multiple regressions. One or more continuous predictors and a continuous response. Note: here, the predictors are also continuous, not categorical.</li> <li>Stepwise regession: This is a technique to find influential predictors, not a type of regression. Methods include forward selection, backward elimination.</li> <li>Nonlinear regression: use a non-linear function to model a set of continuous predictors and predict continuous response variable.</li> </ul>"},{"location":"projects/stats/islr/03_linear_regression/#linear-vs-non-linear-regression-vs-transformation","title":"Linear vs Non linear regression vs Transformation\u00b6","text":"<p>Contrary to popular belief, linear regression can produce curved fits! An equation is considered non linear when the predictor variables are multiplicative rather than additive. Using log, quadratic, cubic functions, you can produce a curved linear fit.</p> <p>In general, you choose the order of the equation based on number of curves you need in the fit.</p> <p></p>"},{"location":"projects/stats/islr/03_linear_regression/#reciprocal-transformation","title":"Reciprocal transformation\u00b6","text":"<p>As X increases, if your data (Y) descends to floor or ascends to a ceiling and flat lines, then the effect of X flattens out as it increases. In these cases, you can do a reciprocal of X and fit against it. Sometimes a quadratic fit of reciprocal of X would be a good fit.</p>"},{"location":"projects/stats/islr/03_linear_regression/#log-transformation","title":"Log transformation\u00b6","text":"<p>Log transform can rescue a model from non-linear to linear territory. When you take log on both sides of equation, multiplication signs become additions and exponential signs become multiplications.</p> <p>For instance: $$ Y = e^{B_{0}}X_{1}^{B_{1}}X_{2}^{B_{2}} $$ transforms to below when you take log on both sides (double-log form): $$ Ln Y = B_{0} + B_{1}lnX_{1} + B_{2}lnX_{2} $$</p>"},{"location":"projects/stats/islr/03_linear_regression/#non-linear-models","title":"Non-linear models\u00b6","text":"<p>Non linear models have predictors that in product with each other. In general, nonlinear models can take any number of formats. The trick is to finding one that will fit the data. Some limits of non-linear models</p> <ul> <li>lack of interpretability. You cannot clearly state the influence each predictors have on the phenomena</li> <li>You cannot find <code>p-values</code> and <code>CI</code> for the coefficients.</li> <li>You don't have $R^2$ statistic to evaluate the goodness of fit.</li> </ul>"},{"location":"projects/thermal/","title":"Thermal Remote Sensing","text":"<p>coming soon...</p>"},{"location":"talks/","title":"Atma's talks","text":""},{"location":"talks/#talks-in-public-conferences","title":"Talks in public conferences","text":"<ul> <li> <p>2019 - Explore How to Use ArcGIS and Jupyter for Geospatial Data Science </p> <ul> <li></li> </ul> </li> <li> <p>2019 FOSS4GNA</p> <ul> <li>Let's take the machines house hunting</li> </ul> </li> <li> <p>2018 Portland GeoDev Meetup</p> <ul> <li></li> </ul> </li> </ul>"},{"location":"talks/#talks-at-esri-sponsored-events","title":"Talks at Esri sponsored events","text":""},{"location":"talks/#2021-esri-developer-summit","title":"2021 Esri Developer Summit","text":"<ul> <li>ArcGIS Pro: Your spatial data science workstation</li> <li>Deploying apps and services with ArcGIS API for Python</li> </ul>"},{"location":"talks/#2020-esri-developer-summit","title":"2020 Esri Developer Summit","text":"<ul> <li>Imagery and raster analysis on your Web GIS</li> <li>Spatial data science in ArcGIS - a Tour</li> <li>ArcGIS API for Python for analysts and data scientists</li> </ul>"},{"location":"talks/#spring-2020-spatial-data-science-mooc","title":"Spring 2020 Spatial Data Science MOOC","text":""},{"location":"talks/#feb-2020-python-libraries-for-spatial-data-science","title":"Feb 2020 Python libraries for Spatial Data Science","text":""},{"location":"talks/#2019-esri-developer-summit","title":"2019 Esri Developer Summit","text":"<ul> <li>Python across the ArcGIS platform</li> <li>Imagery in ArcGIS</li> <li>Spatial Data Science with ArcGIS</li> </ul>"},{"location":"talks/#2018-esri-partner-conference-and-developer-summit","title":"2018 Esri Partner conference and Developer Summit","text":"<ul> <li>Plenary: Automation and analytics in the ArcGIS Platform with Python</li> <li>ArcGIS API for Python for developers, administrators and data scientists</li> <li>Plenary: Automation in the ArcGIS platform</li> <li>ArcGIS Python API: Advanced Scripting</li> <li>Mapping visualization and analysis with ArcGIS API for Python</li> </ul>"},{"location":"talks/#2017-esri-user-conference-talks","title":"2017 Esri User Conference talks","text":"<ul> <li>Introduction to scripting your WebGIS with ArcGIS API for Python</li> <li>Advanced scripting with ArcGIS API for Python</li> <li>ArcGIS API for Python for administrators and content publishers</li> <li>ArcGIS Python API for GIS Analysts and Data Scientists</li> <li>ArcGIS Python API: Introduction to Scripting your Web GIS</li> <li>Cloning Your Portal Users, Groups and Content using ArcGIS API for Python</li> <li>Mapping, Visualization, and Analysis Using ArcGIS API for Python</li> </ul>"},{"location":"talks/#2017-geodev-webinar-python","title":"2017 GeoDev Webinar - Python","text":"<ul> <li>Explore the power of the ArcGIS API for Python</li> </ul>"},{"location":"talks/#2017-esri-developer-summit-talks","title":"2017 Esri Developer Summit talks","text":"<ul> <li>A 2 day Dev Summit precon workshop on ArcGIS API for Python</li> <li>Introduction to scripting your WebGIS with ArcGIS API for Python</li> <li>Advanced scripting with ArcGIS API for Python</li> </ul>"},{"location":"talks/#2015-esri-user-conference-talks","title":"2015 Esri User Conference talks","text":"<ul> <li>Publishing GIS Services to ArcGIS for Server</li> </ul>"},{"location":"talks/2015-esri-uc/","title":"2015 Esri User Conference","text":""},{"location":"talks/2015-esri-uc/#publishing-gis-services-to-arcgis-for-server","title":"Publishing GIS Services to ArcGIS for Server","text":"<p> Get the slide deck here: Esri proceedings</p>"},{"location":"talks/2017-esri-devsummit/","title":"2017 Esri Developer Summit","text":""},{"location":"talks/2017-esri-devsummit/#2017-dev-summit-precon-workshop","title":"2017 Dev Summit precon workshop","text":"<p>This was a 2 day hands-on workshop that I ran. Find the tutorial notebooks here</p>"},{"location":"talks/2017-esri-devsummit/#introduction-to-scripting-your-webgis-with-arcgis-api-for-python","title":"Introduction to scripting your WebGIS with ArcGIS API for Python","text":""},{"location":"talks/2017-esri-devsummit/#advanced-scripting-with-arcgis-api-for-python","title":"Advanced scripting with ArcGIS API for Python","text":""},{"location":"talks/2017-esri-uc/","title":"2017 Esri User Conference talks","text":""},{"location":"talks/2017-esri-uc/#1-introduction-to-scripting-your-webgis-with-arcgis-api-for-python","title":"1. Introduction to scripting your WebGIS with ArcGIS API for Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-esri-uc/#2-advanced-scripting-with-arcgis-api-for-python","title":"2. Advanced scripting with ArcGIS API for Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-esri-uc/#3-arcgis-api-for-python-for-administrators-and-content-publishers","title":"3. ArcGIS API for Python for administrators and content publishers","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-esri-uc/#4-arcgis-python-api-for-gis-analysts-and-data-scientists","title":"4. ArcGIS Python API for GIS Analysts and Data Scientists","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-esri-uc/#5-arcgis-python-api-introduction-to-scripting-your-web-gis","title":"5. ArcGIS Python API: Introduction to Scripting your Web GIS","text":"<p>Get your notebooks from here</p>"},{"location":"talks/2017-esri-uc/#6-cloning-your-portal-users-groups-and-content-using-arcgis-api-for-python","title":"6. Cloning Your Portal Users, Groups and Content using ArcGIS API for Python","text":"<p>Get your notebooks from here</p>"},{"location":"talks/2017-esri-uc/#7-mapping-visualization-and-analysis-using-arcgis-api-for-python","title":"7. Mapping, Visualization, and Analysis Using ArcGIS API for Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-geodev-webinar/","title":"2017 Esri GeoDev webinar","text":""},{"location":"talks/2018-esri-devsummit/","title":"2018 Esri Partner conference and Developer Summit","text":""},{"location":"talks/2018-esri-devsummit/#2018-esri-partner-conference","title":"2018 Esri Partner conference","text":""},{"location":"talks/2018-esri-devsummit/#plenary-automation-and-analytics-in-the-arcgis-platform-with-python","title":"Plenary: Automation and analytics in the ArcGIS Platform with Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2018-esri-devsummit/#arcgis-api-for-python-for-developers-administrators-and-data-scientists","title":"ArcGIS API for Python for developers, administrators and data scientists","text":"<p>This was a closed session. The slide deck and notebooks are not made public.</p>"},{"location":"talks/2018-esri-devsummit/#2018-esri-developer-summit","title":"2018 Esri Developer summit","text":""},{"location":"talks/2018-esri-devsummit/#plenary-automation-in-the-arcgis-platform","title":"Plenary: Automation in the ArcGIS platform","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2018-esri-devsummit/#arcgis-python-api-advanced-scripting","title":"ArcGIS Python API: Advanced Scripting","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2018-esri-devsummit/#mapping-visualization-and-analysis-with-arcgis-api-for-python","title":"Mapping visualization and analysis with ArcGIS API for Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2018-portland-geodev-meetup/","title":"2018 Portland GeoDev Meetup","text":"<p>House hunting the data scientist way </p> <p>Buying a house is a huge financial and personal undertaking for most people. Whether we realize or not, a lot of decisions we make are heavily influenced by the location of the houses. In this talk, I show how Python's data analysis and geospatial analysis packages can be used to analyze the whole gamut of available listings in a market, evaluate and score properties based on various attribute and spatial parameters and arrive at a shortlist. I extend by showing how this process can be used to build a machine learning model that will understand our preferences and continue to learn as more data is fed. I conclude with ideas for future work and how rest of the industry is progressing in this field.</p> <p>This talk was presented at a Python MeetUp. Find the link here. This talk was published as a blog on Medium and ArcUser magazine. </p>"},{"location":"talks/2018-portland-geodev-meetup/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>If you are interested in the technical details of this study, you can view the notebooks below:</p> <ul> <li>Cleaning data</li> <li>Exploratory data analysis</li> <li>Feature engineering - neighboring facilities</li> <li>Feature engineering - batch</li> <li>Ranking properties</li> <li>Building a recommendation engine</li> </ul>"},{"location":"talks/2018-portland-geodev-meetup/#slides","title":"Slides","text":""},{"location":"talks/2018-portland-geodev-meetup/#talk-screencast","title":"Talk screencast:","text":""},{"location":"talks/2019-dirmag-webinar/","title":"Explore How to Use ArcGIS and Jupyter for Geospatial Data Science","text":"<p>  Watch the talk here: https://www.directionsmag.com/webinar/9311</p>"},{"location":"talks/2019-esri-devsummit/","title":"2019 Esri Developer Summit talks","text":""},{"location":"talks/2019-esri-devsummit/#python-across-the-arcgis-platform","title":"Python across the ArcGIS Platform","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2019-esri-devsummit/#imagery-in-arcgis","title":"Imagery in ArcGIS","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2019-esri-devsummit/#spatial-data-science-with-arcgis","title":"Spatial Data Science with ArcGIS","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2019-foss4gna/","title":"Free &amp; Open Source Software 4 Geospatial - North America (FOSS4G-NA), 2019","text":"<p>Visit the program: 2019 FOSS4GNA</p> <p>Lookup the talk: Let's take the machines house hunting</p>"},{"location":"talks/2020-esri-devsummit/","title":"2020 Esri Developer Summit talks","text":"<p>This was in March 2020, the beginning of the pandemic. All speakers had to return back home and cast their talks live from their home offices. We were all learning the ropes of remote work and remote conferences back then.</p>"},{"location":"talks/2020-esri-devsummit/#imagery-and-raster-analysis-on-your-web-gis","title":"Imagery and Raster Analysis on your Web GIS","text":""},{"location":"talks/2020-esri-devsummit/#spatial-data-science-with-arcgis","title":"Spatial Data Science with ArcGIS","text":""},{"location":"talks/2020-esri-devsummit/#arcgis-api-for-python-for-data-scientists-and-analysts","title":"ArcGIS API for Python for Data Scientists and Analysts","text":""},{"location":"talks/2020-esri-seminar/","title":"Python libraries for Spatial Data Science","text":""},{"location":"talks/2020-esri-seminar/#esri-live-training-seminar-feb-2020","title":"Esri, Live Training Seminar, Feb 2020","text":"<p>Register for the seminar here: https://www.esri.com/training/catalog/5e2750afea39935a53625340/python-libraries-for-spatial-data-science/</p>"},{"location":"talks/2020-spatial-ds-mooc/","title":"Spatial Data Science, The New Frontier in Analytics, Esri, 2020","text":"<p>I was lucky to design, review and teach a few chapters in this highly successful and popular MOOC from Esri.</p> <p></p> <p>Visit the course page: https://www.esri.com/training/catalog/5d76dcf7e9ccda09bef61294/</p>"},{"location":"talks/2021-esri-devsummit/","title":"2021 Esri Developer Summit","text":""},{"location":"talks/2021-esri-devsummit/#speaker-profile","title":"Speaker profile:","text":"<p>Atma Mani, Esri</p>"},{"location":"talks/2021-esri-devsummit/#arcgis-pro-your-spatial-data-science-workstation","title":"ArcGIS Pro: Your Spatial Data Science Workstation","text":"<p>If the video does not play, click here</p> <p>Slide deck for this talk can be found here</p>"},{"location":"talks/2021-esri-devsummit/#deploying-apps-and-services-with-arcgis-api-for-python","title":"Deploying Apps and Services with ArcGIS API for Python","text":"<p>Slide deck of this talk can be found here</p>"},{"location":"talks/2024-annauniv-geoinnovation-challenge/","title":"2024 Anna University Geo Innovation Challenge - Invited speaker series","text":""},{"location":"talks/2024-annauniv-geoinnovation-challenge/#talk-disaster-resilience-forecasting-business-interruptions-in-the-face-of-climate-change-using-meteorological-big-data","title":"Talk: Disaster Resilience: Forecasting business interruptions in the face of climate change using meteorological big-data","text":"<p>Talk flyer: </p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/","title":"Clean housing data","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import pandas as pd import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[2]: Copied! <pre>csv1_path = 'resources/file1_2018-09-20-14-42-11_4k.csv'\ncsv2_path = 'resources/file2_2018-09-20-15-04-20_g.csv'\ncsv3_path = 'resources/file3_2018-09-20-16-02-01_b.csv'\n\nprop_df1 = pd.read_csv(csv1_path)\nprop_df2 = pd.read_csv(csv2_path)\nprop_df3 = pd.read_csv(csv3_path)\nprop_df1.head(3)\n</pre> csv1_path = 'resources/file1_2018-09-20-14-42-11_4k.csv' csv2_path = 'resources/file2_2018-09-20-15-04-20_g.csv' csv3_path = 'resources/file3_2018-09-20-16-02-01_b.csv'  prop_df1 = pd.read_csv(csv1_path) prop_df2 = pd.read_csv(csv2_path) prop_df3 = pd.read_csv(csv3_path) prop_df1.head(3) Out[2]: SALE TYPE SOLD DATE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS ... STATUS NEXT OPEN HOUSE START TIME NEXT OPEN HOUSE END TIME URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING) SOURCE MLS# FAVORITE INTERESTED LATITUDE LONGITUDE 0 MLS Listing NaN Single Family Residential 3445 NE Marine Dr Unit BH04 Portland OR 97211.0 27500.0 0.0 1.0 ... Active NaN NaN http://www.redfin.com/OR/Portland/3445-NE-Mari... RMLS 18567126 N Y 45.600583 -122.628508 1 MLS Listing NaN Vacant Land NW Thurman St Portland OR 97210.0 30000.0 NaN NaN ... Active NaN NaN http://www.redfin.com/OR/Portland/NW-Thurman-S... RMLS 18118897 N Y 45.537928 -122.718082 2 MLS Listing NaN Vacant Land NE Rocky Butte Rd Portland OR 97220.0 34777.0 NaN NaN ... Active NaN NaN http://www.redfin.com/OR/Portland/NE-Rocky-But... RMLS 18454531 N Y 45.542424 -122.565746 <p>3 rows \u00d7 27 columns</p> In\u00a0[3]: Copied! <pre>(prop_df1.shape, prop_df2.shape, prop_df3.shape)\n</pre> (prop_df1.shape, prop_df2.shape, prop_df3.shape) Out[3]: <pre>((3716, 27), (387, 27), (133, 27))</pre> In\u00a0[4]: Copied! <pre>prop_df = prop_df1.append(prop_df2)\nprop_df = prop_df.append(prop_df3)\nprop_df.shape\n</pre> prop_df = prop_df1.append(prop_df2) prop_df = prop_df.append(prop_df3) prop_df.shape Out[4]: <pre>(4236, 27)</pre> In\u00a0[5]: Copied! <pre>prop_dup_index = prop_df.duplicated(['MLS#'])\nprop_dup = prop_df[prop_dup_index]\nprop_dup.shape\n</pre> prop_dup_index = prop_df.duplicated(['MLS#']) prop_dup = prop_df[prop_dup_index] prop_dup.shape Out[5]: <pre>(209, 27)</pre> In\u00a0[6]: Copied! <pre>prop_df.drop_duplicates(subset=['MLS#'], inplace=True)\nprop_df.shape\n</pre> prop_df.drop_duplicates(subset=['MLS#'], inplace=True) prop_df.shape Out[6]: <pre>(4027, 27)</pre> In\u00a0[7]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[7]: <pre>Index(['SALE TYPE', 'SOLD DATE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH', 'STATUS',\n       'NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME',\n       'URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)',\n       'SOURCE', 'MLS#', 'FAVORITE', 'INTERESTED', 'LATITUDE', 'LONGITUDE'],\n      dtype='object')</pre> In\u00a0[8]: Copied! <pre>prop_df.rename(index=str, columns={'$/SQUARE FEET':'PRICE PER SQFT',\n                                  'HOA/MONTH':'HOA PER MONTH',\n                                  'URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)':'URL',\n                                  'MLS#':'MLS'}, inplace=True)\nprop_df.columns\n</pre> prop_df.rename(index=str, columns={'$/SQUARE FEET':'PRICE PER SQFT',                                   'HOA/MONTH':'HOA PER MONTH',                                   'URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)':'URL',                                   'MLS#':'MLS'}, inplace=True) prop_df.columns Out[8]: <pre>Index(['SALE TYPE', 'SOLD DATE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME',\n       'URL', 'SOURCE', 'MLS', 'FAVORITE', 'INTERESTED', 'LATITUDE',\n       'LONGITUDE'],\n      dtype='object')</pre> In\u00a0[9]: Copied! <pre>prop_df.drop(columns=['SOLD DATE','NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME',\n                     'FAVORITE', 'INTERESTED'], inplace=True)\n</pre> prop_df.drop(columns=['SOLD DATE','NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME',                      'FAVORITE', 'INTERESTED'], inplace=True) In\u00a0[10]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[10]: <pre>Index(['SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE', 'ZIP',\n       'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE'],\n      dtype='object')</pre> In\u00a0[13]: Copied! <pre># explore distribution of numeric columns\nax_list = prop_df.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> # explore distribution of numeric columns ax_list = prop_df.hist(bins=25, layout=(4,4), figsize=(15,15)) In\u00a0[11]: Copied! <pre># drop rows with missing values in critical columns\nprop_df_nona = prop_df.dropna(axis=0, how='any', # if any of these cols are empty, remove row\n                              subset=['BEDS','BATHS', 'PRICE', 'YEAR BUILT', 'LATITUDE','LONGITUDE'])\nprop_df_nona.shape\n</pre> # drop rows with missing values in critical columns prop_df_nona = prop_df.dropna(axis=0, how='any', # if any of these cols are empty, remove row                               subset=['BEDS','BATHS', 'PRICE', 'YEAR BUILT', 'LATITUDE','LONGITUDE']) prop_df_nona.shape Out[11]: <pre>(3653, 22)</pre> <p>Let us impute for missing values using different strategies for different columns.</p> In\u00a0[23]: Copied! <pre>prop_df_nona['HOA PER MONTH'].fillna(value=0, inplace=True)\nprop_df_nona['LOT SIZE'].fillna(value=prop_df_nona['LOT SIZE'].median(), inplace=True)\nprop_df_nona['PRICE PER SQFT'].fillna(value=prop_df_nona['PRICE PER SQFT'].median(), inplace=True)\nprop_df_nona['SQUARE FEET'].fillna(value=prop_df_nona['SQUARE FEET'].median(), inplace=True)\nprop_df_nona['YEAR BUILT'].fillna(value=prop_df_nona['YEAR BUILT'].mode(), inplace=True)\nprop_df_nona['ZIP'].fillna(value=prop_df_nona['SQUARE FEET'].mode(), inplace=True)\n</pre> prop_df_nona['HOA PER MONTH'].fillna(value=0, inplace=True) prop_df_nona['LOT SIZE'].fillna(value=prop_df_nona['LOT SIZE'].median(), inplace=True) prop_df_nona['PRICE PER SQFT'].fillna(value=prop_df_nona['PRICE PER SQFT'].median(), inplace=True) prop_df_nona['SQUARE FEET'].fillna(value=prop_df_nona['SQUARE FEET'].median(), inplace=True) prop_df_nona['YEAR BUILT'].fillna(value=prop_df_nona['YEAR BUILT'].mode(), inplace=True) prop_df_nona['ZIP'].fillna(value=prop_df_nona['SQUARE FEET'].mode(), inplace=True) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/pandas/core/generic.py:5434: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._update_inplace(new_data)\n</pre> In\u00a0[25]: Copied! <pre># explore distribution of numeric columns\nax_list = prop_df_nona.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> # explore distribution of numeric columns ax_list = prop_df_nona.hist(bins=25, layout=(4,4), figsize=(15,15)) <p>As can be seen from histogram, some numeric columns are heavily sqewed by outliers. Let us pull up statistics for each of these columns</p> In\u00a0[26]: Copied! <pre>prop_df_nona.describe().round(3)\n</pre> prop_df_nona.describe().round(3) Out[26]: ZIP PRICE BEDS BATHS SQUARE FEET LOT SIZE YEAR BUILT DAYS ON MARKET PRICE PER SQFT HOA PER MONTH LATITUDE LONGITUDE count 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 mean 97210.091 609851.635 3.316 2.428 2291.079 13448.155 1973.970 62.268 281.446 105.865 45.513 -122.648 std 93.873 458104.003 1.762 1.340 1342.049 72390.700 36.279 72.488 118.235 223.111 0.046 0.107 min 97002.000 35000.000 0.000 0.500 212.000 25.000 1878.000 1.000 44.000 0.000 45.382 -122.902 25% 97206.000 359900.000 2.000 2.000 1400.000 5227.000 1948.000 15.000 206.000 0.000 45.483 -122.712 50% 97217.000 495000.000 3.000 2.500 2062.000 6969.000 1980.000 42.000 251.000 0.000 45.516 -122.661 75% 97229.000 699934.000 4.000 3.000 2862.000 8712.000 2006.000 84.000 321.000 74.000 45.541 -122.584 max 98664.000 8650000.000 43.000 21.000 14500.000 3167247.000 2019.000 1080.000 1505.000 2091.000 45.703 -122.306 In\u00a0[27]: Copied! <pre>def six_sigma_filter(df, column):\n    sigma = df[column].std()\n    mu = df[column].mean()\n    three_sigma = [mu-(3*sigma), mu+(3*sigma)]\n    print(\"Column:{}, Mean:{}, Sigma:{}, 3sigma_range: {}:{}\".format(column,mu.round(3),\n                                                                       sigma.round(3),\n                                                                       three_sigma[0].round(2),\n                                                                       three_sigma[1].round(2)))\n    \n    # filter\n    df_to_keep = df[(df[column] &gt; three_sigma[0]) &amp; (df[column] &lt; three_sigma[1])]\n    \n    # prints\n    num_rows_dropped = prop_df.shape[0] - df_to_keep.shape[0]\n    print(\"Number of rows dropped: \" + str(num_rows_dropped))\n    \n    return df_to_keep\n</pre> def six_sigma_filter(df, column):     sigma = df[column].std()     mu = df[column].mean()     three_sigma = [mu-(3*sigma), mu+(3*sigma)]     print(\"Column:{}, Mean:{}, Sigma:{}, 3sigma_range: {}:{}\".format(column,mu.round(3),                                                                        sigma.round(3),                                                                        three_sigma[0].round(2),                                                                        three_sigma[1].round(2)))          # filter     df_to_keep = df[(df[column] &gt; three_sigma[0]) &amp; (df[column] &lt; three_sigma[1])]          # prints     num_rows_dropped = prop_df.shape[0] - df_to_keep.shape[0]     print(\"Number of rows dropped: \" + str(num_rows_dropped))          return df_to_keep In\u00a0[28]: Copied! <pre>def iqr_filter(df, column):\n    med = df[column].median()\n    p_25 = df[column].quantile(q=0.25)\n    p_75 = df[column].quantile(q=0.75)\n    \n    # find valid range\n    iqr_range = [med-(2*p_25), med+(2*p_75)]\n    print(\"Column: {}, Median:{}, 25%:{}, 75%:{}, IQR:{}:{}\".format(column,med,\n                                                                    p_25,p_75,\n                                                                    iqr_range[0].round(2),\n                                                                    iqr_range[1].round(2)))\n    \n    # filter\n    df_to_keep = df[(df[column] &gt; iqr_range[0]) &amp; (df[column] &lt; iqr_range[1])]\n    \n    #prints\n    num_rows_dropped = prop_df.shape[0] - df_to_keep.shape[0]\n    print(\"Number of rows dropped: \" + str(num_rows_dropped))\n    \n    return df_to_keep\n</pre> def iqr_filter(df, column):     med = df[column].median()     p_25 = df[column].quantile(q=0.25)     p_75 = df[column].quantile(q=0.75)          # find valid range     iqr_range = [med-(2*p_25), med+(2*p_75)]     print(\"Column: {}, Median:{}, 25%:{}, 75%:{}, IQR:{}:{}\".format(column,med,                                                                     p_25,p_75,                                                                     iqr_range[0].round(2),                                                                     iqr_range[1].round(2)))          # filter     df_to_keep = df[(df[column] &gt; iqr_range[0]) &amp; (df[column] &lt; iqr_range[1])]          #prints     num_rows_dropped = prop_df.shape[0] - df_to_keep.shape[0]     print(\"Number of rows dropped: \" + str(num_rows_dropped))          return df_to_keep In\u00a0[37]: Copied! <pre>prop_df2 = six_sigma_filter(prop_df_nona, 'BATHS')\n</pre> prop_df2 = six_sigma_filter(prop_df_nona, 'BATHS') <pre>Column:BATHS, Mean:2.428, Sigma:1.34, 3sigma_range: -1.59:6.45\nNumber of rows dropped: 422\n</pre> In\u00a0[38]: Copied! <pre>prop_df2_iqr = iqr_filter(prop_df_nona, 'BATHS')\n</pre> prop_df2_iqr = iqr_filter(prop_df_nona, 'BATHS') <pre>Column: BATHS, Median:2.5, 25%:2.0, 75%:3.0, IQR:-1.5:8.5\nNumber of rows dropped: 396\n</pre> In\u00a0[39]: Copied! <pre>prop_df4 = six_sigma_filter(prop_df2, 'BEDS')\n</pre> prop_df4 = six_sigma_filter(prop_df2, 'BEDS') <pre>Column:BEDS, Mean:3.206, Sigma:1.263, 3sigma_range: -0.58:7.0\nNumber of rows dropped: 459\n</pre> In\u00a0[40]: Copied! <pre>prop_df4_iqr = iqr_filter(prop_df2_iqr, 'BEDS')\n</pre> prop_df4_iqr = iqr_filter(prop_df2_iqr, 'BEDS') <pre>Column: BEDS, Median:3.0, 25%:2.0, 75%:4.0, IQR:-1.0:11.0\nNumber of rows dropped: 403\n</pre> In\u00a0[42]: Copied! <pre>(prop_df4.shape, prop_df4_iqr.shape)\n</pre> (prop_df4.shape, prop_df4_iqr.shape) Out[42]: <pre>((3539, 22), (3624, 22))</pre> In\u00a0[43]: Copied! <pre>ax_list = prop_df4.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> ax_list = prop_df4.hist(bins=25, layout=(4,4), figsize=(15,15)) In\u00a0[44]: Copied! <pre>ax_list = prop_df4_iqr.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> ax_list = prop_df4_iqr.hist(bins=25, layout=(4,4), figsize=(15,15)) <p>The <code>IQR</code> filter yields a better result in our case as the resulting histograms of numeric columns show a nice normal distribution. We proceed with this dataset and write that to disk.</p> In\u00a0[45]: Copied! <pre>prop_df4_iqr.to_csv('resources/houses_for_sale_filtered.csv')\n</pre> prop_df4_iqr.to_csv('resources/houses_for_sale_filtered.csv')"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#clean-housing-data","title":"Clean housing data\u00b6","text":"<p>In this notebook, we download housing data from a popular MLS website, merge, clean, filter and prepare it for subsequent spatial and statistical analysis.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#read-data-from-all-csv-files","title":"Read data from all CSV files\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#merge-all-csv-into-a-single-sheet","title":"Merge all CSV into a single sheet\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#find-if-there-are-duplicates","title":"Find if there are duplicates\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#clean-column-names","title":"Clean column names\u00b6","text":"<p>Column names contain illegal characters. Let us rename them</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#drop-unnecessary-columns","title":"Drop unnecessary columns\u00b6","text":"<p>While it is beneficial to have a lot of features (columns) in our dataset, it is also important to drop those that are highly correlated to an existing field or a derivative of one or fields that we will never use. In the following, we drop fields that we will never use in this analysis.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#find-and-fill-missing-values","title":"Find and fill missing values\u00b6","text":"<p>Missing values in certain columns are critical - <code>beds</code>, <code>bath</code>, <code>price</code>, <code>latitude</code>, <code>longitude</code>. We will drop these rows. Missing values in remaining columns are not so critical, we will fill them with <code>0</code> or average values.</p> <p>Before we fill, let us explore the histograms of numerical columns. With pandas, this can be accomplished with a single command as shown below:</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#explore-distribution-of-numeric-columns","title":"Explore distribution of numeric columns\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#build-statistical-filters-to-remove-outliers","title":"Build statistical filters to remove outliers\u00b6","text":"<p>In this segment, we build $6\\sigma$ and Inter Quartile Range (<code>IQR</code>) filters to remove outliers from our dataset.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#6-sigma-filter","title":"6 sigma filter\u00b6","text":"<p> Statistically, about <code>99.5%</code> of data falls within $\\pm 3 \\sigma$ from $\\mu$ (mean). Thus, for fairly normally distributed data, $6\\sigma$ filter yields a good resultant dataset.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#iqr-filter","title":"IQR filter\u00b6","text":"<p> Howerver, certain columns such as <code>BEDS</code>, <code>LOT SIZE</code> are heavily sqewed. For such, the Inter Quartile Range filter (which uses median as a measure of centrality unlike 6sigma which uses mean) yields a robust filter as it is unaffected by outliers like the 6 sigma filter.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#filter-columns-using-both-filters-and-compare","title":"Filter columns using both filters and compare\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#write-cleaned-appended-table-to-disk","title":"Write cleaned, appended table to disk\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/","title":"Portland housing data - exploratory data analysis","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom arcgis.gis import GIS\nfrom arcgis.features import GeoAccessor, GeoSeriesAccessor\ngis = GIS()\n</pre> import pandas as pd import matplotlib.pyplot as plt %matplotlib inline  from arcgis.gis import GIS from arcgis.features import GeoAccessor, GeoSeriesAccessor gis = GIS() In\u00a0[2]: Copied! <pre>csv_path = 'resources/houses_for_sale_filtered.csv'\nprop_df = pd.read_csv(csv_path)\nprop_df.head(3)\n</pre> csv_path = 'resources/houses_for_sale_filtered.csv' prop_df = pd.read_csv(csv_path) prop_df.head(3) Out[2]: Unnamed: 0 SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS ... YEAR BUILT DAYS ON MARKET PRICE PER SQFT HOA PER MONTH STATUS URL SOURCE MLS LATITUDE LONGITUDE 0 3 MLS Listing Mobile/Manufactured Home 6112 SE Clatsop St Portland OR 97206.0 35000.0 2.0 1.0 ... 1981.0 93.0 44.0 0.0 Active http://www.redfin.com/OR/Portland/6112-SE-Clat... RMLS 18331169 45.461282 -122.600153 1 8 For-Sale-by-Owner Listing Single Family Residential 6901 SE Oaks Park Way Slip 32 Portland OR 97202.0 60000.0 1.0 1.0 ... 1939.0 359.0 55.0 0.0 Active http://www.redfin.com/OR/Portland/6901-SE-Oaks... Fizber.com 4933081 45.474369 -122.662307 2 14 For-Sale-by-Owner Listing Mobile/Manufactured Home 11187 SW Royal Villa Dr Portland OR 97224.0 69950.0 2.0 2.0 ... 1976.0 44.0 47.0 0.0 Active http://www.redfin.com/OR/Portland/11187-SW-Roy... Fizber.com 4974567 45.398486 -122.796175 <p>3 rows \u00d7 23 columns</p> In\u00a0[7]: Copied! <pre>prop_df.shape\n</pre> prop_df.shape Out[7]: <pre>(3624, 23)</pre> In\u00a0[8]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[8]: <pre>Index(['Unnamed: 0', 'SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE'],\n      dtype='object')</pre> <p>Drop redundant columns</p> In\u00a0[3]: Copied! <pre>try:\n    prop_df.drop(columns=['Unnamed: 0'], inplace=True)\n    prop_df.drop(columns=['Unnamed: 0.1'], inplace=True)\nexcept:\n    pass\n</pre> try:     prop_df.drop(columns=['Unnamed: 0'], inplace=True)     prop_df.drop(columns=['Unnamed: 0.1'], inplace=True) except:     pass In\u00a0[4]: Copied! <pre>prop_sdf = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE')\ntype(prop_sdf)\n</pre> prop_sdf = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE') type(prop_sdf) Out[4]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_map = gis.map('Portland, OR')\npdx_map.basemap = 'streets'\npdx_map\n</pre> pdx_map = gis.map('Portland, OR') pdx_map.basemap = 'streets' pdx_map <p></p> In\u00a0[6]: Copied! <pre>prop_sdf.spatial.plot(map_widget=pdx_map)\n</pre> prop_sdf.spatial.plot(map_widget=pdx_map) Out[6]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_density_map = gis.map('Portland, OR')\npdx_density_map.basemap='gray'\npdx_density_map\n</pre> pdx_density_map = gis.map('Portland, OR') pdx_density_map.basemap='gray' pdx_density_map <p></p> In\u00a0[10]: Copied! <pre>prop_sdf.spatial.plot(map_widget=pdx_density_map, renderer_type='h')\n</pre> prop_sdf.spatial.plot(map_widget=pdx_density_map, renderer_type='h') Out[10]: <pre>True</pre> <p>There is a hotspot near downtown Portland. There are few other isolated hotspots around the downtown as well. These could be pockets of communities that are spread around a major city.</p> In\u00a0[\u00a0]: Copied! <pre>pdx_age_map = gis.map(\"Portland, OR\")\npdx_age_map.basemap = 'gray-vector'\npdx_age_map\n</pre> pdx_age_map = gis.map(\"Portland, OR\") pdx_age_map.basemap = 'gray-vector' pdx_age_map <p></p> In\u00a0[12]: Copied! <pre>prop_sdf.spatial.plot(map_widget = pdx_age_map, \n                      renderer_type='c', # for classs breaks renderer\n                     method='esriClassifyNaturalBreaks',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='YEAR BUILT',\n                     cmap='Blues',  # matplotlib color map\n                     alpha=0.7,\n                     outline_color=[0,0,0,0])\n</pre> prop_sdf.spatial.plot(map_widget = pdx_age_map,                        renderer_type='c', # for classs breaks renderer                      method='esriClassifyNaturalBreaks',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='YEAR BUILT',                      cmap='Blues',  # matplotlib color map                      alpha=0.7,                      outline_color=[0,0,0,0]) Out[12]: <pre>True</pre> <p>If you notice the syntax above, it mirrors the general syntax of plotting a DataFrame as a chart. It uses the same color map and symbols of <code>matplotlib</code>. Internally, the ArcGIS API for Python converts this syntax to ArcGIS symbology and renders it on a map.</p> <p>Since the colormap and symbols is that of <code>matplotlib</code>, you can collect the class breaks from the map above and use that to plot the same data as a bar chart as shown below. Combining the map and chart gives you a powerful way to interactively explore the spatial and statistical distribution of your dataset.</p> In\u00a0[13]: Copied! <pre>age_class_breaks = pdx_age_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos\n# print(len(age_class_breaks))\ncbs_list = []\ncmap_list = []\nfor cb in age_class_breaks:\n#     print(cb.description)  # print the class break labels\n    cbs_list.append(cb.classMaxValue)\n    cmap_list.append([x/255.0 for x in cb.symbol.color])\n    \n# build a histogram for the same class breaks\nn, bins, patches = plt.hist(prop_sdf['YEAR BUILT'], bins=cbs_list)\n\n# apply the same color for each class to match the map\nidx = 0\nfor c, p in zip(bins, patches):\n    plt.setp(p, 'facecolor', cmap_list[idx])\n    idx+=1\n\nplt.title('Histogram of YEAR BUILT column')\n</pre> age_class_breaks = pdx_age_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos # print(len(age_class_breaks)) cbs_list = [] cmap_list = [] for cb in age_class_breaks: #     print(cb.description)  # print the class break labels     cbs_list.append(cb.classMaxValue)     cmap_list.append([x/255.0 for x in cb.symbol.color])      # build a histogram for the same class breaks n, bins, patches = plt.hist(prop_sdf['YEAR BUILT'], bins=cbs_list)  # apply the same color for each class to match the map idx = 0 for c, p in zip(bins, patches):     plt.setp(p, 'facecolor', cmap_list[idx])     idx+=1  plt.title('Histogram of YEAR BUILT column') Out[13]: <pre>Text(0.5,1,'Histogram of YEAR BUILT column')</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_price_map = gis.map(\"Portland, OR\")\npdx_price_map.basemap = 'gray-vector'\npdx_price_map\n</pre> pdx_price_map = gis.map(\"Portland, OR\") pdx_price_map.basemap = 'gray-vector' pdx_price_map <p></p> In\u00a0[18]: Copied! <pre>prop_sdf.spatial.plot(map_widget = pdx_price_map, \n                      renderer_type='c', # for classs breaks renderer\n                     method='esriClassifyQuantile',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='PRICE',\n                     cmap='BuPu_r',  # matplotlib color map\n                     alpha=0.5,\n                     outline_color=[50,0,0,50], line_width=1)\n</pre> prop_sdf.spatial.plot(map_widget = pdx_price_map,                        renderer_type='c', # for classs breaks renderer                      method='esriClassifyQuantile',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='PRICE',                      cmap='BuPu_r',  # matplotlib color map                      alpha=0.5,                      outline_color=[50,0,0,50], line_width=1) Out[18]: <pre>True</pre> In\u00a0[19]: Copied! <pre>price_class_breaks = pdx_price_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos\n# print(len(age_class_breaks))\ncbs_list = []\ncmap_list = []\nfor cb in price_class_breaks:\n#     print(cb.description)  # print the class break labels\n    cbs_list.append(cb.classMaxValue)\n    cmap_list.append([x/255.0 for x in cb.symbol.color])\n    \n# build a histogram for the same class breaks\nn, bins, patches = plt.hist(prop_sdf['PRICE'], bins=cbs_list)\n\n# apply the same color for each class to match the map\nidx = 0\nfor c, p in zip(bins, patches):\n    plt.setp(p, 'facecolor', cmap_list[idx])\n    idx+=1\n\nplt.title('Histogram of PRICE column')\n</pre> price_class_breaks = pdx_price_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos # print(len(age_class_breaks)) cbs_list = [] cmap_list = [] for cb in price_class_breaks: #     print(cb.description)  # print the class break labels     cbs_list.append(cb.classMaxValue)     cmap_list.append([x/255.0 for x in cb.symbol.color])      # build a histogram for the same class breaks n, bins, patches = plt.hist(prop_sdf['PRICE'], bins=cbs_list)  # apply the same color for each class to match the map idx = 0 for c, p in zip(bins, patches):     plt.setp(p, 'facecolor', cmap_list[idx])     idx+=1  plt.title('Histogram of PRICE column') Out[19]: <pre>Text(0.5,1,'Histogram of PRICE column')</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_size_map = gis.map(\"Portland, OR\")\npdx_size_map.basemap = 'gray-vector'\npdx_size_map\n</pre> pdx_size_map = gis.map(\"Portland, OR\") pdx_size_map.basemap = 'gray-vector' pdx_size_map <p></p> In\u00a0[21]: Copied! <pre>prop_sdf.spatial.plot(map_widget = pdx_size_map, \n                      renderer_type='c', # for classs breaks renderer\n                     method='esriClassifyNaturalBreaks',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='SQUARE FEET',\n                     cmap='RdBu',  # matplotlib color map\n                     alpha=0.7,\n                     outline_color=[50,0,0,50], line_width=1)\n</pre> prop_sdf.spatial.plot(map_widget = pdx_size_map,                        renderer_type='c', # for classs breaks renderer                      method='esriClassifyNaturalBreaks',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='SQUARE FEET',                      cmap='RdBu',  # matplotlib color map                      alpha=0.7,                      outline_color=[50,0,0,50], line_width=1) Out[21]: <pre>True</pre> In\u00a0[22]: Copied! <pre>size_class_breaks = pdx_size_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos\n# print(len(age_class_breaks))\ncbs_list = []\ncmap_list = []\nfor cb in size_class_breaks:\n#     print(cb.description)  # print the class break labels\n    cbs_list.append(cb.classMaxValue)\n    cmap_list.append([x/255.0 for x in cb.symbol.color])\n    \n# build a histogram for the same class breaks\nn, bins, patches = plt.hist(prop_sdf['SQUARE FEET'], bins=cbs_list)\n\n# apply the same color for each class to match the map\nidx = 0\nfor c, p in zip(bins, patches):\n    plt.setp(p, 'facecolor', cmap_list[idx])\n    idx+=1\n\nplt.title('Histogram of SQUARE FEET column')\n</pre> size_class_breaks = pdx_size_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos # print(len(age_class_breaks)) cbs_list = [] cmap_list = [] for cb in size_class_breaks: #     print(cb.description)  # print the class break labels     cbs_list.append(cb.classMaxValue)     cmap_list.append([x/255.0 for x in cb.symbol.color])      # build a histogram for the same class breaks n, bins, patches = plt.hist(prop_sdf['SQUARE FEET'], bins=cbs_list)  # apply the same color for each class to match the map idx = 0 for c, p in zip(bins, patches):     plt.setp(p, 'facecolor', cmap_list[idx])     idx+=1  plt.title('Histogram of SQUARE FEET column') Out[22]: <pre>Text(0.5,1,'Histogram of SQUARE FEET column')</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_hoa_map = gis.map(\"Portland, OR\")\npdx_hoa_map.basemap = 'gray-vector'\npdx_hoa_map\n</pre> pdx_hoa_map = gis.map(\"Portland, OR\") pdx_hoa_map.basemap = 'gray-vector' pdx_hoa_map <p></p> In\u00a0[42]: Copied! <pre>#plot properties without HOA as hollow\n# prop_sdf_hoa_f = prop_df[prop_df['HOA PER MONTH']==0]\n# prop_sdf_hoa_f.spatial.plot(map_widget=pdx_hoa_map, symbol_type='simple',\n#                             symbol_style='+',outline_color='Blues',\n#                             marker_size=7)\n\nprop_sdf_hoa_2 = prop_df[prop_df['HOA PER MONTH']&gt;0]\n\nprop_sdf_hoa_2.spatial.plot(map_widget = pdx_hoa_map, \n                      renderer_type='c', # for classs breaks renderer\n                     method='esriClassifyQuantile',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='HOA PER MONTH',\n                     cmap='RdBu',  # matplotlib color map\n                     alpha=0.7,\n                     outline_color=[0,0,0,0], line_width=0)\n</pre> #plot properties without HOA as hollow # prop_sdf_hoa_f = prop_df[prop_df['HOA PER MONTH']==0] # prop_sdf_hoa_f.spatial.plot(map_widget=pdx_hoa_map, symbol_type='simple', #                             symbol_style='+',outline_color='Blues', #                             marker_size=7)  prop_sdf_hoa_2 = prop_df[prop_df['HOA PER MONTH']&gt;0]  prop_sdf_hoa_2.spatial.plot(map_widget = pdx_hoa_map,                        renderer_type='c', # for classs breaks renderer                      method='esriClassifyQuantile',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='HOA PER MONTH',                      cmap='RdBu',  # matplotlib color map                      alpha=0.7,                      outline_color=[0,0,0,0], line_width=0) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/arcgis/features/geo/_accessor.py:861: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._data[col] = self._data[col]\n/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/arcgis/features/geo/_accessor.py:1968: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._data['OBJECTID'] = list(range(1, self._data.shape[0] + 1))\n</pre> Out[42]: <pre>True</pre> In\u00a0[43]: Copied! <pre>hoa_class_breaks = pdx_hoa_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos\n# print(len(age_class_breaks))\ncbs_list = []\ncmap_list = []\nfor cb in hoa_class_breaks:\n#     print(cb.description)  # print the class break labels\n    cbs_list.append(cb.classMaxValue)\n    cmap_list.append([x/255.0 for x in cb.symbol.color])\n    \n# build a histogram for the same class breaks\nn, bins, patches = plt.hist(prop_sdf['HOA PER MONTH'], bins=cbs_list)\n\n# apply the same color for each class to match the map\nidx = 0\nfor c, p in zip(bins, patches):\n    plt.setp(p, 'facecolor', cmap_list[idx])\n    idx+=1\n\nplt.title('Histogram of HOA PER MONTH column')\n</pre> hoa_class_breaks = pdx_hoa_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos # print(len(age_class_breaks)) cbs_list = [] cmap_list = [] for cb in hoa_class_breaks: #     print(cb.description)  # print the class break labels     cbs_list.append(cb.classMaxValue)     cmap_list.append([x/255.0 for x in cb.symbol.color])      # build a histogram for the same class breaks n, bins, patches = plt.hist(prop_sdf['HOA PER MONTH'], bins=cbs_list)  # apply the same color for each class to match the map idx = 0 for c, p in zip(bins, patches):     plt.setp(p, 'facecolor', cmap_list[idx])     idx+=1  plt.title('Histogram of HOA PER MONTH column') Out[43]: <pre>Text(0.5,1,'Histogram of HOA PER MONTH column')</pre> <p></p> In\u00a0[46]: Copied! <pre>ax_list = prop_df.hist(bins=25, layout=(4,4), figsize=(15,12))\n</pre> ax_list = prop_df.hist(bins=25, layout=(4,4), figsize=(15,12)) <p>Explore the frequency of categorical columns</p> In\u00a0[47]: Copied! <pre>fig2, ax2 = plt.subplots(1,2, figsize=(10,5))\n\nprop_df['CITY'].value_counts().plot(kind='bar', ax=ax2[0], \n                                             title='City name frequency')\nax2[0].tick_params(labelrotation=45)\n\nprop_df['PROPERTY TYPE'].value_counts().plot(kind='bar', ax=ax2[1], \n                                             title='Property type frequency')\nax2[1].tick_params(labelrotation=45)\nplt.tight_layout()\n</pre> fig2, ax2 = plt.subplots(1,2, figsize=(10,5))  prop_df['CITY'].value_counts().plot(kind='bar', ax=ax2[0],                                               title='City name frequency') ax2[0].tick_params(labelrotation=45)  prop_df['PROPERTY TYPE'].value_counts().plot(kind='bar', ax=ax2[1],                                               title='Property type frequency') ax2[1].tick_params(labelrotation=45) plt.tight_layout() In\u00a0[48]: Copied! <pre>filtered_df = prop_sdf[(prop_df['BEDS']&gt;=2) &amp; \n                       (prop_df['BATHS']&gt;1)&amp; \n                       (prop_df['HOA PER MONTH']&lt;=200) &amp; \n                       (prop_df['YEAR BUILT']&gt;=2000) &amp; \n                       (prop_df['SQUARE FEET'] &gt; 2000) &amp; \n                       (prop_df['PRICE']&lt;=700000)]\nfiltered_df.shape\n</pre> filtered_df = prop_sdf[(prop_df['BEDS']&gt;=2) &amp;                         (prop_df['BATHS']&gt;1)&amp;                         (prop_df['HOA PER MONTH']&lt;=200) &amp;                         (prop_df['YEAR BUILT']&gt;=2000) &amp;                         (prop_df['SQUARE FEET'] &gt; 2000) &amp;                         (prop_df['PRICE']&lt;=700000)] filtered_df.shape Out[48]: <pre>(331, 23)</pre> In\u00a0[49]: Copied! <pre>(prop_sdf.shape, filtered_df.shape)\n</pre> (prop_sdf.shape, filtered_df.shape) Out[49]: <pre>((3624, 23), (331, 23))</pre> <p>From <code>3624</code> houses, we shortlisted <code>331</code> of them. Below, let us visualize the statistical distribution of this shortlist.</p> In\u00a0[50]: Copied! <pre>ax_list2 = filtered_df.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> ax_list2 = filtered_df.hist(bins=25, layout=(4,4), figsize=(15,15)) <p>From the histograms above, we notice most of the houses have <code>4</code> beds, while we requested for at least <code>2</code>. Majority of them are newly built and skewed toward upper end of the price spectrum.</p> In\u00a0[\u00a0]: Copied! <pre>pdx_filtered_map = gis.map(\"Portland, OR\")\npdx_filtered_map.basemap = 'gray-vector'\npdx_filtered_map\n</pre> pdx_filtered_map = gis.map(\"Portland, OR\") pdx_filtered_map.basemap = 'gray-vector' pdx_filtered_map <p></p> In\u00a0[\u00a0]: Copied! <pre>filtered_df.spatial.plot(map_widget=pdx_filtered_map, \n                         renderer_type='c',\n                         method='esriClassifyNaturalBreaks',  # classification scheme\n                         class_count=10,\n                         col='PRICE',\n                         cmap='Blues',  # matplotlib color map\n                        alpha=0.7,outline_color=[0,0,0,0])\n</pre> filtered_df.spatial.plot(map_widget=pdx_filtered_map,                           renderer_type='c',                          method='esriClassifyNaturalBreaks',  # classification scheme                          class_count=10,                          col='PRICE',                          cmap='Blues',  # matplotlib color map                         alpha=0.7,outline_color=[0,0,0,0]) <p>The houses in the shortlist are well spread across the Portland market. We notice spatial clustering in the distribution of property prices. Higher priced houses are mostly to the west of downtown while moderately and lower priced are spread across east and south.</p> In\u00a0[53]: Copied! <pre>filtered_df.to_csv('resources/houses_for_sale_att_filtered.csv')\n</pre> filtered_df.to_csv('resources/houses_for_sale_att_filtered.csv')"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#portland-housing-data-exploratory-data-analysis","title":"Portland housing data - exploratory data analysis\u00b6","text":"<p>In this second notebook, we use the cleaned and filtered data produced from notebook 1. Here we explore the spatial distribution of our dataset.</p> <p>So far, we worked with pandas <code>DataFrame</code> objects, to map the properties, we import the ArcGIS API for Python. This adds a <code>GeoAccessor</code> and spatially enables your <code>DataFrame</code> objects. As you will see in this notebook, you can then easily plot your data both as charts and as maps.</p>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#read-data","title":"Read data\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatially","title":"Visualize spatially\u00b6","text":"<p>Convert to Spatially Enabled DataFrame</p>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#explore-density-of-properties-for-sale","title":"Explore density of properties for sale\u00b6","text":"<p>You can render the same data using a heatmap renderer, which visualizes the spatial density of the properties (houses) using a heatmap.</p>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-geographic-distributions-of-housing-properties","title":"Visualize geographic distributions of housing properties\u00b6","text":"<p>Using different renderers such as 'class colored renderer', you can visualize and investigate if there is any spatial phenomena observable for the following columns.</p> <ul> <li>How is property age spatially distributed?</li> <li>Does property price drop outward from city center?</li> <li>Does property size increase in suburbs?</li> <li>hoa vs no hoa</li> </ul>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatial-distribution-by-property-age","title":"Visualize spatial distribution by property age\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatial-distribution-by-price","title":"Visualize spatial distribution by price\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatial-distribution-of-property-size","title":"Visualize spatial distribution of property size\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#spatial-distribution-of-hoa","title":"Spatial distribution of HoA\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#explore-distribution-of-numeric-columns","title":"Explore distribution of numeric columns:\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#filter-based-on-your-criteria","title":"Filter based on your criteria\u00b6","text":"<p>Let us define a few rules based on the intrinsic properties of these houses and shortlist them.</p>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-statistical-distribution-of-shortlisted-properties","title":"Visualize statistical distribution of shortlisted properties\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatial-distribution-of-shortlisted-properties","title":"Visualize spatial distribution of shortlisted properties\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#write-the-shortlisted-properties-to-disk","title":"Write the shortlisted properties to disk\u00b6","text":"<p>So far, we used attribute queries to explore and filter out properties. We have not yet used GIS analysis to narrow them further. Before that, let us save our work to disk.</p>"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/","title":"Feature engineering - quantifying access to facilities","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\n\nfrom arcgis.gis import GIS\nfrom arcgis.geocoding import geocode\nfrom arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor\nfrom arcgis.features import SpatialDataFrame\nfrom arcgis.geometry import Geometry, Point\nfrom arcgis.geometry.functions import buffer\nfrom arcgis.network import RouteLayer\n</pre> import pandas as pd import matplotlib.pyplot as plt from pprint import pprint %matplotlib inline  from arcgis.gis import GIS from arcgis.geocoding import geocode from arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor from arcgis.features import SpatialDataFrame from arcgis.geometry import Geometry, Point from arcgis.geometry.functions import buffer from arcgis.network import RouteLayer <p>Connect to GIS</p> In\u00a0[2]: Copied! <pre>gis = GIS(profile='')\n</pre> gis = GIS(profile='') In\u00a0[3]: Copied! <pre>prop_list_df = pd.read_csv('resources/houses_for_sale_att_filtered.csv')\nprop_list_df.shape\n</pre> prop_list_df = pd.read_csv('resources/houses_for_sale_att_filtered.csv') prop_list_df.shape Out[3]: <pre>(331, 24)</pre> In\u00a0[4]: Copied! <pre>prop_list_df = pd.DataFrame.spatial.from_xy(prop_list_df, 'LONGITUDE','LATITUDE')\ntype(prop_list_df)\n</pre> prop_list_df = pd.DataFrame.spatial.from_xy(prop_list_df, 'LONGITUDE','LATITUDE') type(prop_list_df) Out[4]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[5]: Copied! <pre>prop1 = prop_list_df[prop_list_df['MLS']==18389440]\nprop1\n</pre> prop1 = prop_list_df[prop_list_df['MLS']==18389440] prop1 Out[5]: Unnamed: 0 SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS ... DAYS ON MARKET PRICE PER SQFT HOA PER MONTH STATUS URL SOURCE MLS LATITUDE LONGITUDE SHAPE 203 2328 MLS Listing Single Family Residential 3775 NW Hilton Head Ter Portland OR 97229.0 649900.0 4.0 2.5 ... 21.0 221.0 25.0 Active http://www.redfin.com/OR/Portland/3775-NW-Hilt... RMLS 18389440 45.546644 -122.815658 {\"x\": -122.8156584, \"y\": 45.546644, \"spatialRe... <p>1 rows \u00d7 24 columns</p> In\u00a0[6]: Copied! <pre>house_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Shapes/RedStarLargeB.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24}\ngrocery_symbol = symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Shopping.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\nhospital_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/SafetyHealth/Hospital.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24}\ncoffee_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Coffee.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\nrestaurant_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Dining.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\nbar_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Bar.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\ngas_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Transportation/esriBusinessMarker_72.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\nshops_service_symbol={\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/esriBusinessMarker_58_Red.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10}\ntransport_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Transportation/esriDefaultMarker_195_White.png\",\"contentType\":\"image/png\",\"width\":15,\"height\":15}\nprofessional_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/esriBusinessMarker_64_Yellow.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10}\nparks_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/OutdoorRecreation/RestArea.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10}\neducation_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Note.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10}\narts_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/LiveShow.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\ndestination_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":12,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Basic/RedStickpin.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24}\nfill_symbol = {\"type\": \"esriSFS\",\"style\": \"esriSFSNull\",\n               \"outline\":{\"color\": [255,0,0,255]}}\n\nfill_symbol2 = {\"type\": \"esriSFS\",\"style\": \"esriSFSNull\",\n               \"outline\":{\"color\": [0,0,0,255]}}\n\nroute_symbol = {\"type\": \"esriSLS\",\"style\": \"esriSLSSolid\",\n                \"color\": [0, 120, 255, 255],\"width\": 1.5}\n</pre> house_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Shapes/RedStarLargeB.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24} grocery_symbol = symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Shopping.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} hospital_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/SafetyHealth/Hospital.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24} coffee_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Coffee.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} restaurant_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Dining.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} bar_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Bar.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} gas_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Transportation/esriBusinessMarker_72.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} shops_service_symbol={\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/esriBusinessMarker_58_Red.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10} transport_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Transportation/esriDefaultMarker_195_White.png\",\"contentType\":\"image/png\",\"width\":15,\"height\":15} professional_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/esriBusinessMarker_64_Yellow.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10} parks_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/OutdoorRecreation/RestArea.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10} education_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Note.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10} arts_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/LiveShow.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} destination_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":12,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Basic/RedStickpin.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24} fill_symbol = {\"type\": \"esriSFS\",\"style\": \"esriSFSNull\",                \"outline\":{\"color\": [255,0,0,255]}}  fill_symbol2 = {\"type\": \"esriSFS\",\"style\": \"esriSFSNull\",                \"outline\":{\"color\": [0,0,0,255]}}  route_symbol = {\"type\": \"esriSLS\",\"style\": \"esriSLSSolid\",                 \"color\": [0, 120, 255, 255],\"width\": 1.5} In\u00a0[7]: Copied! <pre>paddress = prop1.ADDRESS + \", \" + prop1.CITY + \", \" + prop1.STATE\nprop_geom_fset = geocode(paddress.values[0], as_featureset=True)\n</pre> paddress = prop1.ADDRESS + \", \" + prop1.CITY + \", \" + prop1.STATE prop_geom_fset = geocode(paddress.values[0], as_featureset=True) <p>Create an envelope around the property using its extent</p> In\u00a0[8]: Copied! <pre>prop_geom = prop_geom_fset.features[0]\nprop_geom.geometry\n</pre> prop_geom = prop_geom_fset.features[0] prop_geom.geometry Out[8]: <pre>{'x': -122.81532305026238,\n 'y': 45.54674878544642,\n 'spatialReference': {'wkid': 4326, 'latestWkid': 4326}}</pre> In\u00a0[9]: Copied! <pre>prop_geom = prop_geom_fset.features[0]\nprop_buffer = buffer([prop_geom.geometry], \n                     in_sr = 102100, buffer_sr=102100,\n                     distances=0.05, unit=9001)[0]\n\nprop_buffer_f = Feature(geometry=prop_buffer)\nprop_buffer_fset = FeatureSet([prop_buffer_f])\n</pre> prop_geom = prop_geom_fset.features[0] prop_buffer = buffer([prop_geom.geometry],                       in_sr = 102100, buffer_sr=102100,                      distances=0.05, unit=9001)[0]  prop_buffer_f = Feature(geometry=prop_buffer) prop_buffer_fset = FeatureSet([prop_buffer_f]) In\u00a0[10]: Copied! <pre>pdx_map = gis.map('Portland, OR')\npdx_map.basemap='gray'\npdx_map\n</pre> pdx_map = gis.map('Portland, OR') pdx_map.basemap='gray' pdx_map <pre>MapView(basemap='gray', layout=Layout(height='400px', width='100%'))</pre> <p></p> In\u00a0[11]: Copied! <pre>pdx_map.draw(prop_buffer_fset, symbol=fill_symbol2)\npdx_map.draw(prop_geom_fset, symbol=house_symbol)\n</pre> pdx_map.draw(prop_buffer_fset, symbol=fill_symbol2) pdx_map.draw(prop_geom_fset, symbol=house_symbol) In\u00a0[15]: Copied! <pre>neighborhood_data_dict = {}\n</pre> neighborhood_data_dict = {} In\u00a0[16]: Copied! <pre>groceries = geocode('groceries', search_extent=prop_buffer.extent, \n                    max_locations=20, as_featureset=True)\nneighborhood_data_dict['groceries'] = []\n\nfor place in groceries:\n    popup={\"title\" : place.attributes['PlaceName'], \n    \"content\" : place.attributes['Place_addr']}\n    pdx_map.draw(place.geometry, symbol=grocery_symbol, popup=popup)\n    neighborhood_data_dict['groceries'].append(place.attributes['PlaceName'])\n</pre> groceries = geocode('groceries', search_extent=prop_buffer.extent,                      max_locations=20, as_featureset=True) neighborhood_data_dict['groceries'] = []  for place in groceries:     popup={\"title\" : place.attributes['PlaceName'],      \"content\" : place.attributes['Place_addr']}     pdx_map.draw(place.geometry, symbol=grocery_symbol, popup=popup)     neighborhood_data_dict['groceries'].append(place.attributes['PlaceName']) <p>We will geocode for the following facilities within the said <code>5</code> mile buffer.</p> <pre><code>Groceries\nRestaurants\nHospitals\nCoffee shops\nBars\nGas stations\nShops and service\nTravel and transport\nParks and outdoors\nEducation</code></pre> In\u00a0[48]: Copied! <pre>pdx_map2 = gis.map('Portland, OR')\npdx_map2.basemap='gray'\npdx_map2\n</pre> pdx_map2 = gis.map('Portland, OR') pdx_map2.basemap='gray' pdx_map2 <pre>MapView(basemap='gray', layout=Layout(height='400px', width='100%'))</pre> <p></p> <p></p> <p></p> In\u00a0[59]: Copied! <pre>pdx_map2.draw(prop_buffer_fset, symbol=fill_symbol2)\npdx_map2.draw(prop_geom_fset, symbol=house_symbol)\n</pre> pdx_map2.draw(prop_buffer_fset, symbol=fill_symbol2) pdx_map2.draw(prop_geom_fset, symbol=house_symbol) In\u00a0[21]: Copied! <pre>restaurants = geocode('restaurant', search_extent=prop_buffer.extent, max_locations=200)\nneighborhood_data_dict['restauruants'] = []\n\nfor place in restaurants:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=restaurant_symbol, popup=popup)\n    neighborhood_data_dict['restauruants'].append(place['attributes']['PlaceName'])\n</pre> restaurants = geocode('restaurant', search_extent=prop_buffer.extent, max_locations=200) neighborhood_data_dict['restauruants'] = []  for place in restaurants:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=restaurant_symbol, popup=popup)     neighborhood_data_dict['restauruants'].append(place['attributes']['PlaceName']) In\u00a0[51]: Copied! <pre>hospitals = geocode('hospital', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['hospitals'] = []\n\nfor place in hospitals:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=hospital_symbol, popup=popup)\n    neighborhood_data_dict['hospitals'].append(place['attributes']['PlaceName'])\n</pre> hospitals = geocode('hospital', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['hospitals'] = []  for place in hospitals:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=hospital_symbol, popup=popup)     neighborhood_data_dict['hospitals'].append(place['attributes']['PlaceName']) In\u00a0[52]: Copied! <pre>coffees = geocode('coffee', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['coffees'] = []\n\nfor place in coffees:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=coffee_symbol, popup=popup)\n    neighborhood_data_dict['coffees'].append(place['attributes']['PlaceName'])\n</pre> coffees = geocode('coffee', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['coffees'] = []  for place in coffees:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=coffee_symbol, popup=popup)     neighborhood_data_dict['coffees'].append(place['attributes']['PlaceName']) In\u00a0[53]: Copied! <pre>bars = geocode('bar', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['bars'] = []\n\nfor place in bars:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=bar_symbol, popup=popup)\n    neighborhood_data_dict['bars'].append(place['attributes']['PlaceName'])\n</pre> bars = geocode('bar', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['bars'] = []  for place in bars:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=bar_symbol, popup=popup)     neighborhood_data_dict['bars'].append(place['attributes']['PlaceName']) In\u00a0[54]: Copied! <pre>gas = geocode('gas station', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['gas'] = []\n\nfor place in gas:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=gas_symbol, popup=popup)\n    neighborhood_data_dict['gas'].append(place['attributes']['PlaceName'])\n</pre> gas = geocode('gas station', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['gas'] = []  for place in gas:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=gas_symbol, popup=popup)     neighborhood_data_dict['gas'].append(place['attributes']['PlaceName']) In\u00a0[55]: Copied! <pre>shops_service = geocode(\"\",category='shops and service', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['shops'] = []\n\nfor place in shops_service:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=shops_service_symbol, popup=popup)\n    neighborhood_data_dict['shops'].append(place['attributes']['PlaceName'])\n</pre> shops_service = geocode(\"\",category='shops and service', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['shops'] = []  for place in shops_service:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=shops_service_symbol, popup=popup)     neighborhood_data_dict['shops'].append(place['attributes']['PlaceName']) In\u00a0[56]: Copied! <pre>transport = geocode(\"\",category='travel and transport', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['transport'] = []\n\nfor place in transport:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=transport_symbol, popup=popup)\n    neighborhood_data_dict['transport'].append(place['attributes']['PlaceName'])\n</pre> transport = geocode(\"\",category='travel and transport', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['transport'] = []  for place in transport:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=transport_symbol, popup=popup)     neighborhood_data_dict['transport'].append(place['attributes']['PlaceName']) In\u00a0[57]: Copied! <pre>parks = geocode(\"\",category='parks and outdoors', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['parks'] = []\n\nfor place in parks:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=parks_symbol, popup=popup)\n    neighborhood_data_dict['parks'].append(place['attributes']['PlaceName'])\n</pre> parks = geocode(\"\",category='parks and outdoors', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['parks'] = []  for place in parks:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=parks_symbol, popup=popup)     neighborhood_data_dict['parks'].append(place['attributes']['PlaceName']) In\u00a0[58]: Copied! <pre>education = geocode(\"\",category='education', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['education'] = []\n\nfor place in education:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=education_symbol, popup=popup)\n    neighborhood_data_dict['education'].append(place['attributes']['PlaceName'])\n</pre> education = geocode(\"\",category='education', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['education'] = []  for place in education:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=education_symbol, popup=popup)     neighborhood_data_dict['education'].append(place['attributes']['PlaceName']) In\u00a0[75]: Copied! <pre>neighborhood_df = pd.DataFrame.from_dict(neighborhood_data_dict, orient='index')\nneighborhood_df = neighborhood_df.transpose()\nneighborhood_df\n</pre> neighborhood_df = pd.DataFrame.from_dict(neighborhood_data_dict, orient='index') neighborhood_df = neighborhood_df.transpose() neighborhood_df Out[75]: groceries restauruants hospitals coffees bars gas shops transport parks education 0 Bales Market Place Coffee. Cup Providence St Vincent Medical Center-ER Coffee. Cup None Shell Anderson Towing &amp; Recovery MAX-Elmonica &amp; SW 170th Ave Jqay House Park Portland Community College-Rock Creek 1 Safeway Papa Murphy's Providence St Vincent Medical Center Starbucks None ARCO Bassitt Auto Co Powder Lodging Jackie Husen Park Cedar Mill Elementary School 2 QFC Tilly's Gelato None Starbucks None 76 Cottman Transmission Homestead Studio Suites-Beaverton The Bluffs Goddard School 3 Dinihanian's Farm Market Oak Hills Brew Pub None Poppa's Haven None Costco Powell Paint Center MAX-Sunset TC Bonny Slope Park St Pius X Elementary School 4 Walmart Neighborhood Market Starbucks None Tazza Cafe None 76 Retied MAX-Merlo &amp; SW 158th Ave Burton Park Terra Linda Elementary School 5 India Supermarket Starbucks None Laughing Planet Cafe None Fred Meyer Fuel Center Chrisman's Picture Frame &amp; Gallery MAX-Beaverton Creek Lost Park Montessori School of Beaverton 6 Apna Bazaar Chiam None Coffee Renaissance None Chevron Team Uniforms Rodeway Inn &amp; Suites-Portland Terra Linda Park A Childs Way Kindergarten 7 Fred Meyer Bandito Taco None Starbucks None Chevron T-Mobile Doubletree-Beaverton Jordan Park Christ United Methodist Preschool 8 Albertsons Chipotle None Bowl &amp; Berry None 76 Holistic Pet Hilton Garden Inn-Beaverton Cedar Mill Woods Park Cornell Children's Village Day Sch 9 WinCo Foods Bollywood Bites None Taiwan Eats None Shell Shell Fairfield Inn &amp; Suites-Beaverton Cedar Mill Park Catlin Gabel School 10 Plaid Pantry Pizza Schmizza None Starbucks None 76 Mike's Auto Parts Homewood Suites-Hillsboro Beaverton Roger Tilbury Memorial Park West Tualatin View Elementary Sch 11 Target Wan Q Restaurant None Bethany Public House None Shell Kaady Car Wash Poehler Airport Merritt Orchard Park Beaver Acres Elementary School 12 None Shari's None Starbucks None 76 7-Eleven Park &amp; Ride-Elmonica/SW 170th Ave Commonwealth Park Ridgewood Elementary School 13 None Ct Bistro None Starbucks None None Motor Sports International Park &amp; Ride-NW Cornell Pioneer Park William Walker Elementary School 14 None McDonald's None Creatures of Habit Espresso &amp; Deli None None Bank of America Park &amp; Ride-Sunset TRANSIT Center Peppertree Park Meadow Park Middle School 15 None Pizzicato Westside None Starbucks None None Bales Market Place Park &amp; Ride-Beaverton Creek Claremont Golf Course Pasitos Spanish School 16 None Lejoi Cafe None Coffee Rush None None US Bank Park &amp; Ride-Cedar Hills Church Kaiser Woods Park Holy Trinity Elementary School 17 None Ichiban Japanese None Starbucks None None Dufresne's Auto Service Dock Bethany Meadows Park Barnes Elementary School 18 None Mac's Market &amp; Deli None None None None US Bank Dock Kaiser Ridge Park Shiny Sparkles Montessori 19 None SUBWAY None None None None Bank of America Fred Meyer Morgans Run Park Stoller Middle School 20 None Salars Mediterranean Grill None None None None Ace Hardware Dock West Union Estates Park Findley Elementary 21 None Poppa's Haven None None None None Du Fresne's Auto Repair Dock College Park Kindercare Learning Center 22 None Papa John's None None None None Jiffy Lube Dock Northeast Neighborhood Park Westview Senior High School 23 None Tazza Cafe None None None None Wells Fargo Dock Ben Graf Park Beaverton School 24 None Teriyaki Beef Bowl None None None None Walgreens 1-1 Spyglass Park Sweet Peas Kidzone 25 None China Rim None None None None Wells Fargo Exit 67/Murray Blvd/E Stoller Farms Park La Petite Academy 26 None SUBWAY None None None None Safeway Exit 65/Bethany Blvd/E Emerald Estates Park Oak Hills Elementary School 27 None Dairy Queen None None None None Team Uniforms Exit 67/Murray Blvd/W Serah Lindsay Estates Park Bethany Elementary School 28 None Si Senor Mexican Restaurant None None None None Xpressolube Exit 69B/Park Way/W Quarry Park Kids of the Kingdom 29 None Laughing Planet Cafe None None None None Mike's Auto Parts Exit 68/Cedar Hills Blvd/W Springville Meadows Park Elmonica Elementary School 30 None Pizza Schmizza None None None None SHERWIN-WILLIAMS Tillamook/N Skyview Park Touchstone School 31 None Bleachers None None None None Pet Barn Exit 65/Cornell Rd/W John Marty Park Five Oaks Middle School 32 None Tazza Cafe None None None None Rock It Resell Exit 69B/Cedar Hills/E George W Otten Park Pacific Academy 33 None Bowl &amp; Berry None None None None Baby &amp; Me Exit 68/Cedar Hills Blvd/E Somerset Meadows Park Merlo Station High School 34 None Sweet Lemon Vegetarian Bistro None None None None Cedar Mill Home Theater Exit 1/Walker Rd/S Oak Hills Park Kinder Prep Private Preschool 35 None Juan Colorado None None None None Dollar Tree Exit 64/185th Ave/W Bronson Creek Park Prince of Peace Lutheran Preschool 36 None Taiwan Eats None None None None Sunset Science Park Federal CU None Autumn Ridge Park Sunset High School 37 None Taiwan Eats None None None None Sunset Science Park Federal CU None Apollo Ridge Park Cedar Hills Kindercare 38 None Cackalack's Hot Chicken Shack None None None None Dennis Market None Willow Creek Nature Park Northwest Montessori School 39 None Chen's Dynasty Restaurant None None None None Laughing Planet Cafe None Dwight S Parr Jr Park Agia Sophia Academy 40 None Bethany Sushi None None None None None None Moshofsky Woods Park Learning Years Day School 41 None SUBWAY None None None None None None Stonegate at Willow Creek ABC Children's Academy 42 None Starbucks None None None None None None Waterhouse Park Arco Iris Spanish Immersion School 43 None Bethany Public House None None None None None None Sunset Park Forest Park Elementary School 44 None Bethany Public House None None None None None None Foege Park None 45 None Bliss Bake Shoppe None None None None None None Peterkort Village Park None 46 None Bliss Bake Shoppe None None None None None None Wanda L Peck Memorial Park None 47 None Biscuits Cafe None None None None None None Howard M Terpenning Complex None 48 None Koi Fusion None None None None None None Forest Heights Park None 49 None Starbucks None None None None None None Forest Heights City Park None In\u00a0[76]: Copied! <pre>neighborhood_df.count().plot(kind='bar')\nplt.title('Facilities within 5 miles of {}'.format(prop1.ADDRESS.values[0]))\n</pre> neighborhood_df.count().plot(kind='bar') plt.title('Facilities within 5 miles of {}'.format(prop1.ADDRESS.values[0])) Out[76]: <pre>Text(0.5,1,'Facilities within 5 miles of 3775 NW Hilton Head Ter')</pre> In\u00a0[32]: Copied! <pre>route_service_url = gis.properties.helperServices.route.url\nroute_service = RouteLayer(route_service_url, gis=gis)\n</pre> route_service_url = gis.properties.helperServices.route.url route_service = RouteLayer(route_service_url, gis=gis) In\u00a0[33]: Copied! <pre>stops = [paddress.values[0], '309 SW 6th Ave #600, Portland, OR 97204']\nfrom arcgis.geocoding import geocode, batch_geocode\nstops_geocoded = batch_geocode(stops)\n\nstops_geocoded = [item['location'] for item in stops_geocoded]\nstops_geocoded2 = '{},{};{},{}'.format(stops_geocoded[0]['x'],stops_geocoded[0]['y'],\n                                       stops_geocoded[1]['x'],stops_geocoded[1]['y'])\nstops_geocoded2\n</pre> stops = [paddress.values[0], '309 SW 6th Ave #600, Portland, OR 97204'] from arcgis.geocoding import geocode, batch_geocode stops_geocoded = batch_geocode(stops)  stops_geocoded = [item['location'] for item in stops_geocoded] stops_geocoded2 = '{},{};{},{}'.format(stops_geocoded[0]['x'],stops_geocoded[0]['y'],                                        stops_geocoded[1]['x'],stops_geocoded[1]['y']) stops_geocoded2 Out[33]: <pre>'-122.81532304999996,45.546748785000034;-122.67727209699996,45.52153932300007'</pre> In\u00a0[34]: Copied! <pre>modes = route_service.retrieve_travel_modes()['supportedTravelModes']\nfor mode in modes:\n    print(mode['name'])\n</pre> modes = route_service.retrieve_travel_modes()['supportedTravelModes'] for mode in modes:     print(mode['name']) <pre>Walking Time\nRural Driving Distance\nDriving Time\nDriving Distance\nWalking Distance\nRural Driving Time\nTrucking Time\nTrucking Distance\n</pre> In\u00a0[35]: Copied! <pre>route_service.properties.impedance\n</pre> route_service.properties.impedance Out[35]: <pre>'TravelTime'</pre> <p>Calculate time it takes to get to work. Set start time as <code>8:00 AM</code> on Mondays. ArcGIS routing service will use historic averages, so we provide this time as <code>8:00 AM, Monday, June 4 1990</code> in Unix epoch time. Read more about this here</p> In\u00a0[36]: Copied! <pre>route_result = route_service.solve(stops_geocoded2, return_routes=True, \n                             return_stops=True, return_directions=True,\n                             impedance_attribute_name='TravelTime',\n                             start_time=644511600000,\n                             return_barriers=False, return_polygon_barriers=False,\n                             return_polyline_barriers=False)\n</pre> route_result = route_service.solve(stops_geocoded2, return_routes=True,                               return_stops=True, return_directions=True,                              impedance_attribute_name='TravelTime',                              start_time=644511600000,                              return_barriers=False, return_polygon_barriers=False,                              return_polyline_barriers=False) In\u00a0[37]: Copied! <pre>route_length = route_result['directions'][0]['summary']['totalLength']\nroute_duration = route_result['directions'][0]['summary']['totalTime']\nroute_duration_str = \"{}m, {}s\".format(int(route_duration), \n                                       round((route_duration %1)*60,2))\nprint(\"route length: {} miles, route duration: {}\".format(round(route_length,3),\n                                                         route_duration_str))\n</pre> route_length = route_result['directions'][0]['summary']['totalLength'] route_duration = route_result['directions'][0]['summary']['totalTime'] route_duration_str = \"{}m, {}s\".format(int(route_duration),                                         round((route_duration %1)*60,2)) print(\"route length: {} miles, route duration: {}\".format(round(route_length,3),                                                          route_duration_str)) <pre>route length: 10.273 miles, route duration: 27m, 48.39s\n</pre> In\u00a0[49]: Copied! <pre>route_features = route_result['routes']['features']\nroute_fset = FeatureSet(route_features)\nstop_features = route_result['stops']['features']\nstop_fset = FeatureSet(stop_features)\n\nroute_pop_up = {'title':'Name',\n               'content':'Total_Miles'}\npdx_map2.draw(route_fset, symbol=route_symbol, popup=route_pop_up)\n</pre> route_features = route_result['routes']['features'] route_fset = FeatureSet(route_features) stop_features = route_result['stops']['features'] stop_fset = FeatureSet(stop_features)  route_pop_up = {'title':'Name',                'content':'Total_Miles'} pdx_map2.draw(route_fset, symbol=route_symbol, popup=route_pop_up) In\u00a0[50]: Copied! <pre>pdx_map2.draw(stop_fset, symbol=destination_symbol)\n</pre> pdx_map2.draw(stop_fset, symbol=destination_symbol)"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#feature-engineering-quantifying-access-to-facilities","title":"Feature engineering - quantifying access to facilities\u00b6","text":"<p>Often, when shortlisting facilities buyers look for access to facilities such as groceries, restaurants, schools, emergency and health care in thier neighborhood. In this notebook, we use the <code>geocoding</code> module to search for such facilities and build a table for each property.</p>"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#read-one-of-the-shortlisted-properties","title":"Read one of the shortlisted properties\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#create-symbols-for-facilities","title":"Create symbols for facilities\u00b6","text":"<p>Get your symbols using this online tool: http://esri.github.io/arcgis-python-api/tools/symbol.html</p>"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#get-5-mile-extent-around-the-property-of-interest","title":"Get 5 mile extent around the property of interest\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#plot-house-and-buffer-on-map","title":"Plot house and buffer on map\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#geocode-for-facilities","title":"Geocode for facilities\u00b6","text":"<p>We use the ArcGIS Geocoding service to search for facilities around this house</p>"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#groceries","title":"Groceries\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#restaurants","title":"Restaurants\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#hospitals","title":"Hospitals\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#coffee-shops","title":"Coffee shops\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#bars","title":"Bars\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#gas-stations","title":"Gas stations\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#shops-and-service","title":"Shops and service\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#travel-and-transport","title":"Travel and transport\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#parks-and-outdoors","title":"Parks and outdoors\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#education","title":"Education\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#present-the-results-in-a-table","title":"Present the results in a table\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#find-duration-to-commute-to-work","title":"Find duration to commute to work\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/","title":"Feature engineering - quantifying access to facilities - batch mode","text":"In\u00a0[9]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\n\nfrom arcgis.gis import GIS\nfrom arcgis.geocoding import geocode, batch_geocode\nfrom arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor\nfrom arcgis.features import SpatialDataFrame\nfrom arcgis.geometry import Geometry, Point\nfrom arcgis.geometry.functions import buffer\nfrom arcgis.network import RouteLayer\n</pre> import pandas as pd import matplotlib.pyplot as plt from pprint import pprint %matplotlib inline  from arcgis.gis import GIS from arcgis.geocoding import geocode, batch_geocode from arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor from arcgis.features import SpatialDataFrame from arcgis.geometry import Geometry, Point from arcgis.geometry.functions import buffer from arcgis.network import RouteLayer <p>Connect to GIS</p> In\u00a0[7]: Copied! <pre>gis = GIS(profile='')\nroute_service_url = gis.properties.helperServices.route.url\nroute_service = RouteLayer(route_service_url, gis=gis)\n</pre> gis = GIS(profile='') route_service_url = gis.properties.helperServices.route.url route_service = RouteLayer(route_service_url, gis=gis) In\u00a0[3]: Copied! <pre>prop_list_df = pd.read_csv('resources/houses_for_sale_att_filtered.csv')\nprop_list_df.shape\n</pre> prop_list_df = pd.read_csv('resources/houses_for_sale_att_filtered.csv') prop_list_df.shape Out[3]: <pre>(331, 24)</pre> In\u00a0[4]: Copied! <pre>prop_list_df = pd.DataFrame.spatial.from_xy(prop_list_df, 'LONGITUDE','LATITUDE')\ntype(prop_list_df)\n</pre> prop_list_df = pd.DataFrame.spatial.from_xy(prop_list_df, 'LONGITUDE','LATITUDE') type(prop_list_df) Out[4]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[10]: Copied! <pre>groceries_count = []\nrestaurants_count = []\nhospitals_count = []\ncoffee_count = []\nbars_count = []\ngas_count = []\nshops_service_count = []\ntravel_transport_count = []\nparks_count = []\neducation_count = []\nroute_length = []\nroute_duration = []\n\ndestination_address = '309 SW 6th Ave #600, Portland, OR 97204'\n</pre> groceries_count = [] restaurants_count = [] hospitals_count = [] coffee_count = [] bars_count = [] gas_count = [] shops_service_count = [] travel_transport_count = [] parks_count = [] education_count = [] route_length = [] route_duration = []  destination_address = '309 SW 6th Ave #600, Portland, OR 97204' In\u00a0[12]: Copied! <pre>count=0\nfor index, prop in prop_list_df.iterrows():\n    count+=1\n    print(str(count), end=\": \")\n    # geocode the property\n    paddress = prop['ADDRESS'] + \", \" + prop['CITY'] + \", \" + prop['STATE']\n    prop_geom_fset = geocode(paddress, as_featureset=True)\n    \n    print(prop['MLS'], end=\" : \")\n    \n    # create an envelope around each property\n    prop_geom = prop_geom_fset.features[0]\n    \n    # create buffer of 5 miles\n    prop_buffer = buffer([prop_geom.geometry], \n                     in_sr = 102100, buffer_sr=102100,\n                     distances=0.05, unit=9001)[0]\n\n    prop_buffer_f = Feature(geometry=prop_buffer)\n    prop_buffer_fset = FeatureSet([prop_buffer_f])\n    \n    # geocode for Groceries\n    groceries = geocode('groceries', search_extent=prop_buffer.extent, \n                    max_locations=20, as_featureset=True)\n    groceries_count.append(len(groceries.features))\n    print('Groc', end=\" : \")\n    \n    # restaurants\n    restaurants = geocode('restaurant', search_extent=prop_buffer.extent, max_locations=200)\n    restaurants_count.append(len(restaurants))\n    print('Rest', end=\" : \")\n    \n    # hospitals\n    hospitals = geocode('hospital', search_extent=prop_buffer.extent, max_locations=50)\n    hospitals_count.append(len(hospitals))\n    print('Hosp', end =\" : \")\n    \n    # coffee shop\n    coffees = geocode('coffee', search_extent=prop_buffer.extent, max_locations=50)\n    coffee_count.append(len(coffees))\n    print('Coffee', end=\" : \")\n    \n    # bars\n    bars = geocode('bar', search_extent=prop_buffer.extent, max_locations=50)\n    bars_count.append(len(bars))\n    print('Bars', end=\" : \")\n    \n    # gas stations\n    gas = geocode('gas station', search_extent=prop_buffer.extent, max_locations=50)\n    gas_count.append(len(gas))\n    print('Gas', end=\" : \")\n    \n    # shops\n    shops_service = geocode(\"\",category='shops and service', \n                            search_extent=prop_buffer.extent, max_locations=50)\n    shops_service_count.append(len(shops_service))\n    print('Shops', end=\" : \")\n    \n    # travel &amp; transport\n    transport = geocode(\"\",category='travel and transport', \n                        search_extent=prop_buffer.extent, max_locations=50)\n    travel_transport_count.append(len(transport))\n    print(\"Travel\", end =\" : \")\n    \n    # parks\n    parks = geocode(\"\",category='parks and outdoors', \n                    search_extent=prop_buffer.extent, max_locations=50)\n    parks_count.append(len(parks))\n    print('Parks', end=\" : \")\n    \n    # education\n    education = geocode(\"\",category='education', search_extent=prop_buffer.extent, \n                        max_locations=50)\n    education_count.append(len(education))\n    print(\"Edu\", end=\" : \")\n    \n    # get route\n    stops = [paddress, destination_address]\n    stops_geocoded = batch_geocode(stops)\n\n    stops_geocoded = [item['location'] for item in stops_geocoded]\n    stops_geocoded2 = '{},{};{},{}'.format(stops_geocoded[0]['x'],stops_geocoded[0]['y'],\n                                           stops_geocoded[1]['x'],stops_geocoded[1]['y'])\n\n    route_result = route_service.solve(stops_geocoded2, return_routes=True, \n                             return_stops=False, return_directions=True,\n                             impedance_attribute_name='TravelTime',\n                             start_time=644511600000,\n                             return_barriers=False, return_polygon_barriers=False,\n                             return_polyline_barriers=False)\n    route_length.append(route_result['directions'][0]['summary']['totalLength'])\n    route_duration.append(route_result['directions'][0]['summary']['totalTime'])\n    print(\"Route\")\n</pre> count=0 for index, prop in prop_list_df.iterrows():     count+=1     print(str(count), end=\": \")     # geocode the property     paddress = prop['ADDRESS'] + \", \" + prop['CITY'] + \", \" + prop['STATE']     prop_geom_fset = geocode(paddress, as_featureset=True)          print(prop['MLS'], end=\" : \")          # create an envelope around each property     prop_geom = prop_geom_fset.features[0]          # create buffer of 5 miles     prop_buffer = buffer([prop_geom.geometry],                       in_sr = 102100, buffer_sr=102100,                      distances=0.05, unit=9001)[0]      prop_buffer_f = Feature(geometry=prop_buffer)     prop_buffer_fset = FeatureSet([prop_buffer_f])          # geocode for Groceries     groceries = geocode('groceries', search_extent=prop_buffer.extent,                      max_locations=20, as_featureset=True)     groceries_count.append(len(groceries.features))     print('Groc', end=\" : \")          # restaurants     restaurants = geocode('restaurant', search_extent=prop_buffer.extent, max_locations=200)     restaurants_count.append(len(restaurants))     print('Rest', end=\" : \")          # hospitals     hospitals = geocode('hospital', search_extent=prop_buffer.extent, max_locations=50)     hospitals_count.append(len(hospitals))     print('Hosp', end =\" : \")          # coffee shop     coffees = geocode('coffee', search_extent=prop_buffer.extent, max_locations=50)     coffee_count.append(len(coffees))     print('Coffee', end=\" : \")          # bars     bars = geocode('bar', search_extent=prop_buffer.extent, max_locations=50)     bars_count.append(len(bars))     print('Bars', end=\" : \")          # gas stations     gas = geocode('gas station', search_extent=prop_buffer.extent, max_locations=50)     gas_count.append(len(gas))     print('Gas', end=\" : \")          # shops     shops_service = geocode(\"\",category='shops and service',                              search_extent=prop_buffer.extent, max_locations=50)     shops_service_count.append(len(shops_service))     print('Shops', end=\" : \")          # travel &amp; transport     transport = geocode(\"\",category='travel and transport',                          search_extent=prop_buffer.extent, max_locations=50)     travel_transport_count.append(len(transport))     print(\"Travel\", end =\" : \")          # parks     parks = geocode(\"\",category='parks and outdoors',                      search_extent=prop_buffer.extent, max_locations=50)     parks_count.append(len(parks))     print('Parks', end=\" : \")          # education     education = geocode(\"\",category='education', search_extent=prop_buffer.extent,                          max_locations=50)     education_count.append(len(education))     print(\"Edu\", end=\" : \")          # get route     stops = [paddress, destination_address]     stops_geocoded = batch_geocode(stops)      stops_geocoded = [item['location'] for item in stops_geocoded]     stops_geocoded2 = '{},{};{},{}'.format(stops_geocoded[0]['x'],stops_geocoded[0]['y'],                                            stops_geocoded[1]['x'],stops_geocoded[1]['y'])      route_result = route_service.solve(stops_geocoded2, return_routes=True,                               return_stops=False, return_directions=True,                              impedance_attribute_name='TravelTime',                              start_time=644511600000,                              return_barriers=False, return_polygon_barriers=False,                              return_polyline_barriers=False)     route_length.append(route_result['directions'][0]['summary']['totalLength'])     route_duration.append(route_result['directions'][0]['summary']['totalTime'])     print(\"Route\") <pre>1: 18517652 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n2: 18465613 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n3: 18005102 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n4: 18216924 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n5: 18647164 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n6: 18229660 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n7: 18586790 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n8: 18314898 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n9: 18085278 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n10: 18406186 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n11: 18020972 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n12: 18283940 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n13: 18166839 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n14: 18390189 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n15: 18281614 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n16: 18159838 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n17: 18697929 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n18: 18184111 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n19: 18381383 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n20: 18036264 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n21: 18352241 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n22: 18415529 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n23: 18052500 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n24: 18615156 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n25: 18663618 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n26: 18524346 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n27: 18287995 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n28: 18496603 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n29: 18306005 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n30: 18017989 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n31: 18630734 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n32: 18362577 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n33: 18185462 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n34: 18525026 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n35: 18268485 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n36: 18077776 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n37: 18404645 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n38: 18543295 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n39: 18268007 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n40: 18565385 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n41: 18327093 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n42: 18300182 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n43: 18130222 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n44: 18390288 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n45: 18244519 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n46: 18533145 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n47: 18317832 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n48: 18625148 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n49: 18046552 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n50: 18613304 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n51: 18035240 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n52: 18170798 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n53: 18479918 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n54: 18134679 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n55: 18175979 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n56: 18489535 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n57: 18136667 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n58: 18330192 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n59: 18035177 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n60: 18117942 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n61: 18467170 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n62: 18021854 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n63: 18453150 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n64: 17228335 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n65: 18385412 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n66: 18303159 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n67: 18260962 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n68: 18077788 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n69: 1443106 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n70: 18363380 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n71: 18038013 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n72: 18592639 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n73: 18031108 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n74: 18529842 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n75: 18346063 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n76: 18058515 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n77: 18312442 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n78: 18062133 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n79: 18292112 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n80: 18021426 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n81: 18243995 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n82: 18225445 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n83: 18294835 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n84: 18036942 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n85: 18164206 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n86: 18112429 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n87: 18401204 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n88: 18185257 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n89: 18056844 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n90: 18158997 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n91: 18089346 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n92: 18432056 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n93: 18248238 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n94: 18042235 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n95: 18376736 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n96: 17468803 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n97: 18002685 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n98: 18609084 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n99: 18115705 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n100: 18197366 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n101: 18367807 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n102: 18289907 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n103: 1456193 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n104: 18047073 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n105: 18379344 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n106: 18487228 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n107: 18123653 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n108: 18310961 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n109: 18671543 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n110: 18384430 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n111: 18292179 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n112: 18545728 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n113: 18317933 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n114: 18131614 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n115: 18359207 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n116: 18188909 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n117: 18257295 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n118: 18454489 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n119: 18644320 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n120: 18103930 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n121: 18213089 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n122: 18436725 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n123: 18217493 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n124: 18534351 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n125: 18391773 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n126: 17138243 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n127: 18580547 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n128: 18385584 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n129: 1516748 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n130: 18108097 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n131: 18343244 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n132: 18518261 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n133: 18496090 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n134: 18084407 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n135: 18622197 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n136: 18590953 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n137: 18548575 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n138: 18361209 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n139: 18190102 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n140: 18306032 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n141: 18546456 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n142: 17218625 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n143: 18360477 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n144: 18447320 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n145: 18127206 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n146: 18489822 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n147: 18218285 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n148: 18647011 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n149: 18193927 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n150: 18595550 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n151: 18317847 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n152: 18108604 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n153: 18410442 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n154: 17515218 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n155: 18578497 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n156: 18293042 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n157: 18401105 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n158: 18095097 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n159: 18030852 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n160: 18299685 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n161: 18583483 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n162: 18171268 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n163: 18053604 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n164: 18622831 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n165: 18586722 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n166: 18467321 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n167: 18383920 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n168: 18599482 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n169: 18500611 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n170: 18345364 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n171: 18116411 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n</pre> <pre>172: 18187676 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n173: 18521135 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n174: 18661709 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n175: 18196732 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n176: 18047304 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n177: 18202505 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n178: 18090694 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n179: 18572038 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n180: 18641153 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n181: 18231117 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n182: 18069841 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n183: 18599386 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n184: 18160313 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n185: 18377250 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n186: 18301272 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n187: 18094956 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n188: 1465894 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n189: 18525186 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n190: 18144594 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n191: 18462595 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n192: 18597525 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n193: 18436901 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n194: 18166594 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n195: 18164582 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n196: 18258361 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n197: 18326643 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n198: 18681920 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n199: 18123789 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n200: 18080838 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n201: 18551307 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n202: 18142190 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n203: 18333327 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n204: 18389440 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n205: 18232692 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n206: 18147756 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n207: 18439349 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n208: 18319633 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n209: 18400923 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n210: 18108336 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n211: 18270738 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n212: 18045246 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n213: 18158354 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n214: 18225617 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n215: 17626574 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n216: 18285532 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n217: 18256758 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n218: 18659423 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n219: 18384499 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n220: 18126242 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n221: 18151979 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n222: 18323631 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n223: 18309110 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n224: 18669250 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n225: 18020854 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n226: 18412385 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n227: 18590261 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n228: 18197963 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n229: 18138336 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n230: 18100325 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n231: 18047136 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n232: 18073472 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n233: 18115891 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n234: 18486871 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n235: 18537072 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n236: 611481 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n237: 18664192 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n238: 18392231 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n239: 18318834 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n240: 18373029 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n241: 18207743 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n242: 18310301 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n243: 18032546 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n244: 18034065 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n245: 18698609 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n246: 18504270 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n247: 18662489 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n248: 18522209 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n249: 18189976 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n250: 18120748 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n251: 18571434 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n252: 18360370 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n253: 18535877 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n254: 18614773 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n255: 18213231 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n256: 18156948 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n257: 18409009 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n258: 18491281 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n259: 632347 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n260: 18638831 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n261: 18423090 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n262: 18076378 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n263: 18277192 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n264: 18492677 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n265: 18299358 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n266: 18404327 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n267: 18316280 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n268: 18495278 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n269: 18588520 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n270: 18421975 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n271: 18328827 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n272: 18683443 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n273: 18171061 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n274: 18034056 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n275: 18427656 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n276: 18251129 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n277: 18611981 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n278: 18430420 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n279: 18407305 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n280: 18396804 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n281: 18003220 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n282: 18043220 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n283: 18126322 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n284: 18561064 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n285: 18053995 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n286: 18413031 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n287: 18186050 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n288: 18372039 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n289: 18496459 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n290: 18456842 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n291: 18344814 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n292: 18070961 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n293: 18545517 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n294: 18089153 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n295: 18472667 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n296: 18041807 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n297: 18599599 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n298: 18206648 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n299: 18027309 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n300: 18498791 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n301: 18296852 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n302: 18309208 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n303: 18220910 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n304: 18370645 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n305: 18161028 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n306: 18043776 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n307: 18007267 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n308: 18334430 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n309: 18639725 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n310: 18476873 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n311: 18620161 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n312: 18280624 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n313: 18439556 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n314: 18186450 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n315: 18359555 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n316: 18695406 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n317: 18175562 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n318: 18074557 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n319: 18679919 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n320: 18355910 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n321: 18474192 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n322: 18672376 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n323: 18510796 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n324: 18247970 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n325: 18538109 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n326: 18570032 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n327: 18584969 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n328: 18427782 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n329: 17071766 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n330: 1516761 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n331: 1516762 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n</pre> In\u00a0[29]: Copied! <pre>prop_list_df['grocery_count'] = groceries_count\nprop_list_df['restaurant_count']= restaurants_count\nprop_list_df['hospitals_count']= hospitals_count\nprop_list_df['coffee_count']= coffee_count\nprop_list_df['bars_count']=bars_count\nprop_list_df['gas_count']=gas_count\nprop_list_df['shops_count']=shops_service_count\nprop_list_df['travel_count']=travel_transport_count\nprop_list_df['parks_count']=parks_count\nprop_list_df['edu_count']=education_count\nprop_list_df['commute_length']=route_length\nprop_list_df['commute_duration']=route_duration\nprop_list_df.head()\n</pre> prop_list_df['grocery_count'] = groceries_count prop_list_df['restaurant_count']= restaurants_count prop_list_df['hospitals_count']= hospitals_count prop_list_df['coffee_count']= coffee_count prop_list_df['bars_count']=bars_count prop_list_df['gas_count']=gas_count prop_list_df['shops_count']=shops_service_count prop_list_df['travel_count']=travel_transport_count prop_list_df['parks_count']=parks_count prop_list_df['edu_count']=education_count prop_list_df['commute_length']=route_length prop_list_df['commute_duration']=route_duration prop_list_df.head() Out[29]: Unnamed: 0 SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS ... hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration 0 618 MLS Listing Single Family Residential 144 SE 191st Pl Portland OR 97233.0 334900.0 4.0 2.5 ... 0 19 0 28 41 50 50 50 13.978589 25.621265 1 776 MLS Listing Single Family Residential 20028 NE Hoyt St Portland OR 97230.0 349950.0 3.0 2.5 ... 2 23 0 31 38 50 50 50 13.913465 24.726944 2 808 MLS Listing Single Family Residential 711 SE 160th Ave Portland OR 97233.0 358999.0 4.0 2.5 ... 0 16 0 34 47 50 50 50 14.644863 27.830024 3 881 MLS Listing Single Family Residential 5536 SE 142nd Pl Portland OR 97236.0 369800.0 4.0 2.5 ... 2 18 0 29 45 48 50 50 13.097669 25.943438 4 955 MLS Listing Single Family Residential 15707 SE Flavel Dr Portland OR 97236.0 379900.0 3.0 2.0 ... 0 13 0 17 44 10 50 41 14.254413 28.403598 <p>5 rows \u00d7 36 columns</p> In\u00a0[33]: Copied! <pre>prop_list_df.columns\n</pre> prop_list_df.columns Out[33]: <pre>Index(['Unnamed: 0', 'SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration'],\n      dtype='object')</pre> In\u00a0[37]: Copied! <pre>facility_list = ['grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration']\n\naxes = prop_list_df[facility_list].hist(bins=25, layout=(3,4), figsize=(15,10))\n</pre> facility_list = ['grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',        'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',        'edu_count', 'commute_length', 'commute_duration']  axes = prop_list_df[facility_list].hist(bins=25, layout=(3,4), figsize=(15,10)) <p>From the histograms above, most houses don't have very many bars in 5 miles around them. The commute length and duration appears to be tightly clustered around the lower end of the spectrum. Most houses have at least 1 hospital or medical center near them and a large number of parks, restaurants, educational institutions.</p> In\u00a0[30]: Copied! <pre>prop_list_df.to_csv('resources/houses_facility_counts.csv')\nprop_list_df.spatial.to_featureclass('resources/shp/houses_facility_counts.shp')\n</pre> prop_list_df.to_csv('resources/houses_facility_counts.csv') prop_list_df.spatial.to_featureclass('resources/shp/houses_facility_counts.shp') Out[30]: <pre>'resources/shp/houses_facility_counts.shp'</pre>"},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#feature-engineering-quantifying-access-to-facilities-batch-mode","title":"Feature engineering - quantifying access to facilities - batch mode\u00b6","text":"<p>This notebook is similar to previous (04_feature-engineering-neighboring-facilities), except, this one runs for all shortlisted facilities and adds these as features.</p>"},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#read-the-shortlisted-properties","title":"Read the shortlisted properties\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#loop-through-each-property-and-build-the-neighborhood-facility-table","title":"Loop through each property and build the neighborhood facility table\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#feature-engineer-with-access-to-amenities","title":"Feature engineer with access to amenities\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#plot-the-distribution-of-facility-access","title":"Plot the distribution of facility access\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#store-to-disk","title":"Store to disk\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/","title":"Score and rank properties using intrinsic and spatial features","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\nimport seaborn as sns\n\nfrom arcgis.gis import GIS\nfrom arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor\n</pre> import pandas as pd import matplotlib.pyplot as plt from pprint import pprint %matplotlib inline import seaborn as sns  from arcgis.gis import GIS from arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor In\u00a0[2]: Copied! <pre>gis = GIS(profile='')\n</pre> gis = GIS(profile='') In\u00a0[3]: Copied! <pre>prop_df = pd.read_csv('resources/houses_facility_counts.csv')\nprop_df = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE')\n</pre> prop_df = pd.read_csv('resources/houses_facility_counts.csv') prop_df = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE') In\u00a0[19]: Copied! <pre>prop_df.head()\n</pre> prop_df.head() Out[19]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration 0 MLS Listing Single Family Residential 144 SE 191st Pl Portland OR 97233.0 334900.0 4.0 2.5 Gresham, Sandy, Troutdale, Corbett ... 0 19 0 28 41 50 50 50 13.978589 25.621265 1 MLS Listing Single Family Residential 20028 NE Hoyt St Portland OR 97230.0 349950.0 3.0 2.5 Reeds Place ... 2 23 0 31 38 50 50 50 13.913465 24.726944 2 MLS Listing Single Family Residential 711 SE 160th Ave Portland OR 97233.0 358999.0 4.0 2.5 Portland Southeast ... 0 16 0 34 47 50 50 50 14.644863 27.830024 3 MLS Listing Single Family Residential 5536 SE 142nd Pl Portland OR 97236.0 369800.0 4.0 2.5 Portland Southeast ... 2 18 0 29 45 48 50 50 13.097669 25.943438 4 MLS Listing Single Family Residential 15707 SE Flavel Dr Portland OR 97236.0 379900.0 3.0 2.0 Portland Southeast ... 0 13 0 17 44 10 50 41 14.254413 28.403598 <p>5 rows \u00d7 35 columns</p> In\u00a0[20]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[20]: <pre>Index(['SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE', 'ZIP',\n       'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration'],\n      dtype='object')</pre> <p>Drop unnecessary columns</p> In\u00a0[4]: Copied! <pre>try:\n    prop_df.drop(columns=['Unnamed: 0'], inplace=True)\n    prop_df.drop(columns=['Unnamed: 0.1'], inplace=True)\nexcept:\n    pass\n</pre> try:     prop_df.drop(columns=['Unnamed: 0'], inplace=True)     prop_df.drop(columns=['Unnamed: 0.1'], inplace=True) except:     pass In\u00a0[5]: Copied! <pre>def set_scores(row):\n    score = ((row['PRICE']*-1.5) + # penalize by 1.5 times\n             (row['BEDS']*1)+\n             (row['BATHS']*1)+\n             (row['SQUARE FEET']*1)+\n             (row['LOT SIZE']*1)+\n             (row['YEAR BUILT']*1)+\n             (row['HOA PER MONTH']*-1)+  # penalize by 1 times\n             (row['grocery_count']*1)+\n             (row['restaurant_count']*1)+\n             (row['hospitals_count']*1.5)+  # reward by 1.5 times\n             (row['coffee_count']*1)+\n             (row['bars_count']*1)+\n             (row['shops_count']*1)+\n             (row['travel_count']*1.5)+  # reward by 1.5 times\n             (row['parks_count']*1)+\n             (row['edu_count']*1)+\n             (row['commute_length']*-1)+  # penalize by 1 times\n             (row['commute_duration']*-2)  # penalize by 2 times\n            )\n    return score\n</pre> def set_scores(row):     score = ((row['PRICE']*-1.5) + # penalize by 1.5 times              (row['BEDS']*1)+              (row['BATHS']*1)+              (row['SQUARE FEET']*1)+              (row['LOT SIZE']*1)+              (row['YEAR BUILT']*1)+              (row['HOA PER MONTH']*-1)+  # penalize by 1 times              (row['grocery_count']*1)+              (row['restaurant_count']*1)+              (row['hospitals_count']*1.5)+  # reward by 1.5 times              (row['coffee_count']*1)+              (row['bars_count']*1)+              (row['shops_count']*1)+              (row['travel_count']*1.5)+  # reward by 1.5 times              (row['parks_count']*1)+              (row['edu_count']*1)+              (row['commute_length']*-1)+  # penalize by 1 times              (row['commute_duration']*-2)  # penalize by 2 times             )     return score In\u00a0[29]: Copied! <pre>%%time\nprop_df['scores'] = prop_df.apply(set_scores, axis=1)\n</pre> %%time prop_df['scores'] = prop_df.apply(set_scores, axis=1) <pre>CPU times: user 78 ms, sys: 1.98 ms, total: 80 ms\nWall time: 101 ms\n</pre> In\u00a0[30]: Copied! <pre>prop_df.head()\n</pre> prop_df.head() Out[30]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores 0 MLS Listing Single Family Residential 144 SE 191st Pl Portland OR 97233.0 334900.0 4.0 2.5 Gresham, Sandy, Troutdale, Corbett ... 19 0 28 41 50 50 50 13.978589 25.621265 -495439.721119 1 MLS Listing Single Family Residential 20028 NE Hoyt St Portland OR 97230.0 349950.0 3.0 2.5 Reeds Place ... 23 0 31 38 50 50 50 13.913465 24.726944 -516614.867353 2 MLS Listing Single Family Residential 711 SE 160th Ave Portland OR 97233.0 358999.0 4.0 2.5 Portland Southeast ... 16 0 34 47 50 50 50 14.644863 27.830024 -528948.304911 3 MLS Listing Single Family Residential 5536 SE 142nd Pl Portland OR 97236.0 369800.0 4.0 2.5 Portland Southeast ... 18 0 29 45 48 50 50 13.097669 25.943438 -545169.484546 4 MLS Listing Single Family Residential 15707 SE Flavel Dr Portland OR 97236.0 379900.0 3.0 2.0 Portland Southeast ... 13 0 17 44 10 50 41 14.254413 28.403598 -559115.061609 <p>5 rows \u00d7 36 columns</p> In\u00a0[39]: Copied! <pre>prop_df['scores'].hist(bins=50);   # trailing ; suppresses matplotlib's prints\n</pre> prop_df['scores'].hist(bins=50);   # trailing ; suppresses matplotlib's prints In\u00a0[37]: Copied! <pre>sns.jointplot('PRICE','scores', data=prop_df);\n</pre> sns.jointplot('PRICE','scores', data=prop_df); <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n</pre> <p>From the chart above, it appears numeric columns with large values (such as Price) weild an undue importance on our model. To rectify this, we need to scale all numeric columns and normalize them with each other. The section below performs this and then tries to generate scores based on the scaled values.</p> In\u00a0[38]: Copied! <pre>prop_df.to_csv('resources/houses_scores_unscaled.csv')\n</pre> prop_df.to_csv('resources/houses_scores_unscaled.csv') In\u00a0[6]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()\n</pre> from sklearn.preprocessing import MinMaxScaler mm_scaler = MinMaxScaler() In\u00a0[7]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[7]: <pre>Index(['SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE', 'ZIP',\n       'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration'],\n      dtype='object')</pre> In\u00a0[8]: Copied! <pre>columns_to_scale = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'HOA PER MONTH','grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration']\n</pre> columns_to_scale = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',        'YEAR BUILT', 'HOA PER MONTH','grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',        'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',        'edu_count', 'commute_length', 'commute_duration'] In\u00a0[9]: Copied! <pre>scaled_array = mm_scaler.fit_transform(prop_df[columns_to_scale])\nprop_scaled = pd.DataFrame(scaled_array, columns=columns_to_scale)\nprop_scaled.head()\n</pre> scaled_array = mm_scaler.fit_transform(prop_df[columns_to_scale]) prop_scaled = pd.DataFrame(scaled_array, columns=columns_to_scale) prop_scaled.head() Out[9]: PRICE BEDS BATHS SQUARE FEET LOT SIZE YEAR BUILT HOA PER MONTH grocery_count restaurant_count hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration 0 0.000000 0.333333 0.1 0.016024 0.023259 0.368421 0.00 1.0 1.0 0.000000 0.38 0.0 0.56 0.82 1.00 1.0 1.00 0.005581 0.009332 1 0.041222 0.166667 0.1 0.049981 0.046518 0.315789 0.00 1.0 1.0 0.166667 0.46 0.0 0.62 0.76 1.00 1.0 1.00 0.005553 0.008905 2 0.066007 0.333333 0.1 0.028233 0.069777 0.157895 0.00 1.0 1.0 0.000000 0.32 0.0 0.68 0.94 1.00 1.0 1.00 0.005865 0.010386 3 0.095590 0.333333 0.1 0.014498 0.069777 0.736842 0.00 1.0 1.0 0.166667 0.36 0.0 0.58 0.90 0.96 1.0 1.00 0.005204 0.009486 4 0.123254 0.166667 0.0 0.025181 0.093036 0.000000 0.17 1.0 1.0 0.000000 0.26 0.0 0.34 0.88 0.20 1.0 0.82 0.005698 0.010660 In\u00a0[10]: Copied! <pre>prop_scaled['scores_scaled'] = prop_scaled.apply(set_scores, axis=1)\n</pre> prop_scaled['scores_scaled'] = prop_scaled.apply(set_scores, axis=1) <p>Check the influence of price on scores</p> In\u00a0[12]: Copied! <pre>sns.distplot(prop_scaled['scores_scaled'], bins=50)\n</pre> sns.distplot(prop_scaled['scores_scaled'], bins=50) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n</pre> Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a216e6198&gt;</pre> In\u00a0[13]: Copied! <pre>sns.jointplot(x='PRICE', y='scores_scaled', data=prop_scaled)\n</pre> sns.jointplot(x='PRICE', y='scores_scaled', data=prop_scaled) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n</pre> Out[13]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a206eab00&gt;</pre> <p></p> <p>After scaling, the scatter plot of <code>PRICE</code> VS <code>SCORES</code> appears random without much correlation. The histogram of scores also looks normally distributed. This is important as we want to consider all aspects of a house and weigh them according to their relative importance to the buyer. It is not much of a model if the scores are entirely based off one attribute (such as PRICE).</p> In\u00a0[13]: Copied! <pre>prop_df['scores_scaled'] = prop_scaled['scores_scaled']\nprop_df.head()\n</pre> prop_df['scores_scaled'] = prop_scaled['scores_scaled'] prop_df.head() Out[13]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores_scaled 0 MLS Listing Single Family Residential 144 SE 191st Pl Portland OR 97233.0 334900.0 4.0 2.5 Gresham, Sandy, Troutdale, Corbett ... 19 0 28 41 50 50 50 13.978589 25.621265 7.516793 1 MLS Listing Single Family Residential 20028 NE Hoyt St Portland OR 97230.0 349950.0 3.0 2.5 Reeds Place ... 23 0 31 38 50 50 50 13.913465 24.726944 7.563760 2 MLS Listing Single Family Residential 711 SE 160th Ave Portland OR 97233.0 358999.0 4.0 2.5 Portland Southeast ... 16 0 34 47 50 50 50 14.644863 27.830024 7.323591 3 MLS Listing Single Family Residential 5536 SE 142nd Pl Portland OR 97236.0 369800.0 4.0 2.5 Portland Southeast ... 18 0 29 45 48 50 50 13.097669 25.943438 8.036890 4 MLS Listing Single Family Residential 15707 SE Flavel Dr Portland OR 97236.0 379900.0 3.0 2.0 Portland Southeast ... 13 0 17 44 10 50 41 14.254413 28.403598 5.162985 <p>5 rows \u00d7 36 columns</p> In\u00a0[14]: Copied! <pre>sns.jointplot('PRICE', 'scores_scaled', data=prop_df)\n</pre> sns.jointplot('PRICE', 'scores_scaled', data=prop_df) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n</pre> Out[14]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a21a4d7b8&gt;</pre> In\u00a0[15]: Copied! <pre>prop_df_sorted = prop_df.sort_values(by='scores_scaled', ascending=False)\nprop_df_sorted.head(3)\n</pre> prop_df_sorted = prop_df.sort_values(by='scores_scaled', ascending=False) prop_df_sorted.head(3) Out[15]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores_scaled 87 MLS Listing Single Family Residential 15986 SE Spokane Ct. Ave Portland OR 97236.0 543900.0 4.0 3.5 Portland Southeast ... 50 2 34 46 50 50 50 5.796321 16.509734 10.179443 170 MLS Listing Multi-Family (2-4 Unit) SE Henderson St Portland OR 97206.0 625000.0 6.0 6.0 LENTS ... 50 1 50 40 50 50 50 8.380589 23.087985 10.174764 101 MLS Listing Single Family Residential 8268 SE Yamhill St Portland OR 97216.0 550000.0 7.0 4.0 Portland Southeast ... 50 1 50 43 50 50 50 6.330796 16.910622 10.140356 <p>3 rows \u00d7 36 columns</p> In\u00a0[16]: Copied! <pre>prop_df_sorted.reset_index(drop=True, inplace=True)\nprop_df_sorted.head()\n</pre> prop_df_sorted.reset_index(drop=True, inplace=True) prop_df_sorted.head() Out[16]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores_scaled 0 MLS Listing Single Family Residential 15986 SE Spokane Ct. Ave Portland OR 97236.0 543900.0 4.0 3.5 Portland Southeast ... 50 2 34 46 50 50 50 5.796321 16.509734 10.179443 1 MLS Listing Multi-Family (2-4 Unit) SE Henderson St Portland OR 97206.0 625000.0 6.0 6.0 LENTS ... 50 1 50 40 50 50 50 8.380589 23.087985 10.174764 2 MLS Listing Single Family Residential 8268 SE Yamhill St Portland OR 97216.0 550000.0 7.0 4.0 Portland Southeast ... 50 1 50 43 50 50 50 6.330796 16.910622 10.140356 3 MLS Listing Single Family Residential 6311 SE Tenino St Portland OR 97206.0 479900.0 4.0 2.5 Portland Southeast ... 50 2 44 48 50 50 50 7.299694 20.389635 10.035397 4 MLS Listing Multi-Family (2-4 Unit) 2028 SE Harold St Portland OR 97202.0 699900.0 5.0 4.0 SELLWOOD - WEST MORELAND ... 50 2 40 45 50 50 50 3.710354 11.486135 9.958218 <p>5 rows \u00d7 36 columns</p> In\u00a0[18]: Copied! <pre>prop_df_sorted['rank'] = prop_df_sorted.index\nprop_df_sorted.head(10)\n</pre> prop_df_sorted['rank'] = prop_df_sorted.index prop_df_sorted.head(10) Out[18]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores_scaled rank 0 MLS Listing Single Family Residential 15986 SE Spokane Ct. Ave Portland OR 97236.0 543900.0 4.0 3.5 Portland Southeast ... 2 34 46 50 50 50 5.796321 16.509734 10.179443 0 1 MLS Listing Multi-Family (2-4 Unit) SE Henderson St Portland OR 97206.0 625000.0 6.0 6.0 LENTS ... 1 50 40 50 50 50 8.380589 23.087985 10.174764 1 2 MLS Listing Single Family Residential 8268 SE Yamhill St Portland OR 97216.0 550000.0 7.0 4.0 Portland Southeast ... 1 50 43 50 50 50 6.330796 16.910622 10.140356 2 3 MLS Listing Single Family Residential 6311 SE Tenino St Portland OR 97206.0 479900.0 4.0 2.5 Portland Southeast ... 2 44 48 50 50 50 7.299694 20.389635 10.035397 3 4 MLS Listing Multi-Family (2-4 Unit) 2028 SE Harold St Portland OR 97202.0 699900.0 5.0 4.0 SELLWOOD - WEST MORELAND ... 2 40 45 50 50 50 3.710354 11.486135 9.958218 4 5 New Construction Home Single Family Residential 16010 SE Spokane Ct Portland OR 97236.0 518750.0 3.0 2.5 Peach Tree Meadows ... 2 34 46 50 50 50 5.796321 16.509734 9.929172 5 6 MLS Listing Multi-Family (2-4 Unit) 5850 SE 86th Ave Portland OR 97266.0 699000.0 6.0 6.0 LENTS ... 1 50 38 50 50 50 10.899918 21.144505 9.924611 6 7 MLS Listing Townhouse 2022 SE Harold St Portland OR 97202.0 699900.0 5.0 4.0 SELLWOOD - WEST MORELAND ... 2 40 45 50 50 50 3.713178 11.513058 9.880673 7 8 MLS Listing Single Family Residential 15965 SE Spokane Ct Portland OR 97236.0 488750.0 3.0 2.5 Portland Southeast ... 2 34 46 50 50 50 5.796321 16.509734 9.794684 8 9 MLS Listing Single Family Residential 16136 SE Tenino St Portland OR 97236.0 479900.0 3.0 2.5 Portland Southeast ... 2 36 46 50 50 50 4.998670 13.217221 9.761623 9 <p>10 rows \u00d7 37 columns</p> In\u00a0[29]: Copied! <pre>top_10_map = gis.map('Portland, OR')\ntop_10_map.basemap = 'gray'\ntop_10_map\n</pre> top_10_map = gis.map('Portland, OR') top_10_map.basemap = 'gray' top_10_map <pre>MapView(basemap='gray', layout=Layout(height='400px', width='100%'))</pre> <p></p> In\u00a0[39]: Copied! <pre>prop_df_sorted.head(50).spatial.plot(map_widget = top_10_map, \n                      renderer_type='c',\n                     method='esriClassifyNaturalBreaks',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='scores_scaled',\n                     cmap='autumn',  # matplotlib color map\n                     alpha=0.7,\n                     outline_color=[0,0,0,0])\n</pre> prop_df_sorted.head(50).spatial.plot(map_widget = top_10_map,                        renderer_type='c',                      method='esriClassifyNaturalBreaks',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='scores_scaled',                      cmap='autumn',  # matplotlib color map                      alpha=0.7,                      outline_color=[0,0,0,0]) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/arcgis/features/geo/_accessor.py:861: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._data[col] = self._data[col]\n/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/arcgis/features/geo/_accessor.py:1968: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._data['OBJECTID'] = list(range(1, self._data.shape[0] + 1))\n</pre> Out[39]: <pre>True</pre> In\u00a0[19]: Copied! <pre>interesting_columns=['grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration','PRICE','BEDS','BATHS','YEAR BUILT','SQUARE FEET']\n\nax_list = prop_df_sorted[interesting_columns].iloc[:49].hist(bins=25, layout=(5,4), figsize=(15,15))\nplt.tight_layout()\n</pre> interesting_columns=['grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',        'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',        'edu_count', 'commute_length', 'commute_duration','PRICE','BEDS','BATHS','YEAR BUILT','SQUARE FEET']  ax_list = prop_df_sorted[interesting_columns].iloc[:49].hist(bins=25, layout=(5,4), figsize=(15,15)) plt.tight_layout() In\u00a0[54]: Copied! <pre>column_set1 = ['hospitals_count','commute_length','commute_duration',\n               'gas_count', 'shops_count', 'rank']\ncolumn_set2 = ['rank','PRICE','BEDS','BATHS','YEAR BUILT','SQUARE FEET']\n\ngrd = sns.pairplot(data=prop_df_sorted[column_set1].iloc[:49])\nplt.title('How is rank correlated with location properties of a house?')\n</pre> column_set1 = ['hospitals_count','commute_length','commute_duration',                'gas_count', 'shops_count', 'rank'] column_set2 = ['rank','PRICE','BEDS','BATHS','YEAR BUILT','SQUARE FEET']  grd = sns.pairplot(data=prop_df_sorted[column_set1].iloc[:49]) plt.title('How is rank correlated with location properties of a house?') Out[54]: <pre>Text(0.5,1,'How is rank correlated with location properties of a house?')</pre> In\u00a0[55]: Copied! <pre>grd = sns.pairplot(data=prop_df_sorted[column_set2].iloc[:49])\nplt.title('How is rank correlated with intrinsic properties of a house?')\n</pre> grd = sns.pairplot(data=prop_df_sorted[column_set2].iloc[:49]) plt.title('How is rank correlated with intrinsic properties of a house?') Out[55]: <pre>Text(0.5,1,'How is rank correlated with intrinsic properties of a house?')</pre> In\u00a0[40]: Copied! <pre>prop_df_sorted.to_csv('resources/houses_ranked.csv')\nprop_df_sorted.spatial.to_featureclass('resources/shp/houses_ranked.shp')\n</pre> prop_df_sorted.to_csv('resources/houses_ranked.csv') prop_df_sorted.spatial.to_featureclass('resources/shp/houses_ranked.shp') Out[40]: <pre>'resources/shp/houses_ranked.shp'</pre>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#score-and-rank-properties-using-intrinsic-and-spatial-features","title":"Score and rank properties using intrinsic and spatial features\u00b6","text":"<p>So far we have shortlisted properties based on their intrinsic properties such as number of rooms, price, HoA etc. We have enriched them with spatial attributes such as access to facilities and distance to a point of interest (such as work). In this notebook, we weigh and sum these attributes to produce a score for each property. We finally sort them to pick the top few.</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#read-spatially-enriched-properties","title":"Read spatially enriched properties\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#apply-weights","title":"Apply weights\u00b6","text":"<p>In the following section, we determine the relative importance of each attribute and set weights accordingly</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#generate-scores-for-all-properties","title":"Generate scores for all properties\u00b6","text":"<p>We calculate the score for each property using the formula we defined in previous section</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#write-unscaled-data-to-disk","title":"Write unscaled data to disk\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#scale-data","title":"Scale data\u00b6","text":"<p>Although price plays an important role while selecting properties, the range and valuds of the <code>PRICE</code> column is much larger than any other. Thus, it influences more than its fair share. To resolve this, we scale the data using different techniques.</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#apply-scores-on-scaled-data","title":"Apply scores on scaled data\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#inverse-transform-data","title":"Inverse transform data\u00b6","text":"<p>Since we made a copy of the DataFrame when we scaled, we don't need to inverse transfrom the scaled data. We can simply copy the <code>scaled_scores</code> from this DataFrame and apply that to the original DataFrame</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#sort-by-scores","title":"Sort by scores\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#apply-a-rank-for-each-property-based-on-the-scores","title":"Apply a rank for each property based on the scores\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#visualize-the-top-50-properties","title":"Visualize the top 50 properties\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#investigate-statistical-distribution-facilities-in-the-top-50-shortlist","title":"Investigate statistical distribution facilities in the top 50 shortlist\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#how-is-rank-correlated-with-location-properties-of-houses","title":"How is rank correlated with location properties of houses?\u00b6","text":"<p>In the charts below we attempt to find if there exists a correlation between different facilities.</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#how-is-rank-correlated-with-intrinsic-properties-of-houses","title":"How is rank correlated with intrinsic properties of houses?\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#save-to-disk","title":"Save to disk\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/","title":"Building a housing recommendation engine","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\nimport seaborn as sns\n\nfrom arcgis.gis import GIS\nfrom arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor\n</pre> import pandas as pd import matplotlib.pyplot as plt from pprint import pprint %matplotlib inline import seaborn as sns  from arcgis.gis import GIS from arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor In\u00a0[2]: Copied! <pre>prop_df = pd.read_csv('resources/houses_ranked.csv')\nprop_df = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE')\n</pre> prop_df = pd.read_csv('resources/houses_ranked.csv') prop_df = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE') In\u00a0[3]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[3]: <pre>Index(['Unnamed: 0', 'SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration', 'scores_scaled',\n       'rank'],\n      dtype='object')</pre> In\u00a0[4]: Copied! <pre>prop_df.shape\n</pre> prop_df.shape Out[4]: <pre>(331, 38)</pre> <p>Generate a prefernce list that is <code>331</code> records long. This list has <code>1</code> for first <code>50</code> records followed by <code>0</code>.</p> In\u00a0[5]: Copied! <pre>preference_list = [1]*50\npreference_list.extend([0]*(331-50))\nlen(preference_list)\n</pre> preference_list = [1]*50 preference_list.extend([0]*(331-50)) len(preference_list) Out[5]: <pre>331</pre> In\u00a0[6]: Copied! <pre>prop_df['favorite'] = preference_list\n</pre> prop_df['favorite'] = preference_list In\u00a0[7]: Copied! <pre>prop_df.drop(columns=['Unnamed: 0','scores_scaled','rank'], inplace=True)\nprop_df.head()\n</pre> prop_df.drop(columns=['Unnamed: 0','scores_scaled','rank'], inplace=True) prop_df.head() Out[7]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration favorite 0 MLS Listing Single Family Residential 15986 SE Spokane Ct. Ave Portland OR 97236.0 543900.0 4.0 3.5 Portland Southeast ... 50 2 34 46 50 50 50 5.796321 16.509734 1 1 MLS Listing Multi-Family (2-4 Unit) SE Henderson St Portland OR 97206.0 625000.0 6.0 6.0 LENTS ... 50 1 50 40 50 50 50 8.380589 23.087985 1 2 MLS Listing Single Family Residential 8268 SE Yamhill St Portland OR 97216.0 550000.0 7.0 4.0 Portland Southeast ... 50 1 50 43 50 50 50 6.330796 16.910622 1 3 MLS Listing Single Family Residential 6311 SE Tenino St Portland OR 97206.0 479900.0 4.0 2.5 Portland Southeast ... 50 2 44 48 50 50 50 7.299694 20.389635 1 4 MLS Listing Multi-Family (2-4 Unit) 2028 SE Harold St Portland OR 97202.0 699900.0 5.0 4.0 SELLWOOD - WEST MORELAND ... 50 2 40 45 50 50 50 3.710354 11.486135 1 <p>5 rows \u00d7 36 columns</p> In\u00a0[8]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[8]: <pre>Index(['SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE', 'ZIP',\n       'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration', 'favorite'],\n      dtype='object')</pre> In\u00a0[11]: Copied! <pre>train_df = prop_df.drop(columns=['SALE TYPE','PROPERTY TYPE','ADDRESS', 'CITY', 'STATE', 'ZIP','LOCATION', \n                                'DAYS ON MARKET','PRICE PER SQFT','STATUS',\n                                 'URL', 'SOURCE', 'MLS', 'SHAPE','LATITUDE', 'LONGITUDE'])\ntrain_df.head()\n</pre> train_df = prop_df.drop(columns=['SALE TYPE','PROPERTY TYPE','ADDRESS', 'CITY', 'STATE', 'ZIP','LOCATION',                                  'DAYS ON MARKET','PRICE PER SQFT','STATUS',                                  'URL', 'SOURCE', 'MLS', 'SHAPE','LATITUDE', 'LONGITUDE']) train_df.head() Out[11]: PRICE BEDS BATHS SQUARE FEET LOT SIZE YEAR BUILT HOA PER MONTH grocery_count restaurant_count hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration favorite 0 543900.0 4.0 3.5 3178.0 6969.0 2018.0 50.0 20 50 6 50 2 34 46 50 50 50 5.796321 16.509734 1 1 625000.0 6.0 6.0 2844.0 6969.0 2018.0 0.0 20 50 6 50 1 50 40 50 50 50 8.380589 23.087985 1 2 550000.0 7.0 4.0 3038.0 6969.0 2018.0 0.0 20 50 4 50 1 50 43 50 50 50 6.330796 16.910622 1 3 479900.0 4.0 2.5 2029.0 3920.0 2018.0 0.0 20 50 6 50 2 44 48 50 50 50 7.299694 20.389635 1 4 699900.0 5.0 4.0 2582.0 6969.0 2016.0 0.0 20 50 8 50 2 40 45 50 50 50 3.710354 11.486135 1 In\u00a0[12]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()\n</pre> from sklearn.preprocessing import MinMaxScaler mm_scaler = MinMaxScaler() In\u00a0[13]: Copied! <pre>columns_to_scale = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'HOA PER MONTH','grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration']\n</pre> columns_to_scale = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',        'YEAR BUILT', 'HOA PER MONTH','grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',        'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',        'edu_count', 'commute_length', 'commute_duration'] In\u00a0[15]: Copied! <pre>scaled_array = mm_scaler.fit_transform(train_df[columns_to_scale])\nprop_scaled = pd.DataFrame(scaled_array, columns=columns_to_scale)\nprop_scaled.head()\n</pre> scaled_array = mm_scaler.fit_transform(train_df[columns_to_scale]) prop_scaled = pd.DataFrame(scaled_array, columns=columns_to_scale) prop_scaled.head() Out[15]: PRICE BEDS BATHS SQUARE FEET LOT SIZE YEAR BUILT HOA PER MONTH grocery_count restaurant_count hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration 0 0.572446 0.333333 0.3 0.448684 0.100778 0.947368 0.25 1.0 1.0 0.500000 1.0 1.0 0.68 0.92 1.0 1.0 1.0 0.002085 0.004983 1 0.794577 0.666667 0.8 0.321251 0.100778 0.947368 0.00 1.0 1.0 0.500000 1.0 0.5 1.00 0.80 1.0 1.0 1.0 0.003189 0.008123 2 0.589154 0.833333 0.4 0.395269 0.100778 0.947368 0.00 1.0 1.0 0.333333 1.0 0.5 1.00 0.86 1.0 1.0 1.0 0.002313 0.005175 3 0.397151 0.333333 0.1 0.010301 0.046518 0.947368 0.00 1.0 1.0 0.500000 1.0 1.0 0.88 0.96 1.0 1.0 1.0 0.002727 0.006835 4 0.999726 0.500000 0.4 0.221290 0.100778 0.842105 0.00 1.0 1.0 0.666667 1.0 1.0 0.80 0.90 1.0 1.0 1.0 0.001193 0.002586 In\u00a0[29]: Copied! <pre>prop_scaled.columns\n</pre> prop_scaled.columns Out[29]: <pre>Index(['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE', 'YEAR BUILT',\n       'HOA PER MONTH', 'grocery_count', 'restaurant_count', 'hospitals_count',\n       'coffee_count', 'bars_count', 'gas_count', 'shops_count',\n       'travel_count', 'parks_count', 'edu_count', 'commute_length',\n       'commute_duration'],\n      dtype='object')</pre> In\u00a0[30]: Copied! <pre>prop_scaled.info()\n</pre> prop_scaled.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 331 entries, 0 to 330\nData columns (total 19 columns):\nPRICE               331 non-null float64\nBEDS                331 non-null float64\nBATHS               331 non-null float64\nSQUARE FEET         331 non-null float64\nLOT SIZE            331 non-null float64\nYEAR BUILT          331 non-null float64\nHOA PER MONTH       331 non-null float64\ngrocery_count       331 non-null float64\nrestaurant_count    331 non-null float64\nhospitals_count     331 non-null float64\ncoffee_count        331 non-null float64\nbars_count          331 non-null float64\ngas_count           331 non-null float64\nshops_count         331 non-null float64\ntravel_count        331 non-null float64\nparks_count         331 non-null float64\nedu_count           331 non-null float64\ncommute_length      331 non-null float64\ncommute_duration    331 non-null float64\ndtypes: float64(19)\nmemory usage: 49.2 KB\n</pre> In\u00a0[31]: Copied! <pre>X = prop_scaled\ny = train_df['favorite']\n</pre> X = prop_scaled y = train_df['favorite'] In\u00a0[32]: Copied! <pre>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33)\n\n(len(X_train), len(X_test))\n</pre> from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33)  (len(X_train), len(X_test)) Out[32]: <pre>(221, 110)</pre> In\u00a0[33]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(verbose=1)\n</pre> from sklearn.linear_model import LogisticRegression log_model = LogisticRegression(verbose=1) In\u00a0[34]: Copied! <pre>log_model.fit(X_train, y_train)\n</pre> log_model.fit(X_train, y_train) <pre>[LibLinear]</pre> Out[34]: <pre>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=1, warm_start=False)</pre> In\u00a0[35]: Copied! <pre>test_predictions = log_model.predict(X_test)\n</pre> test_predictions = log_model.predict(X_test) In\u00a0[36]: Copied! <pre>test_predictions\n</pre> test_predictions Out[36]: <pre>array([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0])</pre> In\u00a0[37]: Copied! <pre>from sklearn.metrics import classification_report, confusion_matrix\n</pre> from sklearn.metrics import classification_report, confusion_matrix In\u00a0[39]: Copied! <pre>from pprint import pprint\npprint(classification_report(y_test, test_predictions, target_names=['not fav','fav']))\n</pre> from pprint import pprint pprint(classification_report(y_test, test_predictions, target_names=['not fav','fav'])) <pre>('             precision    recall  f1-score   support\\n'\n '\\n'\n '    not fav       0.94      0.98      0.96        89\\n'\n '        fav       0.88      0.71      0.79        21\\n'\n '\\n'\n 'avg / total       0.93      0.93      0.92       110\\n')\n</pre> In\u00a0[40]: Copied! <pre>tn, fp, fn, tp = confusion_matrix(y_test, test_predictions).ravel()\ntn, fp, fn, tp\n</pre> tn, fp, fn, tp = confusion_matrix(y_test, test_predictions).ravel() tn, fp, fn, tp Out[40]: <pre>(87, 2, 6, 15)</pre> In\u00a0[41]: Copied! <pre>coeff = log_model.coef_.round(5).tolist()[0]\nlist(zip(X_train.columns, coeff))\n</pre> coeff = log_model.coef_.round(5).tolist()[0] list(zip(X_train.columns, coeff)) Out[41]: <pre>[('PRICE', -0.4817),\n ('BEDS', 0.56799),\n ('BATHS', 0.65258),\n ('SQUARE FEET', 0.09618),\n ('LOT SIZE', -0.10108),\n ('YEAR BUILT', 0.86107),\n ('HOA PER MONTH', 0.02129),\n ('grocery_count', -0.7736),\n ('restaurant_count', -1.22493),\n ('hospitals_count', 1.38967),\n ('coffee_count', 1.27494),\n ('bars_count', 2.9728),\n ('gas_count', 1.16501),\n ('shops_count', -0.71489),\n ('travel_count', 0.24195),\n ('parks_count', -1.02031),\n ('edu_count', -0.45057),\n ('commute_length', -0.58029),\n ('commute_duration', -0.59949)]</pre> In\u00a0[44]: Copied! <pre>log_model.intercept_\n</pre> log_model.intercept_ Out[44]: <pre>array([-1.46905595])</pre>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#building-a-housing-recommendation-engine","title":"Building a housing recommendation engine\u00b6","text":"<p>So far, we have feature engineered our data set with location specific features. We explicitly defined weights for different attributes and arrived at a rank. Instead, we could simply like and dislike a few houses and let a machine learning model infer our preferences based on that. That is what this notebook tries to do.</p> <p>Since it is time consuming to like and dislike a large number of properties, we pick the top 50 notebooks from our previous rank and like them all. We dislike the remaining ones.</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#read-ranked-dataset","title":"Read ranked dataset\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#generate-preference-column","title":"Generate preference column\u00b6","text":"<p>We will pick the top 50 records and provide a positive preference to them. Then we will drop the score and rank columns and let the machine learning algorithm learn our preferences.</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#drop-rank-scores_scaled-columns-from-dataframe","title":"Drop <code>rank</code>, <code>scores_scaled</code> columns from DataFrame\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#one-hot-encoding","title":"One hot encoding\u00b6","text":"<p>We drop more columns that don't really determine a buyer's preference</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#scale-numeric-columns","title":"Scale numeric columns\u00b6","text":"<p>We use the same <code>MinMaxScaler</code> we used earlier to scale the data.</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#split-dataset-into-training-and-test","title":"Split dataset into training and test\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#logistic-regression","title":"Logistic Regression\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#model-evaluation","title":"Model evaluation\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#model-inference","title":"Model inference\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#recommendation-engines","title":"Recommendation engines\u00b6","text":"<p>From the example above, we could build a recommendation engine that runs on periodically on a newer set of properties and determines which ones are worth your time (one's it predicts you would 'like'). The ML model's weights appear similar to what we defined manually. In some cases, it goes way off.</p> <p>This type of recommendation is called 'content based filtering' and for this to work, we need a really large training set. In reality nobody can sit and generate such a large set. In practice, another type of recommendations called 'community based filtering' is used. Based on the features identified for the properties, it tries to find similarity between buyers and pools the training set for all similar buyers together to create a really large training set and learns from that.</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#overall","title":"Overall\u00b6","text":"<p>In these sets of notebooks, we observed how data science and machine learning approaches can be employed in the real estate industry. Buying houses is a very personal process, however a lot of decisions are heavily influenced by the location of the houses. We showed how Python libraries such as Pandas can be used to statistically analyze the properties. We also showed how the ArcGIS API for Python adds spatial capabilities to Pandas allowing to perform spatial data analysis. We enriched the data with information on access to different facilities and used that to compare, score and rank the properties. The shortlist we arrived at can be used for field visits.</p> <p>We conclude with a forward thinking approach to turn this into a recommendation engine and suggest scope for future work in this area.</p>"},{"location":"teaching_resources/front-end/html/","title":"HTML - files that make up the web","text":"<p>TOC</p> <ul> <li>HTML skeleton</li> <li>HTML tags</li> </ul>"},{"location":"teaching_resources/front-end/html/#html-skeleton","title":"HTML skeleton","text":"<ul> <li>HTML - this is the opening and ending tag. All fills within these tags</li> <li><code>&lt;!-- comment --&gt;</code> in html</li> <li>Head - consider like header, import region. It links to important resource files like CSS, JS scripts.</li> <li>Body - main content of the web page</li> <li>Divisions <code>&lt;div&gt; &lt;/div&gt;</code> constitute the various segments of the page. It is common to nest the divisions.</li> <li>Scripts - scripts are generally at the end because browsers load the page in the order they see the html. Hence if the scripts modify the html, they need to follow the html regions they modify.</li> </ul>"},{"location":"teaching_resources/front-end/html/#html-tags","title":"HTML tags","text":"<p>You write content between the tags</p> <pre><code>&lt;html&gt; &lt;/html&gt;\n&lt;head&gt; &lt;/head&gt;\n&lt;!-- --&gt; for comments\n&lt;meta charset='utf-8'&gt; tells browser how to read the document\n&lt;title&gt; &lt;/title&gt; for browser tab text\n&lt;link href='path' rel='root path'&gt; for links to other files\n&lt;body&gt; &lt;/body&gt;\n&lt;div id=''&gt; &lt;/div&gt; for each section of the path is a division. Divisions can be nested within one another\n&lt;p&gt; put new paragraphs &lt;/p&gt;\n&lt;ul&gt; unordered / bulleted lists &lt;/ul&gt;\n&lt;ol&gt; ordered / numbered lists &lt;/ol&gt;\n&lt;li&gt; list item, for both ol and ul &lt;/li&gt;\n&lt;h1&gt; &lt;/h1&gt; for headings. cycle the numbers through. 1 is largest heading\n&lt;a href=\"path\"&gt;hyperlink text &lt;/a&gt;\n</code></pre> <p>Now to the parts of html pertaining to JS</p> <pre><code>&lt;script src='path to js file'&gt;&lt;/script&gt;\n&lt;script&gt; and between the script tags, you can write JS code. &lt;/script&gt;\n// for JS single line comments\n/* for multi line */\n</code></pre>"},{"location":"teaching_resources/front-end/js_essentials/","title":"Javascript essentials","text":"<ul> <li> <p>Use <code>;</code> to terminate lines. JS supports OO and functional programming. HTML5 is HTML + CSS+ JS.</p> </li> <li> <p>JS can be run in browsers and outside - node.js, windows script host</p> </li> <li>JS cannot access file system since the browser runs it in a sandbox. user needs to give explicit permission.</li> </ul> <p>Table of contents</p> <ul> <li>Placement</li> <li>Types and programming constructs</li> <li>switches</li> <li>loops - while</li> <li>functions</li> <li>Objects</li> <li>namespaces</li> <li>Appendix - documentation</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#placement","title":"Placement","text":"<p>Place <code>&lt;script&gt;&lt;/script&gt;</code> within the end of <code>&lt;body&gt;</code> tags. This ensures the page is fully loaded since your JS scripts may have to read elements of the page itself. You can also run a html file with JS in a browser. It loads as a file. This is not scalable for large sites. You need a web server - use <code>node.js</code>.</p> <p>If your JS is in a file, then link to that using the format <code>&lt;script src=\"./scripts/site.js\"&gt;</code> Note Dont self close the script tag <code>(&lt;script/&gt;)</code></p> <ul> <li> <p>sequence of operation, is by the content of the html page. So if you have two scripts 1, 2 they are executed by their precedence in the html page. This same as a stand-alone python script.</p> </li> <li> <p>more nuance. all contents within the same <code>&lt;script&gt;</code> tag is loaded at the same time. If you write a funciton in one <code>&lt;script&gt;</code> tag and call it from another tag, then you need to place the scripts in sequence.</p> </li> <li> <p>JS does a 2 pass loading. It first passes to find all the variables, objects and functions. In the second pass it actually executes. However if you are playing with function assignment, it would not happen until execution, so you need to be careful and not call it before it is available.</p> </li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#types-and-programming-constructs","title":"Types and programming constructs","text":"<ul> <li><code>var</code> is dynamic object</li> <li>comments - use <code>//</code> for single line and <code>/*  */</code> for multiline</li> <li>you can also use html comments <code>&lt;!-- and --&gt;</code></li> <li>ternary operator <code>condition ? expr1 : expr2</code></li> <li>binary operator <code>1+2</code>, <code>false + \"it failed\"</code>, <code>true + true</code>, <code>\"str1\" + \"str2\"</code></li> <li>auto increment <code>++variable</code> is prefix increment and <code>variable++</code> is postfix increment - which is use and then increment.</li> <li><code>a += 10</code> is same as <code>a=a+10</code></li> <li>string, bool, number are primitive types</li> <li>compostite types: objects, arrays, date</li> <li><code>undefined</code> is when a variable is declared but not assigned a value.</li> <li><code>null</code> is not same as Python <code>None</code>, it is also different from <code>undefined</code>. <code>null</code> is the lack of any type.</li> <li><code>typeof(obj)</code> for type inspection</li> <li><code>a===1</code> the triple equals is a script value check</li> <li>checking <code>\"12\" == 12</code> will return true! Also checking <code>\"12.0\"==12\"</code> will return true. If you are checking for object type equality, then use triple equals.</li> <li><code>arrayobj.pop()</code> to get remove and get the last element. Use <code>arrayobj.push(value)</code> to insert at the end.</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#switches","title":"switches","text":"<pre><code>switch(variable){\n    case 'a':\n        break;\n    case 'b':\n        statements\n        break;\n    default:\n        statements\n        break;\n}\n</code></pre>"},{"location":"teaching_resources/front-end/js_essentials/#loops-while","title":"loops - while","text":"<pre><code>while(expression){\n    statements;\n    console.log();\n}\n</code></pre> <p>Note: When looping through arrays, dont use <code>for (element in array)</code> loop. Always use the regular for loop</p>"},{"location":"teaching_resources/front-end/js_essentials/#functions","title":"functions","text":"<ul> <li>JS does not support overloading.</li> </ul> <pre><code>function add(par1, par2){\n    //statement\n    return value;\n}\n</code></pre> <ul> <li>when defining a function, dont use <code>var parameter</code>, use just the variable name</li> <li>all parameters are optional</li> <li>you can also pass more than the declared number of parameters to the function.</li> <li>disambiguate named functions - cannot. Use need to use namespaces and modular pattern.</li> <li>built-in variable <code>arguments[]</code> to get all the arguments passed to the function.</li> <li>declare variables within functions using <code>var</code>. If you use without <code>var</code>, then you are using or overwriting the global version of the same variable!</li> <li>type <code>\"use strict\"</code> in your script to print all errors and break on them. Place the strict within your functions and not at a gobal scope - this is because you will use other folks libraries and they may not be up to high standards of coding.</li> <li>JS does not support default arguments!!</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#objects","title":"Objects","text":"<ul> <li>In object literal notation, you define an object like a JSON file  <code>js  var person={     firstName:\"atma\",     lastName:\"mani\"  };</code></li> <li>You can extend an object with new properties and assign them at runtime. (same as in a property map in Python or a dict during runtime)</li> <li>You can use <code>for in</code> loop to reflect over the kvp of an array  <code>js  for (var key in object){     console.log(key, object[key]);  }</code></li> <li>use <code>JSON.stringify()</code> to serialize it to JSON and to hydrate it back, use <code>JSON.parse()</code>.</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#namespaces","title":"namespaces","text":"<ul> <li>while JS does not give you a great namespace construct, you can fashion one by creating an object (of object class) and add your methods, classes to it.</li> </ul> <pre><code>if (!yourNamespace){\n    var yourNamespace={}; //this is checked everytime yoru scrip is loaded\n}\n\n//extend it\nyourNamespace.logic1 = function(){};\nyourNamespace.obj1 = {\n    property1:'value'\n}\n ```\n\n### strings\n - strings are immutable\n - you can call regular methods like `strobj.length`, `str.split()`, `str.trim()` to get another string returned by that method.\n - you can also access individual elements of the string just like in an array `stringobj[index]`\n\n### dates\n - `var rightNow = new Date();`\n\n## JS and DOM\n - You can write JS into `&lt;button&gt;` tags\n - JS can read the elements of the html page. It uses the `id` property of the tags (elements)\n - YOu can debug JS from chrome, set breakpoints, use watch window and variable explorer.\n - the console window of chrome is a good JS sandbox. You can simply type commands in the console and use JS\n - you can do the same from console after starting `node`.\n\n## Functional programming\nFor essential purposes, functional programming is declaring functions and passing them around using their references. You can extend the functionality or modify it.\n\n### Closures\n - closures mean, if you have nested functions, the inner function has access to the arguments and variables created in the outer and also modify them.\n - with closures pattern, you can return an object that points to the nested functions as methods. with this you can instantiate an object from a method (blown mind) and \n\n## Inheritance\n - `prototyping` concept allows you to not only inherit, but also extend your base class.\n```js\nclassname.prototype.your_extension = function(){logic};\n</code></pre> <ul> <li>when inheriting your own objects, you will use the <code>call()</code> method to call the super's constructor</li> </ul> <pre><code>function Animal(name){\n                this.name=name;\n            }\nfunction Person(name, age){\n                Animal.call(this, name); //calling the super's constructor\n                this.age = age;\n            }\n</code></pre> <ul> <li>if you extended your base class using a constructor, then you got to also inherit the prototypes like below:</li> </ul> <pre><code>Animal.prototype.speak = function(){\n                alert(this.name +  \" is barking\");\n            }\n//then you in herit the prototype as\nPerson.prototype = Object.create(Animal.prototype);\n//when you inherit, you overwrite your own constructor, so you set it back.\nPerson.prototype.constructor = Person;\n</code></pre> <ul> <li>when inheriting prototypes, you can inherit from any other object. Thus, you can inherit from one object and inherit the prototypes from another.</li> <li>you can, in this way, have prototype chaining.</li> <li>The concept <code>defineProperties</code> allows you to declare some properties as read only, making it not possible for inheriting objects to change some properties. It also gives you getters and setters.</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#ecmascript-6-pattern","title":"ECMAScript 6 pattern","text":"<ul> <li>ECMAScript 6 makes creating classes way more like a proper OO language. You get <code>class</code>, <code>constructor()</code> <code>super</code> keywords and looses a lot of bad verbage making things simpler</li> <li>You really dont play with prototypes in ecma6.</li> <li>you get <code>arrow</code> functions, which are lambda functions, similar to that in C#.</li> </ul> <pre><code>()=&gt; {/*function declaration*/}\n(x,y)=&gt; {/*function declaration*/} //for a function wiht one or more args\n</code></pre> <ul> <li>you read <code>=&gt;</code> as <code>goes to</code>.</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#dom","title":"DOM","text":"<ul> <li>tree of nodes. Each tag is a node.</li> <li>JS can parse the DOM, modify it based on event the user is triggering.</li> <li>methods like <code>createElement</code>, <code>getElementById</code>, <code>getElementByName</code>, <code>getElementByTagName</code> lets you read DOM or search DOM.</li> <li>You can inject html by setting <code>&lt;div&gt;</code> tags <code>innerHTML</code> to the html text you need.</li> <li>since the browser support varies a lot, you can use http://caniuse.com to find support across browsers.</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#jquery-for-dom-manipulation","title":"jQuery for DOM manipulation","text":"<ul> <li>jQuery is a library and a very popular one. You can use the Google CDN or download and put it in your server as well. https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js is Google's CDN.</li> <li>If you are installing this locally instead of using the CDN, then use <code>npm</code> and <code>bower</code>. Bower will install it in the root of your site, so when you host your site, you will not forget to host the dependent libraries.</li> </ul> <pre><code>npm install -g bower\nbower install jquery\n</code></pre> <ul> <li>a couple of two different package managers is <code>yarn</code> and <code>webpack</code>.</li> <li>jQuery tutorial https://www.w3schools.com/jquery/</li> <li>a case for installing the lib locally is, you can step-in to the jQuery code when needed to understand the implementaiton.</li> <li>when you call jQuery, you dont get back a DOM. Instead you get back a jQuery result object.</li> <li>in code, <code>$</code> is a shorthand representation of lib name <code>jQuery</code>.</li> </ul> <pre><code>$(document).ready(function(){\n    console.log(\"page loaded, ready for JS\")\n    })\n\n//general syntax is \n$(selector).action()\n</code></pre> <ul> <li>it is good to check if the page has fully loaded before running JS. you can check it like above.</li> <li>a programming pattern is to use <code>$</code> suffix for jQuery result sets to differentiate from JS variables</li> </ul> <pre><code>var divs$ = $.('div'); //get all the selectors of div\n</code></pre>"},{"location":"teaching_resources/front-end/js_essentials/#ajax-jquery-library-for-io-with-backend","title":"AJAX - jQuery library for IO with backend","text":"<ul> <li>operations are async. AJAX - Asynchronous Javascript And XML. AJAX is used to make calls to backend server or any remote server via HTTP, REST.</li> <li>AJAX call will return a value called a <code>promise</code>, while the rest of the script is executed.</li> <li>AJAX object is a <code>XHR</code> - XML HTTP Request object - something microsoft came up. There is no JSON in the name since it was not invented yet. However, today's tech still keeps the name XHR to send and receive JSON.</li> <li>AJAX methods - <code>$.get(url, data, success, dataType)</code>, <code>$.getJSON()</code>, <code>$.post()</code>, <code>.load(url, data, complete)</code> are some popular methods. </li> <li>When doing async work, you use the <code>$.ajax()</code> which will give you a <code>promise</code> and you will call <code>$.then(function(){})</code> which will perform the operation once the async op is complete.</li> <li>use <code>$.ajaxSetup({})</code> to declare some defaults like the url prefix, method, dataType etc and this will apply for all calls made.</li> <li><code>JSONP</code> is JSON with padding.</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#node-js","title":"Node JS","text":"<ul> <li>With node JS, you can power up a server and a client set up. See the node js example.</li> </ul>"},{"location":"teaching_resources/front-end/js_essentials/#appendix-run-web-server-using-nodejs","title":"Appendix - Run web server using node.js","text":"<p>In terminal, after installing node.js, type:</p> <pre><code>npm install http-server -g\n</code></pre> <p>This installs a server, help for that server. Then to start a web server, navigate to the folder with website and run</p> <pre><code>http-server -p 3000\n</code></pre> <p>You can run this command either from terminal or from the terminal window in vs code.</p>"},{"location":"teaching_resources/front-end/js_essentials/#appendix-documentation","title":"Appendix - documentation","text":"<ul> <li>w3 school - gives high level documentation</li> <li>mozilla developer network - mdn js</li> </ul> <p>Typescript transpiles into Javascript!!</p>"},{"location":"teaching_resources/front-end/web_development/","title":"Web Development","text":""},{"location":"teaching_resources/front-end/web_development/#axioms","title":"Axioms","text":"<ul> <li>file names - no spaces, all lower case. Use <code>-</code> or <code>_</code> to separate words</li> <li>file paths - all relative</li> </ul>"},{"location":"teaching_resources/front-end/web_development/#structure-of-websites","title":"Structure of websites","text":"<ul> <li>index.html</li> <li>images folder</li> <li>scripts folder - JS files</li> <li>styles folder - CSS files</li> </ul>"},{"location":"teaching_resources/front-end/web_development/#css","title":"CSS","text":"<p>Cascading Style Sheets. Structure of a CSS <code>ruleset</code>: </p> <ul> <li>use <code>property : property_value</code> notation for a rule</li> <li>within a ruleset, separate each rule by a <code>;</code></li> <li>select and apply same set of ruleset on multiple elements like this  <code>CSS     p,li,h1{             color: red;             width: 50px;     }</code></li> <li>the above are <code>element selectors</code> - they select specific html tags to apply the ruleset</li> <li><code>ID selectors</code> - selects a specific ID of a html tag. use as   <code>CSS     #my_id{             property: value;             other rules;     }</code></li> <li><code>class selectors</code> - select classes <code>.my_class</code></li> <li><code>attribute selectors</code> - all elements with specified attributes - <code>img[src]</code> applies all  tags with <code>src</code> attribute.</li> <li><code>pseudo class selectors</code> - selects elements in a specified state. For instance <code>a:hover</code> selects <code>&lt;a&gt;</code> anchor tags when mouse pointer is hovering over it.</li> </ul> <p>It is intersting to note that in web development, you can make use of a number of resources that are already hosted. For instance, you can pick a font from Google's font index hosted at fonts.google.com. To do this, add the font source as a resource in the html's <code>&lt;head&gt;</code> tags: <code>&lt;link href=\"https://fonts.googleapis.com/css?family=Droid+Serif\" rel=\"stylesheet\"&gt;</code> and edit the CSS to specify which font from this family: </p> <pre><code>html{\n    font-family: 10px;\n    font-family: 'Droid Serif', serif;\n}\n</code></pre>"},{"location":"teaching_resources/front-end/web_development/#js","title":"JS","text":"<p>You got to link the JS files to the html file, preferably in the end before <code>&lt;\\body&gt;</code> section like this : <code>&lt;script src='scripts/main.js'&gt;&lt;/script&gt;</code>. This is because JS modifies the contents of this html file and it should be executed last else it would not know of the contents of this page.</p> <ul> <li>Variable declaration <code>var myVariable = value;</code></li> <li>statements end with <code>;</code></li> <li>symbols are case sensitive</li> <li>Data types - <code>String</code>, <code>Number</code>, <code>Boolean</code>, <code>Array</code>, <code>Object</code>.</li> <li>JS is not strongly typed</li> <li><code>Number</code> can hold both integer and float</li> <li><code>Boolean</code> is <code>true</code> and <code>false</code>, in lower cases</li> <li><code>Array</code> can hold multiple data types <code>[1, 'bob', 39.45, obj2]</code></li> <li><code>Object</code> - everything in JS is an obj.</li> <li>Comments <code>//</code> for single line and <code>/* -- */</code> for multi line</li> <li>Operators in JS</li> <li><code>+ - * /</code> for math operations</li> <li><code>=</code> for assignment</li> <li><code>===</code> for equality checking, not its triple and not double equals</li> <li><code>!</code>, <code>!==</code> for negation</li> </ul>"},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/","title":"GDAL - Inspecting rasters","text":"<p>GDAL - Geospatial Data Abstration Library</p> In\u00a0[2]: Copied! <pre>cd /Users/abharathi/Documents/gis_data/gdal-tools/\n</pre> cd /Users/abharathi/Documents/gis_data/gdal-tools/ <pre>/Users/abharathi/Documents/gis_data/gdal-tools\n</pre> In\u00a0[3]: Copied! <pre>ls\n</pre> ls <pre>1870_southern-india.jpg* geonames/                prism/\nbatch.py*                landsat8/                spatial_query.gpkg\nbatch_parallel.py*       london_1m_dsm/           srtm/\nearth_at_night.jpg*      naip/                    worldcities.csv*\nearthquakes/             precipitation.gpkg\n</pre> <p>GDAL and OGR started as two different programs, hence have a different usage pattern or design.</p> <p>Why use GDAL</p> <ul> <li>faster</li> <li>smaller, fits headless, remote execution</li> <li>FOSS</li> <li>great for server-side execution</li> <li>great for compression</li> </ul> <p>Speed</p> <ul> <li>GDAL is fast. If we use Python to kick off GDAL as a CLI - it is still fast (<code>os.system</code>)</li> <li>If we use Python bindings, they are much slower than using raw GDAL</li> </ul> In\u00a0[5]: Copied! <pre>!which gdalinfo\n</pre> !which gdalinfo <pre>/Users/abharathi/micromamba/envs/opengeo/bin/gdalinfo\n</pre> In\u00a0[13]: Copied! <pre>!gdalinfo --version\n</pre> !gdalinfo --version <pre>GDAL 3.6.2, released 2023/01/02\n</pre> In\u00a0[11]: Copied! <pre>ls /Users/abharathi/micromamba/envs/opengeo/bin/ogr*\n</pre> ls /Users/abharathi/micromamba/envs/opengeo/bin/ogr* <pre>/Users/abharathi/micromamba/envs/opengeo/bin/ogr2ogr*\n/Users/abharathi/micromamba/envs/opengeo/bin/ogr_layer_algebra.py*\n/Users/abharathi/micromamba/envs/opengeo/bin/ogrinfo*\n/Users/abharathi/micromamba/envs/opengeo/bin/ogrlineref*\n/Users/abharathi/micromamba/envs/opengeo/bin/ogrmerge.py*\n/Users/abharathi/micromamba/envs/opengeo/bin/ogrtindex*\n</pre> In\u00a0[15]: Copied! <pre># formats supported by gdal\n!gdalinfo --formats\n</pre> # formats supported by gdal !gdalinfo --formats <pre>Supported Formats:\n  VRT -raster,multidimensional raster- (rw+v): Virtual Raster\n  DERIVED -raster- (ro): Derived datasets using VRT pixel functions\n  GTiff -raster- (rw+vs): GeoTIFF\n  COG -raster- (wv): Cloud optimized GeoTIFF generator\n  NITF -raster- (rw+vs): National Imagery Transmission Format\n  RPFTOC -raster- (rovs): Raster Product Format TOC format\n  ECRGTOC -raster- (rovs): ECRG TOC format\n  HFA -raster- (rw+v): Erdas Imagine Images (.img)\n  SAR_CEOS -raster- (rov): CEOS SAR Image\n  CEOS -raster- (rov): CEOS Image\n  JAXAPALSAR -raster- (rov): JAXA PALSAR Product Reader (Level 1.1/1.5)\n  GFF -raster- (rov): Ground-based SAR Applications Testbed File Format (.gff)\n  ELAS -raster- (rw+v): ELAS\n  ESRIC -raster- (rov): Esri Compact Cache\n  AIG -raster- (rov): Arc/Info Binary Grid\n  AAIGrid -raster- (rwv): Arc/Info ASCII Grid\n  GRASSASCIIGrid -raster- (rov): GRASS ASCII Grid\n  ISG -raster- (rov): International Service for the Geoid\n  SDTS -raster- (rov): SDTS Raster\n  DTED -raster- (rwv): DTED Elevation Raster\n  PNG -raster- (rwv): Portable Network Graphics\n  JPEG -raster- (rwv): JPEG JFIF\n  MEM -raster,multidimensional raster- (rw+): In Memory Raster\n  JDEM -raster- (rov): Japanese DEM (.mem)\n  GIF -raster- (rwv): Graphics Interchange Format (.gif)\n  BIGGIF -raster- (rov): Graphics Interchange Format (.gif)\n  ESAT -raster- (rov): Envisat Image Format\n  FITS -raster,vector- (rw+): Flexible Image Transport System\n  BSB -raster- (rov): Maptech BSB Nautical Charts\n  XPM -raster- (rwv): X11 PixMap Format\n  BMP -raster- (rw+v): MS Windows Device Independent Bitmap\n  DIMAP -raster- (rovs): SPOT DIMAP\n  AirSAR -raster- (rov): AirSAR Polarimetric Image\n  RS2 -raster- (rovs): RadarSat 2 XML Product\n  SAFE -raster- (rov): Sentinel-1 SAR SAFE Product\n  PCIDSK -raster,vector- (rw+v): PCIDSK Database File\n  PCRaster -raster- (rw+): PCRaster Raster File\n  ILWIS -raster- (rw+v): ILWIS Raster Map\n  SGI -raster- (rw+v): SGI Image File Format 1.0\n  SRTMHGT -raster- (rwv): SRTMHGT File Format\n  Leveller -raster- (rw+v): Leveller heightfield\n  Terragen -raster- (rw+v): Terragen heightfield\n  netCDF -raster,multidimensional raster,vector- (rw+s): Network Common Data Format\n  HDF4 -raster,multidimensional raster- (ros): Hierarchical Data Format Release 4\n  HDF4Image -raster- (rw+): HDF4 Dataset\n  ISIS3 -raster- (rw+v): USGS Astrogeology ISIS cube (Version 3)\n  ISIS2 -raster- (rw+v): USGS Astrogeology ISIS cube (Version 2)\n  PDS -raster- (rov): NASA Planetary Data System\n  PDS4 -raster,vector- (rw+vs): NASA Planetary Data System 4\n  VICAR -raster,vector- (rw+v): MIPL VICAR file\n  TIL -raster- (rov): EarthWatch .TIL\n  ERS -raster- (rw+v): ERMapper .ers Labelled\n  JP2OpenJPEG -raster,vector- (rwv): JPEG-2000 driver based on OpenJPEG library\n  L1B -raster- (rovs): NOAA Polar Orbiter Level 1b Data Set\n  FIT -raster- (rwv): FIT Image\n  GRIB -raster,multidimensional raster- (rwv): GRIdded Binary (.grb, .grb2)\n  RMF -raster- (rw+v): Raster Matrix Format\n  WCS -raster- (rovs): OGC Web Coverage Service\n  WMS -raster- (rwvs): OGC Web Map Service\n  MSGN -raster- (rov): EUMETSAT Archive native (.nat)\n  RST -raster- (rw+v): Idrisi Raster A.1\n  GSAG -raster- (rwv): Golden Software ASCII Grid (.grd)\n  GSBG -raster- (rw+v): Golden Software Binary Grid (.grd)\n  GS7BG -raster- (rw+v): Golden Software 7 Binary Grid (.grd)\n  COSAR -raster- (rov): COSAR Annotated Binary Matrix (TerraSAR-X)\n  TSX -raster- (rov): TerraSAR-X Product\n  COASP -raster- (ro): DRDC COASP SAR Processor Raster\n  R -raster- (rwv): R Object Data Store\n  MAP -raster- (rov): OziExplorer .MAP\n  KMLSUPEROVERLAY -raster- (rwv): Kml Super Overlay\n  WEBP -raster- (rwv): WEBP\n  PDF -raster,vector- (rw+vs): Geospatial PDF\n  Rasterlite -raster- (rwvs): Rasterlite\n  MBTiles -raster,vector- (rw+v): MBTiles\n  PLMOSAIC -raster- (ro): Planet Labs Mosaics API\n  CALS -raster- (rwv): CALS (Type 1)\n  WMTS -raster- (rwv): OGC Web Map Tile Service\n  SENTINEL2 -raster- (rovs): Sentinel 2\n  MRF -raster- (rw+v): Meta Raster Format\n  TileDB -raster- (rw+vs): TileDB\n  PNM -raster- (rw+v): Portable Pixmap Format (netpbm)\n  DOQ1 -raster- (rov): USGS DOQ (Old Style)\n  DOQ2 -raster- (rov): USGS DOQ (New Style)\n  PAux -raster- (rw+v): PCI .aux Labelled\n  MFF -raster- (rw+v): Vexcel MFF Raster\n  MFF2 -raster- (rw+): Vexcel MFF2 (HKV) Raster\n  GSC -raster- (rov): GSC Geogrid\n  FAST -raster- (rov): EOSAT FAST Format\n  BT -raster- (rw+v): VTP .bt (Binary Terrain) 1.3 Format\n  LAN -raster- (rw+v): Erdas .LAN/.GIS\n  CPG -raster- (rov): Convair PolGASP\n  NDF -raster- (rov): NLAPS Data Format\n  EIR -raster- (rov): Erdas Imagine Raw\n  DIPEx -raster- (rov): DIPEx\n  LCP -raster- (rwv): FARSITE v.4 Landscape File (.lcp)\n  GTX -raster- (rw+v): NOAA Vertical Datum .GTX\n  LOSLAS -raster- (rov): NADCON .los/.las Datum Grid Shift\n  NTv2 -raster- (rw+vs): NTv2 Datum Grid Shift\n  CTable2 -raster- (rw+v): CTable2 Datum Grid Shift\n  ACE2 -raster- (rov): ACE2\n  SNODAS -raster- (rov): Snow Data Assimilation System\n  KRO -raster- (rw+v): KOLOR Raw\n  ROI_PAC -raster- (rw+v): ROI_PAC raster\n  RRASTER -raster- (rw+v): R Raster\n  BYN -raster- (rw+v): Natural Resources Canada's Geoid\n  ARG -raster- (rwv): Azavea Raster Grid format\n  RIK -raster- (rov): Swedish Grid RIK (.rik)\n  USGSDEM -raster- (rwv): USGS Optional ASCII DEM (and CDED)\n  GXF -raster- (rov): GeoSoft Grid Exchange Format\n  KEA -raster- (rw+v): KEA Image Format (.kea)\n  BAG -raster,multidimensional raster,vector- (rw+v): Bathymetry Attributed Grid\n  HDF5 -raster,multidimensional raster- (rovs): Hierarchical Data Format Release 5\n  HDF5Image -raster- (rov): HDF5 Dataset\n  NWT_GRD -raster- (rw+v): Northwood Numeric Grid Format .grd/.tab\n  NWT_GRC -raster- (rov): Northwood Classified Grid Format .grc/.tab\n  ADRG -raster- (rw+vs): ARC Digitized Raster Graphics\n  SRP -raster- (rovs): Standard Raster Product (ASRP/USRP)\n  BLX -raster- (rwv): Magellan topo (.blx)\n  PostGISRaster -raster- (rws): PostGIS Raster driver\n  SAGA -raster- (rw+v): SAGA GIS Binary Grid (.sdat, .sg-grd-z)\n  XYZ -raster- (rwv): ASCII Gridded XYZ\n  HF2 -raster- (rwv): HF2/HFZ heightfield raster\n  OZI -raster- (rov): OziExplorer Image File\n  CTG -raster- (rov): USGS LULC Composite Theme Grid\n  ZMap -raster- (rwv): ZMap Plus Grid\n  NGSGEOID -raster- (rov): NOAA NGS Geoid Height Grids\n  IRIS -raster- (rov): IRIS data (.PPI, .CAPPi etc)\n  PRF -raster- (rov): Racurs PHOTOMOD PRF\n  EEDAI -raster- (ros): Earth Engine Data API Image\n  DAAS -raster- (ro): Airbus DS Intelligence Data As A Service driver\n  SIGDEM -raster- (rwv): Scaled Integer Gridded DEM .sigdem\n  TGA -raster- (rov): TGA/TARGA Image File Format\n  OGCAPI -raster,vector- (rov): OGCAPI\n  STACTA -raster- (rovs): Spatio-Temporal Asset Catalog Tiled Assets\n  STACIT -raster- (rovs): Spatio-Temporal Asset Catalog Items\n  GPKG -raster,vector- (rw+vs): GeoPackage\n  CAD -raster,vector- (rovs): AutoCAD Driver\n  PLSCENES -raster,vector- (ro): Planet Labs Scenes API\n  NGW -raster,vector- (rw+s): NextGIS Web\n  GenBin -raster- (rov): Generic Binary (.hdr Labelled)\n  ENVI -raster- (rw+v): ENVI .hdr Labelled\n  EHdr -raster- (rw+v): ESRI .hdr Labelled\n  ISCE -raster- (rw+v): ISCE raster\n  Zarr -raster,multidimensional raster- (rw+vs): Zarr\n  HTTP -raster,vector- (ro): HTTP Fetching Wrapper\n</pre> In\u00a0[17]: Copied! <pre>cd srtm/\n</pre> cd srtm/ <pre>/Users/abharathi/Documents/gis_data/gdal-tools/srtm\n</pre> In\u00a0[19]: Copied! <pre>ls -lh\n</pre> ls -lh <pre>total 202624\n-rw-r--r--@ 1 abharathi  staff    25M Sep  7  2020 N27E086.hgt\n-rw-r--r--@ 1 abharathi  staff    25M Sep  7  2020 N27E087.hgt\n-rw-r--r--@ 1 abharathi  staff    25M Sep  7  2020 N28E086.hgt\n-rw-r--r--@ 1 abharathi  staff    25M Sep  7  2020 N28E087.hgt\n</pre> In\u00a0[20]: Copied! <pre>!gdalinfo N28E086.hgt\n</pre> !gdalinfo N28E086.hgt <pre>Driver: SRTMHGT/SRTMHGT File Format\nFiles: N28E086.hgt\nSize is 3601, 3601\nCoordinate System is:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\nData axis to CRS axis mapping: 2,1\nOrigin = (85.999861111111116,29.000138888888888)\nPixel Size = (0.000277777777778,-0.000277777777778)\nMetadata:\n  AREA_OR_POINT=Point\nCorner Coordinates:\nUpper Left  (  85.9998611,  29.0001389) ( 85d59'59.50\"E, 29d 0' 0.50\"N)\nLower Left  (  85.9998611,  27.9998611) ( 85d59'59.50\"E, 27d59'59.50\"N)\nUpper Right (  87.0001389,  29.0001389) ( 87d 0' 0.50\"E, 29d 0' 0.50\"N)\nLower Right (  87.0001389,  27.9998611) ( 87d 0' 0.50\"E, 27d59'59.50\"N)\nCenter      (  86.5000000,  28.5000000) ( 86d30' 0.00\"E, 28d30' 0.00\"N)\nBand 1 Block=3601x1 Type=Int16, ColorInterp=Undefined\n  NoData Value=-32768\n  Unit Type: m\n</pre> In\u00a0[25]: Copied! <pre># Can chain command options\n!gdalinfo -hist N28E086.hgt\n</pre> # Can chain command options !gdalinfo -hist N28E086.hgt <pre>Driver: SRTMHGT/SRTMHGT File Format\nFiles: N28E086.hgt\n       N28E086.hgt.aux.xml\nSize is 3601, 3601\nCoordinate System is:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\nData axis to CRS axis mapping: 2,1\nOrigin = (85.999861111111116,29.000138888888888)\nPixel Size = (0.000277777777778,-0.000277777777778)\nMetadata:\n  AREA_OR_POINT=Point\nCorner Coordinates:\nUpper Left  (  85.9998611,  29.0001389) ( 85d59'59.50\"E, 29d 0' 0.50\"N)\nLower Left  (  85.9998611,  27.9998611) ( 85d59'59.50\"E, 27d59'59.50\"N)\nUpper Right (  87.0001389,  29.0001389) ( 87d 0' 0.50\"E, 29d 0' 0.50\"N)\nLower Right (  87.0001389,  27.9998611) ( 87d 0' 0.50\"E, 27d59'59.50\"N)\nCenter      (  86.5000000,  28.5000000) ( 86d30' 0.00\"E, 28d30' 0.00\"N)\nBand 1 Block=3601x1 Type=Int16, ColorInterp=Undefined\n  Min=2535.000 Max=8275.000 \n  Minimum=2535.000, Maximum=8275.000, Mean=5067.325, StdDev=545.184\n  256 buckets from 2523.75 to 8286.25:\n  7 11 6 4 11 46 36 48 45 42 94 143 242 298 395 482 496 466 429 435 506 491 638 659 734 758 843 861 920 1014 1152 1228 1155 1346 1392 1670 1641 1890 1869 1964 1975 2201 2333 2765 2763 2959 2951 3295 3200 3673 3604 3937 3987 4476 4344 4862 4650 5086 5042 5889 5871 6668 6716 7342 7326 7837 8373 9414 9173 9968 10277 12213 11758 13969 15494 17512 19362 19328 57636 128726 140793 208505 241340 179825 185147 168703 163827 153862 162822 161140 173363 170846 180251 175639 188740 179526 186968 184531 196035 193185 206067 199861 211957 200760 212829 206817 220143 209265 222936 216445 229534 229816 238752 226566 234329 213829 218165 207523 217529 210291 219284 203776 206762 188260 198659 187907 184265 175861 162441 162943 150038 146503 129777 126385 111852 106263 94471 92537 81691 80049 71584 69721 62924 62460 56267 56311 51676 51844 46486 46417 42682 42311 39751 40405 37214 37815 35293 35475 33431 34307 32683 33092 30774 30803 28106 28282 25729 26323 24228 23774 22093 21084 18592 18101 16228 16004 14372 13961 12660 11036 10310 8965 8688 7677 7532 7126 6839 6015 5821 5071 4685 4092 3778 3257 3316 3134 2933 2527 2506 2178 2151 1975 1926 1736 1651 1552 1409 1222 1216 1070 1095 903 854 841 796 763 802 752 729 659 663 649 703 653 633 594 563 516 576 567 461 433 419 405 314 331 288 249 182 201 179 145 119 119 107 144 125 100 79 92 49 23 21 15 19 6 \n  NoData Value=-32768\n  Unit Type: m\n  Metadata:\n    STATISTICS_MAXIMUM=8275\n    STATISTICS_MEAN=5067.3254290577\n    STATISTICS_MINIMUM=2535\n    STATISTICS_STDDEV=545.18375677872\n    STATISTICS_VALID_PERCENT=100\n</pre> In\u00a0[27]: Copied! <pre>ls *.hgt &gt; srtm_filelist.txt\n</pre> ls *.hgt &gt; srtm_filelist.txt In\u00a0[28]: Copied! <pre>cat srtm_filelist.txt\n</pre> cat srtm_filelist.txt <pre>N27E086.hgt\nN27E087.hgt\nN28E086.hgt\nN28E087.hgt\n</pre> In\u00a0[29]: Copied! <pre>!gdalbuildvrt -input_file_list srtm_filelist.txt merged.vrt\n</pre> !gdalbuildvrt -input_file_list srtm_filelist.txt merged.vrt <pre>0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[30]: Copied! <pre>cat merged.vrt\n</pre> cat merged.vrt <pre>&lt;VRTDataset rasterXSize=\"7201\" rasterYSize=\"7201\"&gt;\n  &lt;SRS dataAxisToSRSAxisMapping=\"2,1\"&gt;GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]&lt;/SRS&gt;\n  &lt;GeoTransform&gt;  8.5999861111111116e+01,  2.7777777777777778e-04,  0.0000000000000000e+00,  2.9000138888888888e+01,  0.0000000000000000e+00, -2.7777777777777778e-04&lt;/GeoTransform&gt;\n  &lt;VRTRasterBand dataType=\"Int16\" band=\"1\"&gt;\n    &lt;NoDataValue&gt;-32768&lt;/NoDataValue&gt;\n    &lt;ComplexSource&gt;\n      &lt;SourceFilename relativeToVRT=\"1\"&gt;N27E086.hgt&lt;/SourceFilename&gt;\n      &lt;SourceBand&gt;1&lt;/SourceBand&gt;\n      &lt;SourceProperties RasterXSize=\"3601\" RasterYSize=\"3601\" DataType=\"Int16\" BlockXSize=\"3601\" BlockYSize=\"1\" /&gt;\n      &lt;SrcRect xOff=\"0\" yOff=\"0\" xSize=\"3601\" ySize=\"3601\" /&gt;\n      &lt;DstRect xOff=\"0\" yOff=\"3600\" xSize=\"3601\" ySize=\"3601\" /&gt;\n      &lt;NODATA&gt;-32768&lt;/NODATA&gt;\n    &lt;/ComplexSource&gt;\n    &lt;ComplexSource&gt;\n      &lt;SourceFilename relativeToVRT=\"1\"&gt;N27E087.hgt&lt;/SourceFilename&gt;\n      &lt;SourceBand&gt;1&lt;/SourceBand&gt;\n      &lt;SourceProperties RasterXSize=\"3601\" RasterYSize=\"3601\" DataType=\"Int16\" BlockXSize=\"3601\" BlockYSize=\"1\" /&gt;\n      &lt;SrcRect xOff=\"0\" yOff=\"0\" xSize=\"3601\" ySize=\"3601\" /&gt;\n      &lt;DstRect xOff=\"3600\" yOff=\"3600\" xSize=\"3601\" ySize=\"3601\" /&gt;\n      &lt;NODATA&gt;-32768&lt;/NODATA&gt;\n    &lt;/ComplexSource&gt;\n    &lt;ComplexSource&gt;\n      &lt;SourceFilename relativeToVRT=\"1\"&gt;N28E086.hgt&lt;/SourceFilename&gt;\n      &lt;SourceBand&gt;1&lt;/SourceBand&gt;\n      &lt;SourceProperties RasterXSize=\"3601\" RasterYSize=\"3601\" DataType=\"Int16\" BlockXSize=\"3601\" BlockYSize=\"1\" /&gt;\n      &lt;SrcRect xOff=\"0\" yOff=\"0\" xSize=\"3601\" ySize=\"3601\" /&gt;\n      &lt;DstRect xOff=\"0\" yOff=\"0\" xSize=\"3601\" ySize=\"3601\" /&gt;\n      &lt;NODATA&gt;-32768&lt;/NODATA&gt;\n    &lt;/ComplexSource&gt;\n    &lt;ComplexSource&gt;\n      &lt;SourceFilename relativeToVRT=\"1\"&gt;N28E087.hgt&lt;/SourceFilename&gt;\n      &lt;SourceBand&gt;1&lt;/SourceBand&gt;\n      &lt;SourceProperties RasterXSize=\"3601\" RasterYSize=\"3601\" DataType=\"Int16\" BlockXSize=\"3601\" BlockYSize=\"1\" /&gt;\n      &lt;SrcRect xOff=\"0\" yOff=\"0\" xSize=\"3601\" ySize=\"3601\" /&gt;\n      &lt;DstRect xOff=\"3600\" yOff=\"0\" xSize=\"3601\" ySize=\"3601\" /&gt;\n      &lt;NODATA&gt;-32768&lt;/NODATA&gt;\n    &lt;/ComplexSource&gt;\n  &lt;/VRTRasterBand&gt;\n&lt;/VRTDataset&gt;\n</pre> In\u00a0[31]: Copied! <pre>!gdalinfo -stats merged.vrt\n</pre> !gdalinfo -stats merged.vrt <pre>Driver: VRT/Virtual Raster\nFiles: merged.vrt\n       N27E086.hgt\n       N27E087.hgt\n       N28E086.hgt\n       N28E087.hgt\nSize is 7201, 7201\nCoordinate System is:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\nData axis to CRS axis mapping: 2,1\nOrigin = (85.999861111111116,29.000138888888888)\nPixel Size = (0.000277777777778,-0.000277777777778)\nCorner Coordinates:\nUpper Left  (  85.9998611,  29.0001389) ( 85d59'59.50\"E, 29d 0' 0.50\"N)\nLower Left  (  85.9998611,  26.9998611) ( 85d59'59.50\"E, 26d59'59.50\"N)\nUpper Right (  88.0001389,  29.0001389) ( 88d 0' 0.50\"E, 29d 0' 0.50\"N)\nLower Right (  88.0001389,  26.9998611) ( 88d 0' 0.50\"E, 26d59'59.50\"N)\nCenter      (  87.0000000,  28.0000000) ( 87d 0' 0.00\"E, 28d 0' 0.00\"N)\nBand 1 Block=128x128 Type=Int16, ColorInterp=Undefined\n  Minimum=176.000, Maximum=8748.000, Mean=3840.966, StdDev=1649.758\n  NoData Value=-32768\n  Metadata:\n    STATISTICS_MAXIMUM=8748\n    STATISTICS_MEAN=3840.9655621886\n    STATISTICS_MINIMUM=176\n    STATISTICS_STDDEV=1649.7576436136\n    STATISTICS_VALID_PERCENT=100\n</pre> In\u00a0[33]: Copied! <pre># Get maximum of the merged rasters\n%time\n!gdalinfo -stats -json merged.vrt | jq \".bands[0].maximum\"\n</pre> # Get maximum of the merged rasters %time !gdalinfo -stats -json merged.vrt | jq \".bands[0].maximum\" <pre>CPU times: user 2 \u00b5s, sys: 1e+03 ns, total: 3 \u00b5s\nWall time: 9.06 \u00b5s\n8748\n</pre> In\u00a0[34]: Copied! <pre>!gdal_translate --formats\n</pre> !gdal_translate --formats <pre>Supported Formats:\n  VRT -raster,multidimensional raster- (rw+v): Virtual Raster\n  DERIVED -raster- (ro): Derived datasets using VRT pixel functions\n  GTiff -raster- (rw+vs): GeoTIFF\n  COG -raster- (wv): Cloud optimized GeoTIFF generator\n  NITF -raster- (rw+vs): National Imagery Transmission Format\n  RPFTOC -raster- (rovs): Raster Product Format TOC format\n  ECRGTOC -raster- (rovs): ECRG TOC format\n  HFA -raster- (rw+v): Erdas Imagine Images (.img)\n  SAR_CEOS -raster- (rov): CEOS SAR Image\n  CEOS -raster- (rov): CEOS Image\n  JAXAPALSAR -raster- (rov): JAXA PALSAR Product Reader (Level 1.1/1.5)\n  GFF -raster- (rov): Ground-based SAR Applications Testbed File Format (.gff)\n  ELAS -raster- (rw+v): ELAS\n  ESRIC -raster- (rov): Esri Compact Cache\n  AIG -raster- (rov): Arc/Info Binary Grid\n  AAIGrid -raster- (rwv): Arc/Info ASCII Grid\n  GRASSASCIIGrid -raster- (rov): GRASS ASCII Grid\n  ISG -raster- (rov): International Service for the Geoid\n  SDTS -raster- (rov): SDTS Raster\n  DTED -raster- (rwv): DTED Elevation Raster\n  PNG -raster- (rwv): Portable Network Graphics\n  JPEG -raster- (rwv): JPEG JFIF\n  MEM -raster,multidimensional raster- (rw+): In Memory Raster\n  JDEM -raster- (rov): Japanese DEM (.mem)\n  GIF -raster- (rwv): Graphics Interchange Format (.gif)\n  BIGGIF -raster- (rov): Graphics Interchange Format (.gif)\n  ESAT -raster- (rov): Envisat Image Format\n  FITS -raster,vector- (rw+): Flexible Image Transport System\n  BSB -raster- (rov): Maptech BSB Nautical Charts\n  XPM -raster- (rwv): X11 PixMap Format\n  BMP -raster- (rw+v): MS Windows Device Independent Bitmap\n  DIMAP -raster- (rovs): SPOT DIMAP\n  AirSAR -raster- (rov): AirSAR Polarimetric Image\n  RS2 -raster- (rovs): RadarSat 2 XML Product\n  SAFE -raster- (rov): Sentinel-1 SAR SAFE Product\n  PCIDSK -raster,vector- (rw+v): PCIDSK Database File\n  PCRaster -raster- (rw+): PCRaster Raster File\n  ILWIS -raster- (rw+v): ILWIS Raster Map\n  SGI -raster- (rw+v): SGI Image File Format 1.0\n  SRTMHGT -raster- (rwv): SRTMHGT File Format\n  Leveller -raster- (rw+v): Leveller heightfield\n  Terragen -raster- (rw+v): Terragen heightfield\n  netCDF -raster,multidimensional raster,vector- (rw+s): Network Common Data Format\n  HDF4 -raster,multidimensional raster- (ros): Hierarchical Data Format Release 4\n  HDF4Image -raster- (rw+): HDF4 Dataset\n  ISIS3 -raster- (rw+v): USGS Astrogeology ISIS cube (Version 3)\n  ISIS2 -raster- (rw+v): USGS Astrogeology ISIS cube (Version 2)\n  PDS -raster- (rov): NASA Planetary Data System\n  PDS4 -raster,vector- (rw+vs): NASA Planetary Data System 4\n  VICAR -raster,vector- (rw+v): MIPL VICAR file\n  TIL -raster- (rov): EarthWatch .TIL\n  ERS -raster- (rw+v): ERMapper .ers Labelled\n  JP2OpenJPEG -raster,vector- (rwv): JPEG-2000 driver based on OpenJPEG library\n  L1B -raster- (rovs): NOAA Polar Orbiter Level 1b Data Set\n  FIT -raster- (rwv): FIT Image\n  GRIB -raster,multidimensional raster- (rwv): GRIdded Binary (.grb, .grb2)\n  RMF -raster- (rw+v): Raster Matrix Format\n  WCS -raster- (rovs): OGC Web Coverage Service\n  WMS -raster- (rwvs): OGC Web Map Service\n  MSGN -raster- (rov): EUMETSAT Archive native (.nat)\n  RST -raster- (rw+v): Idrisi Raster A.1\n  GSAG -raster- (rwv): Golden Software ASCII Grid (.grd)\n  GSBG -raster- (rw+v): Golden Software Binary Grid (.grd)\n  GS7BG -raster- (rw+v): Golden Software 7 Binary Grid (.grd)\n  COSAR -raster- (rov): COSAR Annotated Binary Matrix (TerraSAR-X)\n  TSX -raster- (rov): TerraSAR-X Product\n  COASP -raster- (ro): DRDC COASP SAR Processor Raster\n  R -raster- (rwv): R Object Data Store\n  MAP -raster- (rov): OziExplorer .MAP\n  KMLSUPEROVERLAY -raster- (rwv): Kml Super Overlay\n  WEBP -raster- (rwv): WEBP\n  PDF -raster,vector- (rw+vs): Geospatial PDF\n  Rasterlite -raster- (rwvs): Rasterlite\n  MBTiles -raster,vector- (rw+v): MBTiles\n  PLMOSAIC -raster- (ro): Planet Labs Mosaics API\n  CALS -raster- (rwv): CALS (Type 1)\n  WMTS -raster- (rwv): OGC Web Map Tile Service\n  SENTINEL2 -raster- (rovs): Sentinel 2\n  MRF -raster- (rw+v): Meta Raster Format\n  TileDB -raster- (rw+vs): TileDB\n  PNM -raster- (rw+v): Portable Pixmap Format (netpbm)\n  DOQ1 -raster- (rov): USGS DOQ (Old Style)\n  DOQ2 -raster- (rov): USGS DOQ (New Style)\n  PAux -raster- (rw+v): PCI .aux Labelled\n  MFF -raster- (rw+v): Vexcel MFF Raster\n  MFF2 -raster- (rw+): Vexcel MFF2 (HKV) Raster\n  GSC -raster- (rov): GSC Geogrid\n  FAST -raster- (rov): EOSAT FAST Format\n  BT -raster- (rw+v): VTP .bt (Binary Terrain) 1.3 Format\n  LAN -raster- (rw+v): Erdas .LAN/.GIS\n  CPG -raster- (rov): Convair PolGASP\n  NDF -raster- (rov): NLAPS Data Format\n  EIR -raster- (rov): Erdas Imagine Raw\n  DIPEx -raster- (rov): DIPEx\n  LCP -raster- (rwv): FARSITE v.4 Landscape File (.lcp)\n  GTX -raster- (rw+v): NOAA Vertical Datum .GTX\n  LOSLAS -raster- (rov): NADCON .los/.las Datum Grid Shift\n  NTv2 -raster- (rw+vs): NTv2 Datum Grid Shift\n  CTable2 -raster- (rw+v): CTable2 Datum Grid Shift\n  ACE2 -raster- (rov): ACE2\n  SNODAS -raster- (rov): Snow Data Assimilation System\n  KRO -raster- (rw+v): KOLOR Raw\n  ROI_PAC -raster- (rw+v): ROI_PAC raster\n  RRASTER -raster- (rw+v): R Raster\n  BYN -raster- (rw+v): Natural Resources Canada's Geoid\n  ARG -raster- (rwv): Azavea Raster Grid format\n  RIK -raster- (rov): Swedish Grid RIK (.rik)\n  USGSDEM -raster- (rwv): USGS Optional ASCII DEM (and CDED)\n  GXF -raster- (rov): GeoSoft Grid Exchange Format\n  KEA -raster- (rw+v): KEA Image Format (.kea)\n  BAG -raster,multidimensional raster,vector- (rw+v): Bathymetry Attributed Grid\n  HDF5 -raster,multidimensional raster- (rovs): Hierarchical Data Format Release 5\n  HDF5Image -raster- (rov): HDF5 Dataset\n  NWT_GRD -raster- (rw+v): Northwood Numeric Grid Format .grd/.tab\n  NWT_GRC -raster- (rov): Northwood Classified Grid Format .grc/.tab\n  ADRG -raster- (rw+vs): ARC Digitized Raster Graphics\n  SRP -raster- (rovs): Standard Raster Product (ASRP/USRP)\n  BLX -raster- (rwv): Magellan topo (.blx)\n  PostGISRaster -raster- (rws): PostGIS Raster driver\n  SAGA -raster- (rw+v): SAGA GIS Binary Grid (.sdat, .sg-grd-z)\n  XYZ -raster- (rwv): ASCII Gridded XYZ\n  HF2 -raster- (rwv): HF2/HFZ heightfield raster\n  OZI -raster- (rov): OziExplorer Image File\n  CTG -raster- (rov): USGS LULC Composite Theme Grid\n  ZMap -raster- (rwv): ZMap Plus Grid\n  NGSGEOID -raster- (rov): NOAA NGS Geoid Height Grids\n  IRIS -raster- (rov): IRIS data (.PPI, .CAPPi etc)\n  PRF -raster- (rov): Racurs PHOTOMOD PRF\n  EEDAI -raster- (ros): Earth Engine Data API Image\n  DAAS -raster- (ro): Airbus DS Intelligence Data As A Service driver\n  SIGDEM -raster- (rwv): Scaled Integer Gridded DEM .sigdem\n  TGA -raster- (rov): TGA/TARGA Image File Format\n  OGCAPI -raster,vector- (rov): OGCAPI\n  STACTA -raster- (rovs): Spatio-Temporal Asset Catalog Tiled Assets\n  STACIT -raster- (rovs): Spatio-Temporal Asset Catalog Items\n  GPKG -raster,vector- (rw+vs): GeoPackage\n  CAD -raster,vector- (rovs): AutoCAD Driver\n  PLSCENES -raster,vector- (ro): Planet Labs Scenes API\n  NGW -raster,vector- (rw+s): NextGIS Web\n  GenBin -raster- (rov): Generic Binary (.hdr Labelled)\n  ENVI -raster- (rw+v): ENVI .hdr Labelled\n  EHdr -raster- (rw+v): ESRI .hdr Labelled\n  ISCE -raster- (rw+v): ISCE raster\n  Zarr -raster,multidimensional raster- (rw+vs): Zarr\n  HTTP -raster,vector- (ro): HTTP Fetching Wrapper\n</pre> In\u00a0[35]: Copied! <pre>%time\n# gdal will guess the output format\n!gdal_translate merged.vrt merged.tif\n</pre> %time # gdal will guess the output format !gdal_translate merged.vrt merged.tif <pre>CPU times: user 3 \u00b5s, sys: 1e+03 ns, total: 4 \u00b5s\nWall time: 7.87 \u00b5s\nInput file size is 7201, 7201\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[36]: Copied! <pre>ls -lh merged*\n</pre> ls -lh merged* <pre>-rw-r--r--  1 abharathi  staff    99M Apr 25 07:15 merged.tif\n-rw-r--r--  1 abharathi  staff   2.6K Apr 25 06:58 merged.vrt\n</pre> In\u00a0[37]: Copied! <pre>%time\n# gdal will guess the output format\n!gdal_translate merged.vrt merged.zarr\n</pre> %time # gdal will guess the output format !gdal_translate merged.vrt merged.zarr <pre>CPU times: user 2 \u00b5s, sys: 1e+03 ns, total: 3 \u00b5s\nWall time: 11 \u00b5s\nERROR 1: Cannot guess driver for merged.zarr\nOutput driver not found.\n</pre> In\u00a0[38]: Copied! <pre>!gdalinfo merged.tif\n</pre> !gdalinfo merged.tif <pre>Driver: GTiff/GeoTIFF\nFiles: merged.tif\nSize is 7201, 7201\nCoordinate System is:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\nData axis to CRS axis mapping: 2,1\nOrigin = (85.999861111111116,29.000138888888888)\nPixel Size = (0.000277777777778,-0.000277777777778)\nMetadata:\n  AREA_OR_POINT=Area\nImage Structure Metadata:\n  INTERLEAVE=BAND\nCorner Coordinates:\nUpper Left  (  85.9998611,  29.0001389) ( 85d59'59.50\"E, 29d 0' 0.50\"N)\nLower Left  (  85.9998611,  26.9998611) ( 85d59'59.50\"E, 26d59'59.50\"N)\nUpper Right (  88.0001389,  29.0001389) ( 88d 0' 0.50\"E, 29d 0' 0.50\"N)\nLower Right (  88.0001389,  26.9998611) ( 88d 0' 0.50\"E, 26d59'59.50\"N)\nCenter      (  87.0000000,  28.0000000) ( 87d 0' 0.00\"E, 28d 0' 0.00\"N)\nBand 1 Block=7201x1 Type=Int16, ColorInterp=Gray\n  Min=176.000 Max=8748.000 \n  Minimum=176.000, Maximum=8748.000, Mean=3840.966, StdDev=1649.758\n  NoData Value=-32768\n  Metadata:\n    STATISTICS_MAXIMUM=8748\n    STATISTICS_MEAN=3840.9655621886\n    STATISTICS_MINIMUM=176\n    STATISTICS_STDDEV=1649.7576436136\n    STATISTICS_VALID_PERCENT=100\n</pre> <p>Use creation options (<code>-co</code>) to specify compression when creating the resulting raser. Doc: https://gdal.org/drivers/raster/gtiff.html#creation-options</p> <p><code>TILDED=YES</code> - stores tiled TIFF where neighborhood effect is used for compression <code>PREDICTOR=YES</code> - stores diff  in values</p> In\u00a0[43]: Copied! <pre>%time\n!gdal_translate merged.vrt merged.tif -co COMPRESS=DEFLATE -co TILED=YES -co PREDICTOR=2\n</pre> %time !gdal_translate merged.vrt merged.tif -co COMPRESS=DEFLATE -co TILED=YES -co PREDICTOR=2 <pre>CPU times: user 3 \u00b5s, sys: 1e+03 ns, total: 4 \u00b5s\nWall time: 9.78 \u00b5s\nInput file size is 7201, 7201\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[44]: Copied! <pre>ls -lh *.tif\n</pre> ls -lh *.tif <pre>-rw-r--r--  1 abharathi  staff    39M Apr 25 07:31 merged.tif\n</pre> In\u00a0[45]: Copied! <pre>!gdalinfo merged.tif\n</pre> !gdalinfo merged.tif <pre>Driver: GTiff/GeoTIFF\nFiles: merged.tif\nSize is 7201, 7201\nCoordinate System is:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\nData axis to CRS axis mapping: 2,1\nOrigin = (85.999861111111116,29.000138888888888)\nPixel Size = (0.000277777777778,-0.000277777777778)\nMetadata:\n  AREA_OR_POINT=Area\nImage Structure Metadata:\n  COMPRESSION=DEFLATE\n  INTERLEAVE=BAND\n  PREDICTOR=2\nCorner Coordinates:\nUpper Left  (  85.9998611,  29.0001389) ( 85d59'59.50\"E, 29d 0' 0.50\"N)\nLower Left  (  85.9998611,  26.9998611) ( 85d59'59.50\"E, 26d59'59.50\"N)\nUpper Right (  88.0001389,  29.0001389) ( 88d 0' 0.50\"E, 29d 0' 0.50\"N)\nLower Right (  88.0001389,  26.9998611) ( 88d 0' 0.50\"E, 26d59'59.50\"N)\nCenter      (  87.0000000,  28.0000000) ( 87d 0' 0.00\"E, 28d 0' 0.00\"N)\nBand 1 Block=256x256 Type=Int16, ColorInterp=Gray\n  Min=176.000 Max=8748.000 \n  Minimum=176.000, Maximum=8748.000, Mean=3840.966, StdDev=1649.758\n  NoData Value=-32768\n  Metadata:\n    STATISTICS_MAXIMUM=8748\n    STATISTICS_MEAN=3840.9655621886\n    STATISTICS_MINIMUM=176\n    STATISTICS_STDDEV=1649.7576436136\n    STATISTICS_VALID_PERCENT=100\n</pre> In\u00a0[46]: Copied! <pre>%time\n!gdal_translate merged.vrt merged.tif -co COMPRESS=DEFLATE \\\n-co TILED=YES -co PREDICTOR=2 -a_nodata -9999\n</pre> %time !gdal_translate merged.vrt merged.tif -co COMPRESS=DEFLATE \\ -co TILED=YES -co PREDICTOR=2 -a_nodata -9999 <pre>CPU times: user 4 \u00b5s, sys: 0 ns, total: 4 \u00b5s\nWall time: 10 \u00b5s\nInput file size is 7201, 7201\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[57]: Copied! <pre>!gdalinfo -json merged.tif | jq \".bands[0].noDataValue\"\n</pre> !gdalinfo -json merged.tif | jq \".bands[0].noDataValue\" <pre>-9999\n</pre> In\u00a0[59]: Copied! <pre>%time\n!gdal_translate -of COG merged.vrt merged_cog.tif \\\n  -co COMPRESS=DEFLATE -co PREDICTOR=2 -co NUM_THREADS=ALL_CPUS \\\n  -a_nodata -9999\n</pre> %time !gdal_translate -of COG merged.vrt merged_cog.tif \\   -co COMPRESS=DEFLATE -co PREDICTOR=2 -co NUM_THREADS=ALL_CPUS \\   -a_nodata -9999 <pre>CPU times: user 4 \u00b5s, sys: 0 ns, total: 4 \u00b5s\nWall time: 10 \u00b5s\nInput file size is 7201, 7201\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[60]: Copied! <pre>!ls -lh *.tif\n</pre> !ls -lh *.tif <pre>-rw-r--r--  1 abharathi  staff    39M Apr 25 07:40 merged.tif\n-rw-r--r--  1 abharathi  staff    54M Apr 25 07:50 merged_cog.tif\n</pre> In\u00a0[65]: Copied! <pre>!gdalinfo merged_cog.tif -json | jq \".metadata.IMAGE_STRUCTURE\"\n</pre> !gdalinfo merged_cog.tif -json | jq \".metadata.IMAGE_STRUCTURE\" <pre>{\n  \"COMPRESSION\": \"DEFLATE\",\n  \"INTERLEAVE\": \"BAND\",\n  \"LAYOUT\": \"COG\",\n  \"PREDICTOR\": \"2\"\n}\n</pre> In\u00a0[68]: Copied! <pre>!gdalinfo https://oin-hotosm.s3.amazonaws.com/643b06548cae390005a1456c/0/643b06548cae390005a1456d.tif\n</pre> !gdalinfo https://oin-hotosm.s3.amazonaws.com/643b06548cae390005a1456c/0/643b06548cae390005a1456d.tif <pre>Driver: GTiff/GeoTIFF\nFiles: none associated\nSize is 12099, 9659\nCoordinate System is:\nPROJCRS[\"WGS 84 / UTM zone 36S\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 36S\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",33,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",10000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 30\u00b0E and 36\u00b0E, southern hemisphere between 80\u00b0S and equator, onshore and offshore. Burundi. Eswatini (Swaziland). Kenya. Malawi. Mozambique. Rwanda. South Africa. Tanzania. Uganda. Zambia. Zimbabwe.\"],\n        BBOX[-80,30,0,36]],\n    ID[\"EPSG\",32736]]\nData axis to CRS axis mapping: 1,2\nOrigin = (415959.438299251720309,7169051.803544469177723)\nPixel Size = (0.019999931184024,-0.019998767072739)\nMetadata:\n  AREA_OR_POINT=Area\nImage Structure Metadata:\n  COMPRESSION=YCbCr JPEG\n  INTERLEAVE=PIXEL\n  JPEGTABLESMODE=1\n  JPEG_QUALITY=75\n  SOURCE_COLOR_SPACE=YCbCr\nCorner Coordinates:\nUpper Left  (  415959.438, 7169051.804) ( 32d 9'47.25\"E, 25d35'37.10\"S)\nLower Left  (  415959.438, 7168858.635) ( 32d 9'47.20\"E, 25d35'43.38\"S)\nUpper Right (  416201.417, 7169051.804) ( 32d 9'55.92\"E, 25d35'37.15\"S)\nLower Right (  416201.417, 7168858.635) ( 32d 9'55.88\"E, 25d35'43.42\"S)\nCenter      (  416080.428, 7168955.219) ( 32d 9'51.56\"E, 25d35'40.26\"S)\nBand 1 Block=512x512 Type=Byte, ColorInterp=Red\n  Description = red\n  Min=0.000 Max=255.000 \n  Minimum=0.000, Maximum=255.000, Mean=108.198, StdDev=76.230\n  Overviews: 6050x4830, 3025x2415, 1513x1208, 757x604, 379x302\n  Mask Flags: PER_DATASET \n  Overviews of mask band: 6050x4830, 3025x2415, 1513x1208, 757x604, 379x302\n  Metadata:\n    STATISTICS_MAXIMUM=255\n    STATISTICS_MEAN=108.19774015389\n    STATISTICS_MINIMUM=0\n    STATISTICS_STDDEV=76.230400100428\nBand 2 Block=512x512 Type=Byte, ColorInterp=Green\n  Description = green\n  Min=0.000 Max=255.000 \n  Minimum=0.000, Maximum=255.000, Mean=99.308, StdDev=66.015\n  Overviews: 6050x4830, 3025x2415, 1513x1208, 757x604, 379x302\n  Mask Flags: PER_DATASET \n  Overviews of mask band: 6050x4830, 3025x2415, 1513x1208, 757x604, 379x302\n  Metadata:\n    STATISTICS_MAXIMUM=255\n    STATISTICS_MEAN=99.30805629414\n    STATISTICS_MINIMUM=0\n    STATISTICS_STDDEV=66.014549775173\nBand 3 Block=512x512 Type=Byte, ColorInterp=Blue\n  Description = blue\n  Min=0.000 Max=255.000 \n  Minimum=0.000, Maximum=255.000, Mean=79.344, StdDev=57.837\n  Overviews: 6050x4830, 3025x2415, 1513x1208, 757x604, 379x302\n  Mask Flags: PER_DATASET \n  Overviews of mask band: 6050x4830, 3025x2415, 1513x1208, 757x604, 379x302\n  Metadata:\n    STATISTICS_MAXIMUM=255\n    STATISTICS_MEAN=79.343554107368\n    STATISTICS_MINIMUM=0\n    STATISTICS_STDDEV=57.837036128418\n</pre> In\u00a0[70]: Copied! <pre># We specify scale since the vert units are in meters whereas the horz units are in degrees\n%time\n!gdaldem hillshade merged.vrt hillshade.tif -s 111120\n</pre> # We specify scale since the vert units are in meters whereas the horz units are in degrees %time !gdaldem hillshade merged.vrt hillshade.tif -s 111120 <pre>CPU times: user 4 \u00b5s, sys: 1 \u00b5s, total: 5 \u00b5s\nWall time: 8.82 \u00b5s\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[71]: Copied! <pre>ls -lh hill*\n</pre> ls -lh hill* <pre>-rw-r--r--  1 abharathi  staff    49M Apr 25 08:00 hillshade.tif\n</pre> In\u00a0[74]: Copied! <pre># first create a file to store the classes\n1000,101,146,82\n1500,190,202,130\n2000,241,225,145\n2500,244,200,126\n3000,197,147,117\n4000,204,169,170\n5000,251,238,253\n6000,255,255,255\n</pre> # first create a file to store the classes 1000,101,146,82 1500,190,202,130 2000,241,225,145 2500,244,200,126 3000,197,147,117 4000,204,169,170 5000,251,238,253 6000,255,255,255 Out[74]: <pre>(6000, 255, 255, 255)</pre> In\u00a0[77]: Copied! <pre>cat colormap.txt\n</pre> cat colormap.txt <pre>1000,101,146,82\n1500,190,202,130\n2000,241,225,145\n2500,244,200,126\n3000,197,147,117\n4000,204,169,170\n5000,251,238,253\n6000,255,255,255</pre> In\u00a0[78]: Copied! <pre>%time\n!gdaldem color-relief merged.vrt colormap.txt colorized_dem.tif\n</pre> %time !gdaldem color-relief merged.vrt colormap.txt colorized_dem.tif <pre>CPU times: user 2 \u00b5s, sys: 1e+03 ns, total: 3 \u00b5s\nWall time: 11.2 \u00b5s\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[82]: Copied! <pre># Convert to PNG, but a downsampled file that is 10% x 10%\n%time\n!gdal_translate -of PNG colorized_dem.tif colorized.png -outsize 10% 10%\n</pre> # Convert to PNG, but a downsampled file that is 10% x 10% %time !gdal_translate -of PNG colorized_dem.tif colorized.png -outsize 10% 10% <pre>CPU times: user 5 \u00b5s, sys: 3 \u00b5s, total: 8 \u00b5s\nWall time: 16.9 \u00b5s\nInput file size is 7201, 7201\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[83]: Copied! <pre>ls -lh colorized*\n</pre> ls -lh colorized* <pre>-rw-r--r--  1 abharathi  staff   595K Apr 25 08:22 colorized.png\n-rw-r--r--  1 abharathi  staff   712B Apr 25 08:22 colorized.png.aux.xml\n-rw-r--r--  1 abharathi  staff   148M Apr 25 08:12 colorized_dem.tif\n</pre> <p></p> In\u00a0[87]: Copied! <pre>0.00009259259259 * 111320\n</pre> 0.00009259259259 * 111320 Out[87]: <pre>10.3074074071188</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#gdal-inspecting-rasters","title":"GDAL - Inspecting rasters\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#set-up","title":"Set up\u00b6","text":"<p>Use - <code>opengeo</code> micromamba env. Move to <code>gdal-tools</code> folder.</p>"},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#intro","title":"Intro\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#tools-with-gdal","title":"Tools with GDAL\u00b6","text":"<p>We get 2 sets of tools - raster tools that come with GDAL and vector tools that come with OGR.</p> <p>3 Main tools</p> <ul> <li><code>gdalinfo</code> - quick metadata</li> <li><code>gdal_translate</code> - convert formats, apply compression</li> <li><code>gdalwarp</code> - resampling and reprojecting</li> </ul> <p>Other tools</p> <ul> <li><code>gdaladdo</code> - pyramid tiles for images for quicker GUI loading</li> <li><code>gdaltindex</code> - tile index will give overviews of tiles</li> <li><code>gdalbuiltvrt</code> - for creating virtual rasters</li> <li><code>gdaldem</code>, <code>gdalcounter</code> - elevation</li> <li><code>gdal_rasterize</code>, <code>gdal_poligonize</code> - for conversion to and from raster &lt;--&gt; vector</li> <li><code>gdal_grid</code> - interpolations</li> <li><code>nearblack</code> - work with edge artifacts in rasters</li> </ul> <p>Python bindings</p> <ul> <li><code>gdal_merge.py</code> - merge rasters</li> <li><code>gdal_calc.py</code> - raster algebra</li> <li><code>gdal_pansharpen.py</code> - for optical images</li> <li><code>gdal_retile.py</code> - tiling / chipping for deep learning</li> </ul> <p>OGR</p> <ul> <li><code>ogrinfo</code> - lists source and metadata</li> <li><code>ogr2ogr</code> - format conversion</li> <li><code>ogrmerge.py</code> - merge vector layers</li> </ul>"},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#exercises","title":"Exercises\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#basic-raster-processing","title":"Basic raster processing\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#making-virtual-rasters","title":"Making virtual rasters\u00b6","text":"<p>Helps save space. A <code>.vrt</code> file is a pointer to the source files and acts like a container.</p>"},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#converting-formats-gdal_translate","title":"Converting formats - <code>gdal_translate</code>\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#convert-format-and-compress","title":"Convert format and compress\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#assign-a-different-no-data-value","title":"Assign a different no data value\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#writing-gog","title":"Writing GOG\u00b6","text":"<p>Need to specify output format using <code>-of COG</code> as COG also has same <code>*.tif</code> extension</p>"},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#using-gdalinfo-on-cog-files-stored-on-the-cloud","title":"Using GDALINFO on COG files stored on the cloud\u00b6","text":"<p>I am using Open aerial map site for this: https://map.openaerialmap.org/#/-117.81875610351562,33.76544869849223,9/latest/643b06ca8cae390005a1456f?_k=9krcby</p>"},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#dems-using-gdaldem","title":"DEMs using <code>gdaldem</code>\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_inspecting_rasters/#dem-classified-color-relief-file","title":"DEM classified (color relief) file\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_intro/","title":"GDAL an Introduction","text":"<p>Course link: https://courses.spatialthoughts.com/gdal-tools.html</p> <p>GDAL is a FOSS4G library for raster and vector geospatial data IO and manipulation. GDAL is usually the foundational library behind several desktop applications such as ArcGIS, QGIS, Google Earth etc.</p>"},{"location":"teaching_resources/gdal/gdal_intro/#installation","title":"Installation","text":"<p>GDAL is a binary CLI. As such, it can be installed using <code>conda</code> or <code>mamba</code></p> <pre><code>$ micromamba install -c conda-forge gdal\n\n# Verify\n(stac) \u279c  gis_data gdalinfo --version\nGDAL 3.5.3, released 2022/10/21\n</code></pre>"},{"location":"teaching_resources/gdal/gdal_intro/#clif-notes","title":"Clif notes","text":"<ul> <li>gdal vs ogr</li> <li>gdalinfo</li> <li>gdalinfo stats</li> <li>virtual raster</li> </ul>"},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/","title":"GDAL - mosaicking, warping, calculating, pansharpening","text":"<p>GDAL - Geospatial Data Abstration Library</p> In\u00a0[2]: Copied! <pre>cd /Users/abharathi/Documents/gis_data/gdal-tools/\n</pre> cd /Users/abharathi/Documents/gis_data/gdal-tools/ <pre>/Users/abharathi/Documents/gis_data/gdal-tools\n</pre> In\u00a0[3]: Copied! <pre>ls\n</pre> ls <pre>1870_southern-india.jpg*      landsat8/\n5d16a93f1cf0f6000579ad2c.tif  london_1m_dsm/\nbatch.py*                     naip/\nbatch_parallel.py*            precipitation.gpkg\nearth_at_night.jpg*           prism/\nearthquakes/                  spatial_query.gpkg\ngdal_stuff.qgz                srtm/\ngeonames/                     worldcities.csv*\n</pre> In\u00a0[1]: Copied! <pre>!gdalinfo /vsigs/spatialthoughts-public-data/viirs_ntl_2021_global.tif --config GS_NO_SIGN_REQUEST YES\n</pre> !gdalinfo /vsigs/spatialthoughts-public-data/viirs_ntl_2021_global.tif --config GS_NO_SIGN_REQUEST YES <pre>Driver: GTiff/GeoTIFF\nFiles: /vsigs/spatialthoughts-public-data/viirs_ntl_2021_global.tif\nSize is 80152, 80196\nCoordinate System is:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06\u00b0S and 85.06\u00b0N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\nData axis to CRS axis mapping: 1,2\nOrigin = (-20038000.000000000000000,20049000.000000000000000)\nPixel Size = (500.000000000000000,-500.000000000000000)\nMetadata:\n  AREA_OR_POINT=Area\nImage Structure Metadata:\n  COMPRESSION=DEFLATE\n  INTERLEAVE=BAND\n  LAYOUT=COG\n  PREDICTOR=2\nCorner Coordinates:\nUpper Left  (-20038000.000,20049000.000) (179d59'44.10\"E, 85d 3'36.09\"N)\nLower Left  (-20038000.000,-20049000.000) (179d59'44.10\"E, 85d 3'36.09\"S)\nUpper Right (20038000.000,20049000.000) (179d59'44.10\"W, 85d 3'36.09\"N)\nLower Right (20038000.000,-20049000.000) (179d59'44.10\"W, 85d 3'36.09\"S)\nCenter      (   0.0000000,   0.0000000) (  0d 0' 0.01\"E,  0d 0' 0.01\"N)\nBand 1 Block=512x512 Type=Float32, ColorInterp=Gray\n  NoData Value=-9999\n  Overviews: 40076x40098, 20038x20049, 10019x10024, 5009x5012, 2504x2506, 1252x1253, 626x626, 313x313\n</pre> In\u00a0[34]: Copied! <pre>cd ../naip\n</pre> cd ../naip <pre>/Users/abharathi/Documents/gis_data/gdal-tools/naip\n</pre> In\u00a0[35]: Copied! <pre>ls\n</pre> ls <pre>aoi.cpg*        output_2_1.jp2* output_3_8.jp2* output_5_7.jp2* output_7_6.jp2*\naoi.dbf*        output_2_2.jp2* output_4_1.jp2* output_5_8.jp2* output_7_7.jp2*\naoi.prj*        output_2_3.jp2* output_4_2.jp2* output_6_1.jp2* output_7_8.jp2*\naoi.qpj*        output_2_4.jp2* output_4_3.jp2* output_6_2.jp2* output_8_1.jp2*\naoi.shp*        output_2_5.jp2* output_4_4.jp2* output_6_3.jp2* output_8_2.jp2*\naoi.shx*        output_2_6.jp2* output_4_5.jp2* output_6_4.jp2* output_8_3.jp2*\nfilelist.txt    output_2_7.jp2* output_4_6.jp2* output_6_5.jp2* output_8_4.jp2*\noutput_1_1.jp2* output_2_8.jp2* output_4_7.jp2* output_6_6.jp2* output_8_5.jp2*\noutput_1_2.jp2* output_3_1.jp2* output_4_8.jp2* output_6_7.jp2* output_8_6.jp2*\noutput_1_3.jp2* output_3_2.jp2* output_5_1.jp2* output_6_8.jp2* output_8_7.jp2*\noutput_1_4.jp2* output_3_3.jp2* output_5_2.jp2* output_7_1.jp2* output_8_8.jp2*\noutput_1_5.jp2* output_3_4.jp2* output_5_3.jp2* output_7_2.jp2*\noutput_1_6.jp2* output_3_5.jp2* output_5_4.jp2* output_7_3.jp2*\noutput_1_7.jp2* output_3_6.jp2* output_5_5.jp2* output_7_4.jp2*\noutput_1_8.jp2* output_3_7.jp2* output_5_6.jp2* output_7_5.jp2*\n</pre> In\u00a0[6]: Copied! <pre>ls *.jp2 &gt; filelist.txt\n</pre> ls *.jp2 &gt; filelist.txt <p>Alpha band is an additional band that simply states which cells have value and which have no-Data. It is an image mask. Alpha is useful when you compress raster using PNG or JP2 formats and reduce quantization to 8bit Uint8. With such low dynamic range, it becomes tricky to use <code>0</code> or <code>255</code> as no-data.</p> In\u00a0[10]: Copied! <pre>!gdalinfo output_1_1.jp2\n</pre> !gdalinfo output_1_1.jp2 <pre>Driver: JP2OpenJPEG/JPEG-2000 driver based on OpenJPEG library\nFiles: output_1_1.jp2\nSize is 5000, 5000\nCoordinate System is:\nPROJCRS[\"NAD83 / UTM zone 10N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 10N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-123,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"North America - between 126\u00b0W and 120\u00b0W - onshore and offshore. Canada - British Columbia; Northwest Territories; Yukon. United States (USA) - California; Oregon; Washington.\"],\n        BBOX[30.54,-126,81.8,-119.99]],\n    ID[\"EPSG\",26910]]\nData axis to CRS axis mapping: 1,2\nOrigin = (545028.000000000000000,4249325.999946000054479)\nPixel Size = (0.600000000000000,-0.600000000600000)\nImage Structure Metadata:\n  INTERLEAVE=PIXEL\n  COMPRESSION_REVERSIBILITY=LOSSY\nCorner Coordinates:\nUpper Left  (  545028.000, 4249326.000) (122d29' 3.80\"W, 38d23'27.65\"N)\nLower Left  (  545028.000, 4246326.000) (122d29' 4.49\"W, 38d21'50.32\"N)\nUpper Right (  548028.000, 4249326.000) (122d27' 0.14\"W, 38d23'27.08\"N)\nLower Right (  548028.000, 4246326.000) (122d27' 0.87\"W, 38d21'49.76\"N)\nCenter      (  546528.000, 4247826.000) (122d28' 2.32\"W, 38d22'38.71\"N)\nBand 1 Block=1024x1024 Type=Byte, ColorInterp=Undefined\n  Overviews: 2500x2500, 1250x1250, 625x625\n  Overviews: arbitrary\n  Image Structure Metadata:\n    COMPRESSION=JPEG2000\nBand 2 Block=1024x1024 Type=Byte, ColorInterp=Undefined\n  Overviews: 2500x2500, 1250x1250, 625x625\n  Overviews: arbitrary\n  Image Structure Metadata:\n    COMPRESSION=JPEG2000\nBand 3 Block=1024x1024 Type=Byte, ColorInterp=Undefined\n  Overviews: 2500x2500, 1250x1250, 625x625\n  Overviews: arbitrary\n  Image Structure Metadata:\n    COMPRESSION=JPEG2000\n</pre> In\u00a0[36]: Copied! <pre>%time\n!gdalbuildvrt -input_file_list filelist.txt naip.vrt\n</pre> %time !gdalbuildvrt -input_file_list filelist.txt naip.vrt <pre>CPU times: user 4 \u00b5s, sys: 1e+03 ns, total: 5 \u00b5s\nWall time: 12.9 \u00b5s\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[38]: Copied! <pre>%%time\n!gdal_translate -of JPEG -outsize 2% 2% naip.vrt naip_preview.jpg\n</pre> %%time !gdal_translate -of JPEG -outsize 2% 2% naip.vrt naip_preview.jpg <pre>Input file size is 40000, 40000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 38.4 ms, sys: 24.2 ms, total: 62.6 ms\nWall time: 3.22 s\n</pre> In\u00a0[39]: Copied! <pre>%%time\n!gdaltindex -write_absolute_path index.json --optfile filelist.txt\n</pre> %%time !gdaltindex -write_absolute_path index.json --optfile filelist.txt <pre>Creating new index file...\nCPU times: user 20 ms, sys: 15.3 ms, total: 35.4 ms\nWall time: 1.17 s\n</pre> In\u00a0[45]: Copied! <pre>cd ../landsat8/\n</pre> cd ../landsat8/ <pre>/Users/abharathi/Documents/gis_data/gdal-tools/landsat8\n</pre> In\u00a0[46]: Copied! <pre>ls -lh\n</pre> ls -lh <pre>total 1845384\n-rwxr-xr-x@ 1 abharathi  staff   110M Oct 12  2019 RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B2.tif*\n-rwxr-xr-x@ 1 abharathi  staff   115M Oct 12  2019 RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B3.tif*\n-rwxr-xr-x@ 1 abharathi  staff   115M Oct 12  2019 RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.tif*\n-rwxr-xr-x@ 1 abharathi  staff   131M Oct 12  2019 RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B5.tif*\n-rwxr-xr-x@ 1 abharathi  staff   430M Oct 12  2019 RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B8.tif*\n-rw-r--r--  1 abharathi  staff   2.5K Apr 26 06:47 rgb.vrt\n</pre> In\u00a0[30]: Copied! <pre>!gdalbuildvrt -o rgb.vrt -separate \\\n  RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.tif \\\n  RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B3.tif \\\n  RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B2.tif\n</pre> !gdalbuildvrt -o rgb.vrt -separate \\   RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.tif \\   RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B3.tif \\   RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B2.tif <pre>0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[47]: Copied! <pre>!gdalbuildvrt -o allbands.vrt -separate \\\n  RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.tif \\\n  RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B3.tif \\\n  RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B2.tif \\\n  RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B5.tif \\\n  RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B8.tif\n</pre> !gdalbuildvrt -o allbands.vrt -separate \\   RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.tif \\   RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B3.tif \\   RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B2.tif \\   RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B5.tif \\   RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B8.tif <pre>0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[41]: Copied! <pre>%%time\n!gdal_translate rgp.vrt rgb.tif -co PHOTOMETRIC=RGB -co COMPRESS=DEFLATE\n</pre> %%time !gdal_translate rgp.vrt rgb.tif -co PHOTOMETRIC=RGB -co COMPRESS=DEFLATE <pre>ERROR 4: rgp.vrt: No such file or directory\nCPU times: user 7.96 ms, sys: 9.33 ms, total: 17.3 ms\nWall time: 631 ms\n</pre> In\u00a0[50]: Copied! <pre>%%time\n!gdal_translate -scale 0 0.3 0 255 -ot Byte rgb.vrt rgb_stretch.tif\n</pre> %%time !gdal_translate -scale 0 0.3 0 255 -ot Byte rgb.vrt rgb_stretch.tif <pre>Input file size is 7701, 7861\nWarning 1: for band 1, nodata value has been clamped to 0, the original value being out of range.\nWarning 1: for band 2, nodata value has been clamped to 0, the original value being out of range.\nWarning 1: for band 3, nodata value has been clamped to 0, the original value being out of range.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 41.6 ms, sys: 21.8 ms, total: 63.4 ms\nWall time: 4.17 s\n</pre> In\u00a0[52]: Copied! <pre>!which gdal_calc.py\n</pre> !which gdal_calc.py <pre>/Users/abharathi/micromamba/envs/opengeo/bin/gdal_calc.py\n</pre> In\u00a0[66]: Copied! <pre>%run -i /Users/abharathi/micromamba/envs/opengeo/bin/gdal_calc.py\n</pre> %run -i /Users/abharathi/micromamba/envs/opengeo/bin/gdal_calc.py <pre>usage: gdal_calc.py [--help] --calc [expression ...] [-a [filename ...]]\n                    [--a_band [n ...]] [-b [filename ...]] [--b_band [n ...]]\n                    [-c [filename ...]] [--c_band [n ...]] [-d [filename ...]]\n                    [--d_band [n ...]] [-e [filename ...]] [--e_band [n ...]]\n                    [-f [filename ...]] [--f_band [n ...]] [-g [filename ...]]\n                    [--g_band [n ...]] [-h [filename ...]] [--h_band [n ...]]\n                    [-i [filename ...]] [--i_band [n ...]] [-j [filename ...]]\n                    [--j_band [n ...]] [-k [filename ...]] [--k_band [n ...]]\n                    [-l [filename ...]] [--l_band [n ...]] [-m [filename ...]]\n                    [--m_band [n ...]] [-n [filename ...]] [--n_band [n ...]]\n                    [-o [filename ...]] [--o_band [n ...]] [-p [filename ...]]\n                    [--p_band [n ...]] [-q [filename ...]] [--q_band [n ...]]\n                    [-r [filename ...]] [--r_band [n ...]] [-s [filename ...]]\n                    [--s_band [n ...]] [-t [filename ...]] [--t_band [n ...]]\n                    [-u [filename ...]] [--u_band [n ...]] [-v [filename ...]]\n                    [--v_band [n ...]] [-w [filename ...]] [--w_band [n ...]]\n                    [-x [filename ...]] [--x_band [n ...]] [-y [filename ...]]\n                    [--y_band [n ...]] [-z [filename ...]] [--z_band [n ...]]\n                    [-A [filename ...]] [--A_band [n ...]] [-B [filename ...]]\n                    [--B_band [n ...]] [-C [filename ...]] [--C_band [n ...]]\n                    [-D [filename ...]] [--D_band [n ...]] [-E [filename ...]]\n                    [--E_band [n ...]] [-F [filename ...]] [--F_band [n ...]]\n                    [-G [filename ...]] [--G_band [n ...]] [-H [filename ...]]\n                    [--H_band [n ...]] [-I [filename ...]] [--I_band [n ...]]\n                    [-J [filename ...]] [--J_band [n ...]] [-K [filename ...]]\n                    [--K_band [n ...]] [-L [filename ...]] [--L_band [n ...]]\n                    [-M [filename ...]] [--M_band [n ...]] [-N [filename ...]]\n                    [--N_band [n ...]] [-O [filename ...]] [--O_band [n ...]]\n                    [-P [filename ...]] [--P_band [n ...]] [-Q [filename ...]]\n                    [--Q_band [n ...]] [-R [filename ...]] [--R_band [n ...]]\n                    [-S [filename ...]] [--S_band [n ...]] [-T [filename ...]]\n                    [--T_band [n ...]] [-U [filename ...]] [--U_band [n ...]]\n                    [-V [filename ...]] [--V_band [n ...]] [-W [filename ...]]\n                    [--W_band [n ...]] [-X [filename ...]] [--X_band [n ...]]\n                    [-Y [filename ...]] [--Y_band [n ...]] [-Z [filename ...]]\n                    [--Z_band [n ...]] --outfile filename\n                    [--NoDataValue value] [--hideNoData] [--type datatype]\n                    [--format gdal_format] [--creation-option option]\n                    [--allBands [a-z, A-Z]] [--overwrite] [--debug] [--quiet]\n                    [--color-table COLOR_TABLE]\n                    [--extent {ignore,fail,union,intersect} | --projwin ulx uly lrx lry]\n                    [--projectionCheck]\ngdal_calc.py: error: the following arguments are required: --calc, --outfile\n</pre> <pre>\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit: 2\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%time\n%run -i /Users/abharathi/micromamba/envs/opengeo/bin/gdal_calc.py \\\n  -A RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B5.TIF \\\n  -B RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.TIF \\\n  --outfile ndvi.tif --calc=\"(A-B)/(A+B)\" --NoDataValue=-999\n</pre> %%time %run -i /Users/abharathi/micromamba/envs/opengeo/bin/gdal_calc.py \\   -A RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B5.TIF \\   -B RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.TIF \\   --outfile ndvi.tif --calc=\"(A-B)/(A+B)\" --NoDataValue=-999 In\u00a0[69]: Copied! <pre>%%time\n!gdalbuildvrt -o nrg.vrt -separate \\\nRT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B5.tif \\\nRT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.tif \\\nRT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B3.tif\n</pre> %%time !gdalbuildvrt -o nrg.vrt -separate \\ RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B5.tif \\ RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B4.tif \\ RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B3.tif <pre>0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 5.41 ms, sys: 9.25 ms, total: 14.7 ms\nWall time: 424 ms\n</pre> In\u00a0[73]: Copied! <pre>%%time\n!gdal_translate -of JPEG nrg.vrt nrg.jpg -co COMPRESS=JPG -scale 0 0.3 0 255 -ot Byte -outsize 10% 10%\n</pre> %%time !gdal_translate -of JPEG nrg.vrt nrg.jpg -co COMPRESS=JPG -scale 0 0.3 0 255 -ot Byte -outsize 10% 10% <pre>Input file size is 7701, 7861\nWarning 1: for band 1, nodata value has been clamped to 0, the original value being out of range.\nWarning 1: for band 2, nodata value has been clamped to 0, the original value being out of range.\nWarning 1: for band 3, nodata value has been clamped to 0, the original value being out of range.\nWarning 6: driver JPEG does not support creation option COMPRESS\n0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 9.38 ms, sys: 11.1 ms, total: 20.4 ms\nWall time: 589 ms\n</pre> <p></p> In\u00a0[76]: Copied! <pre>!which gdal_pansharpen.py\n</pre> !which gdal_pansharpen.py <pre>/Users/abharathi/micromamba/envs/opengeo/bin/gdal_pansharpen.py\n</pre> In\u00a0[78]: Copied! <pre>%%time\n%run -i /Users/abharathi/micromamba/envs/opengeo/bin/gdal_pansharpen.py \\\nRT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B8.tif \\\nrgb.vrt pansharpened.vrt -r bilinear -co COMPRESS=DEFLATE -co PHOTOMETRIC=RGB\n</pre> %%time %run -i /Users/abharathi/micromamba/envs/opengeo/bin/gdal_pansharpen.py \\ RT_LC08_L1TP_137042_20190920_20190926_01_T1_2019-09-20_B8.tif \\ rgb.vrt pansharpened.vrt -r bilinear -co COMPRESS=DEFLATE -co PHOTOMETRIC=RGB <pre>CPU times: user 6.39 ms, sys: 9.63 ms, total: 16 ms\nWall time: 68.6 ms\n</pre> In\u00a0[80]: Copied! <pre>%%time\n!gdal_translate -of JPEG pansharpened.vrt pansharpened.jpg -scale 0 0.3 0 255 -ot Byte -outsize 40% 40%\n</pre> %%time !gdal_translate -of JPEG pansharpened.vrt pansharpened.jpg -scale 0 0.3 0 255 -ot Byte -outsize 40% 40% <pre>Input file size is 15401, 15721\nWarning 1: for band 1, nodata value has been clamped to 0, the original value being out of range.\nWarning 1: for band 2, nodata value has been clamped to 0, the original value being out of range.\nWarning 1: for band 3, nodata value has been clamped to 0, the original value being out of range.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 97.1 ms, sys: 40.8 ms, total: 138 ms\nWall time: 12.8 s\n</pre> In\u00a0[83]: Copied! <pre>cd ../london_1m_dsm/\n</pre> cd ../london_1m_dsm/ <pre>/Users/abharathi/Documents/gis_data/gdal-tools/london_1m_dsm\n</pre> In\u00a0[84]: Copied! <pre>ls *.asc &gt; filelist.txt\n</pre> ls *.asc &gt; filelist.txt In\u00a0[85]: Copied! <pre>%%time\n!gdalbuildvrt  -input_file_list filelist.txt londondsm.vrt\n</pre> %%time !gdalbuildvrt  -input_file_list filelist.txt londondsm.vrt <pre>0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 25.1 ms, sys: 17.7 ms, total: 42.8 ms\nWall time: 1.55 s\n</pre> In\u00a0[86]: Copied! <pre># No CRS in the vrt\n!gdalinfo londondsm.vrt\n</pre> # No CRS in the vrt !gdalinfo londondsm.vrt <pre>Driver: VRT/Virtual Raster\nFiles: londondsm.vrt\n       tq3080_DSM_1M.asc\n       tq3081_DSM_1M.asc\n       tq3082_DSM_1M.asc\n       tq3083_DSM_1M.asc\n       tq3084_DSM_1M.asc\n       tq3180_DSM_1M.asc\n       tq3181_DSM_1M.asc\n       tq3182_DSM_1M.asc\n       tq3183_DSM_1M.asc\n       tq3184_DSM_1M.asc\n       tq3280_DSM_1M.asc\n       tq3281_DSM_1M.asc\n       tq3282_DSM_1M.asc\n       tq3283_DSM_1M.asc\n       tq3284_DSM_1M.asc\n       tq3380_DSM_1M.asc\n       tq3381_DSM_1M.asc\n       tq3382_DSM_1M.asc\n       tq3383_DSM_1M.asc\n       tq3384_DSM_1M.asc\n       tq3480_DSM_1M.asc\n       tq3481_DSM_1M.asc\n       tq3482_DSM_1M.asc\n       tq3483_DSM_1M.asc\n       tq3484_DSM_1M.asc\nSize is 5000, 5000\nOrigin = (530000.000000000000000,185000.000000000000000)\nPixel Size = (1.000000000000000,-1.000000000000000)\nCorner Coordinates:\nUpper Left  (  530000.000,  185000.000) \nLower Left  (  530000.000,  180000.000) \nUpper Right (  535000.000,  185000.000) \nLower Right (  535000.000,  180000.000) \nCenter      (  532500.000,  182500.000) \nBand 1 Block=128x128 Type=Float32, ColorInterp=Undefined\n  NoData Value=-9999\n</pre> In\u00a0[89]: Copied! <pre>%%time\n#Assign SRS\n!gdal_translate londondsm.vrt londondsm.tif \\\n  -co COMPRESS=DEFLATE -co PREDICTOR=2 \\\n  -a_srs EPSG:27700\n</pre> %%time #Assign SRS !gdal_translate londondsm.vrt londondsm.tif \\   -co COMPRESS=DEFLATE -co PREDICTOR=2 \\   -a_srs EPSG:27700 <pre>Input file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 30.6 ms, sys: 18.2 ms, total: 48.8 ms\nWall time: 3.3 s\n</pre> In\u00a0[\u00a0]: Copied! <pre>!gdaltransform -s_srs EPSG:4326\n</pre> !gdaltransform -s_srs EPSG:4326"},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#gdal-mosaicking-warping-calculating-pansharpening","title":"GDAL - mosaicking, warping, calculating, pansharpening\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#set-up","title":"Set up\u00b6","text":"<p>Use - <code>opengeo</code> micromamba env. Move to <code>gdal-tools</code> folder.</p>"},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#talk-to-cloud-systems-via-streaming","title":"Talk to Cloud systems via streaming\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#working-with-aerial-imagery","title":"Working with aerial imagery\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#mosaicing-aerial-images","title":"Mosaicing aerial images\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#build-virtual-tile-that-mosaics-the-drone-images-using-gdalbuildvrt","title":"Build virtual tile that mosaics the drone images using <code>gdalbuildvrt</code>\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#create-overview-file-for-visualizing-gdal_translate","title":"Create overview file for visualizing <code>gdal_translate</code>\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#processing-satellite-image","title":"Processing satellite image\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#use-gdalbuildvrt-to-composite-bands-into-a-virtual-raster","title":"Use <code>gdalbuildvrt</code> to composite bands into a virtual raster\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#use-gdaltranslate-to-convert-virtual-to-persisted-file","title":"Use <code>gdaltranslate</code> to convert virtual to persisted file\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#build-a-visualization-product-that-applies-histogram-stretch","title":"Build a visualization product that applies histogram stretch\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#raster-algebra-using-gdal_calcpy","title":"Raster algebra using <code>gdal_calc.py</code>\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#performing-ndvi","title":"Performing NDVI\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#exercise-creating-fcc-viz-product","title":"Exercise - creating FCC viz product\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#pansharpen","title":"Pansharpen\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#exercise-lidar-of-uk-viewshed-analysis","title":"Exercise - Lidar of UK viewshed analysis\u00b6","text":"<p>STEPS</p> <ol> <li>mosaic using buildvrt</li> <li>assign SRS of 4326 and persist raster</li> <li>reproject using gdaltransform</li> <li>gdal_viewshed to produce the viewshed raster</li> </ol>"},{"location":"teaching_resources/gdal/gdal_mosaicking_warping_pansharpening/#reproject","title":"Reproject\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_wms/","title":"GDAL - interacting with WMS","text":"<p>GDAL - Geospatial Data Abstration Library</p> In\u00a0[1]: Copied! <pre>cd /Users/abharathi/Documents/gis_data/gdal-tools/\n</pre> cd /Users/abharathi/Documents/gis_data/gdal-tools/ <pre>/Users/abharathi/Documents/gis_data/gdal-tools\n</pre> In\u00a0[2]: Copied! <pre>ls\n</pre> ls <pre>1870_southern-india.jpg*      landsat8/\n5d16a93f1cf0f6000579ad2c.tif  london_1m_dsm/\nbatch.py*                     naip/\nbatch_parallel.py*            precipitation.gpkg\nearth_at_night.jpg*           prism/\nearthquakes/                  spatial_query.gpkg\ngdal_stuff.qgz                srtm/\ngeonames/                     worldcities.csv*\n</pre> In\u00a0[3]: Copied! <pre>!gdalinfo \"WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?version=1.3.0\"\n</pre> !gdalinfo \"WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?version=1.3.0\" <pre>Driver: WMS/OGC Web Map Service\nFiles: none associated\nSize is 512, 512\nSubdatasets:\n  SUBDATASET_1_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=cartographic%3Anational-boundaries&amp;CRS=EPSG:4326&amp;BBOX=-55.792,-180.0,83.667,180.0\n  SUBDATASET_1_DESC=cartographic:national-boundaries\n  SUBDATASET_2_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=cartographic%3A00-global-labels&amp;CRS=CRS:84&amp;BBOX=-177.225,-51.741,178.45,78.877\n  SUBDATASET_2_DESC=Global Labels\n  SUBDATASET_3_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=cartographic%3A00-gpw-v3-national-admin-boundaries&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_3_DESC=National Boundaries Medium Scale\n  SUBDATASET_4_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=cartographic%3A00-gpw-v3-national-admin-boundaries_generalized&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.625\n  SUBDATASET_4_DESC=National Boundaries Small Scale\n  SUBDATASET_5_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=cartographic%3A00-grump-v1-national-admin-boundaries&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.767,180.0,83.633\n  SUBDATASET_5_DESC=National Boundaries Large Scale\n  SUBDATASET_6_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=aglands%3Aaglands-croplands-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.978,89.989\n  SUBDATASET_6_DESC=Global Agricultural Lands: Croplands, 2000\n  SUBDATASET_7_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=aglands%3Aaglands-pastures-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.978,89.989\n  SUBDATASET_7_DESC=Global Agricultural Lands: Pastures, 2000\n  SUBDATASET_8_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=anthromes%3Aanthromes-anthropogenic-biomes-world-v1&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.75,180.0,83.083\n  SUBDATASET_8_DESC=Anthropogenic Biomes v1\n  SUBDATASET_9_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=anthromes%3Aanthromes-anthropogenic-biomes-world-v2-1700&amp;CRS=CRS:84&amp;BBOX=-180.00000000000003,-89.99999999999996,179.99999999999974,89.99999999999993\n  SUBDATASET_9_DESC=Anthropogenic Biomes v2: 1700\n  SUBDATASET_10_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=anthromes%3Aanthromes-anthropogenic-biomes-world-v2-1800&amp;CRS=CRS:84&amp;BBOX=-180.00000000000003,-89.99999999999996,179.99999999999974,89.99999999999993\n  SUBDATASET_10_DESC=Anthropogenic Biomes v2: 1800\n  SUBDATASET_11_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=anthromes%3Aanthromes-anthropogenic-biomes-world-v2-1900&amp;CRS=CRS:84&amp;BBOX=-180.00000000000006,-89.99999999999994,179.99999999999972,89.99999999999994\n  SUBDATASET_11_DESC=Anthropogenic Biomes v2: 1900\n  SUBDATASET_12_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=anthromes%3Aanthromes-anthropogenic-biomes-world-v2-2000&amp;CRS=CRS:84&amp;BBOX=-180.00000000000003,-89.99999999999996,179.99999999999974,89.99999999999993\n  SUBDATASET_12_DESC=Anthropogenic Biomes v2: 2000\n  SUBDATASET_13_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=crop-climate%3Acrop-climate-effects-climate-global-food-production&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.767,180.0,83.633\n  SUBDATASET_13_DESC=Crop Climate: Maize, Rice, and Wheat\n  SUBDATASET_14_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=energy%3Aenergy-pop-exposure-nuclear-plants-locations_plants&amp;CRS=CRS:84&amp;BBOX=-124.209044,-33.968002,166.539698,68.050658\n  SUBDATASET_14_DESC=Nuclear Power Plant Locations\n  SUBDATASET_15_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_15_DESC=EPI 2010\n  SUBDATASET_16_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_agriculture&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_16_DESC=EPI 2010_Agriculture\n  SUBDATASET_17_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_air-pollution-effects-on-ecosystems&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_17_DESC=EPI 2010: Air Pollution Effects on Ecosystems\n  SUBDATASET_18_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_air-pollution-effects-on-human-health&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_18_DESC=EPI 2010: Air Pollution Effects on Human Health\n  SUBDATASET_19_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_biodiversity-and-habitat&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_19_DESC=EPI 2010: Biodiversity and Habitat\n  SUBDATASET_20_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_climate-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_20_DESC=EPI 2010: Climate Change\n  SUBDATASET_21_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_environmental-burden-of-disease&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_21_DESC=EPI 2010: Environmental Burden of Disease\n  SUBDATASET_22_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_fisheries&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_22_DESC=EPI 2010: Fisheries\n  SUBDATASET_23_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_forestry&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_23_DESC=EPI 2010: Forestry\n  SUBDATASET_24_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_water-effects-on-ecosystems&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_24_DESC=EPI 2010: Water Effects on Ecosystems\n  SUBDATASET_25_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2010_water-effects-on-human-health&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_25_DESC=EPI 2010: Water Effects on Human Health\n  SUBDATASET_26_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_26_DESC=Environmental Performance Index 2014\n  SUBDATASET_27_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_eh&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_27_DESC=EPI 2014: Environmental Health Objective\n  SUBDATASET_28_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_eh-air-quality&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_28_DESC=EPI 2014: Environmental Health Objective - Air Quality\n  SUBDATASET_29_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_eh-health-impacts&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_29_DESC=EPI 2014: Environmental Health Objective - Health Impacts\n  SUBDATASET_30_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_eh-water-and-sanitation&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_30_DESC=EPI 2014: Environmental Health Objective - Water and Sanitation\n  SUBDATASET_31_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_ev&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_31_DESC=EPI 2014: Ecosystem Vitality Objective\n  SUBDATASET_32_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_ev-agriculture&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_32_DESC=EPI 2014: Ecosystem Vitality Objective - Agriculture\n  SUBDATASET_33_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_ev-biodiversity-and-habitat&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_33_DESC=EPI 2014: Ecosystem Vitality Objective - Biodiversity and Habitat\n  SUBDATASET_34_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_ev-climate-and-energy&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_34_DESC=EPI 2014: Ecosystem Vitality Objective - Climate and Energy\n  SUBDATASET_35_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_ev-fisheries&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_35_DESC=EPI 2014: Ecosystem Vitality Objective - Fisheries\n  SUBDATASET_36_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_ev-forests&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049&amp;TRANSPARENT=FALSE\n  SUBDATASET_36_DESC=EPI 2014: Ecosystem Vitality Objective - Forests\n  SUBDATASET_37_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_ev-water-resources&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_37_DESC=EPI 2014: Ecosystem Vitality Objective - Water Resources\n  SUBDATASET_38_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2014_percent-change&amp;CRS=CRS:84&amp;BBOX=-179.99999999977967,-55.76666666677943,180.0,83.63333333322049\n  SUBDATASET_38_DESC=EPI 2014: Ten-Year Percent Change\n  SUBDATASET_39_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_39_DESC=EPI 2016\n  SUBDATASET_40_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_eh&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_40_DESC=EPI 2016_Environmental Health Objective\n  SUBDATASET_41_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_eh-air-quality&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_41_DESC=EPI 2016_Environmental Health Objective - Air Quality\n  SUBDATASET_42_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_eh-health-impacts&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_42_DESC=EPI 2016_Environmental Health Objective - Health Impacts\n  SUBDATASET_43_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_eh-water-and-sanitation&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_43_DESC=EPI 2016_Environmental Health Objective - Water and Sanitation\n  SUBDATASET_44_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_ev&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_44_DESC=EPI 2016_Ecosystem Vitality Objective\n  SUBDATASET_45_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_ev-agriculture&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_45_DESC=EPI 2016_Ecosystem Vitality Objective - Agriculture\n  SUBDATASET_46_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_ev-biodiversity-and-habitat&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_46_DESC=EPI 2016_Ecosystem Vitality Objective - Biodiversity and Habitat\n  SUBDATASET_47_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_ev-climate-and-energy&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_47_DESC=EPI 2016_Ecosystem Vitality Objective - Climate and Energy\n  SUBDATASET_48_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_ev-fisheries&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_48_DESC=EPI 2016_Ecosystem Vitality Objective - Fisheries\n  SUBDATASET_49_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_ev-forests&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_49_DESC=EPI 2016_Ecosystem Vitality Objective - Forests\n  SUBDATASET_50_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_ev-water-resources&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_50_DESC=EPI 2016_Ecosystem Vitality Objective - Water Resources\n  SUBDATASET_51_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2016_percent-change&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.484279632568246,180.00000000000043,83.62741851806643\n  SUBDATASET_51_DESC=EPI 2016_Ten-Year Percent Change\n  SUBDATASET_52_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_52_DESC=EPI 2018\n  SUBDATASET_53_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_eco&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_53_DESC=EPI 2018_Ecosystem Vitality Objective\n  SUBDATASET_54_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_eco-agriculture&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_54_DESC=EPI 2018_Ecosystem Vitality Objective - Agriculture\n  SUBDATASET_55_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_eco-air-pollution&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_55_DESC=EPI 2018_Ecosystem Vitality Objective - Air Pollution\n  SUBDATASET_56_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_eco-biodiversity-and-habitat&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_56_DESC=EPI 2018_Ecosystem Vitality Objective - Biodiversity and Habitat\n  SUBDATASET_57_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_eco-climate-and-energy&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_57_DESC=EPI 2018_Ecosystem Vitality Objective - Climate and Energy\n  SUBDATASET_58_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_eco-fisheries&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_58_DESC=EPI 2018_Ecosystem Vitality Objective - Fisheries\n  SUBDATASET_59_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_eco-forests&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_59_DESC=EPI 2018_Ecosystem Vitality Objective - Forests\n  SUBDATASET_60_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_eco-water-resources&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_60_DESC=EPI 2018_Ecosystem Vitality Objective - Water Resources\n  SUBDATASET_61_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_hlt&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_61_DESC=EPI 2018_Environmental Health Objective\n  SUBDATASET_62_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_hlt-air-quality&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_62_DESC=EPI 2018_Environmental Health Objective - Air Quality\n  SUBDATASET_63_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_hlt-heavy-metals&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_63_DESC=EPI 2018_Environmental Health Objective - Heavy Metals\n  SUBDATASET_64_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2018_hlt-water-and-sanitation&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_64_DESC=EPI 2018_Environmental Health Objective - Water and Sanitation\n  SUBDATASET_65_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_65_DESC=EPI 2020\n  SUBDATASET_66_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_eco&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_66_DESC=EPI 2020_Ecosystem Vitality Objective\n  SUBDATASET_67_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_eco-agriculture&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_67_DESC=EPI 2020_Ecosystem Vitality Objective - Agriculture\n  SUBDATASET_68_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_eco-biodiversity-and-habitat&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_68_DESC=EPI 2020_Ecosystem Vitality Objective - Biodiversity and Habitat\n  SUBDATASET_69_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_eco-climate-change&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_69_DESC=EPI 2020_Ecosystem Vitality Objective - Climate Change\n  SUBDATASET_70_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_eco-ecosystem-services&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_70_DESC=EPI 2020_Ecosystem Vitality Objective - Ecosystem Services\n  SUBDATASET_71_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_eco-fisheries&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_71_DESC=EPI 2020_Ecosystem Vitality Objective - Fisheries\n  SUBDATASET_72_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_eco-pollution-emissions&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_72_DESC=EPI 2020_Ecosystem Vitality Objective - Pollution Emissions\n  SUBDATASET_73_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_eco-water-resources&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_73_DESC=EPI 2020_Ecosystem Vitality Objective - Water Resources\n  SUBDATASET_74_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_hlt&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_74_DESC=EPI 2020_Environmental Health Objective\n  SUBDATASET_75_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_hlt-air-quality&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_75_DESC=EPI 2020_Environmental Health Objective - Air Quality\n  SUBDATASET_76_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_hlt-heavy-metals&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_76_DESC=EPI 2020_Environmental Health Objective - Heavy Metals\n  SUBDATASET_77_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_hlt-sanitation-and-drinking-water&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_77_DESC=EPI 2020_Environmental Health Objective - Sanitation and Drinking Water\n  SUBDATASET_78_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=epi%3Aepi-environmental-performance-index-2020_hlt-waste-management&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010002,83.63333333326676\n  SUBDATASET_78_DESC=EPI 2020_Environmental Health Objective - Waste Management\n  SUBDATASET_79_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_79_DESC=Environmental Sustainability Index 2005\n  SUBDATASET_80_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_air-quality&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_80_DESC=ESI 2005: Air Quality\n  SUBDATASET_81_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_biodiversity&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_81_DESC=ESI 2005: Biodiversity\n  SUBDATASET_82_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_eco-efficiency&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_82_DESC=ESI 2005: Eco-Efficiency\n  SUBDATASET_83_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_environ-governance&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_83_DESC=ESI 2005: Environmental Governance\n  SUBDATASET_84_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_environ-health&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_84_DESC=ESI 2005: Environmental Health\n  SUBDATASET_85_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_greenhouse-gas-emissions&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_85_DESC=ESI 2005: Greenhouse Gas Emissions\n  SUBDATASET_86_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_human-sustenance&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_86_DESC=ESI 2005: Human Sustenance\n  SUBDATASET_87_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_land&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_87_DESC=ESI 2005: Land\n  SUBDATASET_88_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_nat-disaster-vulnerability&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_88_DESC=ESI 2005: Natural Disaster Vulnerability\n  SUBDATASET_89_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_nat-resource-mgmt&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_89_DESC=ESI 2005: Natural Resource Management\n  SUBDATASET_90_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_particip-international-collaborate&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_90_DESC=ESI 2005: Participation in International Collaborations\n  SUBDATASET_91_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_private-sector-response&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_91_DESC=ESI 2005: Private Sector Response\n  SUBDATASET_92_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_reduce-air-pollution&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_92_DESC=ESI 2005: Reduction of Air Pollution\n  SUBDATASET_93_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_reduce-ecosystem-stress&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_93_DESC=ESI 2005: Reduction of Ecosystem Stress\n  SUBDATASET_94_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_reduce-pop-press&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_94_DESC=ESI 2005: Reduction of Population Pressure\n  SUBDATASET_95_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_reduce-trans-environ-press&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_95_DESC=ESI 2005: Reduction of Trans-Environmental Pressure\n  SUBDATASET_96_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_reduce-waste-consump-press&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_96_DESC=ESI 2005: Reduction of Waste and Consumption Pressure\n  SUBDATASET_97_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_reduce-water-stress&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_97_DESC=ESI 2005: Reduction of Water Stress\n  SUBDATASET_98_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_science-tech&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_98_DESC=ESI 2005: Science and Technology\n  SUBDATASET_99_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_water-quality&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_99_DESC=ESI 2005: Water Quality\n  SUBDATASET_100_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=esi%3Aesi-environmental-sustainability-index-2005_water-quantity&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_100_DESC=ESI 2005: Water Quantity\n  SUBDATASET_101_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=cartographic%3Aesri-administrative-boundaries_level-1&amp;CRS=CRS:84&amp;BBOX=-180.0,-85.47,180.0,83.624\n  SUBDATASET_101_DESC=ESRI Administrative Boundaries - Level 1\n  SUBDATASET_102_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-nitrogen-fertilizer-application&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.761,180.0,83.739\n  SUBDATASET_102_DESC=Nitrogen Fertilizer Application\n  SUBDATASET_103_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-nitrogen-in-manure-production&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.761,180.0,83.739\n  SUBDATASET_103_DESC=Nitrogen in Manure Production\n  SUBDATASET_104_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-glyphosate-alfalfa-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_104_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Glyphosate Alfalfa High Estimate\n  SUBDATASET_105_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-glyphosate-corn-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_105_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Glyphosate Corn High Estimate\n  SUBDATASET_106_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-glyphosate-cotton-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_106_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Glyphosate Cotton High Estimate\n  SUBDATASET_107_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-glyphosate-other-crops-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_107_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Glyphosate Other Crops High Estimate\n  SUBDATASET_108_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-glyphosate-pasture-hay-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_108_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Glyphosate Pasture and Hay High Estimate\n  SUBDATASET_109_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-glyphosate-soybean-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_109_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Glyphosate Soybean High Estimate\n  SUBDATASET_110_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-glyphosate-wheat-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_110_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Glyphosate Wheat High Estimate\n  SUBDATASET_111_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-metam-vegetables-fruits-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_111_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Metam Vegetables and Fruits High Estimate\n  SUBDATASET_112_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-petroleum-oil-orchards-grapes-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_112_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Petroleum Oil Orchards and Grapes High Estimate\n  SUBDATASET_113_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids-v1-01_app-rate-propanil-rice-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_113_DESC=PEST-CHEMGRIDS_2015, v1.01 Application Rate Propanil Rice High Estimate\n  SUBDATASET_114_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-glyphosate-alfalfa-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_114_DESC=PEST-CHEMGRIDS_2015 Application Rate Glyphosate Alfalfa High Estimate\n  SUBDATASET_115_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-glyphosate-corn-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_115_DESC=PEST-CHEMGRIDS_2015 Application Rate Glyphosate Corn High Estimate\n  SUBDATASET_116_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-glyphosate-cotton-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_116_DESC=PEST-CHEMGRIDS_2015 Application Rate Glyphosate Cotton High Estimate\n  SUBDATASET_117_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-glyphosate-other-crops-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_117_DESC=PEST-CHEMGRIDS_2015 Application Rate Glyphosate Other Crops High Estimate\n  SUBDATASET_118_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-glyphosate-pasture-hay-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_118_DESC=PEST-CHEMGRIDS_2015 Application Rate Glyphosate Pasture and Hay High Estimate\n  SUBDATASET_119_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-glyphosate-soybean-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_119_DESC=PEST-CHEMGRIDS_2015 Application Rate Glyphosate Soybean High Estimate\n  SUBDATASET_120_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-glyphosate-wheat-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_120_DESC=PEST-CHEMGRIDS_2015 Application Rate Glyphosate Wheat High Estimate\n  SUBDATASET_121_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-metam-vegetables-fruits-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_121_DESC=PEST-CHEMGRIDS_2015 Application Rate Metam Vegetables and Fruits High Estimate\n  SUBDATASET_122_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-petroleum-oil-orchards-grapes-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_122_DESC=PEST-CHEMGRIDS_2015 Application Rate Petroleum Oil Orchards and Grapes High Estimate\n  SUBDATASET_123_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-pest-chemgrids_app-rate-propanil-rice-high-est-2015&amp;CRS=CRS:84&amp;BBOX=-178.91666666666666,-56.04249827067055,179.91666666666666,84.04083506266277\n  SUBDATASET_123_DESC=PEST-CHEMGRIDS_2015 Application Rate Propanil Rice High Estimate\n  SUBDATASET_124_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-phosphorus-fertilizer-application&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.761,180.0,83.739\n  SUBDATASET_124_DESC=Phosphorus Fertilizer Application\n  SUBDATASET_125_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ferman-v1%3Aferman-v1-phosphorus-in-manure-production&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.761,180.0,83.739\n  SUBDATASET_125_DESC=Phosphorus in Manure Production\n  SUBDATASET_126_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_average&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_126_DESC=Food Insecurity Hotspots_Average\n  SUBDATASET_127_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_duration-crisis&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_127_DESC=Food Insecurity Hotspots_Duration Crisis\n  SUBDATASET_128_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_duration-emergency&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_128_DESC=Food Insecurity Hotspots_Duration Emergency\n  SUBDATASET_129_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_duration-famine&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_129_DESC=Food Insecurity Hotspots_Duration Famine\n  SUBDATASET_130_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_duration-minimal&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_130_DESC=Food Insecurity Hotspots_Duration Minimal\n  SUBDATASET_131_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_duration-stressed&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_131_DESC=Food Insecurity Hotspots_Duration Stressed\n  SUBDATASET_132_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_frequency-crisis&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_132_DESC=Food Insecurity Hotspots_Frequency Crisis\n  SUBDATASET_133_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_frequency-emergency&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_133_DESC=Food Insecurity Hotspots_Frequency Emergency\n  SUBDATASET_134_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_frequency-famine&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_134_DESC=Food Insecurity Hotspots_Frequency Famine\n  SUBDATASET_135_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_frequency-minimal&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_135_DESC=Food Insecurity Hotspots_Frequency Minimal\n  SUBDATASET_136_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_frequency-stressed&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_136_DESC=Food Insecurity Hotspots_Frequency Stressed\n  SUBDATASET_137_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=food%3Afood-food-insecurity-hotspots_standarddeviation&amp;CRS=CRS:84&amp;BBOX=-92.241493,-26.868685,74.878507,38.483315\n  SUBDATASET_137_DESC=Food Insecurity Hotspots_Standard Deviation\n  SUBDATASET_138_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-centroids&amp;CRS=CRS:84&amp;BBOX=-179.2,-54.752,179.848,79.892\n  SUBDATASET_138_DESC=GPW v3: Centroids\n  SUBDATASET_139_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-coastlines&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.792,180.0,83.667\n  SUBDATASET_139_DESC=GPW v3: Coastlines\n  SUBDATASET_140_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-land-geographic-unit-area&amp;CRS=CRS:84&amp;BBOX=-179.979,-58.021,180.021,84.979\n  SUBDATASET_140_DESC=GPW v3: Geographic Unit Area\n  SUBDATASET_141_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-count-future-estimates_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_141_DESC=GPW v3: Population Count Future Estimates 2005\n  SUBDATASET_142_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-count-future-estimates_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_142_DESC=GPW v3: Population Count Future Estimates 2010\n  SUBDATASET_143_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-count-future-estimates_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_143_DESC=GPW v3: Population Count Future Estimates 2015\n  SUBDATASET_144_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-count_1990&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_144_DESC=GPW v3: Population Count 1990\n  SUBDATASET_145_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-count_1995&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_145_DESC=GPW v3: Population Count 1995\n  SUBDATASET_146_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-count_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_146_DESC=GPW v3: Population Count 2000\n  SUBDATASET_147_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-density-future-estimates_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_147_DESC=GPW v3: Population Density Future Estimates 2005\n  SUBDATASET_148_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-density-future-estimates_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_148_DESC=GPW v3: Population Density Future Estimates 2010\n  SUBDATASET_149_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-density-future-estimates_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_149_DESC=GPW v3: Population Density Future Estimates 2015\n  SUBDATASET_150_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-density_1990&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_150_DESC=GPW v3: Population Density 1990\n  SUBDATASET_151_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-density_1995&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_151_DESC=GPW v3: Population Density 1995\n  SUBDATASET_152_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v3%3Agpw-v3-population-density_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_152_DESC=GPW v3: Population Density 2000\n  SUBDATASET_153_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a000-004bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_153_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 0 to 4\n  SUBDATASET_154_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a000-004ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_154_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 0 to 4\n  SUBDATASET_155_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a000-004mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_155_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 0 to 4\n  SUBDATASET_156_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a000-014bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_156_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 0 to 14\n  SUBDATASET_157_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a000-014ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_157_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 0 to 14\n  SUBDATASET_158_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a000-014mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_158_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 0 to 14\n  SUBDATASET_159_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a005-009bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_159_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 5 to 9\n  SUBDATASET_160_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a005-009ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_160_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 5 to 9\n  SUBDATASET_161_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a005-009mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_161_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 5 to 9\n  SUBDATASET_162_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a010-014bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_162_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 10 to 14\n  SUBDATASET_163_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a010-014ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_163_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 10 to 14\n  SUBDATASET_164_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a010-014mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_164_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 10 to 14\n  SUBDATASET_165_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-019bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_165_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 15 to 19\n  SUBDATASET_166_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-019ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_166_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 15 to 19\n  SUBDATASET_167_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-019mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_167_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 15 to 19\n  SUBDATASET_168_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-049bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_168_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 15 to 49\n  SUBDATASET_169_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-049ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_169_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 15 to 49\n  SUBDATASET_170_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-049mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_170_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 15 to 49\n  SUBDATASET_171_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-064bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_171_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 15 to 64\n  SUBDATASET_172_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-064ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_172_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 15 to 64\n  SUBDATASET_173_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a015-064mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_173_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 15 to 64\n  SUBDATASET_174_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a020-024bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_174_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 20 to 24\n  SUBDATASET_175_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a020-024ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_175_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 20 to 24\n  SUBDATASET_176_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a020-024mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_176_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 20 to 24\n  SUBDATASET_177_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a025-029bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_177_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 25 to 29\n  SUBDATASET_178_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a025-029ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_178_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 25 to 29\n  SUBDATASET_179_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a025-029mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_179_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 25 to 29\n  SUBDATASET_180_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a030-034bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_180_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 30 to 34\n  SUBDATASET_181_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a030-034ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_181_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 30 to 34\n  SUBDATASET_182_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a030-034mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_182_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 30 to 34\n  SUBDATASET_183_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a035-039bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_183_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 35 to 39\n  SUBDATASET_184_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a035-039ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_184_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 35 to 39\n  SUBDATASET_185_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a035-039mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_185_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 35 to 39\n  SUBDATASET_186_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a040-044bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_186_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 40 to 44\n  SUBDATASET_187_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a040-044ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_187_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 40 to 44\n  SUBDATASET_188_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a040-044mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_188_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 40 to 44\n  SUBDATASET_189_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a045-049bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_189_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 45 to 49\n  SUBDATASET_190_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a045-049ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_190_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 45 to 49\n  SUBDATASET_191_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a045-049mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_191_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 45 to 49\n  SUBDATASET_192_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a050-054bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_192_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 50 to 54\n  SUBDATASET_193_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a050-054ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_193_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 50 to 54\n  SUBDATASET_194_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a050-054mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_194_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 50 to 54\n  SUBDATASET_195_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a055-059bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_195_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 55 to 59\n  SUBDATASET_196_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a055-059ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_196_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 55 to 59\n  SUBDATASET_197_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a055-059mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_197_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 55 to 59\n  SUBDATASET_198_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a060-064bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_198_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 60 to 64\n  SUBDATASET_199_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a060-064ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_199_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 60 to 64\n  SUBDATASET_200_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a060-064mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_200_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 60 to 64\n  SUBDATASET_201_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a065-069bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_201_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 65 to 69\n  SUBDATASET_202_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a065-069ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_202_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 65 to 69\n  SUBDATASET_203_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a065-069mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_203_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 65 to 69\n  SUBDATASET_204_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a065plusbt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_204_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 65 and Older\n  SUBDATASET_205_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a065plusft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_205_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 65 and Older\n  SUBDATASET_206_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a065plusmt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_206_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 65 and Older\n  SUBDATASET_207_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a070-074bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_207_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 70 to 74\n  SUBDATASET_208_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a070-074ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_208_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 70 to 74\n  SUBDATASET_209_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a070-074mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_209_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 70 to 74\n  SUBDATASET_210_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a070plusbt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_210_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 70 and Older\n  SUBDATASET_211_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a070plusft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_211_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 70 and Older\n  SUBDATASET_212_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a070plusmt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_212_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 70 and Older\n  SUBDATASET_213_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a075-079bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_213_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 75 to 79\n  SUBDATASET_214_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a075-079ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_214_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 75 to 79\n  SUBDATASET_215_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a075-079mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_215_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 75 to 79\n  SUBDATASET_216_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a075plusbt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_216_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 75 and Older\n  SUBDATASET_217_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a075plusft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_217_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 75 and Older\n  SUBDATASET_218_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a075plusmt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_218_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 75 and Older\n  SUBDATASET_219_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a080-084bt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_219_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 80 to 84\n  SUBDATASET_220_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a080-084ft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_220_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 80 to 84\n  SUBDATASET_221_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a080-084mt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_221_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 80 to 84\n  SUBDATASET_222_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a080plusbt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_222_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 80 and Older\n  SUBDATASET_223_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a080plusft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_223_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 80 and Older\n  SUBDATASET_224_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a080plusmt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_224_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 80 and Older\n  SUBDATASET_225_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a085plusbt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_225_DESC=GPWv4 BDC, v4.11_2010 Total Population Ages 85 and Older\n  SUBDATASET_226_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a085plusft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_226_DESC=GPWv4 BDC, v4.11_2010 Female Population Ages 85 and Older\n  SUBDATASET_227_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_a085plusmt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_227_DESC=GPWv4 BDC, v4.11_2010 Male Population Ages 85 and Older\n  SUBDATASET_228_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_atotpopbt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_228_DESC=GPWv4 BDC, v4.11_2010 Total Population\n  SUBDATASET_229_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_atotpopft-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_229_DESC=GPWv4 BDC, v4.11_2010 Female Population\n  SUBDATASET_230_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-basic-demographic-characteristics-rev11_atotpopmt-2010-cntm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_230_DESC=GPWv4 BDC, v4.11_2010 Male Population\n  SUBDATASET_231_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-data-quality-indicators-rev11_context&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_231_DESC=GPWv4 Data Quality Indicators, v4.11_Data Context\n  SUBDATASET_232_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-data-quality-indicators-rev11_mean-adminunitarea&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_232_DESC=GPWv4 Data Quality Indicators, v4.11_Mean Administrative Unit Area\n  SUBDATASET_233_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-data-quality-indicators-rev11_watermask&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_233_DESC=GPWv4 Data Quality Indicators, v4.11_Water Mask\n  SUBDATASET_234_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-data-quality-indicators_data-context&amp;CRS=CRS:84&amp;BBOX=-180.0,-60.0,179.99999999999983,84.99999999999994\n  SUBDATASET_234_DESC=GPWv4: Data Quality Indicators - Data Context\n  SUBDATASET_235_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-data-quality-indicators_mean-administrative-unit-area&amp;CRS=CRS:84&amp;BBOX=-180.0,-60.0,180.00000000002314,85.00000000000932\n  SUBDATASET_235_DESC=GPWv4: Data Quality Indicators - Mean Administrative Unit Area\n  SUBDATASET_236_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-data-quality-indicators_water-mask&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.0,85.00000000000917\n  SUBDATASET_236_DESC=GPWv4: Data Quality Indicators - Water Mask\n  SUBDATASET_237_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-land-water-area-rev11_landareakm&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_237_DESC=GPWv4 Land and Water Area, v4.11_Land Area\n  SUBDATASET_238_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-land-water-area-rev11_waterareakm&amp;CRS=CRS:84&amp;BBOX=-180.0,-60.0,180.0,85.0\n  SUBDATASET_238_DESC=GPWv4 Land and Water Area, v4.11_Water Area\n  SUBDATASET_239_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-land-water-area_land&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_239_DESC=GPWv4: Land Area\n  SUBDATASET_240_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-land-water-area_water&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_240_DESC=GPWv4: Water Area\n  SUBDATASET_241_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-national-identifier-grid&amp;CRS=CRS:84&amp;BBOX=-180.0,-60.0,180.00000000002314,85.00000000000932\n  SUBDATASET_241_DESC=GPWv4: National Identifier Grid\n  SUBDATASET_242_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-national-identifier-grid-rev11&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.491667,180.0,83.633333\n  SUBDATASET_242_DESC=GPWv4 National Identifier Grid, v4.11\n  SUBDATASET_243_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals-rev11_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_243_DESC=GPWv4 UN WPP-Adjusted Population Count, v4.11_2000\n  SUBDATASET_244_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals-rev11_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_244_DESC=GPWv4 UN WPP-Adjusted Population Count, v4.11_2005\n  SUBDATASET_245_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals-rev11_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_245_DESC=GPWv4 UN WPP-Adjusted Population Count, v4.11_2010\n  SUBDATASET_246_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals-rev11_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_246_DESC=GPWv4 UN WPP-Adjusted Population Count, v4.11_2015\n  SUBDATASET_247_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals-rev11_2020&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_247_DESC=GPWv4 UN WPP-Adjusted Population Count, v4.11_2020\n  SUBDATASET_248_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_248_DESC=GPWv4: UN-Adjusted Population Count - 2000\n  SUBDATASET_249_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_249_DESC=GPWv4: UN-Adjusted Population Count - 2005\n  SUBDATASET_250_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_250_DESC=GPWv4: UN-Adjusted Population Count - 2010\n  SUBDATASET_251_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_251_DESC=GPWv4: UN-Adjusted Population Count - 2015\n  SUBDATASET_252_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-adjusted-to-2015-unwpp-country-totals_2020&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_252_DESC=GPWv4: UN-Adjusted Population Count - 2020\n  SUBDATASET_253_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-rev11_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_253_DESC=GPWv4 Population Count, v4.11_2000\n  SUBDATASET_254_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-rev11_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_254_DESC=GPWv4 Population Count, v4.11_2005\n  SUBDATASET_255_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-rev11_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_255_DESC=GPWv4 Population Count, v4.11_2010\n  SUBDATASET_256_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-rev11_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_256_DESC=GPWv4 Population Count, v4.11_2015\n  SUBDATASET_257_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count-rev11_2020&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_257_DESC=GPWv4 Population Count, v4.11_2020\n  SUBDATASET_258_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_258_DESC=GPWv4: Population Count - 2000\n  SUBDATASET_259_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_259_DESC=GPWv4: Population Count - 2005\n  SUBDATASET_260_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_260_DESC=GPWv4: Population Count - 2010\n  SUBDATASET_261_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_261_DESC=GPWv4: Population Count - 2015\n  SUBDATASET_262_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-count_2020&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.9999999999918,180.00000000000244,85.0000000000092\n  SUBDATASET_262_DESC=GPWv4: Population Count - 2020\n  SUBDATASET_263_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_263_DESC=GPWv4 UN WPP-Adjusted Population Density, v4.11_2000\n  SUBDATASET_264_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_264_DESC=GPWv4 UN WPP-Adjusted Population Density, v4.11_2005\n  SUBDATASET_265_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_265_DESC=GPWv4 UN WPP-Adjusted Population Density, v4.11_2010\n  SUBDATASET_266_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_266_DESC=GPWv4 UN WPP-Adjusted Population Density, v4.11_2015\n  SUBDATASET_267_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11_2020&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_267_DESC=GPWv4 UN WPP-Adjusted Population Density, v4.11_2020\n  SUBDATASET_268_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_268_DESC=GPWv4: UN-Adjusted Population Density - 2000\n  SUBDATASET_269_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_269_DESC=GPWv4: UN-Adjusted Population Density - 2005\n  SUBDATASET_270_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_270_DESC=GPWv4: UN-Adjusted Population Density - 2010\n  SUBDATASET_271_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_271_DESC=GPWv4: UN-Adjusted Population Density - 2015\n  SUBDATASET_272_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-adjusted-to-2015-unwpp-country-totals_2020&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_272_DESC=GPWv4: UN-Adjusted Population Density - 2020\n  SUBDATASET_273_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-rev11_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_273_DESC=GPWv4 Population Density, v4.11_2000\n  SUBDATASET_274_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-rev11_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_274_DESC=GPWv4 Population Density, v4.11_2005\n  SUBDATASET_275_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-rev11_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_275_DESC=GPWv4 Population Density, v4.11_2010\n  SUBDATASET_276_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-rev11_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_276_DESC=GPWv4 Population Density, v4.11_2015\n  SUBDATASET_277_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density-rev11_2020&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_277_DESC=GPWv4 Population Density, v4.11_2020\n  SUBDATASET_278_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_278_DESC=GPWv4: Population Density - 2000\n  SUBDATASET_279_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_279_DESC=GPWv4: Population Density - 2005\n  SUBDATASET_280_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_280_DESC=GPWv4: Population Density - 2010\n  SUBDATASET_281_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density_2015&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_281_DESC=GPWv4: Population Density - 2015\n  SUBDATASET_282_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=gpw-v4%3Agpw-v4-population-density_2020&amp;CRS=CRS:84&amp;BBOX=-180.0,-59.99999999999177,180.00000000000244,85.00000000000922\n  SUBDATASET_282_DESC=GPWv4: Population Density - 2020\n  SUBDATASET_283_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grand-v1%3Agrand-v1-dams-rev01&amp;CRS=CRS:84&amp;BBOX=-153.027,-45.88,176.815,70.387\n  SUBDATASET_283_DESC=Global Reservoir and Dam Database: Dams\n  SUBDATASET_284_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grand-v1%3Agrand-v1-reservoirs-rev01&amp;CRS=CRS:84&amp;BBOX=-153.037,-45.881,176.825,70.398\n  SUBDATASET_284_DESC=Global Reservoir and Dam Database: Reservoirs\n  SUBDATASET_285_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-land-geographic-unit-area&amp;CRS=CRS:84&amp;BBOX=-180.0,-57.0,180.0,84.0\n  SUBDATASET_285_DESC=GRUMP v1: Mean Geographic Unit Area\n  SUBDATASET_286_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-population-count_1990&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_286_DESC=GRUMP v1: Population Count 1990\n  SUBDATASET_287_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-population-count_1995&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_287_DESC=GRUMP v1: Population Count 1995\n  SUBDATASET_288_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-population-count_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_288_DESC=GRUMP v1: Population Count 2000\n  SUBDATASET_289_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-population-density_1990&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_289_DESC=GRUMP v1: Population Density 1990\n  SUBDATASET_290_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-population-density_1995&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_290_DESC=GRUMP v1: Population Density 1995\n  SUBDATASET_291_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-population-density_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_291_DESC=GRUMP v1: Population Density 2000\n  SUBDATASET_292_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-population-density_2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.767,180.0,83.633\n  SUBDATASET_292_DESC=GRUMP v1 Population Density 2010\n  SUBDATASET_293_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-settlement-points&amp;CRS=CRS:84&amp;BBOX=-178.174,-54.8,179.853,78.93\n  SUBDATASET_293_DESC=GRUMP v1: Settlement Points\n  SUBDATASET_294_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grump-v1%3Agrump-v1-urban-extents&amp;CRS=CRS:84&amp;BBOX=-180.0,-57.0,180.0,84.0\n  SUBDATASET_294_DESC=GRUMP v1: Urban Extents\n  SUBDATASET_295_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=icwq%3Aicwq-change-in-chlorophyll-a-concentration-1998-2007&amp;CRS=CRS:84&amp;BBOX=-179.99999999999994,-89.99999999999996,179.99856000000005,89.99928000000004\n  SUBDATASET_295_DESC=Change in Average Annual Chlorophyll-a Concentration from 1998-2000 to 2005-2007 (%)\n  SUBDATASET_296_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-annual-winter-cropped-area-2001-2016_2001&amp;CRS=CRS:84&amp;BBOX=68.10625332,8.08381606,89.150397,37.0738311\n  SUBDATASET_296_DESC=India Annual Winter Cropped Area_2001\n  SUBDATASET_297_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-annual-winter-cropped-area-2001-2016_2016&amp;CRS=CRS:84&amp;BBOX=68.10625332,8.08381606,89.150397,37.0738311\n  SUBDATASET_297_DESC=India Annual Winter Cropped Area_2016\n  SUBDATASET_298_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-village-level-geospatial-socio-econ-1991-2001_female-literates-gj-1991&amp;CRS=CRS:84&amp;BBOX=68.48151121873646,20.121554674908506,74.47763857418519,24.70874662420676\n  SUBDATASET_298_DESC=India Village-Level_Female Literates Gujarat 1991\n  SUBDATASET_299_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-village-level-geospatial-socio-econ-1991-2001_female-literates-gj-2001&amp;CRS=CRS:84&amp;BBOX=68.48151121873646,20.121554674908513,74.47763857418519,24.70874662420676\n  SUBDATASET_299_DESC=India Village-Level_Female Literates Gujarat 2001\n  SUBDATASET_300_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-village-level-geospatial-socio-econ-1991-2001_male-agricultural-laborers-mh-1991&amp;CRS=CRS:84&amp;BBOX=72.6516661583423,15.607726277665256,80.89978274910743,22.029491567915112\n  SUBDATASET_300_DESC=India Village-Level_Male Agricultural Laborers Maharashtra 1991\n  SUBDATASET_301_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-village-level-geospatial-socio-econ-1991-2001_male-agricultural-laborers-mh-2001&amp;CRS=CRS:84&amp;BBOX=72.64414841872758,15.607726277665256,80.89978274910743,22.029491567915116\n  SUBDATASET_301_DESC=India Village-Level_Male Agricultural Laborers Maharashtra 2001\n  SUBDATASET_302_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-village-level-geospatial-socio-econ-1991-2001_male-cultivators-gj-1991&amp;CRS=CRS:84&amp;BBOX=68.48151121873646,20.121554674908506,74.47763857418519,24.70874662420676\n  SUBDATASET_302_DESC=India Village-Level_Male Cultivators Gujarat 1991\n  SUBDATASET_303_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-village-level-geospatial-socio-econ-1991-2001_male-cultivators-gj-2001&amp;CRS=CRS:84&amp;BBOX=68.48151121873646,20.121554674908513,74.47763857418519,24.70874662420676\n  SUBDATASET_303_DESC=India Village-Level_Male Cultivators Gujarat 2001\n  SUBDATASET_304_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-village-level-geospatial-socio-econ-1991-2001_total-population-mh-1991&amp;CRS=CRS:84&amp;BBOX=72.6516661583423,15.607726277665256,80.89978274910743,22.029491567915112\n  SUBDATASET_304_DESC=India Village-Level_Total Population Maharashtra 1991\n  SUBDATASET_305_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-india-village-level-geospatial-socio-econ-1991-2001_total-population-mh-2001&amp;CRS=CRS:84&amp;BBOX=72.64414841872758,15.607726277665256,80.89978274910743,22.029491567915116\n  SUBDATASET_305_DESC=India Village-Level_Total Population Maharashtra 2001\n  SUBDATASET_306_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-spatial-india-census-2011_census-class&amp;CRS=CRS:84&amp;BBOX=67.38333333336669,6.741666667083059,97.40833333216568,37.091666665869056\n  SUBDATASET_306_DESC=India Census 2011_Census Class\n  SUBDATASET_307_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-spatial-india-census-2011_census-ghsl-1pct&amp;CRS=CRS:84&amp;BBOX=67.38333333336669,6.733333333750064,97.40833333216568,37.091666665869056\n  SUBDATASET_307_DESC=India Census 2011_Census GHSL 1 Percent\n  SUBDATASET_308_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=india%3Aindia-spatial-india-census-2011_census-ghsl-50pct&amp;CRS=CRS:84&amp;BBOX=67.38333333336669,6.733333333750064,97.40833333216568,37.091666665869056\n  SUBDATASET_308_DESC=India Census 2011_Census GHSL 50 Percent\n  SUBDATASET_309_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ipcc%3Aipcc-ar4-observed-climate-impacts&amp;CRS=CRS:84&amp;BBOX=-165.0,-70.0,178.9,86.816\n  SUBDATASET_309_DESC=Observed Climate Change Impacts\n  SUBDATASET_310_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ipcc%3Aipcc-synthetic-vulnerability-climate-2005-2050-2100&amp;CRS=CRS:84&amp;BBOX=-179.99999966763818,-55.79166793823248,179.99999966763812,83.66666412494413\n  SUBDATASET_310_DESC=Climate Change Vulnerability Scenarios 2005, 2050, 2100\n  SUBDATASET_311_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lecz%3Alecz-slr-impacts-ramsar-wetlands_centroids&amp;CRS=CRS:84&amp;BBOX=-162.66775634844777,-54.74784871954516,177.9166666670002,81.20970399566674\n  SUBDATASET_311_DESC=Ramsar: Centroids (All Sites)\n  SUBDATASET_312_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lecz%3Alecz-slr-impacts-ramsar-wetlands_polygons&amp;CRS=CRS:84&amp;BBOX=-163.32482910099992,-54.77429199099993,169.26031494000006,81.42047119200004\n  SUBDATASET_312_DESC=Ramsar: Polygons (Sites with Defined Bounds)\n  SUBDATASET_313_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lecz%3Alecz-urban-rural-population-land-area-estimates-v2_urban-rural-zones-10m&amp;CRS=CRS:84&amp;BBOX=-179.99999956399995,-56.99999972400002,179.9999989960001,83.99999971199998\n  SUBDATASET_313_DESC=Urban Rural Extents in the 10m Low Elevation Zone\n  SUBDATASET_314_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_314_DESC=Development Threat Index\n  SUBDATASET_315_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_agriculturalexpansion&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_315_DESC=Development Threat Index_Agricultural Expansion\n  SUBDATASET_316_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_biofuels&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_316_DESC=Development Threat Index_Biofuels\n  SUBDATASET_317_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_coal&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_317_DESC=Development Threat Index_Coal\n  SUBDATASET_318_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_conventionaloilandgas&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_318_DESC=Development Threat Index_Conventional Oil and Gas\n  SUBDATASET_319_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_mining&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_319_DESC=Development Threat Index_Mining\n  SUBDATASET_320_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_solar&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_320_DESC=Development Threat Index_Solar\n  SUBDATASET_321_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_unconventionaloilandgas&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_321_DESC=Development Threat Index_Unconventional Oil and Gas\n  SUBDATASET_322_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_urbanexpansion&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_322_DESC=Development Threat Index_Urban Expansion\n  SUBDATASET_323_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-development-threat-index_wind&amp;CRS=CRS:84&amp;BBOX=-179.99998107889257,-74.47305996713874,179.7631973723046,84.15878108071058\n  SUBDATASET_323_DESC=Development Threat Index_Wind\n  SUBDATASET_324_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-global-grid-prob-urban-expansion-2030&amp;CRS=CRS:84&amp;BBOX=-179.99997803468415,-55.922762245435266,180.02788233587063,83.97845284575614\n  SUBDATASET_324_DESC=Probabilities of Urban Expansion to 2030\n  SUBDATASET_325_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-global-mangrove-forests-distribution-2000&amp;CRS=CRS:84&amp;BBOX=-179.856847,-38.7977830838922,179.4616075095227,31.371126\n  SUBDATASET_325_DESC=Global Mangrove Forests Distribution, 2000\n  SUBDATASET_326_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=lulc%3Alulc-human-modification-terrestrial-systems&amp;CRS=CRS:84&amp;BBOX=-179.99999991978757,-60.000802620970376,179.99711679590365,83.64317286591495\n  SUBDATASET_326_DESC=Human Modification\n  SUBDATASET_327_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=nagdc%3Anagdc-population-landscape-climate-estimates-v3_coastal-proximity-2010&amp;CRS=CRS:84&amp;BBOX=-179.99999966763818,-55.79166793823242,179.9999996676379,83.66666412494396\n  SUBDATASET_327_DESC=Percentage of Population in Proximity to the Coast\n  SUBDATASET_328_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-cyclone-hazard-frequency-distribution&amp;CRS=CRS:84&amp;BBOX=-180.0,-57.0,180.0,72.02\n  SUBDATASET_328_DESC=Cyclone Hazard Frequency and Distribution\n  SUBDATASET_329_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-cyclone-mortality-risks-distribution&amp;CRS=CRS:84&amp;BBOX=-179.9984130859375,-58.015069824218756,179.9976869140625,85.01593017578125\n  SUBDATASET_329_DESC=Cyclone Mortality Risks and Distribution\n  SUBDATASET_330_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-cyclone-proportional-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_330_DESC=Cyclone Proportional Economic Loss Risk Deciles\n  SUBDATASET_331_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-cyclone-total-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,179.996,84.989\n  SUBDATASET_331_DESC=Cyclone Total Economic Loss Risk Deciles\n  SUBDATASET_332_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-drought-hazard-frequency-distribution&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_332_DESC=Drought Hazard Frequency and Distribution\n  SUBDATASET_333_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-drought-mortality-risks-distribution&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_333_DESC=Drought Mortality Risks and Distribution\n  SUBDATASET_334_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-drought-proportional-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_334_DESC=Drought Proportional Economic Loss Risk Deciles\n  SUBDATASET_335_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-drought-total-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,179.996,84.989\n  SUBDATASET_335_DESC=Drought Total Economic Loss Risk Deciles\n  SUBDATASET_336_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-earthquake-distribution-peak-ground-acceleration&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_336_DESC=Earthquake Hazard Distribution - Peak Ground Acceleration\n  SUBDATASET_337_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-earthquake-frequency-distribution&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_337_DESC=Earthquake Hazard Frequency and Distribution\n  SUBDATASET_338_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-earthquake-mortality-risks-distribution&amp;CRS=CRS:84&amp;BBOX=-182.0,-58.015,192.003,85.028\n  SUBDATASET_338_DESC=Earthquake Mortality Risks and Distribution\n  SUBDATASET_339_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-earthquake-proportional-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-182.0,-58.015,192.003,85.028\n  SUBDATASET_339_DESC=Earthquake Proportional Economic Loss Risk Deciles\n  SUBDATASET_340_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-earthquake-total-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,181.998,86.991\n  SUBDATASET_340_DESC=Earthquake Total Economic Loss Risk Deciles\n  SUBDATASET_341_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-flood-hazard-frequency-distribution&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_341_DESC=Flood Hazard Frequency and Distribution\n  SUBDATASET_342_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-flood-mortality-risks-distribution&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_342_DESC=Flood Mortality Risks and Distribution\n  SUBDATASET_343_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-flood-proportional-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_343_DESC=Flood Proportional Economic Loss Risk Deciles\n  SUBDATASET_344_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-flood-total-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,179.996,84.989\n  SUBDATASET_344_DESC=Flood Total Economic Loss Risk Deciles\n  SUBDATASET_345_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-landslide-hazard-distribution&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_345_DESC=Landslide Hazard Distribution\n  SUBDATASET_346_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-landslide-mortality-risks-distribution&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_346_DESC=Landslide Mortality Risks and Distribution\n  SUBDATASET_347_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-landslide-proportional-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_347_DESC=Landslide Proportional Economic Loss Risk Deciles\n  SUBDATASET_348_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-landslide-total-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,179.996,84.989\n  SUBDATASET_348_DESC=Landslide Total Economic Loss Risk Deciles\n  SUBDATASET_349_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-volcano-hazard-frequency-distribution&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_349_DESC=Volcano Hazard Frequency and Distribution\n  SUBDATASET_350_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-volcano-mortality-risks-distribution&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_350_DESC=Volcano Mortality Risks and Distribution\n  SUBDATASET_351_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-volcano-proportional-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-179.998,-58.015,179.998,85.016\n  SUBDATASET_351_DESC=Volcano Proportional Economic Loss Risk Deciles\n  SUBDATASET_352_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=ndh%3Andh-volcano-total-economic-loss-risk-deciles&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,179.996,84.989\n  SUBDATASET_352_DESC=Volcano Total Economic Loss Risk Deciles\n  SUBDATASET_353_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=nrmi%3Anrmi-natural-resource-protection-child-health-indicators-2017_chi&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010004,83.63333333326676\n  SUBDATASET_353_DESC=NRPI-CHI-2017_Child Health Indicator\n  SUBDATASET_354_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=nrmi%3Anrmi-natural-resource-protection-child-health-indicators-2017_nrpi&amp;CRS=CRS:84&amp;BBOX=-179.99999999990004,-59.49166666668327,180.00000000010004,83.63333333326676\n  SUBDATASET_354_DESC=NRPI-CHI-2017_Natural Resource Protection Indicator\n  SUBDATASET_355_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp1-2050&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_355_DESC=Global Population 1-8th Degree_Total Pop Projection SSP1 2050\n  SUBDATASET_356_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp1-2100&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_356_DESC=Global Population 1-8th Degree_Total Pop Projection SSP1 2100\n  SUBDATASET_357_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp1-ssp2-2100-percent-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_357_DESC=Global Population 1-8th Degree_Total Pop Projection Percent Change SSP1-SSP2 2100\n  SUBDATASET_358_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp2-2050&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_358_DESC=Global Population 1-8th Degree_Total Pop Projection SSP2 2050\n  SUBDATASET_359_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp2-2100&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_359_DESC=Global Population 1-8th Degree_Total Pop Projection SSP2 2100\n  SUBDATASET_360_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp3-2050&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_360_DESC=Global Population 1-8th Degree_Total Pop Projection SSP3 2050\n  SUBDATASET_361_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp3-2100&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_361_DESC=Global Population 1-8th Degree_Total Pop Projection SSP3 2100\n  SUBDATASET_362_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp3-ssp2-2100-percent-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_362_DESC=Global Population 1-8th Degree_Total Pop Projection Percent Change SSP3-SSP2 2100\n  SUBDATASET_363_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp4-2050&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_363_DESC=Global Population 1-8th Degree_Total Pop Projection SSP4 2050\n  SUBDATASET_364_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp4-2100&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_364_DESC=Global Population 1-8th Degree_Total Pop Projection SSP4 2100\n  SUBDATASET_365_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp4-ssp2-2100-percent-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_365_DESC=Global Population 1-8th Degree_Total Pop Projection Percent Change SSP4-SSP2 2100\n  SUBDATASET_366_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp5-2050&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_366_DESC=Global Population 1-8th Degree_Total Pop Projection SSP5 2050\n  SUBDATASET_367_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp5-2100&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_367_DESC=Global Population 1-8th Degree_Total Pop Projection SSP5 2100\n  SUBDATASET_368_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp5-ssp2-2100-percent-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_368_DESC=Global Population 1-8th Degree_Total Pop Projection Percent Change SSP5-SSP2 2100\n  SUBDATASET_369_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-ssps-byr-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_369_DESC=Global Population 1-8th Degree_Total Pop Base Year 2000\n  SUBDATASET_370_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-km-downscaled-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp1-2050&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.77500000000887,179.99999999856004,83.64166666610012\n  SUBDATASET_370_DESC=Global Population 1-km Downscaled_Total Pop Projection SSP1 2050\n  SUBDATASET_371_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-km-downscaled-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-proj-ssp1-2100&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.77500000000887,179.99999999856004,83.64166666610012\n  SUBDATASET_371_DESC=Global Population 1-km Downscaled_Total Pop Projection SSP1 2100\n  SUBDATASET_372_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-1-km-downscaled-pop-base-year-projection-ssp-2000-2100-rev01_total-pop-ssps-byr-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.77500000000887,179.99999999856004,83.64166666610012\n  SUBDATASET_372_DESC=Global Population 1-km Downscaled_Total Pop Base Year 2000\n  SUBDATASET_373_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-est-net-migration-grids-1970-2000_1970-1980&amp;CRS=CRS:84&amp;BBOX=-179.9999999999999,-55.766666666999924,179.99999999856016,83.63333333244248\n  SUBDATASET_373_DESC=Global Estimated Net Migration by Decade: 1970-1980\n  SUBDATASET_374_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-est-net-migration-grids-1970-2000_1980-1990&amp;CRS=CRS:84&amp;BBOX=-179.9999999999999,-55.766666666999924,179.99999999856016,83.63333333244248\n  SUBDATASET_374_DESC=Global Estimated Net Migration by Decade: 1980-1990\n  SUBDATASET_375_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-est-net-migration-grids-1970-2000_1990-2000&amp;CRS=CRS:84&amp;BBOX=-179.9999999999999,-55.766666666999924,179.99999999856016,83.63333333244248\n  SUBDATASET_375_DESC=Global Estimated Net Migration by Decade: 1990-2000\n  SUBDATASET_376_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-pop-count-time-series-estimates_1970&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.7750000001369,179.99999999856004,83.63333333263883\n  SUBDATASET_376_DESC=Global Population Count Estimates_1970\n  SUBDATASET_377_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-pop-count-time-series-estimates_1980&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0083333318933,89.99999999928002\n  SUBDATASET_377_DESC=Global Population Count Estimates_1980\n  SUBDATASET_378_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-pop-count-time-series-estimates_1990&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0083333318933,89.99999999928002\n  SUBDATASET_378_DESC=Global Population Count Estimates_1990\n  SUBDATASET_379_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-pop-count-time-series-estimates_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0083333318933,89.99999999928002\n  SUBDATASET_379_DESC=Global Population Count Estimates_2000\n  SUBDATASET_380_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-pop-density-time-series-estimates_1970&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0083333318933,89.99999999928002\n  SUBDATASET_380_DESC=Global Population Density Estimates_1970\n  SUBDATASET_381_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-pop-density-time-series-estimates_1980&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0083333318933,89.99999999928002\n  SUBDATASET_381_DESC=Global Population Density Estimates_1980\n  SUBDATASET_382_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-pop-density-time-series-estimates_1990&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0083333318933,89.99999999928002\n  SUBDATASET_382_DESC=Global Population Density Estimates_1990\n  SUBDATASET_383_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-global-pop-density-time-series-estimates_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0083333318933,89.99999999928002\n  SUBDATASET_383_DESC=Global Population Density Estimates_2000\n  SUBDATASET_384_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp1-2050-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_384_DESC=Global Population Projection_SSP1 2050 - Total Population\n  SUBDATASET_385_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp1-2100-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_385_DESC=Global Population Projection_SSP1 2100 - Total Population\n  SUBDATASET_386_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp1-ssp2-2100-total-percent-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_386_DESC=Global Population Projection_SSP1-SSP2 2100 - Total Percent Change\n  SUBDATASET_387_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp2-2050-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_387_DESC=Global Population Projection_SSP2 2050 - Total Population\n  SUBDATASET_388_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp2-2100-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_388_DESC=Global Population Projection_SSP2 2100 - Total Population\n  SUBDATASET_389_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp3-2050-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_389_DESC=Global Population Projection_SSP3 2050 - Total Population\n  SUBDATASET_390_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp3-2100-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_390_DESC=Global Population Projection_SSP3 2100 - Total Population\n  SUBDATASET_391_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp3-ssp2-2100-total-percent-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_391_DESC=Global Population Projection_SSP3-SSP2 2100 - Total Percent Change\n  SUBDATASET_392_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp4-2050-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_392_DESC=Global Population Projection_SSP4 2050 - Total Population\n  SUBDATASET_393_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp4-2100-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_393_DESC=Global Population Projection_SSP4 2100 - Total Population\n  SUBDATASET_394_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp4-ssp2-2100-total-percent-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_394_DESC=Global Population Projection_SSP4-SSP2 2100 - Total Percent Change\n  SUBDATASET_395_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp5-2050-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_395_DESC=Global Population Projection_SSP5 2050 - Total Population\n  SUBDATASET_396_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp5-2100-total-population&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_396_DESC=Global Population Projection_SSP5 2100 - Total Population\n  SUBDATASET_397_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=popdynamics%3Apopdynamics-pop-projection-ssp-2010-2100_ssp5-ssp2-2100-total-percent-change&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.875000000084995,180.0,83.749999999915\n  SUBDATASET_397_DESC=Global Population Projection_SSP5-SSP2 2100 - Total Percent Change\n  SUBDATASET_398_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates-v2&amp;CRS=CRS:84&amp;BBOX=-179.99999999999997,-59.481221715999936,179.99999999999986,83.62711161733333\n  SUBDATASET_398_DESC=Subnational Infant Mortality Rates 2015\n  SUBDATASET_399_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates-v2-01&amp;CRS=CRS:84&amp;BBOX=-179.999989,-59.481222,179.999989,83.627112\n  SUBDATASET_399_DESC=Subnational Infant Mortality Rates 2015\n  SUBDATASET_400_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates-v2-01_birth15-alloc&amp;CRS=CRS:84&amp;BBOX=-179.999989,-59.481222,179.999989,83.627112\n  SUBDATASET_400_DESC=Subnational Infant Mortality Rates 2015_Births (A.3)\n  SUBDATASET_401_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates-v2-01_cbr15-alloc&amp;CRS=CRS:84&amp;BBOX=-179.999989,-59.481222,179.999989,83.627112\n  SUBDATASET_401_DESC=Subnational Infant Mortality Rates 2015_Births (A.1)\n  SUBDATASET_402_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates-v2-01_infd-birth&amp;CRS=CRS:84&amp;BBOX=-179.999989,-59.475,179.991667,83.625\n  SUBDATASET_402_DESC=Subnational Infant Mortality Rates 2015_Deaths (B.3)\n  SUBDATASET_403_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates-v2-01_infd-cbr&amp;CRS=CRS:84&amp;BBOX=-179.999989,-59.475,179.991667,83.625\n  SUBDATASET_403_DESC=Subnational Infant Mortality Rates 2015_Deaths (B.1)\n  SUBDATASET_404_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates-v2-01_infd-pop0&amp;CRS=CRS:84&amp;BBOX=-179.999989,-59.475,179.991667,83.625\n  SUBDATASET_404_DESC=Subnational Infant Mortality Rates 2015_Deaths (B.2)\n  SUBDATASET_405_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates-v2-01_pop0-2015&amp;CRS=CRS:84&amp;BBOX=-179.999989,-59.481222,179.999989,83.627112\n  SUBDATASET_405_DESC=Subnational Infant Mortality Rates 2015_Births (A.2)\n  SUBDATASET_406_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-infant-mortality-rates_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_406_DESC=Subnational Infant Mortality Rates 2000\n  SUBDATASET_407_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=povmap%3Apovmap-global-subnational-prevalence-child-malnutrition&amp;CRS=CRS:84&amp;BBOX=-180.0,-58.0,180.0,85.0\n  SUBDATASET_407_DESC=Subnational Prevalence of Child Malnutrition: Underweight Children (%)\n  SUBDATASET_408_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-3-year-running-mean-no2-gome-sciamachy-gome2_1996-1998&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_408_DESC=Ground-Level NO2 from GOME, SCIAMACHY and GOME-2, 1996-1998\n  SUBDATASET_409_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-3-year-running-mean-no2-gome-sciamachy-gome2_2010-2012&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_409_DESC=Ground-Level NO2 from GOME, SCIAMACHY and GOME-2, 2010-2012\n  SUBDATASET_410_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-annual-avg-pm2-5-2001-2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,180.0,90.0\n  SUBDATASET_410_DESC=Global Annual Average PM2.5: 2010\n  SUBDATASET_411_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-annual-avg-pm2-5-modis-misr-seawifs-aod-1998-2012_2001-2010&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.0,180.0,70.0\n  SUBDATASET_411_DESC=SDEI: PM 2.5 Grids from MODIS, MISR and SeaWiFS, 2001-2010\n  SUBDATASET_412_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-annual-avg-pm2-5-modis-misr-seawifs-aod-1998-2012_2010-2012&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.0,180.0,70.0\n  SUBDATASET_412_DESC=SDEI: PM 2.5 Grids from MODIS, MISR and SeaWiFS, 2010-2012\n  SUBDATASET_413_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod_2000&amp;CRS=CRS:84&amp;BBOX=-180.0,-54.85000000000001,180.0,69.85\n  SUBDATASET_413_DESC=PM2.5 Grids with GWR_2000\n  SUBDATASET_414_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod_2005&amp;CRS=CRS:84&amp;BBOX=-180.0,-54.85000000000001,180.0,69.85\n  SUBDATASET_414_DESC=PM2.5 Grids with GWR_2005\n  SUBDATASET_415_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod_2010&amp;CRS=CRS:84&amp;BBOX=-180.005,-54.84500000000001,179.995,69.85499999999999\n  SUBDATASET_415_DESC=PM2.5 Grids with GWR_2010\n  SUBDATASET_416_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod_2015&amp;CRS=CRS:84&amp;BBOX=-180.005,-54.84500000000001,179.995,69.85499999999999\n  SUBDATASET_416_DESC=PM2.5 Grids with GWR_2015\n  SUBDATASET_417_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-summer-lst-2013_day-max-global&amp;CRS=CRS:84&amp;BBOX=-180.00523336,-60.0093368,179.99028794,80.00704604\n  SUBDATASET_417_DESC=LST: Summer Daytime Maximum Temperature\n  SUBDATASET_418_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-summer-lst-2013_night-min-global&amp;CRS=CRS:84&amp;BBOX=-180.00329902,-60.00788982,179.99222228000002,80.00012784\n  SUBDATASET_418_DESC=LST: Summer Nighttime Minimum Temperature\n  SUBDATASET_419_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-uhi-2013_daytdif&amp;CRS=CRS:84&amp;BBOX=-176.19999694800072,-54.85800170891167,179.45831298795406,78.27461099660027\n  SUBDATASET_419_DESC=UHI: Summer Daytime Urban-Rural Temperature Difference\n  SUBDATASET_420_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-uhi-2013_dayurbmean&amp;CRS=CRS:84&amp;BBOX=-176.19999694800072,-54.85800170891167,179.45831298795406,78.27461099660027\n  SUBDATASET_420_DESC=UHI: Average Summer Daytime Maximum Surface Temperature\n  SUBDATASET_421_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-uhi-2013_nighttdif&amp;CRS=CRS:84&amp;BBOX=-176.19999694800072,-54.85800170891167,179.45831298795406,78.27461099660027\n  SUBDATASET_421_DESC=UHI: Summer Nighttime Urban-Rural Temperature Difference\n  SUBDATASET_422_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-global-uhi-2013_nighturbmean&amp;CRS=CRS:84&amp;BBOX=-176.19999694800072,-54.85800170891167,179.45831298795406,78.27461099660027\n  SUBDATASET_422_DESC=UHI: Average Summer Nighttime Minimum Surface Temperature\n  SUBDATASET_423_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-trends-freshwater-availability-grace&amp;CRS=CRS:84&amp;BBOX=-180.0,-89.99999999999999,180.0,90.00000000000001\n  SUBDATASET_423_DESC=GRACE Freshwater Availability Trends, 2002-2016\n  SUBDATASET_424_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=sdei%3Asdei-viirs-dmsp-dlight&amp;CRS=CRS:84&amp;BBOX=-179.999989,-60.00012,179.999989,89.0\n  SUBDATASET_424_DESC=VIIRS Plus DMSP dLIGHT\n  SUBDATASET_425_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=other%3Asdei_daytdif&amp;CRS=CRS:84&amp;BBOX=-176.19999694800072,-54.85800170891167,179.45833496269933,78.27533109188835\n  SUBDATASET_425_DESC=UHI: Summer Daytime Urban-Rural Temperature Difference\n  SUBDATASET_426_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=other%3Asdei_dayurbmean&amp;CRS=CRS:84&amp;BBOX=-176.19999694800072,-54.85800170891167,179.45833496269933,78.27533109188835\n  SUBDATASET_426_DESC=UHI: Average Summer Daytime Maximum Surface Temperature\n  SUBDATASET_427_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=other%3Asdei_nighttdif&amp;CRS=CRS:84&amp;BBOX=-176.19999694800072,-54.85800170891167,179.45833496269933,78.27533109188835\n  SUBDATASET_427_DESC=UHI: Summer Nighttime Urban-Rural Temperature Difference\n  SUBDATASET_428_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=other%3Asdei_nighturbmean&amp;CRS=CRS:84&amp;BBOX=-176.19999694800072,-54.85800170891167,179.45833496269933,78.27533109188835\n  SUBDATASET_428_DESC=UHI: Average Summer Nighttime Maximum Surface Temperature\n  SUBDATASET_429_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-amphibian-richness-2015_all-species&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016734,89.99928000008367\n  SUBDATASET_429_DESC=Species: Global Amphibian Richness Grids - All Species\n  SUBDATASET_430_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-amphibian-richness-2015_all-threats&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016717,89.99928000008359\n  SUBDATASET_430_DESC=Species: Global Amphibian Richness Grids - All Threats\n  SUBDATASET_431_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-amphibian-richness-2015_critically-endangered&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016734,89.99928000008367\n  SUBDATASET_431_DESC=Species: Global Amphibian Richness Grids - Critically Endangered\n  SUBDATASET_432_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-amphibian-richness-2015_endangered&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856,89.99928\n  SUBDATASET_432_DESC=Species: Global Amphibian Richness Grids - Endangered\n  SUBDATASET_433_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-amphibian-richness-2015_vulnerable&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016734,89.99928000008367\n  SUBDATASET_433_DESC=Species: Global Amphibian Richness Grids - Vulnerable\n  SUBDATASET_434_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-mammal-richness-2015_all-species&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016734,89.99928000008367\n  SUBDATASET_434_DESC=Species: Global Mammal Richness Grids - All Species\n  SUBDATASET_435_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-mammal-richness-2015_all-threats&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016717,89.99928000008359\n  SUBDATASET_435_DESC=Species: Global Mammal Richness Grids - All Threats\n  SUBDATASET_436_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-mammal-richness-2015_critically-endangered&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016734,89.99928000008367\n  SUBDATASET_436_DESC=Species: Global Mammal Richness Grids - Critically Endangered\n  SUBDATASET_437_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-mammal-richness-2015_endangered&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016734,89.99928000008367\n  SUBDATASET_437_DESC=Species: Global Mammal Richness Grids - Endangered\n  SUBDATASET_438_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=species%3Aspecies-global-mammal-richness-2015_vulnerable&amp;CRS=CRS:84&amp;BBOX=-180.0,-90.0,179.99856000016734,89.99928000008367\n  SUBDATASET_438_DESC=Species: Global Mammal Richness Grids - Vulnerable\n  SUBDATASET_439_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=other%3Asrtm-elevation-above-sea-level_1km&amp;CRS=CRS:84&amp;BBOX=-180.0,-75.12283191281192,179.99999999999983,90.00216808718802\n  SUBDATASET_439_DESC=Global SRTM Elevation above sea-level at 1km resolution\n  SUBDATASET_440_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=superfund%3Asuperfund-atsdr-hazardous-waste-site-ciesin-mod-v2&amp;CRS=CRS:84&amp;BBOX=-176.63197321308894,17.958101310276014,-65.7647552098852,64.83517454134616\n  SUBDATASET_440_DESC=ATSDR Hazardous Waste Sites on National Priorities List\n  SUBDATASET_441_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=superfund%3Asuperfund-atsdr-hazardous-waste-site-v2&amp;CRS=CRS:84&amp;BBOX=-176.63198910824423,13.433573975556829,144.76321045249958,68.36337858084639\n  SUBDATASET_441_DESC=ATSDR Hazardous Waste Sites\n  SUBDATASET_442_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=superfund%3Asuperfund-epa-national-priorities-list-ciesin-mod-v2&amp;CRS=CRS:84&amp;BBOX=-176.65251589420976,-14.360092475602388,145.7199950273824,64.82300179412411\n  SUBDATASET_442_DESC=U.S. EPA National Priorities List Sites\n  SUBDATASET_443_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=urbanspatial%3Aurbanspatial-dar-es-salaam-luse_1982&amp;CRS=CRS:84&amp;BBOX=39.072704026895,-7.00087885814883,39.36490474982419,-6.568036170378123\n  SUBDATASET_443_DESC=Dar es Salaam Land Use and Informal Settlement: 1982\n  SUBDATASET_444_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=urbanspatial%3Aurbanspatial-dar-es-salaam-luse_1992&amp;CRS=CRS:84&amp;BBOX=39.0727034614112,-7.0008788585123805,39.364904184099785,-6.5680361707898625\n  SUBDATASET_444_DESC=Dar es Salaam Land Use and Informal Settlement: 1992\n  SUBDATASET_445_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=urbanspatial%3Aurbanspatial-dar-es-salaam-luse_1998&amp;CRS=CRS:84&amp;BBOX=39.0727034614112,-7.0008788585123805,39.364904184099785,-6.5680361707898625\n  SUBDATASET_445_DESC=Dar es Salaam Land Use and Informal Settlement: 1998\n  SUBDATASET_446_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=urbanspatial%3Aurbanspatial-dar-es-salaam-luse_2002&amp;CRS=CRS:84&amp;BBOX=39.073197667801516,-7.000879003699315,39.36564643649546,-6.568272080256889\n  SUBDATASET_446_DESC=Dar es Salaam Land Use and Informal Settlement: 2002\n  SUBDATASET_447_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=urbanspatial%3Aurbanspatial-urban-extents-viirs-modis-us-2015&amp;CRS=CRS:84&amp;BBOX=-124.735569,24.955199,-66.969404,49.3759\n  SUBDATASET_447_DESC=2015 VIIRS/MODIS Urban Extents Using Neural Network Method\n  SUBDATASET_448_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_housingunitsf1density-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_448_DESC=SF1 2000, Metropolitan Statistical Areas: Housing Units Density\n  SUBDATASET_449_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pct1personhousing-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_449_DESC=SF1 2000, Metropolitan Statistical Areas: One Person Housing Units (%)\n  SUBDATASET_450_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctasian-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_450_DESC=SF1 2000, Metropolitan Statistical Areas: Population that is Asian American (%)\n  SUBDATASET_451_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctblack-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_451_DESC=SF1 2000, Metropolitan Statistical Areas: Population that is Black (%)\n  SUBDATASET_452_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctfemaleheadhousing-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_452_DESC=SF1 2000, Metropolitan Statistical Areas: Female Headed Households with Children under 18 Years of Age (%)\n  SUBDATASET_453_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pcthispanic-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_453_DESC=SF1 2000, Metropolitan Statistical Areas: Population Ethnically Hispanic or Latino (%)\n  SUBDATASET_454_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctnativeamerican-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_454_DESC=SF1 2000, Metropolitan Statistical Areas: Population Native American, American Indian, or Alaska Native (%)\n  SUBDATASET_455_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctnonhispblack-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_455_DESC=SF1 2000, Metropolitan Statistical Areas: Population Non-Hispanic Black (%)\n  SUBDATASET_456_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctnonhispwhite-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_456_DESC=SF1 2000, Metropolitan Statistical Areas: Population Non-Hispanic White (%)\n  SUBDATASET_457_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctoccupiedhousingsf1-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_457_DESC=SF1 2000, Metropolitan Statistical Areas: Occupied Housing Units (%)\n  SUBDATASET_458_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctownerhousing-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_458_DESC=SF1 2000, Metropolitan Statistical Areas: Owner Occupied Housing Units (%)\n  SUBDATASET_459_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctpacificisland-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_459_DESC=SF1 2000, Metropolitan Statistical Areas: Population that is Pacific Islander or Hawaiian Native (%)\n  SUBDATASET_460_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctpop18to24-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_460_DESC=SF1 2000, Metropolitan Statistical Areas: Population 18 to 24 Years Old (%)\n  SUBDATASET_461_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctpop1to4-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_461_DESC=SF1 2000, Metropolitan Statistical Areas: Population 1 to 4 Years Old (%)\n  SUBDATASET_462_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctpop25to64-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_462_DESC=SF1 2000, Metropolitan Statistical Areas: Population 25 to 64 Years Old (%)\n  SUBDATASET_463_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctpop5to17-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_463_DESC=SF1 2000, Metropolitan Statistical Areas: Population 5 to 17 Years Old (%)\n  SUBDATASET_464_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctpop65to79-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_464_DESC=SF1 2000, Metropolitan Statistical Areas: Population 65 to 79 Years Old (%)\n  SUBDATASET_465_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctpop80andup-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_465_DESC=SF1 2000, Metropolitan Statistical Areas: Population 80 Years of Age and Older (%)\n  SUBDATASET_466_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctpopunder1-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_466_DESC=SF1 2000, Metropolitan Statistical Areas: Population Under 1 Year of Age (%)\n  SUBDATASET_467_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctvacanthousing-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_467_DESC=SF1 2000, Metropolitan Statistical Areas: Vacant or Seasonal Housing Units (%)\n  SUBDATASET_468_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_pctwhite-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_468_DESC=SF1 2000, Metropolitan Statistical Areas: Population that is White (%)\n  SUBDATASET_469_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000-msa_popsf1density-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_469_DESC=SF1 2000, Metropolitan Statistical Areas: Population Density\n  SUBDATASET_470_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-housingunitsf1density-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_470_DESC=SF1 2000: Housing Units Density\n  SUBDATASET_471_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pct1personhousing-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_471_DESC=SF1 2000: One Person Housing Units (%)\n  SUBDATASET_472_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctasian-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_472_DESC=SF1 2000: Population Asian American (%)\n  SUBDATASET_473_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctblack-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_473_DESC=SF1 2000: Population Black or African American (%)\n  SUBDATASET_474_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctfemaleheadhousing-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_474_DESC=SF1 2000: Female Headed Households with Children under 18 Years of Age (%)\n  SUBDATASET_475_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pcthispanic-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_475_DESC=SF1 2000: Population Ethnically Hispanic or Latino (%)\n  SUBDATASET_476_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctnativeamerican-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_476_DESC=SF1 2000: Population American Indian &amp; Alaska Native (%)\n  SUBDATASET_477_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctnonhispblack-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_477_DESC=SF1 2000: Population Non-Hispanic Black (%)\n  SUBDATASET_478_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctnonhispwhite-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_478_DESC=SF1 2000: Population Non-Hispanic White (%)\n  SUBDATASET_479_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctoccupiedhousingsf1-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_479_DESC=SF1 2000: Occupied Housing Units (%)\n  SUBDATASET_480_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctownerhousing-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_480_DESC=SF1 2000: Owner Occupied Housing Units (%)\n  SUBDATASET_481_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctpacificisland-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_481_DESC=SF1 2000: Population Pacific Islander or Hawaiian Native (%)\n  SUBDATASET_482_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctpop18to24-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_482_DESC=SF1 2000: Population 18 to 24 Years Old (%)\n  SUBDATASET_483_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctpop1to4-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_483_DESC=SF1 2000: Population 1 to 4 Years Old (%)\n  SUBDATASET_484_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctpop25to64-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_484_DESC=SF1 2000: Population 25 to 64 Years Old (%)\n  SUBDATASET_485_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctpop5to17-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_485_DESC=SF1 2000: Population 5 to 17 Years Old (%)\n  SUBDATASET_486_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctpop65to79-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_486_DESC=SF1 2000: Population 65 to 79 Years Old (%)\n  SUBDATASET_487_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctpop80andup-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_487_DESC=SF1 2000: Population 80 Years of Age and Older (%)\n  SUBDATASET_488_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctpopunder1-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_488_DESC=SF1 2000: Population Under 1 Year of Age (%)\n  SUBDATASET_489_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctvacanthousing-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_489_DESC=SF1 2000: Vacant or Seasonal Housing Units (%)\n  SUBDATASET_490_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-pctwhite-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_490_DESC=SF1 2000: Population that is White (%)\n  SUBDATASET_491_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2000_usa-popsf1density-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_491_DESC=SF1 2000: Population Density\n  SUBDATASET_492_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_a18to24-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_492_DESC=SF1 2010_Population Ages 18 to 24 (%)\n  SUBDATASET_493_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_a1to4-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_493_DESC=SF1 2010_Population Ages 1 to 4 (%)\n  SUBDATASET_494_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_a25to64-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_494_DESC=SF1 2010_Population Age 25 to 64 (%)\n  SUBDATASET_495_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_a5to17-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_495_DESC=SF1 2010_Population Ages 5 to 17 (%)\n  SUBDATASET_496_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_a65to79-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_496_DESC=SF1 2010_Population Age 65 to 79 (%)\n  SUBDATASET_497_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_amind-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_497_DESC=SF1 2010_American Indian and Alaska Native Alone (%)\n  SUBDATASET_498_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_aov80-pct&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_498_DESC=SF1 2010_Population Age 80 and Older (%)\n  SUBDATASET_499_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_asian-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_499_DESC=SF1 2010_Asian Alone (%)\n  SUBDATASET_500_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_aund1-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_500_DESC=SF1 2010_Population Under Age 1 (%)\n  SUBDATASET_501_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_black-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_501_DESC=SF1 2010_Black or African American Alone (%)\n  SUBDATASET_502_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_fem-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_502_DESC=SF1 2010_Female Headed Households with Related Children (%)\n  SUBDATASET_503_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_hawpi-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_503_DESC=SF1 2010_Native Hawaiian and Other Pacific Islander Alone (%)\n  SUBDATASET_504_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_hh-dens-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_504_DESC=SF1 2010_Households Density\n  SUBDATASET_505_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_hhp-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_505_DESC=SF1 2010_Population in Households (%)\n  SUBDATASET_506_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_hu1p-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_506_DESC=SF1 2010_One-Person Housing Units (%)\n  SUBDATASET_507_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_nhblack-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_507_DESC=SF1 2010_Non-Hispanic Black (%)\n  SUBDATASET_508_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_nhisp-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_508_DESC=SF1 2010_Non-Hispanic (%)\n  SUBDATASET_509_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_nhwhite-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_509_DESC=SF1 2010_Non-Hispanic White (%)\n  SUBDATASET_510_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_occ-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_510_DESC=SF1 2010_Occupied Housing Units (%)\n  SUBDATASET_511_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_other-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_511_DESC=SF1 2010_Some Other Race Alone (%)\n  SUBDATASET_512_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_own-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_512_DESC=SF1 2010_Owner-Occupied Housing Units (%)\n  SUBDATASET_513_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_p25-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_513_DESC=SF1 2010_Population Age 25 and Older (%)\n  SUBDATASET_514_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_pop-count-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.216667,71.391667\n  SUBDATASET_514_DESC=SF1 2010_Population Count\n  SUBDATASET_515_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_pop-dens-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_515_DESC=SF1 2010_Population Density\n  SUBDATASET_516_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_sea-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_516_DESC=SF1 2010_Vacant Housing Units (%)\n  SUBDATASET_517_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_twomore-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_517_DESC=SF1 2010_Two or More Races (%)\n  SUBDATASET_518_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_und25-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_518_DESC=SF1 2010_Population Under Age 25 (%)\n  SUBDATASET_519_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_white-pct-2010&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_519_DESC=SF1 2010_White Alone (%)\n  SUBDATASET_520_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file1-2010_womchild-pct&amp;CRS=CRS:84&amp;BBOX=-179.15,17.875,-65.208333,71.4\n  SUBDATASET_520_DESC=SF1 2010_Women of Childbearing Age (%)\n  SUBDATASET_521_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_householdsdensity-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_521_DESC=SF3 2000, Metropolitan Statistical Areas: Household Density\n  SUBDATASET_522_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_housingunitsf3density-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_522_DESC=SF3 2000, Metropolitan Statistical Areas: Housing Units Density\n  SUBDATASET_523_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pcteducationbahigher-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_523_DESC=SF3 2000, Metropolitan Statistical Areas: Population with a Bachelor's Degree or Higher (%)\n  SUBDATASET_524_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pcteducationhshigher-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_524_DESC=SF3 2000, Metropolitan Statistical Areas: Population with a High School Diploma or Higher (%)\n  SUBDATASET_525_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pctforeign-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_525_DESC=SF3 2000, Metropolitan Statistical Areas: Population that is Foreign Born (%)\n  SUBDATASET_526_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pcthouseholdlinguisticisolation-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_526_DESC=SF3 2000, Metropolitan Statistical Areas: Households that are Linguistically Isolated (%)\n  SUBDATASET_527_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pcthousingbuilt50to69-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_527_DESC=SF3 2000, Metropolitan Statistical Areas: Housing Units Built Bewteen 1950 and 1969 (%)\n  SUBDATASET_528_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pcthousingbuilt70to89-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_528_DESC=SF3 2000, Metropolitan Statistical Areas: Housing Units Built Between 1970 and 1989 (%)\n  SUBDATASET_529_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pcthousingbuilt90to00-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_529_DESC=SF3 2000, Metropolitan Statistical Areas: Housing Units Built Between 1990 and 2000 (%)\n  SUBDATASET_530_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pcthousingbuiltbefore50-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_530_DESC=SF3 2000, Metropolitan Statistical Areas: Housing Units Built Before 1950 (%)\n  SUBDATASET_531_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pctoccupiedhousingnocar-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_531_DESC=SF3 2000, Metropolitan Statistical Areas: Occupied Housing Units Without a Car (%)\n  SUBDATASET_532_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pctoccupiedhousingsf3-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_532_DESC=SF3 2000, Metropolitan Statistical Areas: Occupied Housing Units (%)\n  SUBDATASET_533_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pctpop25andover-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_533_DESC=SF3 2000, Metropolitan Statistical Areas: Population Aged 25 Years and Older (%)\n  SUBDATASET_534_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pctpopbelow200pctpovlevel-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_534_DESC=SF3 2000, Metropolitan Statistical Areas: Population Below the 200 Percent Poverty Level (%)\n  SUBDATASET_535_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pctpopbelow50pctpovlevel-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_535_DESC=SF3 2000, Metropolitan Statistical Areas: Population Below the 50 Percent Poverty Level (%)\n  SUBDATASET_536_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pctpopbelowpovlevel-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_536_DESC=SF3 2000, Metropolitan Statistical Areas: Population Below the Poverty Level (%)\n  SUBDATASET_537_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_pctpoppovleveldetermined-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_537_DESC=SF3 2000 MSA_Population where Poverty Level is Determined (%)\n  SUBDATASET_538_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000-msa_popsf3density-2000&amp;CRS=CRS:84&amp;BBOX=-124.0,17.0,-65.0,49.0\n  SUBDATASET_538_DESC=SF3 2000 MSA_Population Density\n  SUBDATASET_539_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-householdsdensity-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_539_DESC=SF3 2000_Household Density\n  SUBDATASET_540_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-housingunitsf3density-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_540_DESC=SF3 2000 MSA_Housing Units Density\n  SUBDATASET_541_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pcteducationbahigher-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_541_DESC=SF3 2000_Population Bachelor's Degree or Higher (%)\n  SUBDATASET_542_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pcteducationhshigher-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_542_DESC=SF3 2000_Population High School Diploma or Higher (%)\n  SUBDATASET_543_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pctforeign-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_543_DESC=SF3 2000_Population Foreign Born (%)\n  SUBDATASET_544_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pcthouseholdlinguisticisolation-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_544_DESC=SF3 2000_Households Linguistically Isolated (%)\n  SUBDATASET_545_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pcthousingbuilt50to69-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_545_DESC=SF3 2000_Housing Units Built Between 1950 and 1969 (%)\n  SUBDATASET_546_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pcthousingbuilt70to89-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_546_DESC=SF3 2000_Housing Units Built Between 1970 and 1989 (%)\n  SUBDATASET_547_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pcthousingbuilt90to00-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_547_DESC=SF3 2000_Housing Units Built Between 1990 and 2000 (%)\n  SUBDATASET_548_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pcthousingbuiltbefore50-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_548_DESC=SF3 2000_Housing Units Built Before 1950 (%)\n  SUBDATASET_549_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pctoccupiedhousingnocar-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_549_DESC=SF3 2000_Occupied Housing Units Without a Car (%)\n  SUBDATASET_550_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pctoccupiedhousingsf3-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_550_DESC=SF3 2000_Occupied Housing Units (%)\n  SUBDATASET_551_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pctpop25andover-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_551_DESC=SF3 2000_Population Aged 25 Years and Older (%)\n  SUBDATASET_552_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pctpopbelow200pctpovlevel-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_552_DESC=SF3 2000_Population Below the 200 Percent Poverty Level (%)\n  SUBDATASET_553_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pctpopbelow50pctpovlevel-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_553_DESC=SF3 2000_Population Below the 50 Percent Poverty Level (%)\n  SUBDATASET_554_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pctpopbelowpovlevel-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_554_DESC=SF3 2000_Population Below the Poverty Level (%)\n  SUBDATASET_555_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-pctpoppovleveldetermined-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_555_DESC=SF3 2000_Population where Poverty Level is Determined (%)\n  SUBDATASET_556_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=usgrid%3Ausgrid-summary-file3-2000_usa-popsf3density-2000&amp;CRS=CRS:84&amp;BBOX=-180.0,17.0,-65.0,72.0\n  SUBDATASET_556_DESC=SF3 2000_Population Density\n  SUBDATASET_557_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=other%3Awcmc-world-database-of-protected-areas&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.749,180.0,83.634\n  SUBDATASET_557_DESC=World Database of Protected Areas\n  SUBDATASET_558_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=wildareas-v2%3Awildareas-v2-human-footprint-geographic&amp;CRS=CRS:84&amp;BBOX=-180.0,-89.919,180.0,89.931\n  SUBDATASET_558_DESC=Wildareas: Human Footprint v2\n  SUBDATASET_559_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=wildareas-v2%3Awildareas-v2-human-influence-index-geographic&amp;CRS=CRS:84&amp;BBOX=-180.0,-89.919,180.008,89.931\n  SUBDATASET_559_DESC=Wildareas: Human Influence Index v2\n  SUBDATASET_560_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=wildareas-v2%3Awildareas-v2-last-of-the-wild-geographic&amp;CRS=CRS:84&amp;BBOX=-180.0,-55.986,180.0,83.623\n  SUBDATASET_560_DESC=Wildareas: Last of the Wild v2\n  SUBDATASET_561_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=wildareas-v3%3Awildareas-v3-1993-human-footprint&amp;CRS=CRS:84&amp;BBOX=-179.999999909593,-55.791667932392016,179.99661110853617,83.66621130702998\n  SUBDATASET_561_DESC=1993 Human Footprint\n  SUBDATASET_562_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=wildareas-v3%3Awildareas-v3-2009-human-footprint&amp;CRS=CRS:84&amp;BBOX=-179.999999909593,-55.791667932392016,179.99661110853617,83.66621130702998\n  SUBDATASET_562_DESC=2009 Human Footprint\n  SUBDATASET_563_NAME=WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=other%3Awwf-terrestrial-biomes&amp;CRS=CRS:84&amp;BBOX=-180.0,-89.892,180.0,83.623\n  SUBDATASET_563_DESC=Terrestrial Biomes from WWF\nCorner Coordinates:\nUpper Left  (    0.0,    0.0)\nLower Left  (    0.0,  512.0)\nUpper Right (  512.0,    0.0)\nLower Right (  512.0,  512.0)\nCenter      (  256.0,  256.0)\n</pre> In\u00a0[4]: Copied! <pre>!gdalinfo \"WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grand-v1%3Agrand-v1-reservoirs-rev01&amp;CRS=CRS:84&amp;BBOX=-153.037,-45.881,176.825,70.398\"\n</pre> !gdalinfo \"WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grand-v1%3Agrand-v1-reservoirs-rev01&amp;CRS=CRS:84&amp;BBOX=-153.037,-45.881,176.825,70.398\" <pre>Driver: WMS/OGC Web Map Service\nFiles: none associated\nSize is 1073741824, 378502602\nCoordinate System is:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ID[\"EPSG\",6326]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433],\n        ID[\"EPSG\",8901]],\n    CS[ellipsoidal,2],\n        AXIS[\"longitude\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]],\n        AXIS[\"latitude\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]]\nData axis to CRS axis mapping: 1,2\nOrigin = (-153.037000000000006,70.397999999999996)\nPixel Size = (0.000000307207927,-0.000000307207928)\nImage Structure Metadata:\n  INTERLEAVE=PIXEL\nCorner Coordinates:\nUpper Left  (-153.0370000,  70.3980000) (153d 2'13.20\"W, 70d23'52.80\"N)\nLower Left  (-153.0370000, -45.8810000) (153d 2'13.20\"W, 45d52'51.60\"S)\nUpper Right ( 176.8250000,  70.3980000) (176d49'30.00\"E, 70d23'52.80\"N)\nLower Right ( 176.8250000, -45.8810000) (176d49'30.00\"E, 45d52'51.60\"S)\nCenter      (  11.8940000,  12.2585000) ( 11d53'38.40\"E, 12d15'30.60\"N)\nBand 1 Block=1024x1024 Type=Byte, ColorInterp=Red\n  Overviews: 536870912x189251301, 268435456x94625651, 134217728x47312825, 67108864x23656413, 33554432x11828206, 16777216x5914103, 8388608x2957052, 4194304x1478526, 2097152x739263, 1048576x369631, 524288x184816, 262144x92408, 131072x46204, 65536x23102, 32768x11551, 16384x5775, 8192x2888, 4096x1444, 2048x722, 1024x361\nBand 2 Block=1024x1024 Type=Byte, ColorInterp=Green\n  Overviews: 536870912x189251301, 268435456x94625651, 134217728x47312825, 67108864x23656413, 33554432x11828206, 16777216x5914103, 8388608x2957052, 4194304x1478526, 2097152x739263, 1048576x369631, 524288x184816, 262144x92408, 131072x46204, 65536x23102, 32768x11551, 16384x5775, 8192x2888, 4096x1444, 2048x722, 1024x361\nBand 3 Block=1024x1024 Type=Byte, ColorInterp=Blue\n  Overviews: 536870912x189251301, 268435456x94625651, 134217728x47312825, 67108864x23656413, 33554432x11828206, 16777216x5914103, 8388608x2957052, 4194304x1478526, 2097152x739263, 1048576x369631, 524288x184816, 262144x92408, 131072x46204, 65536x23102, 32768x11551, 16384x5775, 8192x2888, 4096x1444, 2048x722, 1024x361\n</pre> In\u00a0[5]: Copied! <pre>!gdal_translate -of WMS \"WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grand-v1%3Agrand-v1-reservoirs-rev01&amp;CRS=CRS:84&amp;BBOX=-153.037,-45.881,176.825,70.398\" reservoirs.xml\n</pre> !gdal_translate -of WMS \"WMS:https://sedac.ciesin.columbia.edu/geoserver/ows?SERVICE=WMS&amp;VERSION=1.3.0&amp;REQUEST=GetMap&amp;LAYERS=grand-v1%3Agrand-v1-reservoirs-rev01&amp;CRS=CRS:84&amp;BBOX=-153.037,-45.881,176.825,70.398\" reservoirs.xml <pre>Input file size is 1073741824, 378502602\n</pre> In\u00a0[7]: Copied! <pre>%%time\n!gdalwarp -tr 0.005 0.005 -te 68.106 6.762 97.412 37.078 reservoirs.xml \\\n  reservoirs_india.tif -co COMPRESS=DEFLATE -co TILED=YES\n</pre> %%time !gdalwarp -tr 0.005 0.005 -te 68.106 6.762 97.412 37.078 reservoirs.xml \\   reservoirs_india.tif -co COMPRESS=DEFLATE -co TILED=YES <pre>Creating output file that is 5861P x 6063L.\nProcessing reservoirs.xml [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 333 ms, sys: 111 ms, total: 444 ms\nWall time: 22.3 s\n</pre> In\u00a0[9]: Copied! <pre>%%time\n!gdalwarp -tr 500 500 -te 68.106 6.762 97.412 37.078 -te_srs EPSG:4326 \\\n  reservoirs.xml \\\n  -t_srs EPSG:7755 reservoirs_india_proj.tif -co COMPRESS=DEFLATE -co TILED=YES\n</pre> %%time !gdalwarp -tr 500 500 -te 68.106 6.762 97.412 37.078 -te_srs EPSG:4326 \\   reservoirs.xml \\   -t_srs EPSG:7755 reservoirs_india_proj.tif -co COMPRESS=DEFLATE -co TILED=YES <pre>Creating output file that is 5803P x 6744L.\nProcessing reservoirs.xml [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.\nCPU times: user 125 ms, sys: 48.1 ms, total: 173 ms\nWall time: 8.55 s\n</pre> In\u00a0[10]: Copied! <pre>ls earth*\n</pre> ls earth* <pre>earth_at_night.jpg*\n\nearthquakes:\n2020_01.geojson* 2020_04.geojson* 2020_07.geojson* 2020_10.geojson*\n2020_02.geojson* 2020_05.geojson* 2020_08.geojson* 2020_11.geojson*\n2020_03.geojson* 2020_06.geojson* 2020_09.geojson* 2020_12.geojson*\n</pre> In\u00a0[11]: Copied! <pre>!gdalinfo earth_at_night.jpg\n</pre> !gdalinfo earth_at_night.jpg <pre>Driver: JPEG/JPEG JFIF\nFiles: earth_at_night.jpg\nSize is 13500, 6750\nImage Structure Metadata:\n  COMPRESSION=JPEG\n  INTERLEAVE=PIXEL\n  SOURCE_COLOR_SPACE=YCbCr\nCorner Coordinates:\nUpper Left  (    0.0,    0.0)\nLower Left  (    0.0, 6750.0)\nUpper Right (13500.0,    0.0)\nLower Right (13500.0, 6750.0)\nCenter      ( 6750.0, 3375.0)\nBand 1 Block=13500x1 Type=Byte, ColorInterp=Red\n  Overviews: 6750x3375, 3375x1688, 1688x844\n  Image Structure Metadata:\n    COMPRESSION=JPEG\nBand 2 Block=13500x1 Type=Byte, ColorInterp=Green\n  Overviews: 6750x3375, 3375x1688, 1688x844\n  Image Structure Metadata:\n    COMPRESSION=JPEG\nBand 3 Block=13500x1 Type=Byte, ColorInterp=Blue\n  Overviews: 6750x3375, 3375x1688, 1688x844\n  Image Structure Metadata:\n    COMPRESSION=JPEG\n</pre> In\u00a0[12]: Copied! <pre># use -a_ullr for upper left, lower right coords\n!gdal_translate -a_ullr -180 90 180 -90 -a_srs EPSG:4326 \\\n  earth_at_night.jpg earth_at_night.tif \\\n  -co COMPRESS=JPEG -co TILED=YES -co PHOTOMETRIC=RGB\n</pre> # use -a_ullr for upper left, lower right coords !gdal_translate -a_ullr -180 90 180 -90 -a_srs EPSG:4326 \\   earth_at_night.jpg earth_at_night.tif \\   -co COMPRESS=JPEG -co TILED=YES -co PHOTOMETRIC=RGB <pre>Input file size is 13500, 6750\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[13]: Copied! <pre>!gdal_translate -gcp 418 893 70 15 -gcp 380 2432 70 5 -gcp 3453 2434  90 5 \\\n  -gcp 3407 895 90 15 -gcp 2662 911 85 15 \\\n  1870_southern-india.jpg india-with-gcp.vrt\n</pre> !gdal_translate -gcp 418 893 70 15 -gcp 380 2432 70 5 -gcp 3453 2434  90 5 \\   -gcp 3407 895 90 15 -gcp 2662 911 85 15 \\   1870_southern-india.jpg india-with-gcp.vrt <pre>Input file size is 3957, 3071\n</pre> In\u00a0[14]: Copied! <pre>!gdalwarp -t_srs EPSG:4042 -order 1 -tr 0.005 0.005 \\\n  india-with-gcp.vrt india-reprojected-polynomial.tif \\\n  -co COMPRESS=JPEG -co JPEG_QUALITY=50 -co PHOTOMETRIC=YCBCR\n</pre> !gdalwarp -t_srs EPSG:4042 -order 1 -tr 0.005 0.005 \\   india-with-gcp.vrt india-reprojected-polynomial.tif \\   -co COMPRESS=JPEG -co JPEG_QUALITY=50 -co PHOTOMETRIC=YCBCR <pre>Creating output file that is 5241P x 4012L.\nProcessing india-with-gcp.vrt [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[17]: Copied! <pre>!gdalwarp -t_srs EPSG:4042 -tr 0.005 0.005 \\\n  india-with-gcp.vrt india-reprojected-tps.tif \\\n  -co COMPRESS=JPEG -co JPEG_QUALITY=50 -co PHOTOMETRIC=YCBCR\n</pre> !gdalwarp -t_srs EPSG:4042 -tr 0.005 0.005 \\   india-with-gcp.vrt india-reprojected-tps.tif \\   -co COMPRESS=JPEG -co JPEG_QUALITY=50 -co PHOTOMETRIC=YCBCR <pre>Creating output file that is 5241P x 4012L.\nProcessing india-with-gcp.vrt [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/gdal/gdal_wms/#gdal-interacting-with-wms","title":"GDAL - interacting with WMS\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_wms/#set-up","title":"Set up\u00b6","text":"<p>Use - <code>opengeo</code> micromamba env. Move to <code>gdal-tools</code> folder.</p>"},{"location":"teaching_resources/gdal/gdal_wms/#wms-with-gdal","title":"WMS with GDAL\u00b6","text":"<p>You can download tiles. You can use <code>gdal_translate -of WMS &lt;url&gt; output.xml</code> to create a layer file in XML that you can share the url to consumers of qgis.</p>"},{"location":"teaching_resources/gdal/gdal_wms/#list-layers","title":"List layers\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_wms/#download-wms-as-rasters","title":"Download WMS as rasters\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_wms/#specify-desired-extent-in-an-easy-to-use-srs","title":"Specify desired extent in an easy to use SRS\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_wms/#georeferencing","title":"Georeferencing\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_wms/#with-corner-coords","title":"With corner coords\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_wms/#with-gcp-points","title":"With GCP points\u00b6","text":"<p>GeoTiff can store GCP in header, and can apply the transformation on the fly. We then use gdalwarp to apply the data and persist to disk.</p>"},{"location":"teaching_resources/gdal/gdal_wms/#project-using-a-different-warping-algorithm","title":"Project using a different warping algorithm\u00b6","text":""},{"location":"teaching_resources/gdal/gdal_wms/#idea-georeference-and-animate-weather-data","title":"Idea - georeference and animate weather data?\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/","title":"OGR - inspection, ogr2ogr","text":"<p>Commands</p> <ul> <li>ogrinfo: Lists information about an OGR-supported data source.</li> <li>ogr2ogr: Converts simple features data between file formats.</li> <li>ogrtindex: Creates a tileindex.</li> <li>ogrlineref: Create linear reference and provide some calculations using it.</li> <li>ogrmerge.py: Merge several vector datasets into a single one.</li> <li>ogr_layer_algebra.py: Performs various Vector layer algebraic operations.</li> </ul> In\u00a0[5]: Copied! <pre>cd /Users/abharathi/Documents/gis_data/gdal-tools\n</pre> cd /Users/abharathi/Documents/gis_data/gdal-tools <pre>/Users/abharathi/Documents/gis_data/gdal-tools\n</pre> In\u00a0[6]: Copied! <pre>ls\n</pre> ls <pre>1870_southern-india.jpg*           india-with-gcp.vrt\n1870_southern-india.jpg.aux.xml    landsat8/\n5d16a93f1cf0f6000579ad2c.tif       london_1m_dsm/\nbatch.py*                          naip/\nbatch_parallel.py*                 precipitation.gpkg\nearth_at_night.jpg*                prism/\nearth_at_night.tif                 reservoirs.xml\nearthquakes/                       reservoirs_india.tif\ngdal_stuff.qgz                     reservoirs_india_proj.tif\ngeonames/                          spatial_query.gpkg\nindia-reprojected-polynomial.tif   srtm/\nindia-reprojected-tps.tif          worldcities.csv*\nindia-reprojected-tps.tif.aux.xml\n</pre> In\u00a0[7]: Copied! <pre>!ogrinfo worldcities.csv\n</pre> !ogrinfo worldcities.csv <pre>INFO: Open of `worldcities.csv'\n      using driver `CSV' successful.\n1: worldcities (None)\n</pre> In\u00a0[9]: Copied! <pre>!ogrinfo -al -so worldcities.csv\n</pre> !ogrinfo -al -so worldcities.csv <pre>INFO: Open of `worldcities.csv'\n      using driver `CSV' successful.\n\nLayer name: worldcities\nGeometry: None\nFeature Count: 15493\nLayer SRS WKT:\n(unknown)\ncity: String (0.0)\ncity_ascii: String (0.0)\nlat: String (0.0)\nlng: String (0.0)\ncountry: String (0.0)\niso2: String (0.0)\niso3: String (0.0)\nadmin_name: String (0.0)\ncapital: String (0.0)\npopulation: String (0.0)\nid: String (0.0)\n</pre> In\u00a0[10]: Copied! <pre>%%time\n!ogr2ogr -f GPKG worldcities.gpkg worldcities.csv \\\n  -oo X_POSSIBLE_NAMES=lng -oo Y_POSSIBLE_NAMES=lat -a_srs EPSG:4326\n</pre> %%time !ogr2ogr -f GPKG worldcities.gpkg worldcities.csv \\   -oo X_POSSIBLE_NAMES=lng -oo Y_POSSIBLE_NAMES=lat -a_srs EPSG:4326 <pre>CPU times: user 9.59 ms, sys: 9.28 ms, total: 18.9 ms\nWall time: 887 ms\n</pre> In\u00a0[11]: Copied! <pre>%%time\n!ogr2ogr -f GPKG worldcities.gpkg worldcities.csv \\\n  -oo X_POSSIBLE_NAMES=lng -oo Y_POSSIBLE_NAMES=lat -a_srs EPSG:4326 \\\n  -sql \"SELECT city, country, CAST(population AS integer) as population from worldcities where country = 'India'\"\n</pre> %%time !ogr2ogr -f GPKG worldcities.gpkg worldcities.csv \\   -oo X_POSSIBLE_NAMES=lng -oo Y_POSSIBLE_NAMES=lat -a_srs EPSG:4326 \\   -sql \"SELECT city, country, CAST(population AS integer) as population from worldcities where country = 'India'\" <pre>CPU times: user 4.59 ms, sys: 8.77 ms, total: 13.4 ms\nWall time: 381 ms\n</pre> In\u00a0[12]: Copied! <pre>%%time\n!ogr2ogr -f GPKG worldcities.gpkg worldcities.csv \\\n  -oo X_POSSIBLE_NAMES=lng -oo Y_POSSIBLE_NAMES=lat -a_srs EPSG:4326 \\\n  -sql \"SELECT city, country, CAST(population AS integer) as population from worldcities where country = 'India'\" \\\n  -nln mycities\n</pre> %%time !ogr2ogr -f GPKG worldcities.gpkg worldcities.csv \\   -oo X_POSSIBLE_NAMES=lng -oo Y_POSSIBLE_NAMES=lat -a_srs EPSG:4326 \\   -sql \"SELECT city, country, CAST(population AS integer) as population from worldcities where country = 'India'\" \\   -nln mycities <pre>CPU times: user 4.89 ms, sys: 6.51 ms, total: 11.4 ms\nWall time: 443 ms\n</pre> In\u00a0[14]: Copied! <pre>!ogrinfo -so -al worldcities.gpkg\n</pre> !ogrinfo -so -al worldcities.gpkg <pre>INFO: Open of `worldcities.gpkg'\n      using driver `GPKG' successful.\n\nLayer name: mycities\nGeometry: Point\nFeature Count: 212\nExtent: (69.670000, 8.180400) - (94.900000, 34.300000)\nLayer SRS WKT:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\nData axis to CRS axis mapping: 2,1\nFID Column = fid\nGeometry Column = geom\ncity: String (0.0)\ncountry: String (0.0)\npopulation: Integer (0.0)\n</pre> In\u00a0[16]: Copied! <pre>!ogr2ogr mycities.shp worldcities.gpkg \\\n-t_srs EPSG:7755 -lco ENCODING=UTF-8 \\\n-lco SPATIAL_INDEX=Yes\n</pre> !ogr2ogr mycities.shp worldcities.gpkg \\ -t_srs EPSG:7755 -lco ENCODING=UTF-8 \\ -lco SPATIAL_INDEX=Yes In\u00a0[18]: Copied! <pre>ls\n</pre> ls <pre>1870_southern-india.jpg*           mycities.dbf\n1870_southern-india.jpg.aux.xml    mycities.prj\n5d16a93f1cf0f6000579ad2c.tif       mycities.qix\nbatch.py*                          mycities.shp\nbatch_parallel.py*                 mycities.shx\nearth_at_night.jpg*                naip/\nearth_at_night.tif                 precipitation.gpkg\nearthquakes/                       prism/\ngdal_stuff.qgz                     reservoirs.xml\ngeonames/                          reservoirs_india.tif\nindia-reprojected-polynomial.tif   reservoirs_india_proj.tif\nindia-reprojected-tps.tif          spatial_query.gpkg\nindia-reprojected-tps.tif.aux.xml  srtm/\nindia-with-gcp.vrt                 worldcities.csv*\nlandsat8/                          worldcities.gpkg\nlondon_1m_dsm/                     worldcities.gpkg-shm\nmycities.cpg                       worldcities.gpkg-wal\n</pre> In\u00a0[17]: Copied! <pre># Get country pop from city pop\n!ogr2ogr country_pop.csv worldcities.gpkg \\\n  -sql \"SELECT country, sum(population) as total_population from worldcities GROUP BY country\"\n</pre> # Get country pop from city pop !ogr2ogr country_pop.csv worldcities.gpkg \\   -sql \"SELECT country, sum(population) as total_population from worldcities GROUP BY country\" <pre>ERROR 1: In ExecuteSQL(): sqlite3_prepare_v2(SELECT country, sum(population) as total_population from worldcities GROUP BY country):\n  no such table: worldcities\n</pre> In\u00a0[19]: Copied! <pre>cd earthquakes/\n</pre> cd earthquakes/ <pre>/Users/abharathi/Documents/gis_data/gdal-tools/earthquakes\n</pre> In\u00a0[20]: Copied! <pre>ls\n</pre> ls <pre>2020_01.geojson* 2020_04.geojson* 2020_07.geojson* 2020_10.geojson*\n2020_02.geojson* 2020_05.geojson* 2020_08.geojson* 2020_11.geojson*\n2020_03.geojson* 2020_06.geojson* 2020_09.geojson* 2020_12.geojson*\n</pre> In\u00a0[25]: Copied! <pre>!which ogrmerge.py\n</pre> !which ogrmerge.py <pre>/Users/abharathi/micromamba/envs/opengeo/bin/ogrmerge.py\n</pre> In\u00a0[26]: Copied! <pre>%run -i /Users/abharathi/micromamba/envs/opengeo/bin/ogrmerge.py --help\n</pre> %run -i /Users/abharathi/micromamba/envs/opengeo/bin/ogrmerge.py --help <pre>ERROR: Unrecognized argument : --help\nUsage: ogrmerge.py -o out_dsname src_dsname [src_dsname]*\n            [-f format] [-single] [-nln layer_name_template]\n            [-update | -overwrite_ds] [-append | -overwrite_layer]\n            [-src_geom_type geom_type_name[,geom_type_name]*]\n            [-dsco NAME=VALUE]* [-lco NAME=VALUE]*\n            [-s_srs srs_def] [-t_srs srs_def | -a_srs srs_def]\n            [-progress] [-skipfailures] [--help-general]\n\nOptions specific to -single:\n            [-field_strategy FirstLayer|Union|Intersection]\n            [-src_layer_field_name name]\n            [-src_layer_field_content layer_name_template]\n\n* layer_name_template can contain the following substituable variables:\n     {AUTO_NAME}  : {DS_BASENAME}_{LAYER_NAME} if they are different\n                    or {LAYER_NAME} if they are identical\n     {DS_NAME}    : name of the source dataset\n     {DS_BASENAME}: base name of the source dataset\n     {DS_INDEX}   : index of the source dataset\n     {LAYER_NAME} : name of the source layer\n     {LAYER_INDEX}: index of the source layer\n</pre> <pre>\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit: 2\n</pre> In\u00a0[28]: Copied! <pre>%run -i /Users/abharathi/micromamba/envs/opengeo/bin/ogrmerge.py -o earthquakes.gpkg *.geojson \\\n -single -nln all_earthquakes -overwrite_ds\n</pre> %run -i /Users/abharathi/micromamba/envs/opengeo/bin/ogrmerge.py -o earthquakes.gpkg *.geojson \\  -single -nln all_earthquakes -overwrite_ds In\u00a0[32]: Copied! <pre>!ogr2ogr earthquakes.gpkg earthquakes.gpkg \\\n-where \"mag&gt;4.5\" -nln large_earthquakes -update\n</pre> !ogr2ogr earthquakes.gpkg earthquakes.gpkg \\ -where \"mag&gt;4.5\" -nln large_earthquakes -update In\u00a0[33]: Copied! <pre>cd ..\n</pre> cd .. <pre>/Users/abharathi/Documents/gis_data/gdal-tools\n</pre> In\u00a0[37]: Copied! <pre># %load batch.py\nimport os\n\ninput_dir = 'naip'\n\ncommand = 'gdal_translate -of GTiff -co COMPRESS=JPEG {input} {output}'\nfor file in os.listdir(input_dir):\n  if file.endswith('.jp2'):\n    input = os.path.join(input_dir, file)\n    filename = os.path.splitext(os.path.basename(file))[0]\n    output =  os.path.join(input_dir, filename + '.tif')\n    os.system(command.format(input=input, output=output))\n</pre> # %load batch.py import os  input_dir = 'naip'  command = 'gdal_translate -of GTiff -co COMPRESS=JPEG {input} {output}' for file in os.listdir(input_dir):   if file.endswith('.jp2'):     input = os.path.join(input_dir, file)     filename = os.path.splitext(os.path.basename(file))[0]     output =  os.path.join(input_dir, filename + '.tif')     os.system(command.format(input=input, output=output)) <pre>Input file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\nInput file size is 5000, 5000\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[\u00a0]: Copied! <pre># %load batch_parallel.py\nimport os\nfrom multiprocessing import Pool\nfrom timeit import default_timer as timer\n\ninput_dir = 'naip'\n\ncommand = 'gdal_translate -of GTiff -co COMPRESS=JPEG {input} {output}'\n\ndef process(file):\n    input = os.path.join(input_dir, file)\n    filename = os.path.splitext(os.path.basename(file))[0]\n    output =  os.path.join(input_dir, filename + '.tif')\n    os.system(command.format(input=input, output=output))\n    \nfiles = [file for file in os.listdir(input_dir) if file.endswith('.jp2')]\n\nif __name__ == '__main__':\n  start = timer()\n  p = Pool(4)\n  p.map(process, files)\n  end = timer()\n  print(end - start)\n  \n  # start = timer()\n  # for file in files:\n  #   process(file)\n  # end = timer()\n  # print(end - start)\n</pre> # %load batch_parallel.py import os from multiprocessing import Pool from timeit import default_timer as timer  input_dir = 'naip'  command = 'gdal_translate -of GTiff -co COMPRESS=JPEG {input} {output}'  def process(file):     input = os.path.join(input_dir, file)     filename = os.path.splitext(os.path.basename(file))[0]     output =  os.path.join(input_dir, filename + '.tif')     os.system(command.format(input=input, output=output))      files = [file for file in os.listdir(input_dir) if file.endswith('.jp2')]  if __name__ == '__main__':   start = timer()   p = Pool(4)   p.map(process, files)   end = timer()   print(end - start)      # start = timer()   # for file in files:   #   process(file)   # end = timer()   # print(end - start) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#ogr-inspection-ogr2ogr","title":"OGR - inspection, <code>ogr2ogr</code>\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#info-using-ogrinfo","title":"Info using OGRINFO\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#ogr2ogr-the-swiss-army-knife","title":"<code>ogr2ogr</code> the Swiss army knife\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#convert-csv-to-geopackage","title":"Convert CSV to GeoPackage\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#use-sql-statements","title":"Use SQL statements\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#rename-layer-in-gpkg","title":"Rename layer in gpkg\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#exercise-6","title":"Exercise 6\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#querying-data-using-ogr2ogr","title":"Querying data using <code>ogr2ogr</code>\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#merging-layers","title":"Merging layers\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#write-a-new-layer-into-the-existing-geopackage","title":"Write a new layer into the existing geopackage\u00b6","text":""},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#spatial-sql","title":"Spatial SQL\u00b6","text":"<p>OGR supports dialect called SQLite dialect that is implemented by SpatiaLite. This is not as powerful as postGIS, but still useful.</p>"},{"location":"teaching_resources/gdal/ogr_inspection_ogr2ogr/#etl-section","title":"ETL section\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_geocoding_interactive_plotting/","title":"Geopandas - geocoding, interactive plotting","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon, LineString\nimport seaborn as sns\nimport pyepsg\n%matplotlib inline\nplt.style.use('bmh')\n</pre> import matplotlib.pyplot as plt import pandas as pd import geopandas as gpd from shapely.geometry import Point, Polygon, LineString import seaborn as sns import pyepsg %matplotlib inline plt.style.use('bmh') In\u00a0[2]: Copied! <pre>df = pd.read_csv('data/2015-06-21.csv')\ndf.head()\n</pre> df = pd.read_csv('data/2015-06-21.csv') df.head() Out[2]: Start time End time Calories (kcal) Distance (m) Average heart rate (bpm) Max heart rate (bpm) Min heart rate (bpm) Height (m) Low latitude (deg) Low longitude (deg) ... High longitude (deg) Average speed (m/s) Max speed (m/s) Min speed (m/s) Step count Average weight (kg) Max weight (kg) Min weight (kg) Inactive duration (ms) Walking duration (ms) 0 00:00:00.000-07:00 00:15:00.000-07:00 16.215914 NaN NaN NaN NaN NaN 34.054939 -117.242691 ... -117.242432 NaN NaN NaN NaN NaN NaN NaN 900000 NaN 1 00:15:00.000-07:00 00:30:00.000-07:00 16.215914 NaN NaN NaN NaN NaN 34.054955 -117.242424 ... -117.242401 NaN NaN NaN NaN NaN NaN NaN 900000 NaN 2 00:30:00.000-07:00 00:45:00.000-07:00 16.215914 NaN NaN NaN NaN NaN 34.054939 -117.242706 ... -117.242416 NaN NaN NaN NaN NaN NaN NaN 900000 NaN 3 00:45:00.000-07:00 01:00:00.000-07:00 16.215914 NaN NaN NaN NaN NaN 34.054970 -117.242661 ... -117.242447 NaN NaN NaN NaN NaN NaN NaN 900000 NaN 4 01:00:00.000-07:00 01:15:00.000-07:00 16.215914 NaN NaN NaN NaN NaN 34.054977 -117.242668 ... -117.242409 NaN NaN NaN NaN NaN NaN NaN 900000 NaN <p>5 rows \u00d7 21 columns</p> In\u00a0[3]: Copied! <pre>df.shape\n</pre> df.shape Out[3]: <pre>(96, 21)</pre> In\u00a0[4]: Copied! <pre>df.columns\n</pre> df.columns Out[4]: <pre>Index(['Start time', 'End time', 'Calories (kcal)', 'Distance (m)',\n       'Average heart rate (bpm)', 'Max heart rate (bpm)',\n       'Min heart rate (bpm)', 'Height (m)', 'Low latitude (deg)',\n       'Low longitude (deg)', 'High latitude (deg)', 'High longitude (deg)',\n       'Average speed (m/s)', 'Max speed (m/s)', 'Min speed (m/s)',\n       'Step count', 'Average weight (kg)', 'Max weight (kg)',\n       'Min weight (kg)', 'Inactive duration (ms)', 'Walking duration (ms)'],\n      dtype='object')</pre> <p>filter out columns without data</p> In\u00a0[5]: Copied! <pre>df.describe()\n</pre> df.describe() Out[5]: Calories (kcal) Distance (m) Average heart rate (bpm) Max heart rate (bpm) Min heart rate (bpm) Height (m) Low latitude (deg) Low longitude (deg) High latitude (deg) High longitude (deg) Average speed (m/s) Max speed (m/s) Min speed (m/s) Step count Average weight (kg) Max weight (kg) Min weight (kg) Inactive duration (ms) Walking duration (ms) count 96.000000 2.000000 0.0 0.0 0.0 0.0 91.000000 91.000000 91.000000 91.000000 2.000000 2.000000 2.000000 15.000000 0.0 0.0 0.0 96.000000 5.000000 mean 16.411472 31.000000 NaN NaN NaN NaN 34.055033 -117.242620 34.055120 -117.242453 0.799705 0.799705 0.799705 116.600000 NaN NaN NaN 880833.302083 150074.200000 std 2.665210 2.828427 NaN NaN NaN NaN 0.000113 0.000092 0.000075 0.000097 0.333833 0.333833 0.333833 157.019016 NaN NaN NaN 87443.309515 142937.658018 min 3.434296 29.000000 NaN NaN NaN NaN 34.054150 -117.242844 34.054996 -117.242722 0.563649 0.563649 0.563649 1.000000 NaN NaN NaN 190607.000000 40929.000000 25% 16.215914 30.000000 NaN NaN NaN NaN 34.054996 -117.242691 34.055063 -117.242500 0.681677 0.681677 0.681677 16.000000 NaN NaN NaN 900000.000000 50464.000000 50% 16.215914 31.000000 NaN NaN NaN NaN 34.055023 -117.242653 34.055122 -117.242447 0.799705 0.799705 0.799705 64.000000 NaN NaN NaN 900000.000000 62575.000000 75% 16.215914 32.000000 NaN NaN NaN NaN 34.055098 -117.242538 34.055174 -117.242405 0.917733 0.917733 0.917733 121.000000 NaN NaN NaN 900000.000000 233586.000000 max 34.519853 33.000000 NaN NaN NaN NaN 34.055172 -117.242409 34.055359 -117.242027 1.035761 1.035761 1.035761 581.000000 NaN NaN NaN 900000.000000 362817.000000 In\u00a0[7]: Copied! <pre>df2 = df[['Start time', 'End time','Calories (kcal)','High latitude (deg)',\n          'High longitude (deg)','Average speed (m/s)','Step count']]\ndf2.head(5)\n</pre> df2 = df[['Start time', 'End time','Calories (kcal)','High latitude (deg)',           'High longitude (deg)','Average speed (m/s)','Step count']] df2.head(5) Out[7]: Start time End time Calories (kcal) High latitude (deg) High longitude (deg) Average speed (m/s) Step count 0 00:00:00.000-07:00 00:15:00.000-07:00 16.215914 34.054996 -117.242432 NaN NaN 1 00:15:00.000-07:00 00:30:00.000-07:00 16.215914 34.055000 -117.242401 NaN NaN 2 00:30:00.000-07:00 00:45:00.000-07:00 16.215914 34.055065 -117.242416 NaN NaN 3 00:45:00.000-07:00 01:00:00.000-07:00 16.215914 34.055058 -117.242447 NaN NaN 4 01:00:00.000-07:00 01:15:00.000-07:00 16.215914 34.055061 -117.242409 NaN NaN In\u00a0[8]: Copied! <pre>point_list = [Point(xy) for xy in zip(df2['High longitude (deg)'],df['High latitude (deg)'])]\npoint_list[:4]\n</pre> point_list = [Point(xy) for xy in zip(df2['High longitude (deg)'],df['High latitude (deg)'])] point_list[:4] Out[8]: <pre>[&lt;shapely.geometry.point.Point at 0x10f8b5630&gt;,\n &lt;shapely.geometry.point.Point at 0x10f8b5668&gt;,\n &lt;shapely.geometry.point.Point at 0x10f8b5710&gt;,\n &lt;shapely.geometry.point.Point at 0x10f8b56a0&gt;]</pre> <p>It is not necessary to construct a geoseries, but Folium needs a geoseries and that to have a coordinate system set. So I am building it here</p> In\u00a0[9]: Copied! <pre>gseries = gpd.GeoSeries(point_list)\n</pre> gseries = gpd.GeoSeries(point_list) In\u00a0[10]: Copied! <pre>wgs84=pyepsg.get(4326)\n</pre> wgs84=pyepsg.get(4326) In\u00a0[11]: Copied! <pre># gseries.crs = pyepsg.get(4326)\ngseries.crs ={'init':'epsg:4326'}\n</pre> # gseries.crs = pyepsg.get(4326) gseries.crs ={'init':'epsg:4326'} In\u00a0[12]: Copied! <pre>gdf = gpd.GeoDataFrame(data=df2, geometry=gseries)\ngdf.head(4)\n</pre> gdf = gpd.GeoDataFrame(data=df2, geometry=gseries) gdf.head(4) Out[12]: Start time End time Calories (kcal) High latitude (deg) High longitude (deg) Average speed (m/s) Step count geometry 0 00:00:00.000-07:00 00:15:00.000-07:00 16.215914 34.054996 -117.242432 NaN NaN POINT (-117.242431640625 34.05499649047852) 1 00:15:00.000-07:00 00:30:00.000-07:00 16.215914 34.055000 -117.242401 NaN NaN POINT (-117.2424011230469 34.05500030517578) 2 00:30:00.000-07:00 00:45:00.000-07:00 16.215914 34.055065 -117.242416 NaN NaN POINT (-117.2424163818359 34.0550651550293) 3 00:45:00.000-07:00 01:00:00.000-07:00 16.215914 34.055058 -117.242447 NaN NaN POINT (-117.2424468994141 34.05505752563477) In\u00a0[13]: Copied! <pre>gdf[['High latitude (deg)','High longitude (deg)']].describe()\n</pre> gdf[['High latitude (deg)','High longitude (deg)']].describe() Out[13]: High latitude (deg) High longitude (deg) count 91.000000 91.000000 mean 34.055120 -117.242453 std 0.000075 0.000097 min 34.054996 -117.242722 25% 34.055063 -117.242500 50% 34.055122 -117.242447 75% 34.055174 -117.242405 max 34.055359 -117.242027 <p>Drop the rows that don't have coordinates</p> In\u00a0[14]: Copied! <pre>gdf.dropna(subset=['High latitude (deg)'], inplace=True)\ngdf.shape\n</pre> gdf.dropna(subset=['High latitude (deg)'], inplace=True) gdf.shape Out[14]: <pre>(91, 8)</pre> In\u00a0[15]: Copied! <pre>fig,ax= plt.subplots(1, figsize=(5,5))\ngdf.plot(ax=ax)\n# ax.set_xlim([-117.2,-117.4])\n# ax.set_ylim([34.054, 34.055])\n</pre> fig,ax= plt.subplots(1, figsize=(5,5)) gdf.plot(ax=ax) # ax.set_xlim([-117.2,-117.4]) # ax.set_ylim([34.054, 34.055]) Out[15]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10f99f0f0&gt;</pre> In\u00a0[16]: Copied! <pre>import folium\nfolium.__version__\n</pre> import folium folium.__version__ Out[16]: <pre>'0.5.0'</pre> In\u00a0[17]: Copied! <pre># from folium.plugins import HeatMap\n# hm = HeatMap(data=[xy for xy in zip(gdf['geometry'].x, gdf['geometry'].y)])\n</pre> # from folium.plugins import HeatMap # hm = HeatMap(data=[xy for xy in zip(gdf['geometry'].x, gdf['geometry'].y)]) In\u00a0[135]: Copied! <pre>m = folium.Map(location=[34.054,-117.2], zoom_start=15)\nfolium.GeoJson(data=gdf).add_to(m)\nm\n</pre> m = folium.Map(location=[34.054,-117.2], zoom_start=15) folium.GeoJson(data=gdf).add_to(m) m Out[135]: In\u00a0[18]: Copied! <pre>from geopandas.tools import geocode, geocoding, reverse_geocode\n</pre> from geopandas.tools import geocode, geocoding, reverse_geocode In\u00a0[19]: Copied! <pre>type(geocode), type(reverse_geocode), type(geocoding)\n</pre> type(geocode), type(reverse_geocode), type(geocoding) Out[19]: <pre>(function, function, module)</pre> <p>Get list of providers from <code>geopy</code></p> In\u00a0[15]: Copied! <pre>import geopy, inspect\nprint(geopy.__version__)\n\n# use inspection, but limit to just classes\ninspect.getmembers(geopy, predicate=inspect.isclass)\n</pre> import geopy, inspect print(geopy.__version__)  # use inspection, but limit to just classes inspect.getmembers(geopy, predicate=inspect.isclass) <pre>1.14.0\n</pre> Out[15]: <pre>[('ArcGIS', geopy.geocoders.arcgis.ArcGIS),\n ('Baidu', geopy.geocoders.baidu.Baidu),\n ('Bing', geopy.geocoders.bing.Bing),\n ('DataBC', geopy.geocoders.databc.DataBC),\n ('GeoNames', geopy.geocoders.geonames.GeoNames),\n ('GeocodeFarm', geopy.geocoders.geocodefarm.GeocodeFarm),\n ('GoogleV3', geopy.geocoders.googlev3.GoogleV3),\n ('IGNFrance', geopy.geocoders.ignfrance.IGNFrance),\n ('LiveAddress', geopy.geocoders.smartystreets.LiveAddress),\n ('Location', geopy.location.Location),\n ('Mapzen', geopy.geocoders.mapzen.Mapzen),\n ('Nominatim', geopy.geocoders.osm.Nominatim),\n ('OpenCage', geopy.geocoders.opencage.OpenCage),\n ('OpenMapQuest', geopy.geocoders.openmapquest.OpenMapQuest),\n ('Photon', geopy.geocoders.photon.Photon),\n ('PickPoint', geopy.geocoders.pickpoint.PickPoint),\n ('Point', geopy.point.Point),\n ('What3Words', geopy.geocoders.what3words.What3Words),\n ('Yandex', geopy.geocoders.yandex.Yandex)]</pre> In\u00a0[20]: Copied! <pre>geocode(strings='Hollywood sign') # default provider is googlev3\n</pre> geocode(strings='Hollywood sign') # default provider is googlev3 Out[20]: address geometry 0 Los Angeles, CA 90068, USA POINT (-118.3215482 34.1341151) In\u00a0[21]: Copied! <pre>geocode(strings='Hollywood sign', provider='arcgis')\n</pre> geocode(strings='Hollywood sign', provider='arcgis') Out[21]: address geometry 0 Hollywood Sign POINT (-118.3219799999999 34.13438000000008) In\u00a0[26]: Copied! <pre>address_list = ['hospitals near Redlands, CA', \n                'groceries near Redlands, CA', \n                'schools near Redlands, CA']\n\ngeocoded_gdf = geocode(strings=address_list, provider='arcgis')\ngeocoded_gdf\n</pre> address_list = ['hospitals near Redlands, CA',                  'groceries near Redlands, CA',                  'schools near Redlands, CA']  geocoded_gdf = geocode(strings=address_list, provider='arcgis') geocoded_gdf Out[26]: address geometry 0 Redlands Community Hospital-ER POINT (-117.2044100362101 34.03633005943772) 1 Target POINT (-117.2072099744647 34.07031003505633) 2 Redlands East Valley High School POINT (-117.12776 34.06201000000004) In\u00a0[30]: Copied! <pre>m2 = folium.Map(location=[34.054,-117.2], zoom_start=12)\nfolium.GeoJson(data=geocoded_gdf).add_to(m2)\nm2\n</pre> m2 = folium.Map(location=[34.054,-117.2], zoom_start=12) folium.GeoJson(data=geocoded_gdf).add_to(m2) m2 Out[30]: In\u00a0[31]: Copied! <pre>m2.crs\n</pre> m2.crs Out[31]: <pre>'EPSG3857'</pre> In\u00a0[32]: Copied! <pre>m2.to_json()\n</pre> m2.to_json() Out[32]: <pre>'{\"name\": \"Map\", \"id\": \"94aa73e804f245ec8f4e34436a4a2412\", \"children\": {\"openstreetmap\": {\"name\": \"TileLayer\", \"id\": \"6e327110ed23459f86840082b427835d\", \"children\": {}}, \"geo_json_bd54f09d34e648ffbd9a583dc626582f\": {\"name\": \"GeoJson\", \"id\": \"bd54f09d34e648ffbd9a583dc626582f\", \"children\": {}}}}'</pre>"},{"location":"teaching_resources/geopandas/geopandas_geocoding_interactive_plotting/#geopandas-geocoding-interactive-plotting","title":"Geopandas - geocoding, interactive plotting\u00b6","text":"<p>More IO, interactive visualization using folium and geocoding</p>"},{"location":"teaching_resources/geopandas/geopandas_geocoding_interactive_plotting/#read-csv-data","title":"Read CSV data\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_geocoding_interactive_plotting/#create-a-geopandas-data-frame-from-pandas-dataframe","title":"Create a geopandas data frame from pandas dataframe\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_geocoding_interactive_plotting/#viz-on-folium-map","title":"Viz on Folium map\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_geocoding_interactive_plotting/#geocoding","title":"Geocoding\u00b6","text":"<p>Geocoding is the process of turning addresses into coordinates by plotting them on a map. It is a combination of search and computation (when you interpolate street addresses for unknown extents) that is typically performed on server side due to the amount of data required on a global scale.</p> <p>Geopandas uses <code>geopy</code> to perform geocoding and returns the result of hits as a <code>geodataframe</code>.</p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/","title":"GeoPandas - intro, plotting","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('bmh') # better for plotting geometries vs general plots.\n\nfrom shapely.geometry import Point, Polygon, LineString\nimport pandas as pd\nimport geopandas as gpd\nfrom geopandas import GeoSeries, GeoDataFrame\n</pre> import matplotlib.pyplot as plt import seaborn as sns plt.style.use('bmh') # better for plotting geometries vs general plots.  from shapely.geometry import Point, Polygon, LineString import pandas as pd import geopandas as gpd from geopandas import GeoSeries, GeoDataFrame In\u00a0[2]: Copied! <pre>point_list = [Point(-120,45), Point(-121.2, 46), Point(-122.9, 47.5)]\ngs = GeoSeries(point_list)\ngs\n</pre> point_list = [Point(-120,45), Point(-121.2, 46), Point(-122.9, 47.5)] gs = GeoSeries(point_list) gs Out[2]: <pre>0        POINT (-120 45)\n1      POINT (-121.2 46)\n2    POINT (-122.9 47.5)\ndtype: object</pre> In\u00a0[3]: Copied! <pre>type(gs)\n</pre> type(gs) Out[3]: <pre>geopandas.geoseries.GeoSeries</pre> In\u00a0[4]: Copied! <pre>gs.geometry\n</pre> gs.geometry Out[4]: <pre>0        POINT (-120 45)\n1      POINT (-121.2 46)\n2    POINT (-122.9 47.5)\ndtype: object</pre> In\u00a0[5]: Copied! <pre>gs.geom_type\n</pre> gs.geom_type Out[5]: <pre>0    Point\n1    Point\n2    Point\ndtype: object</pre> In\u00a0[6]: Copied! <pre>gs.iloc[0]\n</pre> gs.iloc[0] Out[6]: In\u00a0[8]: Copied! <pre>gs.crs = {'init':'epsg:4326'}\n</pre> gs.crs = {'init':'epsg:4326'} In\u00a0[9]: Copied! <pre>gs.plot()\n</pre> gs.plot() Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a0e833080&gt;</pre> In\u00a0[16]: Copied! <pre>data_dict = {'name':['a','b','c'],\n            'lat':[45,46,47.5],\n            'lon':[-120,-121.2,-123]}\n\ndf = pd.DataFrame(data_dict)\ndf.set_index(df['name'], inplace=True)\ndf\n</pre> data_dict = {'name':['a','b','c'],             'lat':[45,46,47.5],             'lon':[-120,-121.2,-123]}  df = pd.DataFrame(data_dict) df.set_index(df['name'], inplace=True) df Out[16]: lat lon name name a 45.0 -120.0 a b 46.0 -121.2 b c 47.5 -123.0 c In\u00a0[17]: Copied! <pre>[xy for xy in zip(df['lon'], df['lat'])]\n</pre> [xy for xy in zip(df['lon'], df['lat'])] Out[17]: <pre>[(-120.0, 45.0), (-121.2, 46.0), (-123.0, 47.5)]</pre> In\u00a0[18]: Copied! <pre>point_list = [Point(xy) for xy in zip(df['lon'], df['lat'])]\npoint_list\n</pre> point_list = [Point(xy) for xy in zip(df['lon'], df['lat'])] point_list Out[18]: <pre>[&lt;shapely.geometry.point.Point at 0x1a0e8af5c0&gt;,\n &lt;shapely.geometry.point.Point at 0x1a19123400&gt;,\n &lt;shapely.geometry.point.Point at 0x1a191237f0&gt;]</pre> In\u00a0[20]: Copied! <pre>gdf = GeoDataFrame(df, geometry=point_list)\ngdf\n</pre> gdf = GeoDataFrame(df, geometry=point_list) gdf Out[20]: lat lon name geometry name a 45.0 -120.0 a POINT (-120 45) b 46.0 -121.2 b POINT (-121.2 46) c 47.5 -123.0 c POINT (-123 47.5) In\u00a0[21]: Copied! <pre>gdf.plot()\n</pre> gdf.plot() Out[21]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a191bd9b0&gt;</pre> In\u00a0[22]: Copied! <pre>fire_stations = gpd.read_file('./data/FireStations.shp')\ntype(fire_stations)\n</pre> fire_stations = gpd.read_file('./data/FireStations.shp') type(fire_stations) Out[22]: <pre>geopandas.geodataframe.GeoDataFrame</pre> In\u00a0[23]: Copied! <pre>fire_stations.head()\n</pre> fire_stations.head() Out[23]: OBJECTID post_id Name descriptio cat1 cat2 cat3 addrln1 addrln2 city ... source ext_id use_type dis_status latitude longitude date_updat POINT_X POINT_Y geometry 0 40223266 2022.0 Los Angeles County Fire Department - Battalion... The Battalion provides fire and rescue service... Public Safety Fire Stations None 7643 W. Santa Monica Blvd. None Los Angeles ... 211 None publish None 34.090940 -118.356388 2013-06-01 6.453766e+06 1.855666e+06 POINT (6453766.230470523 1855666.492027521) 1 40223440 2236.0 Los Angeles County Fire Department - Battalion... The Battalion provides fire and rescue service... Public Safety Fire Stations Battalion HQ 6031 Rickenbacker Rd. None Commerce ... 211 None publish None 33.988587 -118.154162 2013-06-01 6.514937e+06 1.818263e+06 POINT (6514937.276763365 1818262.767294437) 2 40223442 2237.0 Los Angeles County Fire Department - Battalion... The Battalion provides fire and rescue service... Public Safety Fire Stations None 1059 N. White Ave. None Pomona ... 211 None publish None 34.067555 -117.759366 2013-06-01 6.634548e+06 1.847052e+06 POINT (6634547.880061358 1847052.198560596) 3 40223459 2262.0 Los Angeles County Fire Department - Battalion... The Battalion provides fire and rescue service... Public Safety Fire Stations None 1260 Encinal Canyon Rd None Malibu ... 211 None publish None 34.084350 -118.866037 2013-06-01 6.299441e+06 1.854207e+06 POINT (6299441.115893021 1854206.637991846) 4 40223463 2269.0 Los Angeles County Fire Department - Battalion... The Battalion provides fire and rescue service... Public Safety Fire Stations Battalion HQ 6301 S. Santa Fe Ave. None Huntington Park ... 211 None publish None 33.983196 -118.230626 2013-06-01 6.491753e+06 1.816345e+06 POINT (6491753.321819022 1816345.240289599) <p>5 rows \u00d7 30 columns</p> In\u00a0[24]: Copied! <pre>fire_stations.crs\n</pre> fire_stations.crs Out[24]: <pre>{'proj': 'lcc',\n 'lat_1': 34.03333333333333,\n 'lat_2': 35.46666666666667,\n 'lat_0': 33.5,\n 'lon_0': -118,\n 'x_0': 2000000,\n 'y_0': 500000.0000000001,\n 'datum': 'NAD83',\n 'units': 'us-ft',\n 'no_defs': True}</pre> In\u00a0[28]: Copied! <pre>fire_stations.columns\n</pre> fire_stations.columns Out[28]: <pre>Index(['OBJECTID', 'post_id', 'Name', 'descriptio', 'cat1', 'cat2', 'cat3',\n       'addrln1', 'addrln2', 'city', 'state', 'zip', 'hours', 'phones', 'url',\n       'email', 'info1', 'info2', 'link', 'org_name', 'source', 'ext_id',\n       'use_type', 'dis_status', 'latitude', 'longitude', 'date_updat',\n       'POINT_X', 'POINT_Y', 'geometry'],\n      dtype='object')</pre> In\u00a0[67]: Copied! <pre>fire_stations['city'].value_counts().head()\n</pre> fire_stations['city'].value_counts().head() Out[67]: <pre>Los Angeles      65\nLong Beach       25\nSanta Clarita    10\nPasadena         10\nPomona            9\nName: city, dtype: int64</pre> In\u00a0[39]: Copied! <pre>fire_stations.plot(figsize=(10,10))\n</pre> fire_stations.plot(figsize=(10,10)) Out[39]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a19347048&gt;</pre> In\u00a0[41]: Copied! <pre>gpd.datasets.available\n</pre> gpd.datasets.available Out[41]: <pre>['naturalearth_cities', 'naturalearth_lowres', 'nybb']</pre> In\u00a0[42]: Copied! <pre>world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld.head()\n</pre> world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) world.head() Out[42]: pop_est continent name iso_a3 gdp_md_est geometry 0 28400000.0 Asia Afghanistan AFG 22270.0 POLYGON ((61.21081709172574 35.65007233330923,... 1 12799293.0 Africa Angola AGO 110300.0 (POLYGON ((16.32652835456705 -5.87747039146621... 2 3639453.0 Europe Albania ALB 21810.0 POLYGON ((20.59024743010491 41.85540416113361,... 3 4798491.0 Asia United Arab Emirates ARE 184300.0 POLYGON ((51.57951867046327 24.24549713795111,... 4 40913584.0 South America Argentina ARG 573900.0 (POLYGON ((-65.50000000000003 -55.199999999999... In\u00a0[45]: Copied! <pre>world.shape\n</pre> world.shape Out[45]: <pre>(177, 6)</pre> In\u00a0[46]: Copied! <pre>world['name'].unique()\n</pre> world['name'].unique() Out[46]: <pre>array(['Afghanistan', 'Angola', 'Albania', 'United Arab Emirates',\n       'Argentina', 'Armenia', 'Antarctica', 'Fr. S. Antarctic Lands',\n       'Australia', 'Austria', 'Azerbaijan', 'Burundi', 'Belgium',\n       'Benin', 'Burkina Faso', 'Bangladesh', 'Bulgaria', 'Bahamas',\n       'Bosnia and Herz.', 'Belarus', 'Belize', 'Bolivia', 'Brazil',\n       'Brunei', 'Bhutan', 'Botswana', 'Central African Rep.', 'Canada',\n       'Switzerland', 'Chile', 'China', \"C\u00f4te d'Ivoire\", 'Cameroon',\n       'Dem. Rep. Congo', 'Congo', 'Colombia', 'Costa Rica', 'Cuba',\n       'N. Cyprus', 'Cyprus', 'Czech Rep.', 'Germany', 'Djibouti',\n       'Denmark', 'Dominican Rep.', 'Algeria', 'Ecuador', 'Egypt',\n       'Eritrea', 'Spain', 'Estonia', 'Ethiopia', 'Finland', 'Fiji',\n       'Falkland Is.', 'France', 'Gabon', 'United Kingdom', 'Georgia',\n       'Ghana', 'Guinea', 'Gambia', 'Guinea-Bissau', 'Eq. Guinea',\n       'Greece', 'Greenland', 'Guatemala', 'Guyana', 'Honduras',\n       'Croatia', 'Haiti', 'Hungary', 'Indonesia', 'India', 'Ireland',\n       'Iran', 'Iraq', 'Iceland', 'Israel', 'Italy', 'Jamaica', 'Jordan',\n       'Japan', 'Kazakhstan', 'Kenya', 'Kyrgyzstan', 'Cambodia', 'Korea',\n       'Kosovo', 'Kuwait', 'Lao PDR', 'Lebanon', 'Liberia', 'Libya',\n       'Sri Lanka', 'Lesotho', 'Lithuania', 'Luxembourg', 'Latvia',\n       'Morocco', 'Moldova', 'Madagascar', 'Mexico', 'Macedonia', 'Mali',\n       'Myanmar', 'Montenegro', 'Mongolia', 'Mozambique', 'Mauritania',\n       'Malawi', 'Malaysia', 'Namibia', 'New Caledonia', 'Niger',\n       'Nigeria', 'Nicaragua', 'Netherlands', 'Norway', 'Nepal',\n       'New Zealand', 'Oman', 'Pakistan', 'Panama', 'Peru', 'Philippines',\n       'Papua New Guinea', 'Poland', 'Puerto Rico', 'Dem. Rep. Korea',\n       'Portugal', 'Paraguay', 'Palestine', 'Qatar', 'Romania', 'Russia',\n       'Rwanda', 'W. Sahara', 'Saudi Arabia', 'Sudan', 'S. Sudan',\n       'Senegal', 'Solomon Is.', 'Sierra Leone', 'El Salvador',\n       'Somaliland', 'Somalia', 'Serbia', 'Suriname', 'Slovakia',\n       'Slovenia', 'Sweden', 'Swaziland', 'Syria', 'Chad', 'Togo',\n       'Thailand', 'Tajikistan', 'Turkmenistan', 'Timor-Leste',\n       'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Taiwan', 'Tanzania',\n       'Uganda', 'Ukraine', 'Uruguay', 'United States', 'Uzbekistan',\n       'Venezuela', 'Vietnam', 'Vanuatu', 'Yemen', 'South Africa',\n       'Zambia', 'Zimbabwe'], dtype=object)</pre> In\u00a0[47]: Copied! <pre>usa = world[world['name']=='United States']\nusa\n</pre> usa = world[world['name']=='United States'] usa Out[47]: pop_est continent name iso_a3 gdp_md_est geometry 168 313973000.0 North America United States USA 15094000.0 (POLYGON ((-155.54211 19.08348000000001, -155.... In\u00a0[49]: Copied! <pre>ax = fire_stations.plot(figsize=(10,10))\nusa.plot(ax = ax)\n</pre> ax = fire_stations.plot(figsize=(10,10)) usa.plot(ax = ax) Out[49]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1939fc18&gt;</pre> In\u00a0[50]: Copied! <pre>usa.plot()\n</pre> usa.plot() Out[50]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a193be9e8&gt;</pre> In\u00a0[51]: Copied! <pre>usa.crs\n</pre> usa.crs Out[51]: <pre>{'init': 'epsg:4326'}</pre> <p>Snap! They are in different projections and geopandas does not handle that. SO lets try to reproject the fire stations to match usa</p> In\u00a0[52]: Copied! <pre>fire_stations_gcs = fire_stations.to_crs(epsg=4326)\n</pre> fire_stations_gcs = fire_stations.to_crs(epsg=4326) In\u00a0[53]: Copied! <pre>fire_stations_gcs.crs\n</pre> fire_stations_gcs.crs Out[53]: <pre>{'init': 'epsg:4326', 'no_defs': True}</pre> In\u00a0[54]: Copied! <pre>fire_stations_gcs.plot()\n</pre> fire_stations_gcs.plot() Out[54]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a193ec0f0&gt;</pre> In\u00a0[60]: Copied! <pre>fire_stations_gcs.total_bounds\n</pre> fire_stations_gcs.total_bounds Out[60]: <pre>array([-118.88357856,   33.32376846, -117.70773665,   34.75793147])</pre> In\u00a0[66]: Copied! <pre>f, ax = plt.subplots(1, figsize=(12, 12))\nax.set_title('Fire stations in Southern California')\nusa.plot(ax=ax, facecolor='lightgray', edgecolor='gray')\nfire_stations_gcs.plot(ax=ax)\n\n#this plots the whole US. To limit the scale to just SoCal, limit the axes\nax.set_xlim([-120, -118])\nax.set_ylim([33, 35])\n</pre> f, ax = plt.subplots(1, figsize=(12, 12)) ax.set_title('Fire stations in Southern California') usa.plot(ax=ax, facecolor='lightgray', edgecolor='gray') fire_stations_gcs.plot(ax=ax)  #this plots the whole US. To limit the scale to just SoCal, limit the axes ax.set_xlim([-120, -118]) ax.set_ylim([33, 35]) Out[66]: <pre>(33, 35)</pre> In\u00a0[2]: Copied! <pre>damsel_gdf = gpd.read_file('./data/damselfish/DAMSELFISH_distributions.shp')\ndamsel_gdf.head(3)\n</pre> damsel_gdf = gpd.read_file('./data/damselfish/DAMSELFISH_distributions.shp') damsel_gdf.head(3) Out[2]: ID_NO BINOMIAL ORIGIN COMPILER YEAR CITATION SOURCE DIST_COMM ISLAND SUBSPECIES ... RL_UPDATE KINGDOM_NA PHYLUM_NAM CLASS_NAME ORDER_NAME FAMILY_NAM GENUS_NAME SPECIES_NA CATEGORY geometry 0 183963.0 Stegastes leucorus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes leucorus VU POLYGON ((-115.6437454219999 29.71392059300007... 1 183963.0 Stegastes leucorus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes leucorus VU POLYGON ((-105.589950704 21.89339825500002, -1... 2 183963.0 Stegastes leucorus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes leucorus VU POLYGON ((-111.159618439 19.01535626700007, -1... <p>3 rows \u00d7 24 columns</p> In\u00a0[3]: Copied! <pre>damsel_gdf.columns\n</pre> damsel_gdf.columns Out[3]: <pre>Index(['ID_NO', 'BINOMIAL', 'ORIGIN', 'COMPILER', 'YEAR', 'CITATION', 'SOURCE',\n       'DIST_COMM', 'ISLAND', 'SUBSPECIES', 'SUBPOP', 'LEGEND', 'SEASONAL',\n       'TAX_COMM', 'RL_UPDATE', 'KINGDOM_NA', 'PHYLUM_NAM', 'CLASS_NAME',\n       'ORDER_NAME', 'FAMILY_NAM', 'GENUS_NAME', 'SPECIES_NA', 'CATEGORY',\n       'geometry'],\n      dtype='object')</pre> In\u00a0[4]: Copied! <pre>damsel_gdf.shape\n</pre> damsel_gdf.shape Out[4]: <pre>(231, 24)</pre> In\u00a0[17]: Copied! <pre>damsel_gdf.plot(column='BINOMIAL', categorical=True, \n                figsize=(12, 4))\n</pre> damsel_gdf.plot(column='BINOMIAL', categorical=True,                  figsize=(12, 4)) Out[17]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1d4a9eb8&gt;</pre> In\u00a0[5]: Copied! <pre>damsel_gdf['BINOMIAL'].value_counts()\n</pre> damsel_gdf['BINOMIAL'].value_counts() Out[5]: <pre>Amphiprion sandaracinos    51\nChromis cyanea             15\nChromis alpha              14\nChromis alta               11\nMicrospathodon bairdii     10\nAbudefduf troschelii       10\nStegastes acapulcoensis     9\nChromis atrilobata          9\nMicrospathodon dorsalis     9\nStegastes arcifrons         8\nStegastes flavilatus        8\nStegastes beebei            8\nChromis limbaughi           7\nTeixeirichthys jordani      7\nNexilosus latifrons         7\nAbudefduf concolor          6\nChrysiptera flavipinnis     5\nAzurina hirundo             5\nHypsypops rubicundus        4\nChromis pembae              4\nStegastes leucorus          3\nChromis intercrusma         3\nAbudefduf declivifrons      3\nStegastes rectifraenum      3\nAzurina eupalama            3\nChromis crusma              3\nChromis flavicauda          2\nStegastes redemptus         2\nStegastes baldwini          1\nChromis punctipinnis        1\nName: BINOMIAL, dtype: int64</pre> In\u00a0[6]: Copied! <pre>grouped = damsel_gdf.groupby(by='BINOMIAL')\ngrouped.head()\n</pre> grouped = damsel_gdf.groupby(by='BINOMIAL') grouped.head() Out[6]: ID_NO BINOMIAL ORIGIN COMPILER YEAR CITATION SOURCE DIST_COMM ISLAND SUBSPECIES ... RL_UPDATE KINGDOM_NA PHYLUM_NAM CLASS_NAME ORDER_NAME FAMILY_NAM GENUS_NAME SPECIES_NA CATEGORY geometry 0 183963.0 Stegastes leucorus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes leucorus VU POLYGON ((-115.6437454219999 29.71392059300007... 1 183963.0 Stegastes leucorus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes leucorus VU POLYGON ((-105.589950704 21.89339825500002, -1... 2 183963.0 Stegastes leucorus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes leucorus VU POLYGON ((-111.159618439 19.01535626700007, -1... 3 183793.0 Chromis intercrusma 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis intercrusma LC POLYGON ((-80.86500229899997 -0.77894492099994... 4 183793.0 Chromis intercrusma 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis intercrusma LC POLYGON ((-67.33922225599997 -55.6761029239999... 5 183793.0 Chromis intercrusma 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis intercrusma LC POLYGON ((-74.81822204599996 -51.4608230589999... 6 183462.0 Stegastes beebei 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes beebei VU POLYGON ((-86.13105102199995 5.598867493000057... 7 183462.0 Stegastes beebei 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes beebei VU POLYGON ((-80.68215272899994 4.037426486000072... 8 183462.0 Stegastes beebei 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes beebei VU POLYGON ((-90.22271068899994 -0.27334342499995... 9 183462.0 Stegastes beebei 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes beebei VU POLYGON ((-83.93571481699996 9.296388679000074... 10 183462.0 Stegastes beebei 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes beebei VU POLYGON ((-79.15961456299993 9.011326790000055... 14 183586.0 Stegastes rectifraenum 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes rectifraenum LC POLYGON ((-114.129066467 31.49156189000007, -1... 15 183586.0 Stegastes rectifraenum 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes rectifraenum LC POLYGON ((-106.7327677389999 23.60688894500004... 16 183586.0 Stegastes rectifraenum 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes rectifraenum LC POLYGON ((-111.159618439 19.01535626700007, -1... 17 154831.0 Chromis punctipinnis 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis punctipinnis LC POLYGON ((-121.799003601 36.75037002600004, -1... 18 183240.0 Chromis crusma 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis crusma LC POLYGON ((-80.82860578399999 -1.22984210599997... 19 183240.0 Chromis crusma 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis crusma LC POLYGON ((-67.33922225599997 -55.6761029239999... 20 183240.0 Chromis crusma 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis crusma LC POLYGON ((-74.81822204599996 -51.4608230589999... 21 154856.0 Chromis pembae 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis pembae LC POLYGON ((32.56497192400008 29.96966743400003,... 22 154856.0 Chromis pembae 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis pembae LC POLYGON ((55.90029544000004 -3.630027441999971... 23 154856.0 Chromis pembae 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis pembae LC POLYGON ((73.86925448500006 7.187452539000049,... 24 154856.0 Chromis pembae 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis pembae LC POLYGON ((39.89708328200004 16.32163810700007,... 25 183567.0 Stegastes redemptus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes redemptus VU POLYGON ((-112.128707413 25.07043759500004, -1... 26 183567.0 Stegastes redemptus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes redemptus VU POLYGON ((-111.159618439 19.01535626700007, -1... 27 154915.0 Teixeirichthys jordani 1 None 2012 Red List Index (Sampled Approach), Zoological ... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Teixeirichthys jordani LC POLYGON ((121.6300326400001 33.04248618400004,... 28 154915.0 Teixeirichthys jordani 1 None 2012 Red List Index (Sampled Approach), Zoological ... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Teixeirichthys jordani LC POLYGON ((32.56219482400007 29.97488975500005,... 29 154915.0 Teixeirichthys jordani 1 None 2012 Red List Index (Sampled Approach), Zoological ... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Teixeirichthys jordani LC POLYGON ((130.9052090560001 34.02498196400006,... 30 154915.0 Teixeirichthys jordani 1 None 2012 Red List Index (Sampled Approach), Zoological ... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Teixeirichthys jordani LC POLYGON ((56.32233070000007 -3.707270205999976... 31 154915.0 Teixeirichthys jordani 1 None 2012 Red List Index (Sampled Approach), Zoological ... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Teixeirichthys jordani LC POLYGON ((40.64476131800006 -10.85502363999996... 34 183452.0 Chromis limbaughi 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis limbaughi LC POLYGON ((-112.417701721 29.37484550400006, -1... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 187 183483.0 Abudefduf concolor 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf concolor LC POLYGON ((-91.09391784599995 -0.58096068999992... 189 183397.0 Abudefduf troschelii 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf troschelii LC POLYGON ((-90.09187344499998 13.72481517500006... 190 183397.0 Abudefduf troschelii 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf troschelii LC POLYGON ((-121.284812927 35.67465591400003, -1... 191 183397.0 Abudefduf troschelii 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf troschelii LC POLYGON ((-106.72576642 23.59774532600005, -10... 192 183397.0 Abudefduf troschelii 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf troschelii LC POLYGON ((-86.13105102199995 5.598867493000057... 193 183397.0 Abudefduf troschelii 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf troschelii LC POLYGON ((-111.159618439 19.01535626700007, -1... 199 154920.0 Chrysiptera flavipinnis 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chrysiptera flavipinnis LC POLYGON ((144.147216637 -7.778888740999946, 14... 200 154920.0 Chrysiptera flavipinnis 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chrysiptera flavipinnis LC POLYGON ((156.671737653 -6.481586890999949, 15... 201 154920.0 Chrysiptera flavipinnis 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chrysiptera flavipinnis LC POLYGON ((164.5374754960001 -20.08774569199994... 202 154920.0 Chrysiptera flavipinnis 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chrysiptera flavipinnis LC POLYGON ((151.2978177690001 -16.88691080299998... 203 154920.0 Chrysiptera flavipinnis 1 None 2012 International Union for Conservation of Nature... None None None None ... 2012.2 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chrysiptera flavipinnis LC POLYGON ((159.58249191 -21.61306719099997, 159... 204 183653.0 Chromis atrilobata 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis atrilobata LC POLYGON ((-90.09187344499998 13.72481517500006... 205 183653.0 Chromis atrilobata 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis atrilobata LC POLYGON ((-113.422749196 30.12412492800007, -1... 206 183653.0 Chromis atrilobata 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis atrilobata LC POLYGON ((-106.72576642 23.59774532600005, -10... 207 183653.0 Chromis atrilobata 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis atrilobata LC POLYGON ((-86.13105102199995 5.598867493000057... 208 183653.0 Chromis atrilobata 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Chromis atrilobata LC POLYGON ((-111.159618439 19.01535626700007, -1... 213 183431.0 Stegastes acapulcoensis 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes acapulcoensis LC POLYGON ((-90.09187344499998 13.72481517500006... 214 183431.0 Stegastes acapulcoensis 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes acapulcoensis LC POLYGON ((-106.726318976 23.59928014100007, -1... 215 183431.0 Stegastes acapulcoensis 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes acapulcoensis LC POLYGON ((-109.2706279609999 26.29294047400003... 216 183431.0 Stegastes acapulcoensis 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes acapulcoensis LC POLYGON ((-86.13105102199995 5.598867493000057... 217 183431.0 Stegastes acapulcoensis 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Stegastes acapulcoensis LC POLYGON ((-111.170232026 19.01577707400003, -1... 222 183367.0 Hypsypops rubicundus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Hypsypops rubicundus LC POLYGON ((-124.1523361 41.05706766400004, -124... 223 183367.0 Hypsypops rubicundus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Hypsypops rubicundus LC POLYGON ((-117.4136322419999 29.18823463600006... 224 183367.0 Hypsypops rubicundus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Hypsypops rubicundus LC POLYGON ((-117.330311632 21.41966897300006, -1... 225 183367.0 Hypsypops rubicundus 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Hypsypops rubicundus LC POLYGON ((-110.714172363 25.30718421900008, -1... 226 183774.0 Azurina hirundo 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Azurina hirundo NT POLYGON ((-120.118286133 34.47283172600004, -1... 227 183774.0 Azurina hirundo 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Azurina hirundo NT POLYGON ((-117.4136322419999 29.18823463600006... 228 183774.0 Azurina hirundo 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Azurina hirundo NT POLYGON ((-114.638387688 28.39007998000005, -1... 229 183774.0 Azurina hirundo 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Azurina hirundo NT POLYGON ((-111.159618439 19.01535626700007, -1... 230 183774.0 Azurina hirundo 1 IUCN 2010 International Union for Conservation of Nature... None None None None ... 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Azurina hirundo NT POLYGON ((-114.244282766 22.71134688500007, -1... <p>122 rows \u00d7 24 columns</p> In\u00a0[9]: Copied! <pre>type(grouped), type(grouped['BINOMIAL'])\n</pre> type(grouped), type(grouped['BINOMIAL']) Out[9]: <pre>(pandas.core.groupby.DataFrameGroupBy, pandas.core.groupby.SeriesGroupBy)</pre> In\u00a0[10]: Copied! <pre>!mkdir './data/fish_split'\n</pre> !mkdir './data/fish_split' <p>We can iterate over the <code>DataFrameGroupBy</code> object and perform aggregations like <code>std</code>, <code>mean</code>, <code>sum</code> etc. However, here, we can split by column and write each individual fish species to its own shape file.</p> In\u00a0[12]: Copied! <pre>import os\nfor key, value in grouped:\n    # create output name using key. Replace space with _\n    out_name = '{}.shp'.format(key.replace(\" \",\"_\"))\n    print(\"Writing: \" + out_name)\n    \n    #write to disk using geopandas\n    value.to_file(os.path.join(\"./data/fish_split\", out_name))\n</pre> import os for key, value in grouped:     # create output name using key. Replace space with _     out_name = '{}.shp'.format(key.replace(\" \",\"_\"))     print(\"Writing: \" + out_name)          #write to disk using geopandas     value.to_file(os.path.join(\"./data/fish_split\", out_name)) <pre>Writing: Abudefduf_concolor.shp\nWriting: Abudefduf_declivifrons.shp\nWriting: Abudefduf_troschelii.shp\nWriting: Amphiprion_sandaracinos.shp\nWriting: Azurina_eupalama.shp\nWriting: Azurina_hirundo.shp\nWriting: Chromis_alpha.shp\nWriting: Chromis_alta.shp\nWriting: Chromis_atrilobata.shp\nWriting: Chromis_crusma.shp\nWriting: Chromis_cyanea.shp\nWriting: Chromis_flavicauda.shp\nWriting: Chromis_intercrusma.shp\nWriting: Chromis_limbaughi.shp\nWriting: Chromis_pembae.shp\nWriting: Chromis_punctipinnis.shp\nWriting: Chrysiptera_flavipinnis.shp\nWriting: Hypsypops_rubicundus.shp\nWriting: Microspathodon_bairdii.shp\nWriting: Microspathodon_dorsalis.shp\nWriting: Nexilosus_latifrons.shp\nWriting: Stegastes_acapulcoensis.shp\nWriting: Stegastes_arcifrons.shp\nWriting: Stegastes_baldwini.shp\nWriting: Stegastes_beebei.shp\nWriting: Stegastes_flavilatus.shp\nWriting: Stegastes_leucorus.shp\nWriting: Stegastes_rectifraenum.shp\nWriting: Stegastes_redemptus.shp\nWriting: Teixeirichthys_jordani.shp\n</pre> <p>Plot the last fish to see if its a smaller area than original:</p> In\u00a0[14]: Copied! <pre>print(key)\nvalue.plot()\n</pre> print(key) value.plot() <pre>Teixeirichthys jordani\n</pre> Out[14]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x110808eb8&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>grouped.aggregate()\n</pre> grouped.aggregate() In\u00a0[18]: Copied! <pre>damsel_gdf_dissolved = damsel_gdf.dissolve(by='BINOMIAL', aggfunc='first')\ndamsel_gdf_dissolved.shape\n</pre> damsel_gdf_dissolved = damsel_gdf.dissolve(by='BINOMIAL', aggfunc='first') damsel_gdf_dissolved.shape Out[18]: <pre>(30, 23)</pre> <p>This took a while, but we managed to shrink <code>231</code> features to <code>30</code> features. Let us plot and write it to disk for future processing.</p> In\u00a0[20]: Copied! <pre>damsel_gdf_dissolved.head(3)\n</pre> damsel_gdf_dissolved.head(3) Out[20]: geometry ID_NO ORIGIN COMPILER YEAR CITATION SOURCE DIST_COMM ISLAND SUBSPECIES ... TAX_COMM RL_UPDATE KINGDOM_NA PHYLUM_NAM CLASS_NAME ORDER_NAME FAMILY_NAM GENUS_NAME SPECIES_NA CATEGORY BINOMIAL Abudefduf concolor (POLYGON ((-86.13105102199995 5.59886749300005... 183483.0 1 IUCN 2010 International Union for Conservation of Nature... NaN NaN NaN NaN ... NaN 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf concolor LC Abudefduf declivifrons (POLYGON ((-90.09187344499998 13.7248151750000... 183460.0 1 IUCN 2010 International Union for Conservation of Nature... NaN NaN NaN NaN ... NaN 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf declivifrons LC Abudefduf troschelii (POLYGON ((-91.24864959699994 -0.6798474789999... 183397.0 1 IUCN 2010 International Union for Conservation of Nature... NaN NaN NaN NaN ... NaN 2012.1 ANIMALIA CHORDATA ACTINOPTERYGII PERCIFORMES POMACENTRIDAE Abudefduf troschelii LC <p>3 rows \u00d7 23 columns</p> In\u00a0[24]: Copied! <pre>damsel_gdf_dissolved.plot(cmap='Paired', figsize=(12,4))\n</pre> damsel_gdf_dissolved.plot(cmap='Paired', figsize=(12,4)) Out[24]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1eb3feb8&gt;</pre> In\u00a0[25]: Copied! <pre>damsel_gdf_dissolved.to_file('./data/damselfish/damselfish_dissolved.shp')\n</pre> damsel_gdf_dissolved.to_file('./data/damselfish/damselfish_dissolved.shp') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#geopandas-intro-plotting","title":"GeoPandas - intro, plotting\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#geometry-types","title":"Geometry types\u00b6","text":"<p>Geopandas uses <code>shapely.geometry</code> geometry objects. Geopandas has <code>6</code> types of geometry objects</p> <ul> <li>Point</li> <li>Line (LineString)</li> <li>Polygon</li> <li>Multi-Point</li> <li>Multi-Line</li> <li>Multi-Polygon</li> </ul>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#gotchas","title":"Gotchas\u00b6","text":"<ul> <li>Geopandas is a growing project and its API could change over time</li> <li>Geopandas does not restrict or check for consistency in geometry type of its series.</li> </ul>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#constructing-geoseries-from-shapely-points","title":"Constructing <code>GeoSeries</code> from Shapely <code>Point</code>s\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#plot-the-geometries","title":"Plot the geometries\u00b6","text":"<p>This internally uses <code>descartes</code> and <code>matplotlib</code></p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#constructing-geoseries-from-a-pandas-dataframe","title":"Constructing <code>GeoSeries</code> from a Pandas <code>DataFrame</code>\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#reading-a-shp-file","title":"Reading a SHP file\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#data-exploration-using-pandas","title":"Data exploration using pandas\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#plotting","title":"Plotting\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#overlay-usa-map","title":"Overlay USA map\u00b6","text":"<p>Read the bundled dataset and overlay it on top of this</p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#reprojection","title":"Reprojection\u00b6","text":"<p>Projecting the fire stations layer to match the world dataset from geopandas</p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#overlay-plot","title":"Overlay plot\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#filtering-and-writing-data","title":"Filtering and writing data\u00b6","text":"<p>In this part, we will read the <code>Damselfish</code> shape file, do some filtering, column calculations and split the shape file into many smaller ones grouped by a column.</p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#read-from-shape-file","title":"Read from Shape file\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#plot-a-map-by-category","title":"Plot a map by category\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#groupby-a-column","title":"Groupby a column\u00b6","text":"<p>The <code>BINOMIAL</code> column contains the full <code>genus</code> and <code>species</code> names of the fishes. We could split the shape file by each fish type</p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#split-by-column-value-and-write-to-disk","title":"Split by column value and write to disk\u00b6","text":"<p>Create output folder is not available</p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#dissolve-polygons","title":"Dissolve polygons\u00b6","text":"<p>An option is to dissolve each of the individual polygons into one for each fish. This way we can measure the area of each species. This is a proper <code>geoprocessing</code> implemented from <code>Shapely</code> lib.</p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#make-a-choropleth-map","title":"Make a choropleth map\u00b6","text":"<p>Since the dataframe is reshaped by applying the categorical column as the index, I just create a simple <code>choropleth</code> map. Simply specify a <code>cmap</code> and that does the trick</p>"},{"location":"teaching_resources/geopandas/geopandas_intro_plotting/#export-to-disk","title":"Export to disk\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/","title":"Geopandas - data classification, PySAL, OMS","text":"Table of Contents <ul><li>Data reclassification<ul><li>Visualize travel times on map</li></ul></li><li>Classify with Pysal<ul><li>Custom classifier</li></ul></li><li>Download from OSM<ul><li>Convert DiGraph to GeoDataFrame</li><li>Download buildings from OSM</li><li>Plot on map</li></ul></li></ul> In\u00a0[26]: Copied! <pre>import geopandas as gpd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import geopandas as gpd import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>helsinki_traveltimes = gpd.read_file('./data/travel_times/travel_times_overlay_helsinki.geojson')\nhelsinki_traveltimes.head(3)\n</pre> helsinki_traveltimes = gpd.read_file('./data/travel_times/travel_times_overlay_helsinki.geojson') helsinki_traveltimes.head(3) Out[2]: car_m_d car_m_t car_r_d car_r_t from_id pt_m_d pt_m_t pt_m_tt pt_r_d pt_r_t pt_r_tt to_id walk_d walk_t GML_ID NAMEFIN NAMESWE NATCODE geometry 0 15981 36 15988 41 6002702 14698 65 73 14698 61 72 5975375 14456 207 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349226 6667750.00004299, ... 1 16190 34 16197 39 6002701 14661 64 73 14661 60 72 5975375 14419 206 27517366 Helsinki Helsingfors 091 POLYGON ((390750.0001349644 6668000.000042951,... 2 15727 33 15733 37 6001132 14256 59 69 14256 55 62 5975375 14014 200 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349143 6668000.000042943,... <p>The <code>pt_r_tt</code> column contains time taken to reach city center. The <code>walk_d</code> column is the network distance to city center.</p> In\u00a0[3]: Copied! <pre># remove bad values, no data values\nhelsinki_traveltimes = helsinki_traveltimes[helsinki_traveltimes['pt_r_tt'] &gt; 0]\n</pre> # remove bad values, no data values helsinki_traveltimes = helsinki_traveltimes[helsinki_traveltimes['pt_r_tt'] &gt; 0] In\u00a0[4]: Copied! <pre>helsinki_traveltimes.plot(column='pt_r_tt', scheme='Fisher_Jenks',\n                         k=9, cmap='RdYlBu', linewidth=0)\n# scheme is the classification scheme\n# k=9 represents 9 classes in the scheme\n# cmap is Red, Yellow, Blue transitions\n\nplt.title('Travel times to city center')\n</pre> helsinki_traveltimes.plot(column='pt_r_tt', scheme='Fisher_Jenks',                          k=9, cmap='RdYlBu', linewidth=0) # scheme is the classification scheme # k=9 represents 9 classes in the scheme # cmap is Red, Yellow, Blue transitions  plt.title('Travel times to city center') Out[4]: <pre>Text(0.5,1,'Travel times to city center')</pre> In\u00a0[5]: Copied! <pre>helsinki_traveltimes.plot(column='walk_d', scheme='Fisher_Jenks',\n                         k=9, cmap='RdYlBu', linewidth=0)\n# scheme is the classification scheme\n# k=9 represents 9 classes in the scheme\n# cmap is Red, Yellow, Blue transitions\n\nplt.title('Network distance to city center')\nplt.tight_layout()\n</pre> helsinki_traveltimes.plot(column='walk_d', scheme='Fisher_Jenks',                          k=9, cmap='RdYlBu', linewidth=0) # scheme is the classification scheme # k=9 represents 9 classes in the scheme # cmap is Red, Yellow, Blue transitions  plt.title('Network distance to city center') plt.tight_layout() In\u00a0[6]: Copied! <pre>import pysal as ps\nn_classes = 9\n</pre> import pysal as ps n_classes = 9 <p>Create a classifier object that can be used with <code>apply()</code> method of the <code>DataFrame</code> object. There are two ways to classify this. One is to send data to Pysal and classify, the other is to get a classifier function and use that to iterate over the rows of the data frame. As you expect, we will do the latter.</p> <p>The <code>make()</code> method of the <code>Natural_Breaks</code> class is like a partial constructor. It accepts all parameters of the constructor other than the full data</p> In\u00a0[7]: Copied! <pre>nb_classifier = ps.esda.mapclassify.Natural_Breaks.make(k=n_classes)\ntype(nb_classifier)\n</pre> nb_classifier = ps.esda.mapclassify.Natural_Breaks.make(k=n_classes) type(nb_classifier) Out[7]: <pre>function</pre> In\u00a0[15]: Copied! <pre>classification = helsinki_traveltimes[['pt_r_tt']].apply(nb_classifier)\n</pre> classification = helsinki_traveltimes[['pt_r_tt']].apply(nb_classifier) In\u00a0[17]: Copied! <pre>classification.head()\n</pre> classification.head() Out[17]: pt_r_tt 0 7 1 7 2 6 3 7 4 7 <p>Thus we have classified the time to city center into <code>9</code> classes. The <code>Natural Breaks</code> classifier iterates over the whole data making <code>n_classes</code> groups such that the variability within groups is minimum and variability between groups is higher.</p> In\u00a0[19]: Copied! <pre># rename the column\nclassification.rename(columns={'pt_r_tt':'nb_pt_r_tt'}, inplace=True)\n</pre> # rename the column classification.rename(columns={'pt_r_tt':'nb_pt_r_tt'}, inplace=True) In\u00a0[21]: Copied! <pre>classification.head()\n</pre> classification.head() Out[21]: nb_pt_r_tt 0 7 1 7 2 6 3 7 4 7 <p>Join with original geodataframe</p> In\u00a0[24]: Copied! <pre>helsinki_traveltimes.join(classification, how='outer')\nhelsinki_traveltimes.head()\n</pre> helsinki_traveltimes.join(classification, how='outer') helsinki_traveltimes.head() Out[24]: car_m_d car_m_t car_r_d car_r_t from_id pt_m_d pt_m_t pt_m_tt pt_r_d pt_r_t pt_r_tt to_id walk_d walk_t GML_ID NAMEFIN NAMESWE NATCODE geometry 0 15981 36 15988 41 6002702 14698 65 73 14698 61 72 5975375 14456 207 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349226 6667750.00004299, ... 1 16190 34 16197 39 6002701 14661 64 73 14661 60 72 5975375 14419 206 27517366 Helsinki Helsingfors 091 POLYGON ((390750.0001349644 6668000.000042951,... 2 15727 33 15733 37 6001132 14256 59 69 14256 55 62 5975375 14014 200 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349143 6668000.000042943,... 3 15975 33 15982 37 6001131 14512 62 73 14512 58 70 5975375 14270 204 27517366 Helsinki Helsingfors 091 POLYGON ((390750.0001349644 6668000.000042951,... 4 16136 35 16143 40 6001138 14730 65 73 14730 61 72 5975375 14212 203 27517366 Helsinki Helsingfors 091 POLYGON ((392500.0001346234 6668000.000042901,... In\u00a0[25]: Copied! <pre>helsinki_traveltimes.columns\n</pre> helsinki_traveltimes.columns Out[25]: <pre>Index(['car_m_d', 'car_m_t', 'car_r_d', 'car_r_t', 'from_id', 'pt_m_d',\n       'pt_m_t', 'pt_m_tt', 'pt_r_d', 'pt_r_t', 'pt_r_tt', 'to_id', 'walk_d',\n       'walk_t', 'GML_ID', 'NAMEFIN', 'NAMESWE', 'NATCODE', 'geometry'],\n      dtype='object')</pre> In\u00a0[42]: Copied! <pre># find places that are within 20 mins of drive time, but at least 5 KM away from city\n# center\ndef binary_classifier(row):\n    if row['pt_r_tt'] &lt;= 20 and row['walk_d'] &gt;= 4000:\n        return 1\n    else:\n        return 0\n</pre> # find places that are within 20 mins of drive time, but at least 5 KM away from city # center def binary_classifier(row):     if row['pt_r_tt'] &lt;= 20 and row['walk_d'] &gt;= 4000:         return 1     else:         return 0 In\u00a0[43]: Copied! <pre># time this classification\n%time\n\n# apply the classifier\nhelsinki_traveltimes['suitable_grid'] = helsinki_traveltimes.apply(binary_classifier, axis=1)\n\n# see output\nhelsinki_traveltimes.head(5)\n</pre> # time this classification %time  # apply the classifier helsinki_traveltimes['suitable_grid'] = helsinki_traveltimes.apply(binary_classifier, axis=1)  # see output helsinki_traveltimes.head(5) <pre>CPU times: user 4 \u00b5s, sys: 1e+03 ns, total: 5 \u00b5s\nWall time: 8.82 \u00b5s\n</pre> Out[43]: car_m_d car_m_t car_r_d car_r_t from_id pt_m_d pt_m_t pt_m_tt pt_r_d pt_r_t pt_r_tt to_id walk_d walk_t GML_ID NAMEFIN NAMESWE NATCODE geometry suitable_grid 0 15981 36 15988 41 6002702 14698 65 73 14698 61 72 5975375 14456 207 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349226 6667750.00004299, ... 0 1 16190 34 16197 39 6002701 14661 64 73 14661 60 72 5975375 14419 206 27517366 Helsinki Helsingfors 091 POLYGON ((390750.0001349644 6668000.000042951,... 0 2 15727 33 15733 37 6001132 14256 59 69 14256 55 62 5975375 14014 200 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349143 6668000.000042943,... 0 3 15975 33 15982 37 6001131 14512 62 73 14512 58 70 5975375 14270 204 27517366 Helsinki Helsingfors 091 POLYGON ((390750.0001349644 6668000.000042951,... 0 4 16136 35 16143 40 6001138 14730 65 73 14730 61 72 5975375 14212 203 27517366 Helsinki Helsingfors 091 POLYGON ((392500.0001346234 6668000.000042901,... 0 In\u00a0[62]: Copied! <pre>helsinki_traveltimes.shape\n</pre> helsinki_traveltimes.shape Out[62]: <pre>(3816, 20)</pre> In\u00a0[44]: Copied! <pre>helsinki_traveltimes['suitable_grid'].value_counts()\n</pre> helsinki_traveltimes['suitable_grid'].value_counts() Out[44]: <pre>0    3794\n1      22\nName: suitable_grid, dtype: int64</pre> In\u00a0[45]: Copied! <pre>helsinki_traveltimes.plot(column='suitable_grid')\n</pre> helsinki_traveltimes.plot(column='suitable_grid') Out[45]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10e1cfe80&gt;</pre> In\u00a0[11]: Copied! <pre>import osmnx as ox\nchennai_graph = ox.graph_from_place('Chennai, Tamil Nadu, India')\ntype(chennai_graph)\n</pre> import osmnx as ox chennai_graph = ox.graph_from_place('Chennai, Tamil Nadu, India') type(chennai_graph) Out[11]: <pre>networkx.classes.multidigraph.MultiDiGraph</pre> In\u00a0[15]: Copied! <pre>ox.plot_graph(chennai_graph)\n</pre> ox.plot_graph(chennai_graph) Out[15]: <pre>(&lt;Figure size 216.459x432 with 1 Axes&gt;,\n &lt;matplotlib.axes._subplots.AxesSubplot at 0x1542a5ecf8&gt;)</pre> <p>Chennai is quite large and it takes a long time. Let us try an address. OSMnx will apply a bound box around it and get the network around it.</p> In\u00a0[17]: Copied! <pre>redlands_graph = ox.graph_from_address('380 New York St, Redlands, CA, USA')\n</pre> redlands_graph = ox.graph_from_address('380 New York St, Redlands, CA, USA') In\u00a0[18]: Copied! <pre>ox.plot_graph_folium(redlands_graph)\n</pre> ox.plot_graph_folium(redlands_graph) Out[18]: In\u00a0[20]: Copied! <pre>redlands_nodes, redlands_edges = ox.graph_to_gdfs(redlands_graph)\nredlands_nodes.head()\n</pre> redlands_nodes, redlands_edges = ox.graph_to_gdfs(redlands_graph) redlands_nodes.head() Out[20]: highway osmid ref x y geometry 54282058 traffic_signals 54282058 NaN -117.2 34.0646 POINT (-117.2002004 34.0646087) 54307215 NaN 54307215 NaN -117.201 34.0652 POINT (-117.2014769 34.0652388) 54310829 NaN 54310829 NaN -117.198 34.052 POINT (-117.1978832 34.0519842) 54310830 turning_loop 54310830 NaN -117.198 34.0511 POINT (-117.1979513 34.0511416) 54313777 motorway_junction 54313777 79 -117.191 34.0617 POINT (-117.1909685 34.0616596) <p>If you notice, the nodes are of <code>Point</code> geometry. I think these represent the intersections</p> In\u00a0[21]: Copied! <pre>redlands_edges.head()\n</pre> redlands_edges.head() Out[21]: access bridge geometry highway key lanes length maxspeed name oneway osmid ref service u v 0 NaN NaN LINESTRING (-117.200091 34.0592561, -117.19667... residential 0 NaN 315.100 NaN West Park Avenue False 7512595 NaN NaN 1235207168 2921241165 1 NaN NaN LINESTRING (-117.200091 34.0592561, -117.20073... residential 0 NaN 59.410 NaN West Park Avenue False 7512595 NaN NaN 1235207168 4496777852 2 NaN NaN LINESTRING (-117.200091 34.0592561, -117.20009... tertiary 0 [4, 5] 165.086 40 mph Tennessee Street False [237035432, 475113846] NaN NaN 1235207168 3844123278 3 NaN NaN LINESTRING (-117.200091 34.0592561, -117.20008... tertiary 0 [4, 5] 199.419 40 mph Tennessee Street False [475113850, 475113854] NaN NaN 1235207168 3843278118 4 NaN NaN LINESTRING (-117.1967853 34.0568395, -117.1967... footway 0 NaN 13.175 NaN NaN False 380911240 NaN NaN 1333960705 3842059569 <p>The <code>edges</code> are of <code>linestring</code> geometry and a host of information.</p> In\u00a0[23]: Copied! <pre>redlands_buildings = ox.buildings_from_address('380 New York St, Redlands, CA, USA',\n                                              distance=1000)\n</pre> redlands_buildings = ox.buildings_from_address('380 New York St, Redlands, CA, USA',                                               distance=1000) In\u00a0[24]: Copied! <pre>type(redlands_buildings)\n</pre> type(redlands_buildings) Out[24]: <pre>geopandas.geodataframe.GeoDataFrame</pre> In\u00a0[25]: Copied! <pre>redlands_buildings.head()\n</pre> redlands_buildings.head() Out[25]: abutters addr:city addr:country addr:housename addr:housenumber addr:postcode addr:state addr:street alt_name amenity ... shop smoking source source:address source_ref subject tourism website wikidata wikipedia 17005724 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN bing_imagery_0.06m_200706 NaN NaN NaN NaN NaN Q18393818 en:Redlands Mall 19787719 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 19787787 commercial NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 28147457 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN no NaN NaN NaN NaN NaN NaN NaN NaN 32194340 NaN Redlands US ESRI HQ 350 92373 NaN New York Street NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN <p>5 rows \u00d7 45 columns</p> In\u00a0[34]: Copied! <pre>fig, ax = plt.subplots(figsize=(12,12))\n\n# plot the streets, colored by the type of street\nredlands_edges.plot(ax=ax, column='highway', legend=True)\n\n# plot the buildings\nredlands_buildings.plot(ax=ax)\n</pre> fig, ax = plt.subplots(figsize=(12,12))  # plot the streets, colored by the type of street redlands_edges.plot(ax=ax, column='highway', legend=True)  # plot the buildings redlands_buildings.plot(ax=ax) Out[34]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1522ce02e8&gt;</pre>"},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#geopandas-data-classification-pysal-oms","title":"Geopandas - data classification, PySAL, OMS\u00b6","text":"Use Pysal to classify data, download vector data from Open Street Maps."},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#data-reclassification","title":"Data reclassification\u00b6","text":"<p>This is the process of grouping data into set intervals or classes. The class intervals can be manually set or a classification scheme (such as Natural Breaks (Jenks), Quantiles, equal interval..) can be used.</p>"},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#visualize-travel-times-on-map","title":"Visualize travel times on map\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#classify-with-pysal","title":"Classify with Pysal\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#custom-classifier","title":"Custom classifier\u00b6","text":"<p>We can define our own classifier as a function and do the rest as in regular pandas</p>"},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#download-from-osm","title":"Download from OSM\u00b6","text":"<p>Use OSMnx library to download vector, network data from open streets maps server.</p>"},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#convert-digraph-to-geodataframe","title":"Convert DiGraph to GeoDataFrame\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#download-buildings-from-osm","title":"Download buildings from OSM\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_reclass_pysal_osm/#plot-on-map","title":"Plot on map\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/","title":"Geopandas - spatial operations","text":"In\u00a0[1]: Copied! <pre>import geopandas as gpd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import geopandas as gpd import matplotlib.pyplot as plt %matplotlib inline In\u00a0[38]: Copied! <pre>from shapely.geometry import Point, Polygon\n\np1 = Point(24.952242, 60.1696017)\np2 = Point(24.976567, 60.1612500)\n\n# Create a Polygon\ncoords = [(24.950899, 60.169158), (24.953492, 60.169158), (24.953510, 60.170104), (24.950958, 60.169990)]\npoly1 = Polygon(coords)\n</pre> from shapely.geometry import Point, Polygon  p1 = Point(24.952242, 60.1696017) p2 = Point(24.976567, 60.1612500)  # Create a Polygon coords = [(24.950899, 60.169158), (24.953492, 60.169158), (24.953510, 60.170104), (24.950958, 60.169990)] poly1 = Polygon(coords) In\u00a0[41]: Copied! <pre>gdf_points = gpd.GeoDataFrame(geometry=[p1,p2])\ngdf_poly = gpd.GeoDataFrame(geometry=[poly1])\n</pre> gdf_points = gpd.GeoDataFrame(geometry=[p1,p2]) gdf_poly = gpd.GeoDataFrame(geometry=[poly1]) In\u00a0[42]: Copied! <pre>fig, ax1 = plt.subplots()\ngdf_poly.plot(ax=ax1)\ngdf_points.plot(ax = ax1)\n</pre> fig, ax1 = plt.subplots() gdf_poly.plot(ax=ax1) gdf_points.plot(ax = ax1) Out[42]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x11032d898&gt;</pre> <p>Use the <code>within()</code> and <code>contains()</code> spatial topology operators</p> In\u00a0[39]: Copied! <pre>p1.within(poly1)\n</pre> p1.within(poly1) Out[39]: <pre>True</pre> In\u00a0[43]: Copied! <pre>p2.within(poly1)\n</pre> p2.within(poly1) Out[43]: <pre>False</pre> In\u00a0[44]: Copied! <pre>poly1.contains(p1)\n</pre> poly1.contains(p1) Out[44]: <pre>True</pre> In\u00a0[50]: Copied! <pre>from shapely.geometry import LineString, MultiLineString\n\nline_a = LineString([(0, 0), (1, 1)])\nline_b = LineString([(1, 1), (0, 2)])\n\ngdf_lines = gpd.GeoDataFrame(geometry=[line_a, line_b])\ngdf_lines.plot(cmap='viridis')\n</pre> from shapely.geometry import LineString, MultiLineString  line_a = LineString([(0, 0), (1, 1)]) line_b = LineString([(1, 1), (0, 2)])  gdf_lines = gpd.GeoDataFrame(geometry=[line_a, line_b]) gdf_lines.plot(cmap='viridis') Out[50]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x11041aac8&gt;</pre> In\u00a0[51]: Copied! <pre>line_a.touches(line_b)\n</pre> line_a.touches(line_b) Out[51]: <pre>True</pre> In\u00a0[52]: Copied! <pre>line_a.intersects(line_b)\n</pre> line_a.intersects(line_b) Out[52]: <pre>True</pre> In\u00a0[13]: Copied! <pre>geocoded_address = gpd.read_file('./data/addresses_geocoded_3879.shp')\npop_poly = gpd.read_file('./data/population_grid/Vaestotietoruudukko_2015.shp')\n</pre> geocoded_address = gpd.read_file('./data/addresses_geocoded_3879.shp') pop_poly = gpd.read_file('./data/population_grid/Vaestotietoruudukko_2015.shp') In\u00a0[15]: Copied! <pre>fig3, ax3 = plt.subplots(1)\npop_poly.plot(ax=ax3)\ngeocoded_address.plot(ax=ax3, cmap = 'viridis')\n</pre> fig3, ax3 = plt.subplots(1) pop_poly.plot(ax=ax3) geocoded_address.plot(ax=ax3, cmap = 'viridis') Out[15]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd5cf98&gt;</pre> <p>Above you can see the polygons and the geocoded address points. Lets plot their dataframes to see the columns</p> In\u00a0[16]: Copied! <pre>geocoded_address.head()\n</pre> geocoded_address.head() Out[16]: address geometry 0 It\u00e4merenkatu 14, 00180, Helsinki POINT (25495255.82235641 6672275.260963614) 1 Kampinkuja 1, 00100, Helsinki POINT (25496127.70873243 6672843.287593854) 2 Kaivokatu 8, 00100, Helsinki POINT (25496750.13674767 6673047.518228112) 3 Hermanstads strandv\u00e4g 1, 00580, Helsingfors POINT (25498747.0563857 6674958.07898162) 4 It\u00e4v\u00e4yl\u00e4 900, 00890, Helsinki POINT (25508815.55214499 6681781.436864837) In\u00a0[17]: Copied! <pre>pop_poly.head()\n</pre> pop_poly.head() Out[17]: INDEX ASUKKAITA ASVALJYYS IKA0_9 IKA10_19 IKA20_29 IKA30_39 IKA40_49 IKA50_59 IKA60_69 IKA70_79 IKA_YLI80 geometry 0 688 8 31.0 99 99 99 99 99 99 99 99 99 POLYGON ((25472499.99532626 6689749.005069185,... 1 703 6 42.0 99 99 99 99 99 99 99 99 99 POLYGON ((25472499.99532626 6685998.998064222,... 2 710 8 44.0 99 99 99 99 99 99 99 99 99 POLYGON ((25472499.99532626 6684249.004130407,... 3 711 7 64.0 99 99 99 99 99 99 99 99 99 POLYGON ((25472499.99532626 6683999.004997005,... 4 715 19 23.0 99 99 99 99 99 99 99 99 99 POLYGON ((25472499.99532626 6682998.998461431,... <p>Rename a column</p> In\u00a0[18]: Copied! <pre>pop_poly.rename(columns={'ASUKKAITA':'pop15'}, inplace=True)\npop_poly.columns\n</pre> pop_poly.rename(columns={'ASUKKAITA':'pop15'}, inplace=True) pop_poly.columns Out[18]: <pre>Index(['INDEX', 'pop15', 'ASVALJYYS', 'IKA0_9', 'IKA10_19', 'IKA20_29',\n       'IKA30_39', 'IKA40_49', 'IKA50_59', 'IKA60_69', 'IKA70_79', 'IKA_YLI80',\n       'geometry'],\n      dtype='object')</pre> <p>Filter and retain just the columns that are necessary. Then assert the CRS is same before doing spatial join</p> In\u00a0[21]: Copied! <pre>pop_poly_filtered = pop_poly[['pop15', 'geometry']]\npop_poly_filtered.crs == geocoded_address.crs\n</pre> pop_poly_filtered = pop_poly[['pop15', 'geometry']] pop_poly_filtered.crs == geocoded_address.crs Out[21]: <pre>True</pre> <p>You specify the <code>spatial operation</code> when doing a join. Here the topology looks for <code>within</code>.</p> In\u00a0[22]: Copied! <pre>geocoded_address_joined = gpd.sjoin(left_df=geocoded_address,\n                                   right_df=pop_poly_filtered,\n                                   how=\"inner\", op='within')\ngeocoded_address_joined.head()\n</pre> geocoded_address_joined = gpd.sjoin(left_df=geocoded_address,                                    right_df=pop_poly_filtered,                                    how=\"inner\", op='within') geocoded_address_joined.head() Out[22]: address geometry index_right pop15 0 It\u00e4merenkatu 14, 00180, Helsinki POINT (25495255.82235641 6672275.260963614) 3214 521 1 Kampinkuja 1, 00100, Helsinki POINT (25496127.70873243 6672843.287593854) 3326 173 2 Kaivokatu 8, 00100, Helsinki POINT (25496750.13674767 6673047.518228112) 3449 31 11 Rautatientori 1, 00100, Helsinki POINT (25496939.39933316 6673189.887604699) 3449 31 4 It\u00e4v\u00e4yl\u00e4 900, 00890, Helsinki POINT (25508815.55214499 6681781.436864837) 5680 28 In\u00a0[23]: Copied! <pre>geocoded_address_joined.to_file('./data/address_geocoded_joined.shp')\n</pre> geocoded_address_joined.to_file('./data/address_geocoded_joined.shp') <p>Plot the data using classed colored renderer, based on the population column</p> In\u00a0[26]: Copied! <pre>geocoded_address_joined.plot(column='pop15', cmap='Reds',\n                            scheme='fisher_jenks', legend=True)\nplt.title(\"Number of people living in the address' district\")\n</pre> geocoded_address_joined.plot(column='pop15', cmap='Reds',                             scheme='fisher_jenks', legend=True) plt.title(\"Number of people living in the address' district\") Out[26]: <pre>Text(0.5,1,\"Number of people living in the address' district\")</pre> In\u00a0[2]: Copied! <pre>borders_gdf = gpd.read_file('./data/helsinki_borders/Helsinki_borders.shp')\ntravel_grid_gdf = gpd.read_file('./data/travel_times/TravelTimes_to_5975375_RailwayStation.shp')\n\nborders_gdf.head(3)\n</pre> borders_gdf = gpd.read_file('./data/helsinki_borders/Helsinki_borders.shp') travel_grid_gdf = gpd.read_file('./data/travel_times/TravelTimes_to_5975375_RailwayStation.shp')  borders_gdf.head(3) Out[2]: GML_ID NAMEFIN NAMESWE NATCODE geometry 0 27517366 Helsinki Helsingfors 091 POLYGON ((399936.363 6684600.244, 399937.63 66... In\u00a0[3]: Copied! <pre>travel_grid_gdf.head(3)\n</pre> travel_grid_gdf.head(3) Out[3]: car_m_d car_m_t car_r_d car_r_t from_id pt_m_d pt_m_t pt_m_tt pt_r_d pt_r_t pt_r_tt to_id walk_d walk_t geometry 0 32297 43 32260 48 5785640 32616 116 147 32616 108 139 5975375 32164 459 POLYGON ((382000.0001358641 6697750.000038058,... 1 32508 43 32471 49 5785641 32822 119 145 32822 111 133 5975375 29547 422 POLYGON ((382250.0001358146 6697750.000038053,... 2 30133 50 31872 56 5785642 32940 121 146 32940 113 133 5975375 29626 423 POLYGON ((382500.0001357661 6697750.000038046,... In\u00a0[4]: Copied! <pre>travel_grid_gdf.shape\n</pre> travel_grid_gdf.shape Out[4]: <pre>(13231, 15)</pre> In\u00a0[10]: Copied! <pre># plot the boundary as basemap layer\nbasemap_ax = borders_gdf.plot(color='white', edgecolor='blue', figsize=(8, 12))\n\n# do a class colored renderer on travel time by car column\ntravel_grid_gdf.plot(ax = basemap_ax, column='car_r_t', \n                     alpha=0.5, cmap='plasma')\n\nplt.tight_layout()\n</pre> # plot the boundary as basemap layer basemap_ax = borders_gdf.plot(color='white', edgecolor='blue', figsize=(8, 12))  # do a class colored renderer on travel time by car column travel_grid_gdf.plot(ax = basemap_ax, column='car_r_t',                       alpha=0.5, cmap='plasma')  plt.tight_layout() <p>The city of Helsinki is to the south. Thus we see the travel times increasing as we fan out. Now let us intersect the travel grid with basemap layer and select only the polygons that intersect</p> In\u00a0[13]: Copied! <pre>%%time\nintersect_result = gpd.overlay(travel_grid_gdf, borders_gdf, how='intersection')\n</pre> %%time intersect_result = gpd.overlay(travel_grid_gdf, borders_gdf, how='intersection') <pre>CPU times: user 1min 18s, sys: 318 ms, total: 1min 18s\nWall time: 1min 19s\n</pre> <p>This is an expensive operations and is time consuming.</p> In\u00a0[14]: Copied! <pre>intersect_result.head(4)\n</pre> intersect_result.head(4) Out[14]: car_m_d car_m_t car_r_d car_r_t from_id pt_m_d pt_m_t pt_m_tt pt_r_d pt_r_t pt_r_tt to_id walk_d walk_t GML_ID NAMEFIN NAMESWE NATCODE geometry 0 15981 36 15988 41 6002702 14698 65 73 14698 61 72 5975375 14456 207 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349226 6667750.00004299, ... 1 16190 34 16197 39 6002701 14661 64 73 14661 60 72 5975375 14419 206 27517366 Helsinki Helsingfors 091 POLYGON ((390750.0001349644 6668000.000042951,... 2 15727 33 15733 37 6001132 14256 59 69 14256 55 62 5975375 14014 200 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349143 6668000.000042943,... 3 15975 33 15982 37 6001131 14512 62 73 14512 58 70 5975375 14270 204 27517366 Helsinki Helsingfors 091 POLYGON ((390750.0001349644 6668000.000042951,... In\u00a0[15]: Copied! <pre>intersect_result.shape\n</pre> intersect_result.shape Out[15]: <pre>(3836, 19)</pre> <p>Thus the number of records / polygons has reduced from around <code>13,000</code> to around <code>3800</code>. Let us plot this to visualize the result</p> In\u00a0[16]: Copied! <pre>intersect_result.plot()\n</pre> intersect_result.plot() Out[16]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10dce8668&gt;</pre> In\u00a0[17]: Copied! <pre>intersect_result.to_file('./data/travel_times/travel_times_overlay_helsinki.geojson',\n                        driver='GeoJSON')\n</pre> intersect_result.to_file('./data/travel_times/travel_times_overlay_helsinki.geojson',                         driver='GeoJSON') In\u00a0[7]: Copied! <pre>intersect_result = gpd.read_file('./data/travel_times/travel_times_overlay_helsinki.geojson')\nintersect_result.head(3)\n</pre> intersect_result = gpd.read_file('./data/travel_times/travel_times_overlay_helsinki.geojson') intersect_result.head(3) Out[7]: car_m_d car_m_t car_r_d car_r_t from_id pt_m_d pt_m_t pt_m_tt pt_r_d pt_r_t pt_r_tt to_id walk_d walk_t GML_ID NAMEFIN NAMESWE NATCODE geometry 0 15981 36 15988 41 6002702 14698 65 73 14698 61 72 5975375 14456 207 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349226 6667750.00004299, ... 1 16190 34 16197 39 6002701 14661 64 73 14661 60 72 5975375 14419 206 27517366 Helsinki Helsingfors 091 POLYGON ((390750.0001349644 6668000.000042951,... 2 15727 33 15733 37 6001132 14256 59 69 14256 55 62 5975375 14014 200 27517366 Helsinki Helsingfors 091 POLYGON ((391000.0001349143 6668000.000042943,... In\u00a0[9]: Copied! <pre>intersect_result['car_r_t'].value_counts()\n</pre> intersect_result['car_r_t'].value_counts() Out[9]: <pre> 25    194\n 26    177\n 24    159\n 30    155\n 29    153\n 28    151\n 35    149\n 23    148\n 33    147\n 36    145\n 27    144\n 32    144\n 31    142\n 34    137\n 21    129\n 22    122\n 37    110\n 18    105\n 19    105\n 20    105\n 38     99\n 16     81\n 40     80\n 39     77\n 15     73\n 17     72\n 14     63\n 42     53\n 41     52\n 12     48\n 13     45\n 43     37\n 10     36\n 11     35\n 45     28\n 44     27\n 46     23\n 9      18\n-1      12\n 47     12\n 8      10\n 48      7\n 51      6\n 7       5\n 50      5\n 49      5\n 56      2\n 53      1\n 52      1\n 54      1\n 0       1\nName: car_r_t, dtype: int64</pre> In\u00a0[11]: Copied! <pre>result_aggregated = intersect_result.dissolve(by='car_r_t')\nresult_aggregated.shape\n</pre> result_aggregated = intersect_result.dissolve(by='car_r_t') result_aggregated.shape Out[11]: <pre>(51, 18)</pre> In\u00a0[12]: Copied! <pre>result_aggregated.head(3)\n</pre> result_aggregated.head(3) Out[12]: geometry car_m_d car_m_t car_r_d from_id pt_m_d pt_m_t pt_m_tt pt_r_d pt_r_t pt_r_tt to_id walk_d walk_t GML_ID NAMEFIN NAMESWE NATCODE car_r_t -1 (POLYGON ((388000.0001354737 6669000.000042855... -1 -1 -1 5996387 -1 -1 -1 -1 -1 -1 -1 -1 -1 27517366 Helsinki Helsingfors 091 0 POLYGON ((386000.0001357812 6672000.000042388,... 0 0 0 5975375 0 0 0 0 0 0 5975375 0 0 27517366 Helsinki Helsingfors 091 7 POLYGON ((386250.0001357396 6671750.000042424,... 1059 7 1059 5977007 447 6 6 447 6 6 5975375 447 6 27517366 Helsinki Helsingfors 091 In\u00a0[15]: Copied! <pre>result_aggregated.plot(cmap='plasma', figsize=(6,10), linewidth=0)\n</pre> result_aggregated.plot(cmap='plasma', figsize=(6,10), linewidth=0) Out[15]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10e37a6a0&gt;</pre> <p>Thus through aggregation, we have dissolved the number of features from <code>3800</code>s to <code>51</code>.</p> In\u00a0[18]: Copied! <pre>result_aggregated.to_file('./data/travel_times/helsinki_aggregated.geojson', \n                          driver='GeoJSON')\n</pre> result_aggregated.to_file('./data/travel_times/helsinki_aggregated.geojson',                            driver='GeoJSON')"},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/#geopandas-spatial-operations","title":"Geopandas - spatial operations\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/#spatial-overlays","title":"Spatial overlays\u00b6","text":"<p>Spatial overlays is the process of overlaying two or more layers on top of each other and performing operations based on how they overlay</p>"},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/#point-in-polygon-problem","title":"Point in polygon problem\u00b6","text":"<p><code>Geopandas</code> is capable of higher level spatial overlay operations, but we can use <code>shapely</code> to perform low level geometry <code>predicate</code> operations as below:</p>"},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/#topology-inspections","title":"Topology inspections\u00b6","text":""},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/#spatial-joins","title":"Spatial joins\u00b6","text":"<p>the <code>sjoin()</code> is a powerful method. Internally it does the overlay operations and performs a spatial join.</p>"},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/#spatial-intersections","title":"Spatial intersections\u00b6","text":"<p>So far we saw how to join features based on their location, below we see how to intersect two layers and create new geometries.</p> <p>Spatial joins preserve geometry and only get attributes. However, intersections and unions will create new (often smaller) geometries that include the characteristics of both the layers.</p>"},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/#save-as-geojson","title":"Save as GeoJSON\u00b6","text":"<p>The shape is similar to the shaded polygon in the previous plot. Let us save it to disk as a GeoJSON for future use.</p>"},{"location":"teaching_resources/geopandas/geopandas_spatial_overlays/#attribute-aggregation","title":"Attribute aggregation\u00b6","text":"<p>We can use regular pandas expressions to aggregate by attributes. For instance, we can aggregate all neighboring polygons grids that have the same travel time.</p>"},{"location":"teaching_resources/golang/","title":"Go lang - an introduction","text":""},{"location":"teaching_resources/golang/#history-of-go","title":"History of Go","text":"<p>Go was created in 2007 and open sourced in 2009 to take advantage of the changing landscape of server and cloud technology. Increasingly the servers on the cloud were becoming distributed and multi-threaded and a lot more capable. This meant, there was an increasing need for a simpler, higher-level language that was intended for back-ends on the cloud. Thus Go was designed to have a readable syntax as that of a dynamically typed language such as Python, but with the efficiency and speed of a lower-level statically typed language like C++.</p> <p>Today, Go is popular as a back-end language for cloud based, microservice based architecture. Its characteristics of being a compiled language and platform agnostic runtimes make it highly desirable.</p>"},{"location":"teaching_resources/golang/#setting-up-go","title":"Setting up Go","text":"<p>To install Go, run <code>brew install go</code>. Use <code>brew install go --dry-run</code> to check the version using a dry run.</p> <p>Next, use VSCode as the editor. Use the extensions tab to search and install the Go extension by Google. Install all other sub tools that it suggests.</p>"},{"location":"teaching_resources/golang/#quick-start","title":"Quick-start","text":"<p>Go programs are typically thought of as projects. This means, there is a set entry-point and a set of supporting files, each with its own business logic. Thus, your Go programs would be organized as such:</p> <pre><code>repo-root\n    .gitignore\n    readme.md\n    project-root\n        main.go   # compulsory\n        go.mod    # defines project packages and architecture.\n        file1.go    # files in same package\n        file2.go\n        pkg1        # user defined packages\n            pkg1file1.go    # files within a package\n            pkg1file2.go\n        pkg2\n            pkg2file1.go\n            pkg2file2.go\n        ...\n    build-dir  # for circle CI, github/gitlab CI etc.\n</code></pre>"},{"location":"teaching_resources/golang/#a-minimal-go-project","title":"A minimal Go project","text":"<p>At a minimum, a Go project can be a single file application. The snippet below shows the essential components of Go project.</p> <pre><code>package main // the name of your package\n\nimport \"fmt\" // the package you are importing - from std lib in this case\n\n// main() is the default entry point. Every go project needs an entry point.\nfunc main() {\n    fmt.Println(\"Hello world\")\n}\n</code></pre> <p>There can only be one <code>main()</code> in a Go project. To run this program, use:</p> <pre><code>$ go run main.go\nHello world\n$\n</code></pre>"},{"location":"teaching_resources/golang/#go-lang-conventions","title":"Go lang conventions","text":"<p>Every programming language and community has a conventional way of doing things. Let's what those are for Golang.</p> <ol> <li>Variable names use camelCasing convention.</li> <li>Declare variables, functions before actual usage. (Just as in an interpreted language)</li> </ol>"},{"location":"teaching_resources/golang/golang-basics/","title":"Go basics","text":""},{"location":"teaching_resources/golang/golang-basics/#creating-variables","title":"Creating variables","text":"<p>Create variables by prefixing with <code>var</code> keyword:</p> <pre><code>var variableName = &lt;value&gt;\n</code></pre> <p>Since a value is assigned to the variable in the same line, go infers the datatype automatically. In other cases, you need to specify the data type:</p> <pre><code>var userName string\nvar userTickets int\n\nuserName = \"ponniyinselvan\"\nuserTickets = 800\n</code></pre> <p>Similarly, you can create constants using the <code>const</code> keyword:</p> <pre><code>const constantName = &lt;value&gt;\n</code></pre> <p>In certain cases, you may want to override go's default datatypes. In such case, you can instantiate, assign value and still declare the type:</p> <pre><code>var remainingTickets uint = 50  //switch from default int to uint, to prevent negative values\n</code></pre>"},{"location":"teaching_resources/golang/golang-basics/#short-code-for-creating-and-assigning-vars","title":"Short code for creating and assigning vars","text":"<p>Go allows the <code>:=</code> short code to create and assign vars:</p> <pre><code>confName := \"go land conf\"\n</code></pre>"},{"location":"teaching_resources/golang/golang-basics/#scope-of-variables-and-functions","title":"Scope of variables (and functions)","text":"<p>In Go, there are 3 levels of scope:</p> <ul> <li>local: only available within the a specific block of code - such as for block, if block, a function</li> <li>package: available to all files within the same package</li> <li>global: available across packages. Created with Capitalized first letter. Available after importing the package that has the member defined The best practice is to define the variable as local as possible.</li> </ul> <pre><code>import (...)\nvar varName datatype = {} //values. Defining pacakge level variables\n\nfunc main(){\n\n}\n</code></pre>"},{"location":"teaching_resources/golang/golang-basics/#printing","title":"Printing","text":"<p>You can print statements to standard output using print functions that are available in the <code>fmt</code> module. This needs to be imported first.</p> <pre><code>import fmt\n\nfmt.Print(\"text here\")      //prints a single line. No new line at the end.\nfmt.Println(\"text here\")    //prints with a new line at the end\nfmt.Println(\"text\", varName, \"text\", varName, \"text\")   //mixing vars and text\nfmt.Printf(\"text %v text %v\", var1, var2)               //Using formated print output\n</code></pre>"},{"location":"teaching_resources/golang/golang-basics/#getting-user-input","title":"Getting user input","text":"<p>To get an input from user using std input, use <code>fmt.Scan()</code> function. You need to pass a placeholder for the value. For this you use pointers. A pointer in go is another variable that holds the address of a given variable.</p> <pre><code>var userName string\nfmt.Print(\"Enter your username: \")\nfmt.Scan(&amp;userName)  //user input is saved to userName var\n</code></pre>"},{"location":"teaching_resources/golang/golang-basics/#modularization","title":"Modularization","text":""},{"location":"teaching_resources/golang/golang-basics/#functions-in-go","title":"Functions in go","text":"<p>Functions are defined using <code>func</code> keyword, using the syntax: <code>func &lt;name&gt; {...}</code>. Values are returned using <code>return</code> keyword. In golang, you can return any number of values.</p> <pre><code>func &lt;funcname&gt; (arg1 datatype1, arg2 datatype2) return_datatype {\n    //statemetnts\n\n    return &lt;variable&gt;\n}\n</code></pre> <p>If returning multiple values, then:</p> <pre><code>func &lt;funcname&gt; (arg1 datatype1, arg2 datatype2) (returntype1, returntype2){\n    //statemnts\n    return value1, value2\n}\n</code></pre> <pre><code>func greetUsers(confName string, remianingTx uint) string{\n    return fmt.Printf(\"Welcome to %v .There are %v tix available\", confName, remainingTx)\n}\n\n//call the fn\ngreetUsers(\"goland conf\", 50)\n</code></pre>"},{"location":"teaching_resources/golang/golang-basics/#pass-by-value-vs-reference","title":"Pass by value vs reference","text":"<p>In Go, functions pass by value. To pass a var by reference do this:</p> <pre><code>func main(){\n    var myVal = make([]map[string]string, 0)\n    myVal[\"keuy\"] = \"value\"\n\n    someFunc(param1, param3, &amp;myVal)    // passes address of myVal\n}\n\nfunc someFunc(param1 type, param2 type, param3 *[]map[string]string){\n    //gets pointer to myVal\n}\n</code></pre>"},{"location":"teaching_resources/golang/golang-basics/#packages-in-go","title":"Packages in Go","text":"<p>Go apps can be organized using packages. A package is a collection of go files. The first line in a go file is usually the <code>package &lt;pkgname&gt;</code> which indicates which package the file belongs to. Files in the same package can share functions, package level variables etc without having to import the same package.</p> <pre><code>//main.go file\npackage main\nimport ()\n//statements\n\n\n//common.go file\npackage main\n//statements\n</code></pre> <p>Both <code>main.go</code> and <code>common.go</code> are now part of the <code>main</code> package.</p>"},{"location":"teaching_resources/golang/golang-basics/#multiple-packages","title":"Multiple packages","text":"<p>Larger applications often have multiple packages to organize code. By convention, each package has its own folder within the app's root folder as shown in Quickstart. To use code or variables from a different package, you need to import the package. However, for go to find the local, user-defined package, you need to import it by prefixing package with module name defined in <code>go.mod</code> file. </p> <p>Further, for go to find members from a different package, that package needs to be of global scope, for which it should it start with a Capital letter. This allows go to \"export\" that member to global scope. If you notice, the functions you use from even std packages such as <code>fmt</code> start with capitals, for instance <code>fmt.Println()</code>.</p> <pre><code>//go.mod file\nmodule booking-app\n\n// helper/helper.go file -- helper package\npackage helper\nimport \"fmt\"    //all regular imports\n\nfunc GreetUsers(){      //starts with Capital letter, so exported\n\n}\n\n// main.go file -- main package\npackage main\nimport (\n    \"fmt\"\n    \"strings\"\n    \"booking-app/helper\"    //module name (from go.mod file) / pkg name\n)\n\nfunc main(){\n    //statements\n    val = helper.GreetUsers()   //calls func from helper package.\n}\n</code></pre>"},{"location":"teaching_resources/golang/golang-control-flow/","title":"Go lang control flow - iteration and conditionals","text":"<p>In Go lang, loops are simplified. There is no <code>for</code>, <code>foreach</code>, <code>while</code>, <code>dowhile</code> etc. There is just a for loop.</p>"},{"location":"teaching_resources/golang/golang-control-flow/#for-loop","title":"<code>for</code> loop","text":"<p>A minimum and an infinite for loop. Ended by <code>break</code> statement:</p> <pre><code>for {\n    //statements\n    break\n}\n</code></pre> <p>Typical for loop pattern - follows C pattern:</p> <pre><code>for i=0; i&lt;count; i++{\n    //statements\n    //executes until i reaches count\n}\n</code></pre> <p>Conditional for loop (like a while loop):</p> <pre><code>for &lt;condition&gt;{\n    // executes until condition remains true\n}\n</code></pre> <p>Foreach style loop:</p> <pre><code>for index,element := range &lt;slice/array&gt; {\n    //statements\n    // range is an enumerator keyword\n    break\n}\n</code></pre>"},{"location":"teaching_resources/golang/golang-control-flow/#if-statements","title":"<code>if</code> statements","text":"<p>Comparison operators commonly used with <code>if</code>: <code>&lt;, &gt;, &lt;=, &gt;=, ==, !=</code>. Logical operators: <code>&amp;&amp;, ||, !</code> for AND, OR, NOT evaluations.</p> <pre><code>if &lt;condition&gt; {\n    //statements, executes if eval to True\n}\n\n//single condition\nif remainingTickets &lt; 0 {\n    //\n}\n\n// complex condition\nif (remainingTickets &lt; 0) &amp;&amp; (len(bookings) &gt; 10) {\n    // two evals using and operator\n}\n</code></pre>"},{"location":"teaching_resources/golang/golang-control-flow/#if-else-if-else-if-statements","title":"<code>if-else</code>, <code>if-else-if</code> statements","text":"<pre><code>if &lt;condition&gt; {\n\n}\nelse {\n\n}\n</code></pre> <pre><code>if &lt;condition&gt; {\n\n}\nelse if{\n\n}\nelse {\n\n}\n</code></pre>"},{"location":"teaching_resources/golang/golang-control-flow/#switch-statements","title":"Switch statements","text":"<pre><code>city := \"London\"\n\nswitch city {\n    case \"New York\":\n        //statements\n    case \"Mumbai\":\n        //statements\n    case \"London\":\n        //statements\n    case \"Los Angeles\", \"Pasadena\":  //common outcome\n        // statments for LA and Pas\n    default:\n        //statements\n}\n</code></pre>"},{"location":"teaching_resources/golang/golang-datatypes/","title":"Go Lang DataTypes","text":"<p>Go lang data types:</p> <ul> <li>Basic type: Numbers (integer variety, float variety, complex variety), strings, and booleans come under this category.</li> <li>Aggregate type: Array and structs come under this category.</li> <li>Reference type: Pointers, slices, maps, functions, and channels come under this category.</li> <li>Interface type</li> </ul>"},{"location":"teaching_resources/golang/golang-datatypes/#aggregate-types","title":"Aggregate types","text":""},{"location":"teaching_resources/golang/golang-datatypes/#arrays","title":"Arrays","text":"<p>Arrays can store multiple values of same datatype. Arrays must be declared by specifying type and size. Arrays are <code>0</code> indexed as in other languages.</p> <pre><code>var arrayName = [length]dataype{value1, value2}     // size is declared &amp; initialized, cannot expand\nvar arrayName = [...]datatype{values,values}        // size is expandable, length is inferred\nvar arrayName = [length]datatype{}     // if size is declared and initliazied empty\nvar arrayName [length]datatype         // declared, not initialized\n\n// can also use the short notation\narrayName := [&lt;length&gt;]datatype{values}\n\n// initialize specific indices\narr1 := [5]int{1:10,2:40}   // sets 2nd element to 10 and 3rd element to 40 = [0 10 40 0 0] \n</code></pre> <p>Array elements can be accessed using their index number:</p> <pre><code>var prices = [10]int{10,30,40,80}\n\nprices[2] = 100     //modify value\n</code></pre>"},{"location":"teaching_resources/golang/golang-datatypes/#strings","title":"Strings","text":"<p>String functions</p> <p>Contains</p>"},{"location":"teaching_resources/golang/golang-datatypes/#reference-types","title":"Reference types","text":""},{"location":"teaching_resources/golang/golang-datatypes/#slices","title":"Slices","text":"<p>Slice is an abstraction of an Array. It is more flexible and can grow as it is of variable length. It is also index based. You create a slice using <code>var sliceName []string</code> to create a slice of type String.</p> <pre><code>var sliceName []datatype\nsliceName = append(sliceName, value)    // to add values\n</code></pre>"},{"location":"teaching_resources/golang/golang-datatypes/#maps","title":"Maps","text":"<p>Maps are like dicts in Python. Maps are created using the syntax <code>var mapName = make(map[key-datatype]value-datatype)</code></p> <pre><code>var userData = make(map[string]string)     //use make to create empty map\n\n//populate values\nuserData[\"firstName\"] = \"adhitha\"\nuserData[\"lastName\"] = \"karikalan\"\n</code></pre> <p>Sometimes, you would like to create a list or slice of maps:</p> <pre><code>var userData = make(map[string][string])    //stores info about 1 user\n\n// create a slice of maps. The 2nd parameter is the initial size of the slice\n//slices grow dynamically. So the init size is just syntactical.\nvar allUsers = make([]map[string][string], 10)  //stores all users\n</code></pre>"},{"location":"teaching_resources/golang/golang-datatypes/#features-of-a-map","title":"Features of a map","text":"<ul> <li>Unlike dicts in Python, Maps can only store values that match the data type specified during declaration</li> </ul>"},{"location":"teaching_resources/golang/golang-datatypes/#structs","title":"Structs","text":"<p>Structs hold values of mixed or multiple data types. It is built on arrays and maps. Structs are actually a custom <code>type</code>. To create a struct:</p> <pre><code>type structName struct {    //use type keyword to create new struct type\n    key1 &lt;datatype&gt;\n    key2 &lt;datatype&gt;\n    key3 &lt;datatype&gt;\n}\n</code></pre> <p>for example you can define a <code>User</code> type as below:</p> <pre><code>type UserData struct {\n    firstName string\n    lastName string\n    address string\n    age uint\n    previousAddresses []string  //slice\n    flightsBooked []map[string]string   //slice of maps\n    mvp bool\n}\n</code></pre> <p>You can create an instance as shown below:</p> <pre><code>var phoenixUsers = make([]UserData, 0)  //use make to create a slice of UserData types\n\nvar u1 = UserData {\n    firstName: \"aditha\",\n    lastName: \"karikalan\",\n    address: \"chola kingdom\",\n    age: 40\n}\n\nphoenixUsers = append(phoenixUsers, u1)\n\n// to query the struct var, use . notation\nfmt.Println(u1.age)     // using . notation\n</code></pre>"},{"location":"teaching_resources/golang/goroutine/","title":"Goroutines - concurrency in Golang","text":"<p>Writing concurrent programs is fairly easy in Go lang. This is one of the highlights of Go lang over interpreted languages like Python.</p> <p>Goroutines in Go run on thread abstractions. These are not OS-level threads, instead are a higher-level abstraction of them. There can be multiple goroutines running in a single thread and Go takes care of their lifecycle. Using the abstractions allow faster creation and removal of concurrent code without the overhead of creating threads. Further, in other languages, inter-thread communication is a problem. However, in Go, you can create channels to communicate between goroutines.</p>"},{"location":"teaching_resources/golang/goroutine/#quickstart","title":"Quickstart","text":"<p>In go lang, you can easily run a function concurrently by simply prefixing <code>go</code> where you call the function.</p> <pre><code>func someSlowLogic() {\n    //statements\n}\n\nfunc main(){\n    someSlowLogic() // calls the func to run synchronously\n\n    go someSlowLogic()  // calls the same func in a separate thread.\n}\n</code></pre>"},{"location":"teaching_resources/golang/goroutine/#waitgroups","title":"WaitGroups","text":"<p>In the simple pattern, the main thread will not wait for worker thread is done. So, it is possible for the main to exit even before worker is done. To solve this we need to establish dependency of one thread to another. This is accomplished using <code>WaitGroup</code>s.</p> <pre><code>import \"sync\"   //sync pkg provides thread synchronization functionality\n\nvar wg = sync.WaitGroup{}   //this is a struct\n\nfunc main(){\n    //statements\n    wg.Add(1)   //number of goroutines to add. One for each func\n    go someFunc()   //func is added to wg WaitGroup\n\n    //statements\n    wg.Wait()   // waits for all goroutines to finish\n}\n\nfunc someFunc(){\n    //statements\n    //last line\n    wg.Done()       // indicates that a WG can be released.\n}\n</code></pre>"},{"location":"teaching_resources/matplotlib/matplotlib_geo/","title":"Geographical plotting with matplotlib","text":"In\u00a0[1]: Copied! <pre>%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\n</pre> %matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap In\u00a0[2]: Copied! <pre>plt.figure(figsize=(8,8))\nm = Basemap(projection='ortho', resolution=None, lat_0=35, lon_0=-100)\nm.bluemarble(scale=0.6);\n</pre> plt.figure(figsize=(8,8)) m = Basemap(projection='ortho', resolution=None, lat_0=35, lon_0=-100) m.bluemarble(scale=0.6); <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> In\u00a0[10]: Copied! <pre>type(m)\n</pre> type(m) Out[10]: <pre>mpl_toolkits.basemap.Basemap</pre> <p>Let us set the view to India, change figure size and the resolution..</p> In\u00a0[11]: Copied! <pre>plt.figure(figsize=(6,10))\nm = Basemap(projection='ortho', resolution=None, lat_0=20.59, lon_0=78.96)\nm.bluemarble(scale=0.5);\n</pre> plt.figure(figsize=(6,10)) m = Basemap(projection='ortho', resolution=None, lat_0=20.59, lon_0=78.96) m.bluemarble(scale=0.5); <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> In\u00a0[4]: Copied! <pre>type(m)\n</pre> type(m) Out[4]: <pre>mpl_toolkits.basemap.Basemap</pre> In\u00a0[5]: Copied! <pre>fig = plt.figure(figsize=(8, 8))\nm2 = Basemap(projection='lcc', resolution=None,\n            width=8E6, height=8E6, \n            lat_0=45, lon_0=-100,)\nm2.etopo(scale=0.5, alpha=0.5)\n\n# Map (long, lat) to (x, y) for plotting\nx, y = m2(-122.3, 47.6)\nplt.plot(x, y, 'ok', markersize=5)\nplt.text(x, y, ' Seattle', fontsize=12);\n</pre> fig = plt.figure(figsize=(8, 8)) m2 = Basemap(projection='lcc', resolution=None,             width=8E6, height=8E6,              lat_0=45, lon_0=-100,) m2.etopo(scale=0.5, alpha=0.5)  # Map (long, lat) to (x, y) for plotting x, y = m2(-122.3, 47.6) plt.plot(x, y, 'ok', markersize=5) plt.text(x, y, ' Seattle', fontsize=12); <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> <p>You can even save the map to an image file.</p> In\u00a0[13]: Copied! <pre>fig.savefig('./seattle.png')\nplt.imshow(plt.imread('./seattle.png'))\n</pre> fig.savefig('./seattle.png') plt.imshow(plt.imread('./seattle.png')) Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x7ff67f7b6358&gt;</pre> In\u00a0[9]: Copied! <pre>m2()\n</pre> m2() Out[9]: <pre>mpl_toolkits.basemap.Basemap</pre> In\u00a0[7]: Copied! <pre>y\n</pre> y Out[7]: <pre>4518079.266407734</pre> In\u00a0[8]: Copied! <pre>type(x)\n</pre> type(x) Out[8]: <pre>float</pre> In\u00a0[9]: Copied! <pre>%%time\nfig = plt.figure(figsize=(8, 8))\nm2 = Basemap(projection='lcc', resolution='f',\n            width=4E5, height=4E5, \n            lat_0=47, lon_0=-123,)\nm2.etopo(scale=1, alpha=0.5)\nm2.drawrivers()\nm2.drawstates()\nm2.drawcounties();\n# Map (long, lat) to (x, y) for plotting\n# x, y = m2(-122.3, 47.6)\n# plt.plot(x, y, 'ok', markersize=5)\n# plt.text(x, y, ' Seattle', fontsize=12);\n</pre> %%time fig = plt.figure(figsize=(8, 8)) m2 = Basemap(projection='lcc', resolution='f',             width=4E5, height=4E5,              lat_0=47, lon_0=-123,) m2.etopo(scale=1, alpha=0.5) m2.drawrivers() m2.drawstates() m2.drawcounties(); # Map (long, lat) to (x, y) for plotting # x, y = m2(-122.3, 47.6) # plt.plot(x, y, 'ok', markersize=5) # plt.text(x, y, ' Seattle', fontsize=12); <pre>CPU times: user 18.3 s, sys: 1.16 s, total: 19.5 s\nWall time: 19.5 s\n</pre> Out[9]: <pre>&lt;matplotlib.collections.PolyCollection at 0x7fde5cc98d30&gt;</pre> In\u00a0[13]: Copied! <pre>fig = plt.figure(figsize=(8, 8))\nm3 = Basemap(projection='cyl', resolution=None, \n            lat_0=47, lon_0=-123,)\nm3.etopo(scale=1, alpha=0.5)\n</pre> fig = plt.figure(figsize=(8, 8)) m3 = Basemap(projection='cyl', resolution=None,              lat_0=47, lon_0=-123,) m3.etopo(scale=1, alpha=0.5) Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x7fde714675c0&gt;</pre> In\u00a0[12]: Copied! <pre># https://services.arcgisonline.com/ArcGIS/rest/services/NatGeo_World_Map/MapServer\nm3.arcgisimage(service='NatGeo_World_Map')\n</pre> # https://services.arcgisonline.com/ArcGIS/rest/services/NatGeo_World_Map/MapServer m3.arcgisimage(service='NatGeo_World_Map') <pre>/Users/atma6951/anaconda3/envs/basemap/lib/python3.6/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  return _prepare_from_string(\" \".join(pjargs))\n/Users/atma6951/anaconda3/envs/basemap/lib/python3.6/site-packages/pyproj/crs/crs.py:294: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  projstring = _prepare_from_string(\" \".join((projstring, projkwargs)))\n</pre> Out[12]: <pre>&lt;matplotlib.image.AxesImage at 0x7fde5fc1c6a0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/matplotlib/matplotlib_geo/#geographical-plotting-with-matplotlib","title":"Geographical plotting with <code>matplotlib</code>\u00b6","text":"<p>To get the features, install the <code>basemap</code> package using</p> <ul> <li><code>conda install basemap</code> and the high res dataset with</li> <li><code>conda install -c conda-forge basemap-data-hires</code>.</li> </ul> <p>I ran into multiple conflicts trying to install <code>basemap</code> in an environment with <code>geopandas</code>. I was successful in creating a new Python env and installing basemap first.</p> <p>Thanks to https://jakevdp.github.io/PythonDataScienceHandbook/04.13-geographic-data-with-basemap.html for this tutorial.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_geo/#common-steps-in-plotting-with-basemap-package","title":"Common steps in plotting with <code>basemap</code> package\u00b6","text":"<ol> <li>First you set the figure size and other properties of the matplotlib figure</li> <li>Next you set the projection using the <code>Basemap</code> constructor. You pass the name of the projection from the choice list, a resolution of the built-in dataset to use. Higher res is slower to draw. You pass either the coordinates of lower left &amp; upper right, or coordinates of center. This gives you a <code>Basemap</code> object which is like an axis. If you are doing a subplot, you can bind it to an existing axes.</li> <li>On the <code>Basemap</code> object, you call the appropriate basemap type - such as <code>etopo()</code>, <code>bluemarble()</code>, <code>shadedrelief</code> etc. and pass appropriate parameters like scale, alpha etc. Scale is not map scale or tiling level, instead it is the factor to scale up or down the rendered image. The default resolution is <code>5400x2700</code> which is computational intensive. You can pass any additional <code>imshow</code> parameters to this call.</li> <li>If you want to overlay any vectors, you call the <code>basemap</code> object like a function and pass the coordinates in lat, lon. The function will return in the appropriate projection system you used to create the object.</li> <li>Then you use the regular <code>plt.plot(x,y, symbol_spec)</code> syntax to plot those points on the map and <code>plt.text(x,y, text_spec)</code> for labels.</li> </ol>"},{"location":"teaching_resources/matplotlib/matplotlib_geo/#plot-a-sphere","title":"Plot a sphere\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_geo/#plot-and-overlay-annotation","title":"Plot and overlay annotation\u00b6","text":"<p>Plot with a different basemap, using Lambert Conformal Conic projection</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/","title":"Matplotlib - intro, subplots","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt %matplotlib inline <p>Let us create some data for X and Y for the plots</p> In\u00a0[12]: Copied! <pre>x = list(range(0,100))\ny = list(map(lambda x:x**2, x))\n</pre> x = list(range(0,100)) y = list(map(lambda x:x**2, x)) In\u00a0[14]: Copied! <pre>plt.plot(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('x vs y')\n</pre> plt.plot(x,y) plt.xlabel('x') plt.ylabel('y') plt.title('x vs y') Out[14]: <pre>&lt;matplotlib.text.Text at 0x10e2918d0&gt;</pre> In\u00a0[15]: Copied! <pre>plt.subplot(1,2,1) #one row, 2 cols, 1st plot:\nplt.plot(x,y)\n\nplt.subplot(1,2,2) #the second plot:\nplt.plot(y,x)\n</pre> plt.subplot(1,2,1) #one row, 2 cols, 1st plot: plt.plot(x,y)  plt.subplot(1,2,2) #the second plot: plt.plot(y,x) Out[15]: <pre>[&lt;matplotlib.lines.Line2D at 0x1108489b0&gt;]</pre> In\u00a0[23]: Copied! <pre>fig = plt.figure()\nax = fig.add_axes([0.1,0.1,0.8,0.8]) #rectangle's [left, bottom, width, height]\nax.plot(x,y)\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('x vs y')\n</pre> fig = plt.figure() ax = fig.add_axes([0.1,0.1,0.8,0.8]) #rectangle's [left, bottom, width, height] ax.plot(x,y)  ax.set_xlabel('x') ax.set_ylabel('y') ax.set_title('x vs y') Out[23]: <pre>&lt;matplotlib.text.Text at 0x111098940&gt;</pre> In\u00a0[25]: Copied! <pre>fig2 = plt.figure()\nax1 = fig2.add_axes([0.1,0.1,0.8,0.8])\n\nax_ins = fig2.add_axes([0.2,0.5,0.3,0.3]) #insert in upper left side of plot\n</pre> fig2 = plt.figure() ax1 = fig2.add_axes([0.1,0.1,0.8,0.8])  ax_ins = fig2.add_axes([0.2,0.5,0.3,0.3]) #insert in upper left side of plot <p>Consider the rectangle has max length and width = 1. You can create the first plot as big as you want filling this canvas. Then create the second sort of independent of first, using the same canvas coordinates.</p> In\u00a0[26]: Copied! <pre>fig3 = plt.figure()\nax1 = fig3.add_axes([0,0,1,1]) #absolute - full size fig\n\nax_ins = fig3.add_axes([0.5,0.1,0.4,0.4]) #insert in lower right side of plot\n</pre> fig3 = plt.figure() ax1 = fig3.add_axes([0,0,1,1]) #absolute - full size fig  ax_ins = fig3.add_axes([0.5,0.1,0.4,0.4]) #insert in lower right side of plot In\u00a0[27]: Copied! <pre>fig3 = plt.figure()\nax1 = fig3.add_axes([0,0,0.4,1]) # about half in width, full height\n\nax2 = fig3.add_axes([0.5,0,0.4,1]) #same, but to the right\n</pre> fig3 = plt.figure() ax1 = fig3.add_axes([0,0,0.4,1]) # about half in width, full height  ax2 = fig3.add_axes([0.5,0,0.4,1]) #same, but to the right In\u00a0[30]: Copied! <pre>fig, axes = plt.subplots(nrows=1,ncols=2)\n\naxes[0].plot(x,y)\naxes[0].set_title('x vs y')\n\naxes[1].plot(x,x)\naxes[1].set_title('x vs x')\n</pre> fig, axes = plt.subplots(nrows=1,ncols=2)  axes[0].plot(x,y) axes[0].set_title('x vs y')  axes[1].plot(x,x) axes[1].set_title('x vs x') Out[30]: <pre>&lt;matplotlib.text.Text at 0x111997be0&gt;</pre> In\u00a0[33]: Copied! <pre>fig, axes = plt.subplots(nrows=1,ncols=2, figsize=(5,5)) #specifying as **kwargs\naxes[0].plot(x,y)\naxes[1].plot(x,x)\n\n#use tight layout to resize plots within the canvas so there is no overlaps\nfig.tight_layout()\n</pre> fig, axes = plt.subplots(nrows=1,ncols=2, figsize=(5,5)) #specifying as **kwargs axes[0].plot(x,y) axes[1].plot(x,x)  #use tight layout to resize plots within the canvas so there is no overlaps fig.tight_layout() In\u00a0[34]: Copied! <pre>fig.savefig('my_plots.png', dpi=300)\n</pre> fig.savefig('my_plots.png', dpi=300) In\u00a0[38]: Copied! <pre>fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(5,3)) #just 1 large plot\naxes.plot(x,y, label='x vs x^2')\naxes.plot(x,x, label ='a straight line')\naxes.legend(loc=0) #loc=0 corresponds to best position available.\n\n#use tight layout to resize plots within the canvas so there is no overlaps\nfig.tight_layout()\n</pre> fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(5,3)) #just 1 large plot axes.plot(x,y, label='x vs x^2') axes.plot(x,x, label ='a straight line') axes.legend(loc=0) #loc=0 corresponds to best position available.  #use tight layout to resize plots within the canvas so there is no overlaps fig.tight_layout()  <p>Note : <code>fig.suplots()</code> does not always return a vector <code>axis</code> object array. As shown above, if you have just 1 plot, it has only 1 axis object.</p> In\u00a0[51]: Copied! <pre>#linewidth or lw - ranges from 1 (default) to any high up\n#colors - takes names and HTML notations\n#alpha - is for transparency and ranges from [0-1]\n#marker - for tick marks and specify in characters\n#markersize\n#markerfacecolor\n#markeredgecolor\n#markeredgewidth\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nst_line = list(range(0,10000, 100))\n\nax.plot(x,y,color='orange', linewidth='3', alpha=0.3, marker='*',\n       markersize=4, markerfacecolor='green', label='x vs y')\n\nax.plot(x,st_line, color='green', marker='o', markersize=10, markerfacecolor='green',\n       label='straight line')\nax.legend()\n</pre> #linewidth or lw - ranges from 1 (default) to any high up #colors - takes names and HTML notations #alpha - is for transparency and ranges from [0-1] #marker - for tick marks and specify in characters #markersize #markerfacecolor #markeredgecolor #markeredgewidth  fig = plt.figure() ax = fig.add_axes([0,0,1,1]) st_line = list(range(0,10000, 100))  ax.plot(x,y,color='orange', linewidth='3', alpha=0.3, marker='*',        markersize=4, markerfacecolor='green', label='x vs y')  ax.plot(x,st_line, color='green', marker='o', markersize=10, markerfacecolor='green',        label='straight line') ax.legend() Out[51]: <pre>&lt;matplotlib.legend.Legend at 0x113249b00&gt;</pre> In\u00a0[54]: Copied! <pre>fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nst_line = list(range(0,10000, 100))\n\nax.plot(x,y,color='orange', linewidth='3', alpha=0.3, marker='*',\n       markersize=4, markerfacecolor='green', label='x vs y')\n\nax.plot(x,st_line, color='green', marker='o', markersize=10, markerfacecolor='green',\n       label='straight line')\nax.legend()\n\nax.set_xlim(0,20)\nax.set_ylim(0,3000)\n</pre> fig = plt.figure() ax = fig.add_axes([0,0,1,1]) st_line = list(range(0,10000, 100))  ax.plot(x,y,color='orange', linewidth='3', alpha=0.3, marker='*',        markersize=4, markerfacecolor='green', label='x vs y')  ax.plot(x,st_line, color='green', marker='o', markersize=10, markerfacecolor='green',        label='straight line') ax.legend()  ax.set_xlim(0,20) ax.set_ylim(0,3000) Out[54]: <pre>(0, 3000)</pre> In\u00a0[3]: Copied! <pre>values = [400, 280, 10]\nlabels = ['apple', 'android', 'windows']\n\n# you can just call plt.pie. However it prints a bunch of objs on the notebook. I do this just to suppress that.\nfix, ax1 = plt.subplots()\n\n# you get the returns from ax1.pie because the font is tiny and to make it bigger\n# the autopct is to get the percentage values.\n_, texts, autotexts = ax1.pie(values, labels=labels, shadow=True, autopct='%1.1f%%')\n\n# make the font bigger by calling set_fontsize method each obj in texts, autotexts\nlist(map(lambda x:x.set_fontsize(15), texts))\nlist(map(lambda x:x.set_fontsize(15), autotexts))\n\n# by default the pie has a perspective. so you here you make it flat\nax1.axis('equal')\nax1.set_title('Cell phone OS by popularity', fontsize=15)\n</pre> values = [400, 280, 10] labels = ['apple', 'android', 'windows']  # you can just call plt.pie. However it prints a bunch of objs on the notebook. I do this just to suppress that. fix, ax1 = plt.subplots()  # you get the returns from ax1.pie because the font is tiny and to make it bigger # the autopct is to get the percentage values. _, texts, autotexts = ax1.pie(values, labels=labels, shadow=True, autopct='%1.1f%%')  # make the font bigger by calling set_fontsize method each obj in texts, autotexts list(map(lambda x:x.set_fontsize(15), texts)) list(map(lambda x:x.set_fontsize(15), autotexts))  # by default the pie has a perspective. so you here you make it flat ax1.axis('equal') ax1.set_title('Cell phone OS by popularity', fontsize=15) Out[3]: <pre>&lt;matplotlib.text.Text at 0x11843d390&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#matplotlib-intro-subplots","title":"Matplotlib - intro, subplots\u00b6","text":"<p>Matplotlib is a popular open source 2D plotting library for Python, modeled after Matlab. There are two approaches of using this - functional approach &amp; objected-oriented approach. The latter is preferred.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#importing","title":"importing\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#functional-plotting","title":"Functional plotting\u00b6","text":"<p>Call the plotting as functions.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#subplots","title":"subplots\u00b6","text":"<p>Use <code>subplot</code> method and specify <code>nrows</code>, <code>ncols</code>, <code>plot number</code> as arguments. Thus specify (1,2,1) for two plots side by side, (2,2,1) for four plots like quadrants.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#object-oriented-plotting","title":"Object oriented plotting\u00b6","text":"<p>Here we create a <code>figure</code> object, set <code>axes</code>, then add plots to it.</p> <p>Specify the axes as a list of a rectangle's <code>[left, bottom, width, height]</code>. The values always range from <code>0-1</code>.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#multiplots","title":"multiplots\u00b6","text":"<p>You can create multiple plots, subplots, insert plots easily in OO approach once the axes is defined.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#creating-side-by-side-plots","title":"Creating side-by-side plots\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#easier-subplots","title":"Easier subplots\u00b6","text":"<p>If you are going to be doing side-by-side subplots, then use the easier API as shown below. Matplotlib will auto arrange the axes and plots for you.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#figsize","title":"Figsize\u00b6","text":"<p>Specify the figure size of the plots. You specify this in <code>inches</code>.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#saving-your-plots","title":"Saving your plots\u00b6","text":"<p>call the <code>savefig()</code> method of <code>figure</code> object</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#labels-and-legends","title":"Labels and legends\u00b6","text":"<p>You can add legend to the <code>axis</code> object. You first populate the <code>label</code> property of the plot for any text to show up in the legend as shown below:</p> <p>When inserting a legend, you can specify <code>loc=0</code> for auto positioning. Value <code>loc=1</code> is upper right, <code>loc=2</code> is upper left and so on.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#decorating-with-colors-markers-transparency","title":"Decorating with colors markers transparency\u00b6","text":"<p>You can go town here and do the full customization, however it is adviced to use a higher level plotting API like seaborn if you find yourself writing a lot of styling code. The <code>plot()</code> method accepts a lot of these arguments</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#set-limits-on-axes","title":"Set limits on axes\u00b6","text":"<p>In the chart above, if you want to zoom and only show the chart for values from 0-20 on X you can do so by limiting the axes. You can also set it such that the axes extends beyond the range of your data</p>"},{"location":"teaching_resources/matplotlib/matplotlib_intro_subplots/#pie-charts","title":"pie charts\u00b6","text":"<p>You need to send values for pie charts as numbers. You cannot pass a text column and expect matplotlib to count values and make a pie out of it.</p>"},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/","title":"Matplotlib - working with axes","text":"<p>It is also possible to set a logarithmic scale for one or both axes. This functionality is in fact only one application of a more general transformation system in Matplotlib. Each of the axes' scales are set seperately using <code>set_xscale</code> and <code>set_yscale</code> methods which accept one parameter (with the value \"log\" in this case):</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt import numpy as np %matplotlib inline In\u00a0[2]: Copied! <pre>x = np.linspace(0, 5, 11)\ny = x**2\n</pre> x = np.linspace(0, 5, 11) y = x**2 In\u00a0[3]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(10,4))\n      \naxes[0].plot(x, x**2, x, np.exp(x))\naxes[0].set_title(\"Normal scale\")\n\naxes[1].plot(x, x**2, x, np.exp(x))\naxes[1].set_yscale(\"log\") #the log transformation\naxes[1].set_title(\"Logarithmic scale (y)\");\n</pre> fig, axes = plt.subplots(1, 2, figsize=(10,4))        axes[0].plot(x, x**2, x, np.exp(x)) axes[0].set_title(\"Normal scale\")  axes[1].plot(x, x**2, x, np.exp(x)) axes[1].set_yscale(\"log\") #the log transformation axes[1].set_title(\"Logarithmic scale (y)\"); <p>We can explicitly determine where we want the axis ticks with <code>set_xticks</code> and <code>set_yticks</code>, which both take a list of values for where on the axis the ticks are to be placed. We can also use the <code>set_xticklabels</code> and <code>set_yticklabels</code> methods to provide a list of custom text labels for each tick location:</p> In\u00a0[4]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 4))\n\nax.plot(x, x**2, x, x**3, lw=2)\n\nax.set_xticks([1, 2, 3, 4, 5])\nax.set_xticklabels([r'$\\alpha$', r'$\\beta$', r'$\\gamma$', r'$\\delta$', r'$\\epsilon$'], \n                   fontsize=18) #tex code\n\nyticks = [0, 50, 100, 150]\nax.set_yticks(yticks)\nax.set_yticklabels([\"$%.1f$\" % y for y in yticks], fontsize=18); # use LaTeX formatted labels\n</pre> fig, ax = plt.subplots(figsize=(10, 4))  ax.plot(x, x**2, x, x**3, lw=2)  ax.set_xticks([1, 2, 3, 4, 5]) ax.set_xticklabels([r'$\\alpha$', r'$\\beta$', r'$\\gamma$', r'$\\delta$', r'$\\epsilon$'],                     fontsize=18) #tex code  yticks = [0, 50, 100, 150] ax.set_yticks(yticks) ax.set_yticklabels([\"$%.1f$\" % y for y in yticks], fontsize=18); # use LaTeX formatted labels <p>There are a number of more advanced methods for controlling major and minor tick placement in matplotlib figures, such as automatic placement according to different policies. See http://matplotlib.org/api/ticker_api.html for details.</p> <p>With large numbers on axes, it is often better use scientific notation:</p> In\u00a0[5]: Copied! <pre>fig, ax = plt.subplots(1, 1)\n      \nax.plot(x, x**2, x, np.exp(x))\nax.set_title(\"scientific notation\")\n\nax.set_yticks([0, 50, 100, 150])\n\nfrom matplotlib import ticker\nformatter = ticker.ScalarFormatter(useMathText=True)\nformatter.set_scientific(True) \nformatter.set_powerlimits((-1,1)) \nax.yaxis.set_major_formatter(formatter)\n</pre> fig, ax = plt.subplots(1, 1)        ax.plot(x, x**2, x, np.exp(x)) ax.set_title(\"scientific notation\")  ax.set_yticks([0, 50, 100, 150])  from matplotlib import ticker formatter = ticker.ScalarFormatter(useMathText=True) formatter.set_scientific(True)  formatter.set_powerlimits((-1,1))  ax.yaxis.set_major_formatter(formatter)  In\u00a0[97]: Copied! <pre># distance between x and y axis and the numbers on the axes\nmatplotlib.rcParams['xtick.major.pad'] = 5\nmatplotlib.rcParams['ytick.major.pad'] = 5\n\nfig, ax = plt.subplots(1, 1)\n      \nax.plot(x, x**2, x, np.exp(x))\nax.set_yticks([0, 50, 100, 150])\n\nax.set_title(\"label and axis spacing\")\n\n# padding between axis label and axis numbers\nax.xaxis.labelpad = 5\nax.yaxis.labelpad = 5\n\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\");\n</pre> # distance between x and y axis and the numbers on the axes matplotlib.rcParams['xtick.major.pad'] = 5 matplotlib.rcParams['ytick.major.pad'] = 5  fig, ax = plt.subplots(1, 1)        ax.plot(x, x**2, x, np.exp(x)) ax.set_yticks([0, 50, 100, 150])  ax.set_title(\"label and axis spacing\")  # padding between axis label and axis numbers ax.xaxis.labelpad = 5 ax.yaxis.labelpad = 5  ax.set_xlabel(\"x\") ax.set_ylabel(\"y\"); In\u00a0[98]: Copied! <pre># restore defaults\nmatplotlib.rcParams['xtick.major.pad'] = 3\nmatplotlib.rcParams['ytick.major.pad'] = 3\n</pre> # restore defaults matplotlib.rcParams['xtick.major.pad'] = 3 matplotlib.rcParams['ytick.major.pad'] = 3 <p>Unfortunately, when saving figures the labels are sometimes clipped, and it can be necessary to adjust the positions of axes a little bit. This can be done using <code>subplots_adjust</code>:</p> In\u00a0[99]: Copied! <pre>fig, ax = plt.subplots(1, 1)\n      \nax.plot(x, x**2, x, np.exp(x))\nax.set_yticks([0, 50, 100, 150])\n\nax.set_title(\"title\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\n\nfig.subplots_adjust(left=0.15, right=.9, bottom=0.1, top=0.9);\n</pre> fig, ax = plt.subplots(1, 1)        ax.plot(x, x**2, x, np.exp(x)) ax.set_yticks([0, 50, 100, 150])  ax.set_title(\"title\") ax.set_xlabel(\"x\") ax.set_ylabel(\"y\")  fig.subplots_adjust(left=0.15, right=.9, bottom=0.1, top=0.9); <p>With the <code>grid</code> method in the axis object, we can turn on and off grid lines. We can also customize the appearance of the grid lines using the same keyword arguments as the <code>plot</code> function:</p> In\u00a0[100]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(10,3))\n\n# default grid appearance\naxes[0].plot(x, x**2, x, x**3, lw=2)\naxes[0].grid(True)\n\n# custom grid appearance\naxes[1].plot(x, x**2, x, x**3, lw=2)\naxes[1].grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5)\n</pre> fig, axes = plt.subplots(1, 2, figsize=(10,3))  # default grid appearance axes[0].plot(x, x**2, x, x**3, lw=2) axes[0].grid(True)  # custom grid appearance axes[1].plot(x, x**2, x, x**3, lw=2) axes[1].grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5) <p>We can also change the properties of axis spines:</p> In\u00a0[101]: Copied! <pre>fig, ax = plt.subplots(figsize=(6,2))\n\nax.spines['bottom'].set_color('blue')\nax.spines['top'].set_color('blue')\n\nax.spines['left'].set_color('red')\nax.spines['left'].set_linewidth(2)\n\n# turn off axis spine to the right\nax.spines['right'].set_color(\"none\")\nax.yaxis.tick_left() # only ticks on the left side\n</pre> fig, ax = plt.subplots(figsize=(6,2))  ax.spines['bottom'].set_color('blue') ax.spines['top'].set_color('blue')  ax.spines['left'].set_color('red') ax.spines['left'].set_linewidth(2)  # turn off axis spine to the right ax.spines['right'].set_color(\"none\") ax.yaxis.tick_left() # only ticks on the left side <p>Sometimes it is useful to have dual x or y axes in a figure; for example, when plotting curves with different units together. Matplotlib supports this with the <code>twinx</code> and <code>twiny</code> functions:</p> In\u00a0[102]: Copied! <pre>fig, ax1 = plt.subplots()\n\nax1.plot(x, x**2, lw=2, color=\"blue\")\nax1.set_ylabel(r\"area $(m^2)$\", fontsize=18, color=\"blue\")\nfor label in ax1.get_yticklabels():\n    label.set_color(\"blue\")\n    \nax2 = ax1.twinx()\nax2.plot(x, x**3, lw=2, color=\"red\")\nax2.set_ylabel(r\"volume $(m^3)$\", fontsize=18, color=\"red\")\nfor label in ax2.get_yticklabels():\n    label.set_color(\"red\")\n</pre> fig, ax1 = plt.subplots()  ax1.plot(x, x**2, lw=2, color=\"blue\") ax1.set_ylabel(r\"area $(m^2)$\", fontsize=18, color=\"blue\") for label in ax1.get_yticklabels():     label.set_color(\"blue\")      ax2 = ax1.twinx() ax2.plot(x, x**3, lw=2, color=\"red\") ax2.set_ylabel(r\"volume $(m^3)$\", fontsize=18, color=\"red\") for label in ax2.get_yticklabels():     label.set_color(\"red\") In\u00a0[103]: Copied! <pre>fig, ax = plt.subplots()\n\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data',0)) # set position of x spine to x=0\n\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))   # set position of y spine to y=0\n\nxx = np.linspace(-0.75, 1., 100)\nax.plot(xx, xx**3);\n</pre> fig, ax = plt.subplots()  ax.spines['right'].set_color('none') ax.spines['top'].set_color('none')  ax.xaxis.set_ticks_position('bottom') ax.spines['bottom'].set_position(('data',0)) # set position of x spine to x=0  ax.yaxis.set_ticks_position('left') ax.spines['left'].set_position(('data',0))   # set position of y spine to y=0  xx = np.linspace(-0.75, 1., 100) ax.plot(xx, xx**3); <p>In addition to the regular <code>plot</code> method, there are a number of other functions for generating different kind of plots. See the matplotlib plot gallery for a complete list of available plot types: http://matplotlib.org/gallery.html. Some of the more useful ones are show below:</p> In\u00a0[104]: Copied! <pre>n = np.array([0,1,2,3,4,5])\n</pre> n = np.array([0,1,2,3,4,5]) In\u00a0[105]: Copied! <pre>fig, axes = plt.subplots(1, 4, figsize=(12,3))\n\naxes[0].scatter(xx, xx + 0.25*np.random.randn(len(xx)))\naxes[0].set_title(\"scatter\")\n\naxes[1].step(n, n**2, lw=2)\naxes[1].set_title(\"step\")\n\naxes[2].bar(n, n**2, align=\"center\", width=0.5, alpha=0.5)\naxes[2].set_title(\"bar\")\n\naxes[3].fill_between(x, x**2, x**3, color=\"green\", alpha=0.5);\naxes[3].set_title(\"fill_between\");\n</pre> fig, axes = plt.subplots(1, 4, figsize=(12,3))  axes[0].scatter(xx, xx + 0.25*np.random.randn(len(xx))) axes[0].set_title(\"scatter\")  axes[1].step(n, n**2, lw=2) axes[1].set_title(\"step\")  axes[2].bar(n, n**2, align=\"center\", width=0.5, alpha=0.5) axes[2].set_title(\"bar\")  axes[3].fill_between(x, x**2, x**3, color=\"green\", alpha=0.5); axes[3].set_title(\"fill_between\"); <p>Annotating text in matplotlib figures can be done using the <code>text</code> function. It supports LaTeX formatting just like axis label texts and titles:</p> In\u00a0[108]: Copied! <pre>fig, ax = plt.subplots()\n\nax.plot(xx, xx**2, xx, xx**3)\n\nax.text(0.15, 0.2, r\"$y=x^2$\", fontsize=20, color=\"blue\")\nax.text(0.65, 0.1, r\"$y=x^3$\", fontsize=20, color=\"green\");\n</pre> fig, ax = plt.subplots()  ax.plot(xx, xx**2, xx, xx**3)  ax.text(0.15, 0.2, r\"$y=x^2$\", fontsize=20, color=\"blue\") ax.text(0.65, 0.1, r\"$y=x^3$\", fontsize=20, color=\"green\"); <p>Axes can be added to a matplotlib Figure canvas manually using <code>fig.add_axes</code> or using a sub-figure layout manager such as <code>subplots</code>, <code>subplot2grid</code>, or <code>gridspec</code>:</p> In\u00a0[109]: Copied! <pre>fig, ax = plt.subplots(2, 3)\nfig.tight_layout()\n</pre> fig, ax = plt.subplots(2, 3) fig.tight_layout() In\u00a0[110]: Copied! <pre>fig = plt.figure()\nax1 = plt.subplot2grid((3,3), (0,0), colspan=3)\nax2 = plt.subplot2grid((3,3), (1,0), colspan=2)\nax3 = plt.subplot2grid((3,3), (1,2), rowspan=2)\nax4 = plt.subplot2grid((3,3), (2,0))\nax5 = plt.subplot2grid((3,3), (2,1))\nfig.tight_layout()\n</pre> fig = plt.figure() ax1 = plt.subplot2grid((3,3), (0,0), colspan=3) ax2 = plt.subplot2grid((3,3), (1,0), colspan=2) ax3 = plt.subplot2grid((3,3), (1,2), rowspan=2) ax4 = plt.subplot2grid((3,3), (2,0)) ax5 = plt.subplot2grid((3,3), (2,1)) fig.tight_layout() In\u00a0[111]: Copied! <pre>import matplotlib.gridspec as gridspec\n</pre> import matplotlib.gridspec as gridspec In\u00a0[112]: Copied! <pre>fig = plt.figure()\n\ngs = gridspec.GridSpec(2, 3, height_ratios=[2,1], width_ratios=[1,2,1])\nfor g in gs:\n    ax = fig.add_subplot(g)\n    \nfig.tight_layout()\n</pre> fig = plt.figure()  gs = gridspec.GridSpec(2, 3, height_ratios=[2,1], width_ratios=[1,2,1]) for g in gs:     ax = fig.add_subplot(g)      fig.tight_layout() <p>Manually adding axes with <code>add_axes</code> is useful for adding insets to figures:</p> In\u00a0[113]: Copied! <pre>fig, ax = plt.subplots()\n\nax.plot(xx, xx**2, xx, xx**3)\nfig.tight_layout()\n\n# inset\ninset_ax = fig.add_axes([0.2, 0.55, 0.35, 0.35]) # X, Y, width, height\n\ninset_ax.plot(xx, xx**2, xx, xx**3)\ninset_ax.set_title('zoom near origin')\n\n# set axis range\ninset_ax.set_xlim(-.2, .2)\ninset_ax.set_ylim(-.005, .01)\n\n# set axis tick locations\ninset_ax.set_yticks([0, 0.005, 0.01])\ninset_ax.set_xticks([-0.1,0,.1]);\n</pre> fig, ax = plt.subplots()  ax.plot(xx, xx**2, xx, xx**3) fig.tight_layout()  # inset inset_ax = fig.add_axes([0.2, 0.55, 0.35, 0.35]) # X, Y, width, height  inset_ax.plot(xx, xx**2, xx, xx**3) inset_ax.set_title('zoom near origin')  # set axis range inset_ax.set_xlim(-.2, .2) inset_ax.set_ylim(-.005, .01)  # set axis tick locations inset_ax.set_yticks([0, 0.005, 0.01]) inset_ax.set_xticks([-0.1,0,.1]); <p>Colormaps and contour figures are useful for plotting functions of two variables. In most of these functions we will use a colormap to encode one dimension of the data. There are a number of predefined colormaps. It is relatively straightforward to define custom colormaps. For a list of pre-defined colormaps, see: http://www.scipy.org/Cookbook/Matplotlib/Show_colormaps</p> In\u00a0[114]: Copied! <pre>alpha = 0.7\nphi_ext = 2 * np.pi * 0.5\n\ndef flux_qubit_potential(phi_m, phi_p):\n    return 2 + alpha - 2 * np.cos(phi_p) * np.cos(phi_m) - alpha * np.cos(phi_ext - 2*phi_p)\n</pre> alpha = 0.7 phi_ext = 2 * np.pi * 0.5  def flux_qubit_potential(phi_m, phi_p):     return 2 + alpha - 2 * np.cos(phi_p) * np.cos(phi_m) - alpha * np.cos(phi_ext - 2*phi_p) In\u00a0[115]: Copied! <pre>phi_m = np.linspace(0, 2*np.pi, 100)\nphi_p = np.linspace(0, 2*np.pi, 100)\nX,Y = np.meshgrid(phi_p, phi_m)\nZ = flux_qubit_potential(X, Y).T\n</pre> phi_m = np.linspace(0, 2*np.pi, 100) phi_p = np.linspace(0, 2*np.pi, 100) X,Y = np.meshgrid(phi_p, phi_m) Z = flux_qubit_potential(X, Y).T In\u00a0[116]: Copied! <pre>fig, ax = plt.subplots()\n\np = ax.pcolor(X/(2*np.pi), Y/(2*np.pi), Z, cmap=matplotlib.cm.RdBu, vmin=abs(Z).min(), vmax=abs(Z).max())\ncb = fig.colorbar(p, ax=ax)\n</pre> fig, ax = plt.subplots()  p = ax.pcolor(X/(2*np.pi), Y/(2*np.pi), Z, cmap=matplotlib.cm.RdBu, vmin=abs(Z).min(), vmax=abs(Z).max()) cb = fig.colorbar(p, ax=ax) In\u00a0[117]: Copied! <pre>fig, ax = plt.subplots()\n\nim = ax.imshow(Z, cmap=matplotlib.cm.RdBu, vmin=abs(Z).min(), vmax=abs(Z).max(), extent=[0, 1, 0, 1])\nim.set_interpolation('bilinear')\n\ncb = fig.colorbar(im, ax=ax)\n</pre> fig, ax = plt.subplots()  im = ax.imshow(Z, cmap=matplotlib.cm.RdBu, vmin=abs(Z).min(), vmax=abs(Z).max(), extent=[0, 1, 0, 1]) im.set_interpolation('bilinear')  cb = fig.colorbar(im, ax=ax) In\u00a0[118]: Copied! <pre>fig, ax = plt.subplots()\n\ncnt = ax.contour(Z, cmap=matplotlib.cm.RdBu, vmin=abs(Z).min(), vmax=abs(Z).max(), extent=[0, 1, 0, 1])\n</pre> fig, ax = plt.subplots()  cnt = ax.contour(Z, cmap=matplotlib.cm.RdBu, vmin=abs(Z).min(), vmax=abs(Z).max(), extent=[0, 1, 0, 1]) <p>To use 3D graphics in matplotlib, we first need to create an instance of the <code>Axes3D</code> class. 3D axes can be added to a matplotlib figure canvas in exactly the same way as 2D axes; or, more conveniently, by passing a <code>projection='3d'</code> keyword argument to the <code>add_axes</code> or <code>add_subplot</code> methods.</p> In\u00a0[119]: Copied! <pre>from mpl_toolkits.mplot3d.axes3d import Axes3D\n</pre> from mpl_toolkits.mplot3d.axes3d import Axes3D In\u00a0[121]: Copied! <pre>fig = plt.figure(figsize=(14,6))\n\n# `ax` is a 3D-aware axis instance because of the projection='3d' keyword argument to add_subplot\nax = fig.add_subplot(1, 2, 1, projection='3d')\n\np = ax.plot_surface(X, Y, Z, rstride=4, cstride=4, linewidth=0)\n\n# surface_plot with color grading and color bar\nax = fig.add_subplot(1, 2, 2, projection='3d')\np = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0, antialiased=False)\ncb = fig.colorbar(p, shrink=0.5)\n</pre> fig = plt.figure(figsize=(14,6))  # `ax` is a 3D-aware axis instance because of the projection='3d' keyword argument to add_subplot ax = fig.add_subplot(1, 2, 1, projection='3d')  p = ax.plot_surface(X, Y, Z, rstride=4, cstride=4, linewidth=0)  # surface_plot with color grading and color bar ax = fig.add_subplot(1, 2, 2, projection='3d') p = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0, antialiased=False) cb = fig.colorbar(p, shrink=0.5) In\u00a0[122]: Copied! <pre>fig = plt.figure(figsize=(8,6))\n\nax = fig.add_subplot(1, 1, 1, projection='3d')\n\np = ax.plot_wireframe(X, Y, Z, rstride=4, cstride=4)\n</pre> fig = plt.figure(figsize=(8,6))  ax = fig.add_subplot(1, 1, 1, projection='3d')  p = ax.plot_wireframe(X, Y, Z, rstride=4, cstride=4) In\u00a0[123]: Copied! <pre>fig = plt.figure(figsize=(8,6))\n\nax = fig.add_subplot(1,1,1, projection='3d')\n\nax.plot_surface(X, Y, Z, rstride=4, cstride=4, alpha=0.25)\ncset = ax.contour(X, Y, Z, zdir='z', offset=-np.pi, cmap=matplotlib.cm.coolwarm)\ncset = ax.contour(X, Y, Z, zdir='x', offset=-np.pi, cmap=matplotlib.cm.coolwarm)\ncset = ax.contour(X, Y, Z, zdir='y', offset=3*np.pi, cmap=matplotlib.cm.coolwarm)\n\nax.set_xlim3d(-np.pi, 2*np.pi);\nax.set_ylim3d(0, 3*np.pi);\nax.set_zlim3d(-np.pi, 2*np.pi);\n</pre> fig = plt.figure(figsize=(8,6))  ax = fig.add_subplot(1,1,1, projection='3d')  ax.plot_surface(X, Y, Z, rstride=4, cstride=4, alpha=0.25) cset = ax.contour(X, Y, Z, zdir='z', offset=-np.pi, cmap=matplotlib.cm.coolwarm) cset = ax.contour(X, Y, Z, zdir='x', offset=-np.pi, cmap=matplotlib.cm.coolwarm) cset = ax.contour(X, Y, Z, zdir='y', offset=3*np.pi, cmap=matplotlib.cm.coolwarm)  ax.set_xlim3d(-np.pi, 2*np.pi); ax.set_ylim3d(0, 3*np.pi); ax.set_zlim3d(-np.pi, 2*np.pi); <ul> <li>http://www.matplotlib.org - The project web page for matplotlib.</li> <li>https://github.com/matplotlib/matplotlib - The source code for matplotlib.</li> <li>http://matplotlib.org/gallery.html - A large gallery showcaseing various types of plots matplotlib can create. Highly recommended!</li> <li>http://www.loria.fr/~rougier/teaching/matplotlib - A good matplotlib tutorial.</li> <li>http://scipy-lectures.github.io/matplotlib/matplotlib.html - Another good matplotlib reference.</li> </ul>"},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#matplotlib-working-with-axes","title":"Matplotlib - working with axes\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#logarithmic-scale","title":"Logarithmic scale\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#placement-of-ticks-and-custom-tick-labels","title":"Placement of ticks and custom tick labels\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#numbers-on-axes-in-scientific-notation","title":"Numbers on axes in scientific notation\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#axis-number-and-axis-label-spacing","title":"Axis number and axis label spacing\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#axis-position-adjustments","title":"Axis position adjustments\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#axis-grid","title":"Axis grid\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#axis-spines","title":"Axis spines\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#twin-axes","title":"Twin axes\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#axes-where-x-and-y-is-zero","title":"Axes where x and y is zero\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#other-2d-plot-styles","title":"Other 2D plot styles\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#text-annotation","title":"Text annotation\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#figures-with-multiple-subplots-and-insets","title":"Figures with multiple subplots and insets\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#subplots","title":"subplots\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#subplot2grid","title":"subplot2grid\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#gridspec","title":"gridspec\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#add_axes","title":"add_axes\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#colormap-and-contour-figures","title":"Colormap and contour figures\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#pcolor","title":"pcolor\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#imshow","title":"imshow\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#contour","title":"contour\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#3d-figures","title":"3D figures\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#surface-plots","title":"Surface plots\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#wire-frame-plot","title":"Wire-frame plot\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#coutour-plots-with-projections","title":"Coutour plots with projections\u00b6","text":""},{"location":"teaching_resources/matplotlib/matplotlib_working_with_axes/#further-reading","title":"Further reading\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/","title":"Numpy - Array creation, inspection and manipulation","text":"In\u00a0[1]: Copied! <pre>import numpy\n</pre> import numpy In\u00a0[2]: Copied! <pre>l1 = [1,2,3]\narr1 = numpy.array(l1)\narr1\n</pre> l1 = [1,2,3] arr1 = numpy.array(l1) arr1 Out[2]: <pre>array([1, 2, 3])</pre> In\u00a0[3]: Copied! <pre>type(arr1)\n</pre> type(arr1) Out[3]: <pre>numpy.ndarray</pre> In\u00a0[4]: Copied! <pre>l2d = [[1,2,3],[4,5,6]]\narr2d = numpy.array(l2d)\narr2d\n</pre> l2d = [[1,2,3],[4,5,6]] arr2d = numpy.array(l2d) arr2d Out[4]: <pre>array([[1, 2, 3],\n       [4, 5, 6]])</pre> In\u00a0[5]: Copied! <pre>type(arr2d)\n</pre> type(arr2d) Out[5]: <pre>numpy.ndarray</pre> In\u00a0[6]: Copied! <pre>l3d = [[[1,2],[3,4]], [[1,2],[3,4]]]\narr3d = numpy.array(l3d)\narr3d\n</pre> l3d = [[[1,2],[3,4]], [[1,2],[3,4]]] arr3d = numpy.array(l3d) arr3d Out[6]: <pre>array([[[1, 2],\n        [3, 4]],\n\n       [[1, 2],\n        [3, 4]]])</pre> In\u00a0[10]: Copied! <pre>mat = numpy.mat('1,2,3;4,5,6;7,8,9')\ntype(mat)\n</pre> mat = numpy.mat('1,2,3;4,5,6;7,8,9') type(mat) Out[10]: <pre>numpy.matrix</pre> In\u00a0[11]: Copied! <pre>mat\n</pre> mat Out[11]: <pre>matrix([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])</pre> In\u00a0[12]: Copied! <pre>numpy.array(mat)\n</pre> numpy.array(mat) Out[12]: <pre>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])</pre> In\u00a0[7]: Copied! <pre>numpy.ones(shape=(4,4)) #pass shapre as a tuple\n</pre> numpy.ones(shape=(4,4)) #pass shapre as a tuple Out[7]: <pre>array([[ 1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.]])</pre> In\u00a0[8]: Copied! <pre>numpy.zeros(shape = (2,2))\n</pre> numpy.zeros(shape = (2,2)) Out[8]: <pre>array([[ 0.,  0.],\n       [ 0.,  0.]])</pre> In\u00a0[11]: Copied! <pre>numpy.eye(3)\n</pre> numpy.eye(3) Out[11]: <pre>array([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])</pre> In\u00a0[12]: Copied! <pre>numpy.arange(start=0, stop=10, step=1) #stop is non inclusive\n</pre> numpy.arange(start=0, stop=10, step=1) #stop is non inclusive Out[12]: <pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[13]: Copied! <pre>numpy.arange(21) #the stop arg is the only compulsory arg\n</pre> numpy.arange(21) #the stop arg is the only compulsory arg Out[13]: <pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20])</pre> <p>linspace is similar, returns contiguous numbers in linearly spaced intervals. These numbers conform to uniform distribution</p> In\u00a0[15]: Copied! <pre>numpy.linspace(start=3, stop=21, num=7) #return 7 numbers between 3 and 21 (inclusive)\n</pre> numpy.linspace(start=3, stop=21, num=7) #return 7 numbers between 3 and 21 (inclusive) Out[15]: <pre>array([  3.,   6.,   9.,  12.,  15.,  18.,  21.])</pre> In\u00a0[14]: Copied! <pre>numpy.random.rand(3, 3) #specify shape in individual arguments\n</pre> numpy.random.rand(3, 3) #specify shape in individual arguments Out[14]: <pre>array([[0.13036297, 0.44737951, 0.85299833],\n       [0.94987992, 0.63348932, 0.42257387],\n       [0.54558662, 0.17654393, 0.84926165]])</pre> In\u00a0[15]: Copied! <pre>numpy.random.rand(3,3,3)\n</pre> numpy.random.rand(3,3,3) Out[15]: <pre>array([[[0.30709651, 0.26703385, 0.56179506],\n        [0.00490574, 0.79481741, 0.08887725],\n        [0.01482835, 0.41434266, 0.6668987 ]],\n\n       [[0.68209787, 0.9032678 , 0.87111643],\n        [0.83040063, 0.40690954, 0.43069501],\n        [0.44060348, 0.6184007 , 0.43669199]],\n\n       [[0.25732679, 0.90222633, 0.86477316],\n        [0.59079063, 0.23197552, 0.73234759],\n        [0.02420268, 0.58581809, 0.04223459]]])</pre> <p><code>randn</code> Create random numbers that follow standard normal distribution.</p> In\u00a0[23]: Copied! <pre>vals = numpy.random.randn(5000)\nvals.shape\n</pre> vals = numpy.random.randn(5000) vals.shape Out[23]: <pre>(5000,)</pre> In\u00a0[25]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\nplt.hist(vals, bins=50);\n</pre> import matplotlib.pyplot as plt %matplotlib inline plt.hist(vals, bins=50); In\u00a0[19]: Copied! <pre>numpy.random.randint(low=30, high=200, size=10)\n</pre> numpy.random.randint(low=30, high=200, size=10) Out[19]: <pre>array([ 37,  43,  76,  36,  67,  45, 165,  75, 165,  40])</pre> In\u00a0[20]: Copied! <pre>arr2 = numpy.random.rand(3,3,3)\narr2.shape\n</pre> arr2 = numpy.random.rand(3,3,3) arr2.shape Out[20]: <pre>(3, 3, 3)</pre> In\u00a0[21]: Copied! <pre>arr3 = numpy.random.randint(low=30, high=200, size=10)\narr3.shape\n</pre> arr3 = numpy.random.randint(low=30, high=200, size=10) arr3.shape Out[21]: <pre>(10,)</pre> <p>thus shape is returned as a tuple.</p> In\u00a0[22]: Copied! <pre>arr2.dtype\n</pre> arr2.dtype Out[22]: <pre>dtype('float64')</pre> In\u00a0[23]: Copied! <pre>arr3.dtype\n</pre> arr3.dtype Out[23]: <pre>dtype('int32')</pre> In\u00a0[25]: Copied! <pre>arr2\n</pre> arr2 Out[25]: <pre>array([[[ 0.04539372,  0.63141413,  0.89693763],\n        [ 0.55265413,  0.16925386,  0.83917698],\n        [ 0.75055999,  0.26155305,  0.33921729]],\n\n       [[ 0.35913789,  0.20122447,  0.10491535],\n        [ 0.01784351,  0.20815688,  0.90825816],\n        [ 0.69680734,  0.3975908 ,  0.63961161]],\n\n       [[ 0.06461796,  0.99271516,  0.02077921],\n        [ 0.26578436,  0.40538054,  0.58002467],\n        [ 0.53456854,  0.85680407,  0.66601052]]])</pre> In\u00a0[24]: Copied! <pre>arr2.max() #max element in the entire 3d array\n</pre> arr2.max() #max element in the entire 3d array Out[24]: <pre>0.99271516081323574</pre> In\u00a0[26]: Copied! <pre>arr2.max(axis=1) #max in each column\n</pre> arr2.max(axis=1) #max in each column Out[26]: <pre>array([[ 0.75055999,  0.63141413,  0.89693763],\n       [ 0.69680734,  0.3975908 ,  0.90825816],\n       [ 0.53456854,  0.99271516,  0.66601052]])</pre> In\u00a0[32]: Copied! <pre>arr2.max(axis=2) #max in each row\n</pre> arr2.max(axis=2) #max in each row Out[32]: <pre>array([[ 0.89693763,  0.83917698,  0.75055999],\n       [ 0.35913789,  0.90825816,  0.69680734],\n       [ 0.99271516,  0.58002467,  0.85680407]])</pre> <p>min method to find the minimum</p> In\u00a0[34]: Copied! <pre>arr2.min()\n</pre> arr2.min() Out[34]: <pre>0.017843505898512246</pre> In\u00a0[35]: Copied! <pre>arr2.min(axis=1)\n</pre> arr2.min(axis=1) Out[35]: <pre>array([[ 0.04539372,  0.16925386,  0.33921729],\n       [ 0.01784351,  0.20122447,  0.10491535],\n       [ 0.06461796,  0.40538054,  0.02077921]])</pre> In\u00a0[36]: Copied! <pre>arr2.argmax() #location max element in the 3D array\n</pre> arr2.argmax() #location max element in the 3D array Out[36]: <pre>19</pre> In\u00a0[38]: Copied! <pre>arr2.argmax(1) #indices of each max element\n</pre> arr2.argmax(1) #indices of each max element Out[38]: <pre>array([[2, 0, 0],\n       [2, 2, 1],\n       [2, 0, 2]], dtype=int64)</pre> In\u00a0[39]: Copied! <pre>arr2.argmin(1)\n</pre> arr2.argmin(1) Out[39]: <pre>array([[0, 1, 2],\n       [1, 0, 0],\n       [0, 1, 0]], dtype=int64)</pre> In\u00a0[41]: Copied! <pre>arr3\n</pre> arr3 Out[41]: <pre>array([145, 152, 101, 130, 152,  84, 148, 160, 121, 137])</pre> In\u00a0[42]: Copied! <pre>arr3.shape\n</pre> arr3.shape Out[42]: <pre>(10,)</pre> In\u00a0[43]: Copied! <pre>arr3.reshape((5,2))\n</pre> arr3.reshape((5,2)) Out[43]: <pre>array([[145, 152],\n       [101, 130],\n       [152,  84],\n       [148, 160],\n       [121, 137]])</pre>"},{"location":"teaching_resources/numpy/numpy_array_creation/#numpy-array-creation-inspection-and-manipulation","title":"Numpy - Array creation, inspection and manipulation\u00b6","text":"<p> NumPy is the primary matrix laboratory for Python. Many other libraries such as pandas, tensorflow, scikit-learn etc are built on top of this. Some tutorials to go with this cheat sheet: Numpy quick start - scipy, numpy python course eu, datacamp numpy wiki, numpy.org</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#creating-numpy-arrays-from-lists","title":"Creating numpy arrays from lists\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/#2d-arrays","title":"2D arrays\u00b6","text":"<p>Create using list of lists</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#3d-arrays","title":"3D arrays\u00b6","text":"<p>Same way using list of lists</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#creating-using-matlab-style-syntax","title":"Creating using MATLAB style syntax\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/#special-numpy-arrays","title":"Special numpy arrays\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/#ones-zeros-and-eye","title":"ones zeros and eye\u00b6","text":"<p>Creating a matrix of ones</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#zeros","title":"zeros\u00b6","text":"<p>sometimes it is useful to just get a matrix of zeros</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#eye","title":"eye\u00b6","text":"<p>eye for identity matrix - has values only in the diagonal. Identity matrices are square, 2D</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#arange-and-linspace","title":"arange and linspace\u00b6","text":"<p>arange is array range. Works same as <code>range</code> function in Python, but returns an array.</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#random-numbers","title":"Random numbers\u00b6","text":"<p>generate random numbers using random module</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#rand","title":"<code>rand</code>\u00b6","text":"<p>returns numbers in normal distribution between -1 and 1</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#randint","title":"<code>randint</code>\u00b6","text":"<p>randint returns randomly distributed integers between specified range</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#array-inspection","title":"Array inspection\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/#shape","title":"shape\u00b6","text":"<p>Use shape property to get the dimensions</p>"},{"location":"teaching_resources/numpy/numpy_array_creation/#datatype-of-the-elements","title":"datatype of the elements\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/#max-and-min-elements","title":"max and min elements\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/#argmax-and-argmin-to-find-the-location-of-max-element","title":"<code>argmax</code> and <code>argmin</code> to find the location of max element\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/#array-manipulation","title":"Array manipulation\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_creation/#reshape-arrays","title":"Reshape arrays\u00b6","text":"<p><code>reshape()</code> method on an array object and send a tuple of the rows and column dimensions</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/","title":"Array operations - slicing, dicing, searching","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[2]: Copied! <pre>arr1 = np.random.randint(10,30, size=8)\narr1\n</pre> arr1 = np.random.randint(10,30, size=8) arr1 Out[2]: <pre>array([25, 10, 18, 10, 16, 22, 14, 26])</pre> In\u00a0[3]: Copied! <pre>arr2 = np.random.randint(20,200,size=50).reshape(5,10)  #method chaining - numbers from 0 to 50\narr2\n</pre> arr2 = np.random.randint(20,200,size=50).reshape(5,10)  #method chaining - numbers from 0 to 50 arr2 Out[3]: <pre>array([[147, 134,  58,  21,  90, 193, 135, 179, 129, 113],\n       [ 85, 161,  31, 123, 191, 166,  52,  25,  94, 184],\n       [174, 149, 143, 123, 126, 143,  59, 180, 116, 105],\n       [ 78, 198, 161, 152, 167,  84, 104, 128, 173, 140],\n       [181,  47, 114, 145, 139, 180, 183, 125,  41,  46]])</pre> In\u00a0[4]: Copied! <pre>arr1[0]\n</pre> arr1[0] Out[4]: <pre>25</pre> In\u00a0[5]: Copied! <pre>arr1[3]\n</pre> arr1[3] Out[5]: <pre>10</pre> In\u00a0[6]: Copied! <pre>arr1[:3] #get the first 3 elements. Gets lower bounds inclusive, upper bound exclusive\n</pre> arr1[:3] #get the first 3 elements. Gets lower bounds inclusive, upper bound exclusive Out[6]: <pre>array([25, 10, 18])</pre> In\u00a0[7]: Copied! <pre>arr1[2:] #lower bound inclusive\n</pre> arr1[2:] #lower bound inclusive Out[7]: <pre>array([18, 10, 16, 22, 14, 26])</pre> In\u00a0[8]: Copied! <pre>arr1[2:5] #get elements at index 2,3,4\n</pre> arr1[2:5] #get elements at index 2,3,4 Out[8]: <pre>array([18, 10, 16])</pre> In\u00a0[9]: Copied! <pre>arr2\n</pre> arr2 Out[9]: <pre>array([[147, 134,  58,  21,  90, 193, 135, 179, 129, 113],\n       [ 85, 161,  31, 123, 191, 166,  52,  25,  94, 184],\n       [174, 149, 143, 123, 126, 143,  59, 180, 116, 105],\n       [ 78, 198, 161, 152, 167,  84, 104, 128, 173, 140],\n       [181,  47, 114, 145, 139, 180, 183, 125,  41,  46]])</pre> In\u00a0[10]: Copied! <pre>arr2[0,0] #style 1 - you pass in a list of indices\n</pre> arr2[0,0] #style 1 - you pass in a list of indices Out[10]: <pre>147</pre> In\u00a0[11]: Copied! <pre>arr2[0][0] #style 2 - parse it as list of lists - not so popular\n</pre> arr2[0][0] #style 2 - parse it as list of lists - not so popular Out[11]: <pre>147</pre> In\u00a0[12]: Copied! <pre>arr2[1] # get a full row\n</pre> arr2[1] # get a full row Out[12]: <pre>array([ 85, 161,  31, 123, 191, 166,  52,  25,  94, 184])</pre> In\u00a0[13]: Copied! <pre>#get the second column\narr2[:,1]\n</pre> #get the second column arr2[:,1] Out[13]: <pre>array([134, 161, 149, 198,  47])</pre> <p>Thus, you specify <code>:</code> for all columns, followed by <code>1</code> for column. And you get a 1D array of the result</p> In\u00a0[14]: Copied! <pre>#get the 3rd row\narr2[2,:] #which is same as arr2[2]\n</pre> #get the 3rd row arr2[2,:] #which is same as arr2[2] Out[14]: <pre>array([174, 149, 143, 123, 126, 143,  59, 180, 116, 105])</pre> In\u00a0[15]: Copied! <pre>#get the center 3,3 elements - columns 4,5,6 and rows 1,2,3\narr2[1:4, 4:7]\n</pre> #get the center 3,3 elements - columns 4,5,6 and rows 1,2,3 arr2[1:4, 4:7] Out[15]: <pre>array([[191, 166,  52],\n       [126, 143,  59],\n       [167,  84, 104]])</pre> In\u00a0[16]: Copied! <pre>arr2\n</pre> arr2 Out[16]: <pre>array([[147, 134,  58,  21,  90, 193, 135, 179, 129, 113],\n       [ 85, 161,  31, 123, 191, 166,  52,  25,  94, 184],\n       [174, 149, 143, 123, 126, 143,  59, 180, 116, 105],\n       [ 78, 198, 161, 152, 167,  84, 104, 128, 173, 140],\n       [181,  47, 114, 145, 139, 180, 183, 125,  41,  46]])</pre> In\u00a0[17]: Copied! <pre>arr2_subset = arr2[1:4, 4:7]\narr2_subset\n</pre> arr2_subset = arr2[1:4, 4:7] arr2_subset Out[17]: <pre>array([[191, 166,  52],\n       [126, 143,  59],\n       [167,  84, 104]])</pre> In\u00a0[18]: Copied! <pre>arr2_subset[:,:] = 999 #assign this entire numpy the same values\narr2_subset\n</pre> arr2_subset[:,:] = 999 #assign this entire numpy the same values arr2_subset Out[18]: <pre>array([[999, 999, 999],\n       [999, 999, 999],\n       [999, 999, 999]])</pre> In\u00a0[19]: Copied! <pre>arr2 #notice the 999 in the middle\n</pre> arr2 #notice the 999 in the middle Out[19]: <pre>array([[147, 134,  58,  21,  90, 193, 135, 179, 129, 113],\n       [ 85, 161,  31, 123, 999, 999, 999,  25,  94, 184],\n       [174, 149, 143, 123, 999, 999, 999, 180, 116, 105],\n       [ 78, 198, 161, 152, 999, 999, 999, 128, 173, 140],\n       [181,  47, 114, 145, 139, 180, 183, 125,  41,  46]])</pre> In\u00a0[20]: Copied! <pre>arr2_subset_a = arr2_subset\narr2_subset_a is arr2_subset\n</pre> arr2_subset_a = arr2_subset arr2_subset_a is arr2_subset Out[20]: <pre>True</pre> <p>Notice they are same obj in memory</p> In\u00a0[21]: Copied! <pre>arr3_subset = arr2_subset.copy()\narr3_subset\n</pre> arr3_subset = arr2_subset.copy() arr3_subset Out[21]: <pre>array([[999, 999, 999],\n       [999, 999, 999],\n       [999, 999, 999]])</pre> In\u00a0[22]: Copied! <pre>arr3_subset is arr2_subset\n</pre> arr3_subset is arr2_subset Out[22]: <pre>False</pre> <p>Notice they are different objects in memory. Thus changing arr3_subset will not affect its source</p> In\u00a0[23]: Copied! <pre>arr3_subset[:,:] = 0.1\narr2_subset\n</pre> arr3_subset[:,:] = 0.1 arr2_subset Out[23]: <pre>array([[999, 999, 999],\n       [999, 999, 999],\n       [999, 999, 999]])</pre> In\u00a0[24]: Copied! <pre>arr1\n</pre> arr1 Out[24]: <pre>array([25, 10, 18, 10, 16, 22, 14, 26])</pre> In\u00a0[28]: Copied! <pre>arr1&gt;15  # gives truth vector\n</pre> arr1&gt;15  # gives truth vector Out[28]: <pre>array([ True, False,  True, False,  True,  True, False,  True])</pre> <p>You can use the Truth vector as an index to search. Get all numbers greater than 15</p> In\u00a0[29]: Copied! <pre>arr1[arr1 &gt; 15]\n</pre> arr1[arr1 &gt; 15] Out[29]: <pre>array([25, 18, 16, 22, 26])</pre> In\u00a0[30]: Copied! <pre>arr1[arr1 &gt; 20]\n</pre> arr1[arr1 &gt; 20] Out[30]: <pre>array([25, 22, 26])</pre> <p>just the condition returns a boolean matrix of same dimension as the one being queried</p> In\u00a0[31]: Copied! <pre>arr1 &gt; 12\n</pre> arr1 &gt; 12 Out[31]: <pre>array([ True, False,  True, False,  True,  True,  True,  True])</pre> In\u00a0[32]: Copied! <pre>arr2[arr2 &gt; 50] #looses the original shape as its impossible to keep the 2D shape\n</pre> arr2[arr2 &gt; 50] #looses the original shape as its impossible to keep the 2D shape Out[32]: <pre>array([147, 134,  58,  90, 193, 135, 179, 129, 113,  85, 161, 123, 999,\n       999, 999,  94, 184, 174, 149, 143, 123, 999, 999, 999, 180, 116,\n       105,  78, 198, 161, 152, 999, 999, 999, 128, 173, 140, 181, 114,\n       145, 139, 180, 183, 125])</pre> In\u00a0[33]: Copied! <pre>arr2[arr2 &lt; 30]\n</pre> arr2[arr2 &lt; 30] Out[33]: <pre>array([21, 25])</pre> In\u00a0[35]: Copied! <pre>arr1[(arr1&gt;16) &amp; (arr1&lt;23)]\n</pre> arr1[(arr1&gt;16) &amp; (arr1&lt;23)] Out[35]: <pre>array([18, 22])</pre> In\u00a0[36]: Copied! <pre>arr1\n</pre> arr1 Out[36]: <pre>array([25, 10, 18, 10, 16, 22, 14, 26])</pre> In\u00a0[37]: Copied! <pre>arr_sum = arr1 + arr1 # elementwise addition\narr_sum\n</pre> arr_sum = arr1 + arr1 # elementwise addition arr_sum Out[37]: <pre>array([50, 20, 36, 20, 32, 44, 28, 52])</pre> In\u00a0[38]: Copied! <pre>arr_cubed = arr1 ** 2 # elementwise exponentiation\narr_cubed\n</pre> arr_cubed = arr1 ** 2 # elementwise exponentiation arr_cubed Out[38]: <pre>array([625, 100, 324, 100, 256, 484, 196, 676])</pre> <p>Similarly, you can add a scalar to an array and NumPy will <code>broadcast</code> that operation on all the elements.</p> In\u00a0[39]: Copied! <pre>arr_cubed - 100 # element wise subtraction by a scalar\n</pre> arr_cubed - 100 # element wise subtraction by a scalar Out[39]: <pre>array([525,   0, 224,   0, 156, 384,  96, 576])</pre> In\u00a0[41]: Copied! <pre>arr1\n</pre> arr1 Out[41]: <pre>array([25, 10, 18, 10, 16, 22, 14, 26])</pre> In\u00a0[40]: Copied! <pre>np.dot(arr1, arr1)\n</pre> np.dot(arr1, arr1) Out[40]: <pre>2761</pre> <p>Above it automatically transposed the second array input to calculate the matrix multiplication of <code>1xn</code>x<code>nx1</code></p> In\u00a0[42]: Copied! <pre>arr_cubed[0] = 0\narr_cubed\n</pre> arr_cubed[0] = 0 arr_cubed Out[42]: <pre>array([  0, 100, 324, 100, 256, 484, 196, 676])</pre> In\u00a0[43]: Copied! <pre>arr_cubed / 0\n</pre> arr_cubed / 0 <pre>/Users/atma6951/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in true_divide\n  \"\"\"Entry point for launching an IPython kernel.\n/Users/atma6951/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n  \"\"\"Entry point for launching an IPython kernel.\n</pre> Out[43]: <pre>array([nan, inf, inf, inf, inf, inf, inf, inf])</pre> <p>Thus 0/0 = <code>nan</code> and num/0 = <code>inf</code></p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#array-operations-slicing-dicing-searching","title":"Array operations - slicing, dicing, searching\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_searching/#array-slicing","title":"Array slicing\u00b6","text":"<p>get elements using index like in a List</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#nd-array-slicing","title":"nD array slicing\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_searching/#array-dicing","title":"Array dicing\u00b6","text":""},{"location":"teaching_resources/numpy/numpy_array_searching/#array-broadcasting","title":"Array broadcasting\u00b6","text":"<p>NumPy allows bulk assigning values, just like in matlab</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#deep-copy","title":"Deep copy\u00b6","text":"<p>NumPy Arrays like Python objects are always shallow copied. Hence any modification made in derivative affects the source. Make deep copies using <code>copy()</code> method</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#array-searching","title":"Array searching\u00b6","text":"<p>Use matlab style array searching</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#compound-searching","title":"Compound searching\u00b6","text":"<p>Find elements within a range for instance:</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#math-operations-elemenwise","title":"Math operations - elemenwise\u00b6","text":"<p>NumPy has operators like <code>+</code>, <code>-</code>, <code>/</code>, <code>*</code> overloaded so you can add two matrices like scalars</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#math-operations-matrix-math","title":"Math operations - matrix math\u00b6","text":"<p>Use built-in functions for matrix operations</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#caveats","title":"Caveats\u00b6","text":"<p>Numpy does not throw errors for divide by zero or for 0/0. Intead it sets value to <code>inf</code> and <code>nan</code>.</p>"},{"location":"teaching_resources/numpy/numpy_array_searching/#universal-functions","title":"Universal functions\u00b6","text":"<p>Numpy has a bunch of universal functions that work on the array elements one at a time and allow arrays to be used or treated as scalars.</p> <p>Before writing a loop, look up the function list here</p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/","title":"Octave / MATLAB - lops, conditional processing, functions","text":"<p>Octave is an open source scientific computing package. It has a GUI, kernel and a programming language modeled after MATLAB. File extensions are <code>.m</code>. This page is a guide to the octave programming language.</p> <p></p> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#general-caveats","title":"General caveats","text":"<ul> <li>You end statements with a <code>;</code>. If not, the kernel does not throw an error, instead will echo the output of each line.</li> <li>If you do not catch the output of an operation in a variable, a default variable called <code>ans</code> is created and that holds the value. At any time, <code>ans</code> holds value of latest operation.</li> <li>Matlab / octave functions can return multiple values.</li> </ul>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#variables","title":"Variables","text":"<p>Variables can be assigned without initializing.</p> <pre><code>a = 1;\nb = [1,2,4]; % row vector\n% Use % for comments\nc = [5;6;7]; % col vector\n</code></pre> <p>2D arrays can be initialized as </p> <pre><code>&gt;&gt; a2d = [1,2,3;4,5,6;7,8,9] % ';' is used to separate rows.\na2d =\n   1   2   3\n   4   5   6\n   7   8   9\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#standard-matrix-creation-functions","title":"Standard matrix creation functions","text":"<pre><code>&gt;&gt; eye(3)  % identity\nans =\nDiagonal Matrix\n   1   0   0\n   0   1   0\n   0   0   1\n\n&gt;&gt; zeros(2,3)\nans =\n   0   0   0\n   0   0   0\n\n&gt;&gt; rand(2,5)  % uniform random numbers between 1,1\nans =\n   0.86289   0.18309   0.48379   0.36600   0.63669\n   0.32459   0.89961   0.79682   0.39961   0.15351\n\n&gt;&gt; randn(3,4)\nans =\n   1.151159   0.022623  -1.030988   0.460505\n  -0.386885  -0.188437   0.339421  -1.131755\n   0.573182   0.451552  -0.307362  -1.717639\n\n&gt;&gt; magic(3)  % creates matrix whose each row, col, diagonal sum to same value\nans =\n   8   1   6\n   3   5   7\n   4   9   2\n</code></pre>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#automatic-map-operations","title":"Automatic <code>map</code> operations","text":"<p>Octave is designed to work primarily with arrays and vectors. Thus when you pass an array or vector to a standard function or operator, it automatically maps the function on each element and runs it and returns either an array/vector or scalar, depending on the operation.</p> <pre><code>&gt;&gt; squares = [4,9,16,25,36,49];\n&gt;&gt; sqrt(squares) % returns a vector output\nans = 2   3   4   5   6   7\n\n&gt;&gt; min(squares)\nans =  4  % scalar output\n</code></pre> <p>When you run functions on a 2D matrix, they run on each column vector:</p> <pre><code>&gt;&gt; sum(magic(3)) % default is columnwise operation\nans =\n   15   15   15\n\n&gt;&gt; sum(magic(3), 2)  % does a row-wise operation\nans =\n\n   15\n   15\n   15\n\n&gt;&gt; prod(magic(3))  % multiply each element in the matrix. Here multiplies each in col vector\nans =\n   96   45   84\n\n&gt;&gt; max(magic(3))\nans =\n   8   9   7\n</code></pre> <p>Functions in Octave can return multiple values. For instance:</p> <pre><code>&gt;&gt; [val, index] = min(squares)\nval =  4\nindex =  1\n&gt;&gt;\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#automatic-broadcasting","title":"Automatic broadcasting","text":"<p>When you operate a scalar against a vector, matlab will automatically scale up that scalar into a vector and perform an element wise computation as shown below:</p> <pre><code>&gt;&gt; x=-2:0.7:2\nx = -2.0000   -1.3000   -0.6000    0.1000    0.8000    1.5000\n&gt;&gt; x+10\nans = 8.0000     8.7000     9.4000    10.1000    10.8000    11.5000\n&gt;&gt;\n</code></pre> <p>Similarly when you multiply, subtract or divide a scalar from a vector, it is automatically broadcasted.</p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#looping-through-a-matrix","title":"Looping through a matrix","text":"<p>The automatic map operations should have most things covered for you. Still, you might need to write a loop. Below is syntax of <code>for</code> loop.</p> <pre><code>for iterator=&lt;range&gt;,\n   operation1;\n   operation2;\n   condition check\n      operation3;\n      break;\n   condition check2\n      operation4;\n      continue;\nend;\n</code></pre> <p>Note the <code>,</code> at end of the line with <code>for</code>.</p> <pre><code>&gt;&gt; indices=1:10;\n&gt;&gt; for i=indices,\n       disp(i);\n   end;\n 1\n 2\n 3\n .\n .\n .\n</code></pre>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#while-loop","title":"While loop","text":"<p>The syntax for while loop is</p> <pre><code>while condition,\n   operation;\nend;\n</code></pre> <p>Once again, note the <code>,</code> at line 1.</p> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#array-multiplication","title":"Array multiplication","text":"<p>There are two types of multiplication  - element wise - which is accomplished by prefixing the operator with <code>.</code> such as: <code>.&lt;operator&gt;</code> or <code>.*</code>  - matrix multiplication - which will happen when you use <code>*</code>.</p> <p>Thus:</p> <pre><code>&gt;&gt; x\nx = -2.0000   -1.3000   -0.6000    0.1000    0.8000    1.5000\n\n&gt;&gt; x2 = [1,2,3,4,5,6]\nx2 = 1   2   3   4   5   6\n\n&gt;&gt; x.*x2\nans = -2.00000  -2.60000  -1.80000   0.40000   4.00000   9.00000\n&gt;&gt;\n</code></pre> <p>When you want to multiply two matrixes, you do:</p> <pre><code>&gt;&gt; x*transpose(x2)\nans =  7.0000\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#transposing-a-matrix","title":"Transposing a matrix","text":"<p>You can call the built-in <code>transpose()</code> function (shown previously) or use the <code>'</code> operator.</p> <pre><code>&gt;&gt; x2'\nans =\n   1\n   2\n   3\n   4\n   5\n   6\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#reshaping-matrixes","title":"Reshaping matrixes","text":"<p>You can reshape using the <code>reshape(&lt;arr&gt;, nrows, ncols)</code> function. Note, it flows elements column wise, followed by rows as shown below:</p> <pre><code>&gt;&gt; vec = 1:16;\n&gt;&gt; reshape(vec, 2,8)\nans =\n    1    3    5    7    9   11   13   15\n    2    4    6    8   10   12   14   16\n\n&gt;&gt; reshape(vec, 4,4)  % note how elements flow along columns, not rows.\nans =\n    1    5    9   13\n    2    6   10   14\n    3    7   11   15\n    4    8   12   16\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#range-functions","title":"Range functions","text":"<p>You can generate uniformly spaced vectors, using 2 methods. If you want uniformly spaced vectors, use the <code>:</code> operator. If you want a specific number of elements within a range, then use the <code>linspace</code> function.</p> <p>The <code>:</code> operator takes syntax <code>start:spacing:end</code></p> <pre><code>first_10_even_numbers = 0:2:21\nfirst_10_even_numbers = 0    2    4    6    8   10   12   14   16   18   20\n</code></pre> <p>The <code>linspace</code> function takes arguments <code>linspace(start, end, numPoints)</code> where start and end are included.</p> <pre><code>&gt;&gt; linspace(1,10,7)\nans = 1.0000    2.5000    4.0000    5.5000    7.0000    8.5000   10.0000\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#conditional-processing","title":"Conditional processing","text":"<p>Logical operators in octave are <code>&lt;, &gt;, &lt;=, &gt;=, ~, ==, ~=, &amp;, |</code>. Matlab returns <code>1</code> for True and <code>0</code> for False. Thus</p> <pre><code>&gt;&gt; heat = 74\nheat =  74\n&gt;&gt; heat &lt; 100\nans = 1\n&gt;&gt; heat &gt; 100\nans = 0\n&gt;&gt; heat == 74\nans = 1\n</code></pre> <p>Conditional operators also <code>map</code> on all elements of the vector. Thus to find elements in a vector greater than a threshold, do:</p> <pre><code>&gt;&gt; v1 = linspace(1,10,7);\n&gt;&gt; ind = v1 &gt; 5  % returns truth vector\nind = 0  0  0  1  1  1  1 \n&gt;&gt; vals = v1(ind) % use truth vector as index\nvals = 5.5000    7.0000    8.5000   10.0000\n</code></pre> <p>To find values that fall between a range:</p> <pre><code>&gt;&gt; v1(v1&lt;8 &amp; v1&gt;3)\nans = 4.0000   5.5000   7.0000\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#if-elseif-else-blocks","title":"<code>if-elseif-else</code> blocks","text":"<p>Jump to defining functions if you want to read that first.</p> <pre><code>function charge = parkingRates(hours)\n  %Calculates parking charge based on number of hours%\n  if hours &lt;=1\n    charge = 2;\n  elseif hours&lt;=8 &amp;&amp; hours&gt;1\n    charge = hours*0.75;\n  else\n    charge = 9;\n  end\nend\n</code></pre> <p>The <code>&amp;&amp;</code> operator is a performance opt that Matlab editor suggested. Else I would use only one of it.</p> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#styling-print-outputs","title":"Styling print outputs","text":"<p>Use <code>disp</code> to display. You can style print outputs with <code>sprintf</code> and sending the string composed to <code>disp()</code>.</p> <pre><code>&gt;&gt; vec1 = [2,3,4,5];\n&gt;&gt; disp(vec1)\n   2   3   4   5\n</code></pre> <pre><code>&gt;&gt; disp(sprintf(\"Mean of the array is %0.3f\", mean(vec1)))\nMean of the array is 3.500\n</code></pre> <p>Use <code>C</code> style statements within <code>sprintf</code>.</p> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#array-indexing-slicing-dicing","title":"Array indexing, slicing, dicing","text":"<p>Consider this <code>4x4</code> matrix:</p> <pre><code>&gt;&gt; readings=linspace(1,10,16);\n&gt;&gt; readings = reshape(readings, 4,4)\nreadings =\n\n    1.0000    3.4000    5.8000    8.2000\n    1.6000    4.0000    6.4000    8.8000\n    2.2000    4.6000    7.0000    9.4000\n    2.8000    5.2000    7.6000   10.0000\n&gt;&gt;\n</code></pre> <p>To get the first column vector, use <code>:,n</code> where <code>:</code> means all rows here and replace <code>n</code> with column number, starting with <code>1</code> instead of <code>0</code>.:</p> <pre><code>&gt;&gt; readings(:,1)  % Note: matlab is 1 indexed, not 0 indexed\nans =\n   1.0000\n   1.6000\n   2.2000\n   2.8000\n&gt;&gt;\n</code></pre> <p>Also note, I am using <code>()</code> to index arrays, not <code>[]</code> as in Python and other languages.</p> <p>To dice the array by pulling 2nd, 3rd rows, 2nd, 3rd columns, do the following:</p> <pre><code>&gt;&gt; readings([2,3], [2,3])\nans =\n   4.0000   6.4000\n   4.6000   7.0000\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#array-concatenation","title":"Array concatenation","text":"<p>You can concatenate along rows or columns using the <code>,</code> or <code>;</code> operators:</p> <pre><code>&gt;&gt; [eye(3), randn(3)]  % use , to concatenate along columns\nans =\n   1.00000   0.00000   0.00000   0.68098  -1.07370  -1.99950\n   0.00000   1.00000   0.00000   0.19421   0.06390   0.29864\n   0.00000   0.00000   1.00000  -0.03696  -0.24735  -0.18180\n\n&gt;&gt; [eye(3); randn(3)]  % use ; to concatenate along rows\nans =\n   1.00000   0.00000   0.00000\n   0.00000   1.00000   0.00000\n   0.00000   0.00000   1.00000\n   0.78756  -0.22399   0.59963\n  -0.93718   0.34779   0.27082\n  -2.00364  -0.45362  -1.47926\n&gt;&gt;\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#statistical-operations-on-arrays","title":"Statistical operations on arrays","text":"<p>Let us start with an array as below:</p> <pre><code>&gt;&gt; vec45 = reshape(linspace(1,10,20), 4,5)\nvec45 =\n    1.0000    2.8947    4.7895    6.6842    8.5789\n    1.4737    3.3684    5.2632    7.1579    9.0526\n    1.9474    3.8421    5.7368    7.6316    9.5263\n    2.4211    4.3158    6.2105    8.1053   10.0000\n&gt;&gt;\n</code></pre> <p>Find mean of each column</p> <pre><code>&gt;&gt; mean(vec45)\nans =   1.7105   3.6053   5.5000   7.3947   9.2895\n</code></pre> <p>Find mean of each row</p> <pre><code>&gt;&gt; mean(vec45,2)  % pass the dimension arg. Set it to 2 for mean along rows\nans =\n   4.7895\n   5.2632\n   5.7368\n   6.2105\n</code></pre> <p>Another way of doing this is to transpose the matrix and take regular mean.</p> <p>Find mean of entire matrix Turn the matrix into a column vector and pass that to the mean.</p> <pre><code>&gt;&gt; mean(vec45(:))\nans =  5.5000\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#writing-functions","title":"Writing functions","text":"<p>You call functions as </p> <pre><code>y = func_name(arg1, arg2);\n</code></pre> <p>The syntax to write functions is:</p> <pre><code>function [out1, out2...] = functionName(in1, in2, ...)\n    %Doc string%\n    statement1;\n    statement2;\n    out1 = statement;\n    out2 = statement;\n    ...\nend\n</code></pre> <p>There is no <code>return</code> statement. All variables you define in the out parameter array (or scalar) will get returned. Matlab allows you to return more than one variable.</p> <pre><code>function result = hmean(vec1)\n  %calculates harmonic mean of the given vector%\n  vec1 = vec1(:)'; %convert to vectors\n  reciprocals = 1./vec1;  %convert to reciprocals\n  sum_reciprocals = sum(reciprocals); % sum of reciprocals - the denominator\n\n  result = size(vec1)(2) / sum_reciprocals;\nendfunction\n\n&gt;&gt; hmean([2,3,4,5])\nans =  3.1169\n&gt;&gt;\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_loops_conditional_functions/#reusing-functions","title":"Reusing functions","text":"<p>A quick and easy way to reuse functions in other scripts is to put just the function (not anything else) in a separate <code>.m</code> file with the same name as the function. Matlab will automatically search for it and import it.</p> <p>To hack the search path, use <code>addpath('full_path')</code> to add a folder to Octave path. This way, you can load a function or module from a different directory.</p>"},{"location":"teaching_resources/octave/octave_plotting/","title":"Handling and Plotting data in Octave / MATLAB","text":""},{"location":"teaching_resources/octave/octave_plotting/#whats-in-memory","title":"What's in memory","text":"<p>To quickly view the variables in memory, use the <code>who</code> command. Use <code>whos</code> for detailed view</p> <pre><code>&gt;&gt; who\nVariables in the current scope:\nans       ar1       heat      readings  vec       vec1      vec44     vec45     x         y         z\n\n&gt;&gt; whos\nVariables in the current scope:\n   Attr Name          Size                     Bytes  Class\n   ==== ====          ====                     =====  =====\n        ans           1x78                        78  char\n        ar1           1x4                         32  double\n        heat          1x1                          8  double\n        readings      4x4                        128  double\n        vec           1x16                        24  double\n        vec1          1x4                         32  double\n        vec44         4x4                        128  double\n        vec45         4x5                        160  double\n        x             1x21                        24  double\n        y             1x21                       168  double\n        z             1x21                       168  double\n\nTotal is 218 elements using 950 bytes\n</code></pre> <p>To delete a variable from memory, use the <code>clear &lt;variable name&gt;</code> command. Just using the <code>clear</code> command will remove all variables.</p>"},{"location":"teaching_resources/octave/octave_plotting/#reading-data-files","title":"Reading data files","text":"<p>Read datasets with <code>load</code> command. <code>&gt;&gt; load('ex1data1.txt')</code> A variable with the name of the file is created in memory. You can use array slicing, dicing to move a part of this data into a new variable.</p> <pre><code>&gt;&gt; load('ex1data1.txt');\n&gt;&gt; subset = ex1data1(1:10,:)\nsubset =\n    6.1101   17.5920\n    5.5277    9.1302\n    8.5186   13.6620\n    7.0032   11.8540\n    5.8598    6.8233\n    8.3829   11.8860\n    7.4764    4.3483\n    8.5781   12.0000\n    6.4862    6.5987\n    5.0546    3.8166\n</code></pre>"},{"location":"teaching_resources/octave/octave_plotting/#saving-data-to-disk","title":"Saving data to disk","text":"<p>Use <code>save &lt;filename.extn&gt; &lt;variable&gt; &lt;-options&gt;</code> to save a variable to disk.</p> <pre><code>&gt;&gt; save 'subset.mat' subset\n</code></pre> <p>You can load this back into memory using the <code>load</code> command: <code>load('subset.mat')</code>. This time, since it is a <code>mat</code> file, Octave will load it with the original variable name.</p> <p>To save data in a human readable form, use <code>save &lt;filename.txt&gt; &lt;variable&gt; -ascii</code>.</p>"},{"location":"teaching_resources/octave/octave_plotting/#plotting","title":"Plotting","text":"<p>Most 2D plots can be accomplished using <code>plot(&lt;arr1&gt;, &lt;arr2&gt;, 'srt:options')</code> function.</p> <p></p>"},{"location":"teaching_resources/octave/octave_plotting/#line-plots","title":"Line plots","text":"<pre><code>plot(x,y); % will plot in a new window\n</code></pre> <p>We can customize the appearance of ticks and line by passing them as a string. For instance, <code>r:*</code> will make lines in red, <code>*</code> for points and <code>:</code> for dotted lines.</p> <p>You can also customize the title, labels, legend as shown:</p> <pre><code>&gt;&gt; plot(x,y);\n&gt;&gt; xlabel('time[x]');\n&gt;&gt; ylabel('y=x^2');\n&gt;&gt; legend('y(x)')\n&gt;&gt; title('Function of time')\n</code></pre> <p></p>"},{"location":"teaching_resources/octave/octave_plotting/#overlaying-plots","title":"Overlaying plots","text":"<p>To overlay multiple plots on the same frame, use <code>hold on</code> command.</p> <pre><code>&gt;&gt; z = x.^3;\n&gt;&gt; plot(x,y, 'r:o')\n&gt;&gt; hold on\n&gt;&gt; plot(x,z, 'g--*')\n</code></pre> <p></p> <p>You can also plot multiple plots in the same command as <code>plot(x,y, x,z)</code> which will overlay both <code>y</code> and <code>z</code> on the same plot window.</p> <p>To close the current figure, call the <code>close</code> command.</p>"},{"location":"teaching_resources/octave/octave_plotting/#printing-plots-to-disk","title":"Printing plots to disk","text":"<p>To print to disk, use the <code>print</code> command as <code>print -dpng 'myplot.png'</code>. This will print the current plot to disk.</p>"},{"location":"teaching_resources/octave/octave_plotting/#multiple-plot-windows","title":"Multiple plot windows","text":"<p>Use the <code>figure(n)</code> command to create new plot windows:</p> <pre><code>&gt;&gt; figure(1); plot(x,y1);\n&gt;&gt; figure(2); plot(y1,y2);\n</code></pre> <p>will open 2 plot windows, one for each plot command.</p>"},{"location":"teaching_resources/octave/octave_plotting/#subplots","title":"Subplots","text":"<p>Use the <code>subplot(nrows, ncols, current_cell)</code> command to create and activate a plot window:</p> <pre><code>&gt;&gt; subplot(1,2,1);\n&gt;&gt; plot(x,y1);\n&gt;&gt; axis([0,1]);  % Sets axis limits. Syntax is axis([xmin, xmax, ymin, ymax])\n&gt;&gt; subplot(1,2,2);\n&gt;&gt; plot(x,y2);\n&gt;&gt; legend('y2');\n</code></pre> <p>which produces:</p> <p></p>"},{"location":"teaching_resources/octave/octave_plotting/#visualizing-matrices-as-images","title":"Visualizing matrices as images","text":"<p>To quickly 'see' a matrix as a color coded image, use <code>imagesc</code>:</p> <pre><code>&gt;&gt; x=randn(900,1);  % produces a standard normal dist, mean=0\n&gt;&gt; size(x)\nans =\n   900     1\n&gt;&gt; x2d= reshape(x,30,30); % turns this vector to a 2d matrix\n&gt;&gt; imagesc(x2d);  % turns this matrix to an image\n&gt;&gt; colorbar;  % adds a colorbar legend\n</code></pre> <p></p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/","title":"Data visualization with Pandas","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import pandas as pd import numpy as np %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns In\u00a0[2]: Copied! <pre>df1 = pd.read_csv('/Users/atma6951/Documents/code/pychakras/pychakras/udemy_ml_bootcamp/Python-for-Data-Visualization/Pandas Built-in Data Viz/df1', index_col=0)\ndf2 = pd.read_csv('/Users/atma6951/Documents/code/pychakras/pychakras/udemy_ml_bootcamp/Python-for-Data-Visualization/Pandas Built-in Data Viz/df2')\n</pre> df1 = pd.read_csv('/Users/atma6951/Documents/code/pychakras/pychakras/udemy_ml_bootcamp/Python-for-Data-Visualization/Pandas Built-in Data Viz/df1', index_col=0) df2 = pd.read_csv('/Users/atma6951/Documents/code/pychakras/pychakras/udemy_ml_bootcamp/Python-for-Data-Visualization/Pandas Built-in Data Viz/df2') In\u00a0[3]: Copied! <pre>df1.head()\n</pre> df1.head() Out[3]: A B C D 2000-01-01 1.339091 -0.163643 -0.646443 1.041233 2000-01-02 -0.774984 0.137034 -0.882716 -2.253382 2000-01-03 -0.921037 -0.482943 -0.417100 0.478638 2000-01-04 -1.738808 -0.072973 0.056517 0.015085 2000-01-05 -0.905980 1.778576 0.381918 0.291436 In\u00a0[4]: Copied! <pre>df2.head()\n</pre> df2.head() Out[4]: a b c d 0 0.039762 0.218517 0.103423 0.957904 1 0.937288 0.041567 0.899125 0.977680 2 0.780504 0.008948 0.557808 0.797510 3 0.672717 0.247870 0.264071 0.444358 4 0.053829 0.520124 0.552264 0.190008 In\u00a0[8]: Copied! <pre>df1.plot(x='A', kind='hist')\n</pre> df1.plot(x='A', kind='hist') Out[8]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1142ece80&gt;</pre> In\u00a0[10]: Copied! <pre>df1['A'].plot.hist(bins=30)\n</pre> df1['A'].plot.hist(bins=30) Out[10]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1169717f0&gt;</pre> In\u00a0[7]: Copied! <pre>df1.hist()\n</pre> df1.hist() Out[7]: <pre>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x11a872eb8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11abd27f0&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x11abf7e80&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11ac27550&gt;]],\n      dtype=object)</pre> <p>In reality, you have a lot more columns. You can prettify the above by creating a layout and figsize:</p> In\u00a0[8]: Copied! <pre>ax_list = df1.hist(bins=25, layout=(2,2), figsize=(7,7))\nplt.tight_layout()\n</pre> ax_list = df1.hist(bins=25, layout=(2,2), figsize=(7,7)) plt.tight_layout() In\u00a0[14]: Copied! <pre>ax_list = df1.hist(bins=25, sharex=True, sharey=True, layout=(1,4), figsize=(15,4))\n</pre> ax_list = df1.hist(bins=25, sharex=True, sharey=True, layout=(1,4), figsize=(15,4)) In\u00a0[15]: Copied! <pre>ax_list = df1.hist(bins=25, sharex=True, sharey=True, layout=(2,2), figsize=(8,8))\n</pre> ax_list = df1.hist(bins=25, sharex=True, sharey=True, layout=(2,2), figsize=(8,8)) In\u00a0[13]: Copied! <pre>plt.style.use('dark_background')\ndf2.plot.area()\n</pre> plt.style.use('dark_background') df2.plot.area() Out[13]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x116b35f98&gt;</pre> In\u00a0[14]: Copied! <pre>plt.style.use('fivethirtyeight')\ndf2.plot.bar()\n</pre> plt.style.use('fivethirtyeight') df2.plot.bar() Out[14]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x116c92cc0&gt;</pre> In\u00a0[18]: Copied! <pre>#reset the style\nplt.style.use('default')\n\n# pass figsize to the matplotlib backend engine and `lw` is line width\ndf1.plot.line(x=df1.index, y='A', figsize=(12,2), lw=1)\n</pre> #reset the style plt.style.use('default')  # pass figsize to the matplotlib backend engine and `lw` is line width df1.plot.line(x=df1.index, y='A', figsize=(12,2), lw=1) Out[18]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x116f61f98&gt;</pre> In\u00a0[21]: Copied! <pre>df1.plot.scatter(x='A', y='B',c='C', cmap='coolwarm')\n</pre> df1.plot.scatter(x='A', y='B',c='C', cmap='coolwarm') Out[21]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1176e09b0&gt;</pre> In\u00a0[22]: Copied! <pre># you could specify size s='c' however the points come out tiny.\n# had to scale it by 100, hence using actual series data and not the column name\ndf2.plot.scatter(x='a',y='b', s=df2['c']*100)\n</pre> # you could specify size s='c' however the points come out tiny. # had to scale it by 100, hence using actual series data and not the column name df2.plot.scatter(x='a',y='b', s=df2['c']*100) Out[22]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x117440f60&gt;</pre> In\u00a0[23]: Copied! <pre>df1['A'].plot.kde()\n</pre> df1['A'].plot.kde() Out[23]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x117a9d860&gt;</pre> <p>Visualize the density of all columns in one plot</p> In\u00a0[24]: Copied! <pre>df1.plot.kde()\n</pre> df1.plot.kde() Out[24]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x117d5e048&gt;</pre> In\u00a0[26]: Copied! <pre>df2.plot.density() #I think density is an alias to KDE\n</pre> df2.plot.density() #I think density is an alias to KDE Out[26]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1180b45f8&gt;</pre> In\u00a0[2]: Copied! <pre>registrant_df = pd.read_csv('./registrant.csv')\nregistrant_df.head()\n</pre> registrant_df = pd.read_csv('./registrant.csv') registrant_df.head() Out[2]: Unnamed: 0 Registration Date Country Organization Current customer? What would you like to learn? 0 0 11/08/2019 06:09 PM EST Jamaica The University of the West Indies NaN NaN 1 1 11/08/2019 06:09 PM EST Japan iLand6 Co.,Ltd. no I am interested ArcGIS. 2 2 11/08/2019 05:56 PM EST Canada Safe Software Inc yes data science workflos 3 3 11/08/2019 05:51 PM EST Canada Le Groupe GeoInfo Inc yes general information 4 4 11/08/2019 05:26 PM EST Canada Safe Software Inc. NaN NaN <p>Now, let us plot the responses from the column <code>What would you like to learn?</code> as a word cloud. First, we need to turn the series into a paragraph.</p> In\u00a0[5]: Copied! <pre>obj_series = registrant_df['What would you like to learn?'].dropna()\nobj_list = list(obj_series)\nobj_string = ' '.join(obj_list)\nobj_string\n</pre> obj_series = registrant_df['What would you like to learn?'].dropna() obj_list = list(obj_series) obj_string = ' '.join(obj_list) obj_string Out[5]: <pre>\"I am interested ArcGIS. data science workflos general information integration of arcgis and jupyter notebooks Using tools like pandas to perform ETL functions on GIS data; reading/writing to/from spatial data frames to GIS formats quickly working with Jupyter notebooks More use cases for Jupytr notebooks in ArcGIS Datascience and GIS Integration, ArcGIS Deep learning concepts ands evaluation How to carefully clean and prepare data for statistical computations The jupyter integration better use of the web environment via Jupyter, including how to share with colleagues, etc Notebook stuff All about ESRI: Modellization, Land use land cover mapping, mapping land degradation, mapping soil erosion using this for community projects Jupyter Integration of Jupyter Notebooks to workflows all the things Anything Deep Learning with Remote Sensing use python Improved humanitarian data collection, analysis and visualization Process Flow The basics How to use Jupyter and ArcGIS notebooks to facilitate efficient workflows Jupiter's analytical capability Methods for automating workflows, increasing accuracy and documenting methods Geoanalytics How to use ArcGIS and Jupyter for Geospatial science data analysis More basics about Jupyter and the Esri Notebooks tool too. Workflow to clean, analyse and visualize data from start to finish how to discuss with executives and professors to use this collaboration platform How to use Jupyter and ArcGIS API for Python Any thing new. open source Python libraries . Notebook Data Science for Geospatial Analysis How we can best use Jupyter in our existing and future solutions How to use Jupyter notebook. More details about Jupyter using python with arcgis Learn about using Jupyter notebooks in spatial analysis How to link between ArcGIS and Jupyter Geospatial analysis using python More about how to use Jupiter Notebooks. Jupyter notebooks a bit better colleague mail python scripting Use of Jupyter Potential for analysis workflows notebook, jupyter Use of Jupyter Notebooks stuff Better ways to integrate Python into engineering workflows throughout our company whats new About Jupyter How to setup arcgis and jupyter notebooks Setting up Jupiter notebooks / environments management install and environment set up, licensing Python Programming, Data Science, Machine Learning Data Science tools in ArcPy How to combine ArcGIS Tools and Jupyter Data Science techniques things to make my job more efficient Please provide examples with data items that don't expires (like the majority of those on your developer examples website) Data interoperability opportunities and constraints between ESRI's proprietary data formats and Python open source standards In general, more about data science The integration between the two platforms What is and how to use ArcGIS Notebooks and Jupyter Use case to notebooks as developer How to Use ArcGIS and Jupyter use Jupyter to increase transparency and build reproducible research one of the topics I have been lately exploring is how to make the maps web-enabled? More about Jupyter Notebooks I'm familiar with Jupyter but I'd like to know best practices Geospatial data science Geospatial Datascienc How Jupyter can help my organization. the things i don't know that i don't know. awareness How to use the Jupyter Notebook environment to perform analysis that's not available in ArcGIS Pro because of licensing. Data Science and use of python Looking to improve my data analysis capabilities and how I can integrate into ArcGIS more about Juypter notebooks and how it can fit into ESRI workflows How to use the Jupyter notebooks version of ESRI and what do I need to get started within my company... tools for extracting centreline from real time point data(i.e. cell phone locations, AIS locations) I would like to see if these workflows would be applicable to my job. general knowledge What notebooks and Jupyter can do for disaster response. Data analytics Content as advertised How to apply ArcGIS Notebooks DATA SCIENCE WITH GIS Geospatial data science workflow analysis Current trends in data science I would like to learn both how I can improve my GIS skills and how to use other software relevant to GIS and mapping. More about notebooks and GIS All related to this topics Jupyter usage General knowledge About Notebooks about the topic How to implement ArcGIS notebooks at an enterrpise scale. What it says! initiation to data science applied to geomatics exmples of application of ArcGIS with Jupyter data science, jupyter noteboook\"</pre> In\u00a0[12]: Copied! <pre>from wordcloud import WordCloud\nwc = WordCloud(width=1000, height=600, background_color='white')\n</pre> from wordcloud import WordCloud wc = WordCloud(width=1000, height=600, background_color='white') In\u00a0[13]: Copied! <pre>obj_wc_img = wc.generate_from_text(obj_string)\n</pre> obj_wc_img = wc.generate_from_text(obj_string) In\u00a0[14]: Copied! <pre>plt.figure(figsize=(20,10))\nplt.imshow(obj_wc_img, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('What would you like to learn?');\n</pre> plt.figure(figsize=(20,10)) plt.imshow(obj_wc_img, interpolation=\"bilinear\") plt.axis('off') plt.title('What would you like to learn?');"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#data-visualization-with-pandas","title":"Data visualization with Pandas\u00b6","text":"<p>Pandas implements some high level plotting functions using matplotlib. Note If you have seaborn imported, pandas will relay the plotting through seaborn and you get better looking plots for the same data and commands.</p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#read-some-data","title":"Read some data\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_data_viz_1/#3-ways-of-calling-plot-from-a-dataframe","title":"3 ways of calling plot from a DataFrame\u00b6","text":"<ul> <li><code>df.plot()</code> and specify the plot type, the X and Y columns etc</li> <li><code>df.plot.hist()</code> calling plot in OO fashion. Only specify teh X and Y and color or size columns</li> <li><code>df['column'].plot.plotname()</code> - calling plot on a series</li> </ul> <p>Types of plot that can be called: area, bar, line, scatter, box, hexbin, kde etc.</p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#ways-of-plotting-histogram","title":"Ways of plotting histogram\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_data_viz_1/#plotting-a-histogram-of-all-numeric-columns-in-the-dataframe","title":"Plotting a histogram of all numeric columns in the dataframe:\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_data_viz_1/#plotting-histogram-of-all-columns-and-sharing-axes","title":"Plotting histogram of all columns and sharing axes\u00b6","text":"<p>The chart above might make more sense if you shared the X as well as Y axes for different columns. This helps in comparing the distribution of values visually.</p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#backgrounds","title":"Backgrounds\u00b6","text":"<p>You can specify dark or white background and style info to the matplotlib that is used behind the scenes.</p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#area-plot","title":"Area plot\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_data_viz_1/#bar-chart","title":"Bar chart\u00b6","text":"<p>Another style is <code>fivethirtyeight</code></p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#line-plot","title":"Line plot\u00b6","text":"<p>This is suited for time series data</p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#scatter-plot","title":"Scatter plot\u00b6","text":"<p>Use <code>colormap</code> or <code>size</code> to bring in a visualize a 3rd variable in your scatter</p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#kde-plots","title":"KDE plots\u00b6","text":"<p>To visualize the density of data</p>"},{"location":"teaching_resources/pandas/pandas_data_viz_1/#making-wordclouds-from-text-fields","title":"Making wordclouds from text fields\u00b6","text":"<p>Word clouds are a great way to visualize frequency of certain terms that appear in the data set. This is accomplished using the library wordcloud. You can install it as</p> <pre><code>conda install -c conda-forge wordcloud\n</code></pre>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/","title":"Pandas - DataFrame creation and exploration","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[2]: Copied! <pre>#from a list\nl1 = [1,2,3,4,5]\nser1 = pd.Series(data = l1)  #when you dont specify labels for index, it is autogenerated\nser1\n</pre> #from a list l1 = [1,2,3,4,5] ser1 = pd.Series(data = l1)  #when you dont specify labels for index, it is autogenerated ser1 Out[2]: <pre>0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64</pre> In\u00a0[3]: Copied! <pre>#from a numpy array\narr1 = np.array(l1)\nl2 = ['a', 'b', 'c','e', 'd']\nser2 = pd.Series(data=arr1, index=l2) #indices can of any data type, here string\nser2\n</pre> #from a numpy array arr1 = np.array(l1) l2 = ['a', 'b', 'c','e', 'd'] ser2 = pd.Series(data=arr1, index=l2) #indices can of any data type, here string ser2 Out[3]: <pre>a    1\nb    2\nc    3\ne    4\nd    5\ndtype: int32</pre> In\u00a0[4]: Copied! <pre>#from a dictionary\nd1 = {'usa':1, 'india':2, 'germany':3, 'japan':'china', 'china':4}\nser3 = pd.Series(d1)\nser3\n</pre> #from a dictionary d1 = {'usa':1, 'india':2, 'germany':3, 'japan':'china', 'china':4} ser3 = pd.Series(d1) ser3 Out[4]: <pre>china          4\ngermany        3\nindia          2\njapan      china\nusa            1\ndtype: object</pre> <p>Thus Series can have different datatypes.</p> In\u00a0[5]: Copied! <pre>ser1a = pd.Series(l1)\nser1 + ser1a #each individual element with matching index/label is summed\n</pre> ser1a = pd.Series(l1) ser1 + ser1a #each individual element with matching index/label is summed Out[5]: <pre>0     2\n1     4\n2     6\n3     8\n4    10\ndtype: int64</pre> <p>When labels dont match, it puts a nan. Thus when two series are added, you may or may not get the same number of elements</p> In\u00a0[6]: Copied! <pre>ser1 + ser3\n</pre> ser1 + ser3 Out[6]: <pre>0          NaN\n1          NaN\n2          NaN\n3          NaN\n4          NaN\nchina      NaN\ngermany    NaN\nindia      NaN\njapan      NaN\nusa        NaN\ndtype: object</pre> In\u00a0[2]: Copied! <pre>arr1 = np.random.rand(4,4)\narr1\n</pre> arr1 = np.random.rand(4,4) arr1 Out[2]: <pre>array([[ 0.13430231,  0.62520675,  0.97098126,  0.71760504],\n       [ 0.71376565,  0.77318189,  0.05968879,  0.45089927],\n       [ 0.05898965,  0.90430093,  0.43148674,  0.08768307],\n       [ 0.50989094,  0.50103695,  0.24427914,  0.76313491]])</pre> In\u00a0[3]: Copied! <pre>row_lables = ['Car1', 'Car2', 'Car3', 'Car4']\ncol_labels = ['reliability', 'cost', 'competition', 'halflife']\n\n#create a dataframe\ndf1 = pd.DataFrame(data=arr1, index=row_lables, columns=col_labels)\ndf1\n</pre> row_lables = ['Car1', 'Car2', 'Car3', 'Car4'] col_labels = ['reliability', 'cost', 'competition', 'halflife']  #create a dataframe df1 = pd.DataFrame(data=arr1, index=row_lables, columns=col_labels) df1 Out[3]: reliability cost competition halflife Car1 0.134302 0.625207 0.970981 0.717605 Car2 0.713766 0.773182 0.059689 0.450899 Car3 0.058990 0.904301 0.431487 0.087683 Car4 0.509891 0.501037 0.244279 0.763135 In\u00a0[15]: Copied! <pre># Accessing a whole column\ndf1['reliability']\n</pre> # Accessing a whole column df1['reliability'] Out[15]: <pre>Car1    0.894051\nCar2    0.014127\nCar3    0.351610\nCar4    0.601929\nName: reliability, dtype: float64</pre> In\u00a0[16]: Copied! <pre>#can access as a property, but this is not advisable \n#since it can clobber builtin methods and properties\ndf1.reliability\n</pre> #can access as a property, but this is not advisable  #since it can clobber builtin methods and properties df1.reliability Out[16]: <pre>Car1    0.894051\nCar2    0.014127\nCar3    0.351610\nCar4    0.601929\nName: reliability, dtype: float64</pre> In\u00a0[18]: Copied! <pre>df1.loc['Car4']\n</pre> df1.loc['Car4'] Out[18]: <pre>reliability    0.601929\ncost           0.854320\ncompetition    0.391956\nhalflife       0.759363\nName: Car4, dtype: float64</pre> In\u00a0[19]: Copied! <pre>type(df1.loc['Car3'])\n</pre> type(df1.loc['Car3']) Out[19]: <pre>pandas.core.series.Series</pre> In\u00a0[21]: Copied! <pre>#get first row, first col\nval1 = df1.iloc[0,0]\nprint(val1)\nprint(type(val1))\n</pre> #get first row, first col val1 = df1.iloc[0,0] print(val1) print(type(val1)) <pre>0.894051123737\n&lt;class 'numpy.float64'&gt;\n</pre> In\u00a0[23]: Copied! <pre>#get full first row\nval2 = df1.iloc[0,:]\nval2\n</pre> #get full first row val2 = df1.iloc[0,:] val2 Out[23]: <pre>reliability    0.894051\ncost           0.849727\ncompetition    0.538400\nhalflife       0.863986\nName: Car1, dtype: float64</pre> In\u00a0[24]: Copied! <pre>type(val2)\n</pre> type(val2) Out[24]: <pre>pandas.core.series.Series</pre> In\u00a0[26]: Copied! <pre>#Get cost and competition of cars 2,3\ndf1.loc[['Car2', 'Car3'], ['cost', 'competition']]\n</pre> #Get cost and competition of cars 2,3 df1.loc[['Car2', 'Car3'], ['cost', 'competition']] Out[26]: cost competition Car2 0.935368 0.719570 Car3 0.659950 0.605077 <p>With index number, dice using</p> <pre><code>DataFrameObj.iloc[[row_indices], [col_indices]]</code></pre> In\u00a0[27]: Copied! <pre>df1.iloc[[1,2], [1,2]]\n</pre> df1.iloc[[1,2], [1,2]] Out[27]: cost competition Car2 0.935368 0.719570 Car3 0.659950 0.605077 In\u00a0[9]: Copied! <pre>df1\n</pre> df1 Out[9]: reliability cost competition halflife Car1 0.776415 0.435083 0.236151 0.169087 Car2 0.790403 0.987459 0.370570 0.734146 Car3 0.884783 0.233803 0.691639 0.725398 Car4 0.693038 0.716824 0.766937 0.490821 In\u00a0[10]: Copied! <pre># find cars with reliability &gt; 0.85\ndf1['reliability'] &gt; 0.85\n</pre> # find cars with reliability &gt; 0.85 df1['reliability'] &gt; 0.85 Out[10]: <pre>Car1    False\nCar2    False\nCar3     True\nCar4    False\nName: reliability, dtype: bool</pre> In\u00a0[11]: Copied! <pre>#to get the car select the data elements using the bool series\ndf1[df1['reliability'] &gt; 0.85]\n</pre> #to get the car select the data elements using the bool series df1[df1['reliability'] &gt; 0.85] Out[11]: reliability cost competition halflife Car3 0.884783 0.233803 0.691639 0.725398 In\u00a0[20]: Copied! <pre>#To get only the car name, which in this case is the index\ndf1[df1['reliability'] &gt; 0.85].index[0]\n</pre> #To get only the car name, which in this case is the index df1[df1['reliability'] &gt; 0.85].index[0] Out[20]: <pre>'Car3'</pre> In\u00a0[21]: Copied! <pre>#to get the actual value of reliablity for this car\ndf1[df1['reliability'] &gt; 0.85]['reliability']\n</pre> #to get the actual value of reliablity for this car df1[df1['reliability'] &gt; 0.85]['reliability'] Out[21]: <pre>Car3    0.884783\nName: reliability, dtype: float64</pre> In\u00a0[22]: Copied! <pre># get both reliability and cost\ndf1[df1['reliability'] &gt; 0.85][['reliability', 'cost']]\n</pre> # get both reliability and cost df1[df1['reliability'] &gt; 0.85][['reliability', 'cost']] Out[22]: reliability cost Car3 0.884783 0.233803 In\u00a0[24]: Copied! <pre>#select cars that have reliability &gt; 0.7 but competition less than 0.5\ndf1[(df1['reliability'] &gt; 0.7) &amp; (df1['competition'] &lt; 0.5)]\n</pre> #select cars that have reliability &gt; 0.7 but competition less than 0.5 df1[(df1['reliability'] &gt; 0.7) &amp; (df1['competition'] &lt; 0.5)] Out[24]: reliability cost competition halflife Car1 0.776415 0.435083 0.236151 0.169087 Car2 0.790403 0.987459 0.370570 0.734146 In\u00a0[26]: Copied! <pre># select cars that have half life &gt; 0.5 or competition &lt; 0.4\ndf1[(df1['halflife'] &gt; 0.5) | (df1['competition'] &lt; 0.4)]\n</pre> # select cars that have half life &gt; 0.5 or competition &lt; 0.4 df1[(df1['halflife'] &gt; 0.5) | (df1['competition'] &lt; 0.4)] Out[26]: reliability cost competition halflife Car1 0.776415 0.435083 0.236151 0.169087 Car2 0.790403 0.987459 0.370570 0.734146 Car3 0.884783 0.233803 0.691639 0.725398 In\u00a0[4]: Copied! <pre>#add full life column\ndf1['full_life'] = df1['halflife'] * 2 #similar to array, series broadcast multiplication\ndf1\n</pre> #add full life column df1['full_life'] = df1['halflife'] * 2 #similar to array, series broadcast multiplication df1 Out[4]: reliability cost competition halflife full_life Car1 0.134302 0.625207 0.970981 0.717605 1.435210 Car2 0.713766 0.773182 0.059689 0.450899 0.901799 Car3 0.058990 0.904301 0.431487 0.087683 0.175366 Car4 0.509891 0.501037 0.244279 0.763135 1.526270 In\u00a0[5]: Copied! <pre>df1.drop('full_life', axis=1, inplace=False)\n</pre> df1.drop('full_life', axis=1, inplace=False) Out[5]: reliability cost competition halflife Car1 0.134302 0.625207 0.970981 0.717605 Car2 0.713766 0.773182 0.059689 0.450899 Car3 0.058990 0.904301 0.431487 0.087683 Car4 0.509891 0.501037 0.244279 0.763135 In\u00a0[6]: Copied! <pre>df1.drop('Car3') #all else is the default\n</pre> df1.drop('Car3') #all else is the default Out[6]: reliability cost competition halflife full_life Car1 0.134302 0.625207 0.970981 0.717605 1.435210 Car2 0.713766 0.773182 0.059689 0.450899 0.901799 Car4 0.509891 0.501037 0.244279 0.763135 1.526270 <p>Drop a row based on a condition.</p> In\u00a0[7]: Copied! <pre>df1.drop(df1[df1['cost'] &gt; 0.65].index, inplace=False)\n</pre> df1.drop(df1[df1['cost'] &gt; 0.65].index, inplace=False) Out[7]: reliability cost competition halflife full_life Car1 0.134302 0.625207 0.970981 0.717605 1.43521 Car4 0.509891 0.501037 0.244279 0.763135 1.52627 In\u00a0[30]: Copied! <pre>#set car names as index for the data frame\ncar_names = 'altima outback taurus mustang'.split()\ncar_names\n</pre> #set car names as index for the data frame car_names = 'altima outback taurus mustang'.split() car_names Out[30]: <pre>['altima', 'outback', 'taurus', 'mustang']</pre> In\u00a0[36]: Copied! <pre>df1['car_names'] = car_names\ndf1\n</pre> df1['car_names'] = car_names df1 Out[36]: reliability cost competition halflife car_names Car1 0.776415 0.435083 0.236151 0.169087 altima Car2 0.790403 0.987459 0.370570 0.734146 outback Car3 0.884783 0.233803 0.691639 0.725398 taurus Car4 0.693038 0.716824 0.766937 0.490821 mustang In\u00a0[37]: Copied! <pre>df_new_index = df1.set_index(keys= df1['car_names'], inplace=False)\ndf_new_index\n</pre> df_new_index = df1.set_index(keys= df1['car_names'], inplace=False) df_new_index Out[37]: reliability cost competition halflife car_names car_names altima 0.776415 0.435083 0.236151 0.169087 altima outback 0.790403 0.987459 0.370570 0.734146 outback taurus 0.884783 0.233803 0.691639 0.725398 taurus mustang 0.693038 0.716824 0.766937 0.490821 mustang <p>Note, the old index is lost.</p> In\u00a0[38]: Copied! <pre>#reset df1 index to numerals and convert existing to a column\ndf1.reset_index()\n</pre> #reset df1 index to numerals and convert existing to a column df1.reset_index() Out[38]: index reliability cost competition halflife car_names 0 Car1 0.776415 0.435083 0.236151 0.169087 altima 1 Car2 0.790403 0.987459 0.370570 0.734146 outback 2 Car3 0.884783 0.233803 0.691639 0.725398 taurus 3 Car4 0.693038 0.716824 0.766937 0.490821 mustang"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#pandas-dataframe-creation-and-exploration","title":"Pandas - DataFrame creation and exploration\u00b6","text":"<p>Pandas is Python Data Analysis library. Series and Dataframes are major data structures in Pandas. Pandas is built on top of NumPy arrays.</p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#series","title":"Series\u00b6","text":"<p>Series is 1 dimensional data structure. It is similar to numpy array, but each data point has a label in the place of an index.</p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#create-a-series","title":"Create a series\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#operations-on-series","title":"Operations on series\u00b6","text":"<p>You can add, multiply and other numerical opertions on Series just like on numpy arrays.</p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#dataframes","title":"DataFrames\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#creating-dataframes","title":"Creating dataFrames\u00b6","text":"<p>Pandas DataFrames are built on top of Series. It looks similar to a NumPy array, but has labels for both columns and rows.</p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#slicing-and-dicing-dataframes","title":"Slicing and dicing DataFrames\u00b6","text":"<p>You can access DataFrames similar to Series and slice it similar to NumPy arrays</p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#access-columns","title":"Access columns\u00b6","text":"<pre><code>DataFrameObj['column_name'] ==&gt; returns a pandas.core.series.Series</code></pre>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#access-rows","title":"Access rows\u00b6","text":"<pre><code>DataFrameobj.loc['row_label'] also returns a Series. Notice the .loc</code></pre>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#accessing-using-index-number","title":"Accessing using index number\u00b6","text":"<p>If you don't know the labels, but know the index like in an array, use <code>iloc</code> and pass the index number.</p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#dicing-dataframes","title":"Dicing DataFrames\u00b6","text":"<p>Dicing using labels ==&gt; use <code>DataFrameObj.loc[[row_labels],[col_labels]]</code></p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#conditional-selection","title":"Conditional selection\u00b6","text":"<p>When running a condition on a DataFrame, you are returned a Bool dataframe.</p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#chaining-conditions","title":"Chaining conditions\u00b6","text":"<p>In a Pythonic way, you can chain conditions</p> <pre><code>df[df condition][selection][selection]</code></pre>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#multiple-conditions","title":"Multiple conditions\u00b6","text":"<p>You can select dataframe elements with multiple conditions. Note cannot use Python <code>and</code> , <code>or</code>. Instead use <code>&amp;</code>, <code>|</code></p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#operations-on-dataframes","title":"Operations on DataFrames\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#adding-new-columns","title":"Adding new columns\u00b6","text":"<p>Create new columns just like adding a kvp to a dictionary.</p> <pre><code>DataFrameObj['new_col'] = Series</code></pre>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#dropping-rows-and-columns","title":"Dropping rows and columns\u00b6","text":"<pre><code>DataFrameObj.drop(label, axis, inplace=True / False)</code></pre> <p>Row labels are <code>axis = 0</code> and columns are <code>axis = 1</code></p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#dataframe-index","title":"DataFrame Index\u00b6","text":"<p>So far, <code>Car1</code>, <code>Car2</code>.. is the index for rows. If you would like to set a different column as an index, use <code>set_index</code>. If you want to make index as a column rather, and use numerals for index, use <code>reset_index</code></p>"},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#set-index","title":"Set index\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_df_creation_exploration/#rest-index","title":"Rest index\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/","title":"Pandas - Indices, Sorting, Merging","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[2]: Copied! <pre># Index Levels\noutside = ['G1','G1','G1','G2','G2','G2']\ninside = [1,2,3,1,2,3]\nhier_index = list(zip(outside,inside)) #create a list of tuples\nhier_index\n</pre> # Index Levels outside = ['G1','G1','G1','G2','G2','G2'] inside = [1,2,3,1,2,3] hier_index = list(zip(outside,inside)) #create a list of tuples hier_index Out[2]: <pre>[('G1', 1), ('G1', 2), ('G1', 3), ('G2', 1), ('G2', 2), ('G2', 3)]</pre> In\u00a0[3]: Copied! <pre>#create a multiindex\nhier_index = pd.MultiIndex.from_tuples(hier_index)\nhier_index\n</pre> #create a multiindex hier_index = pd.MultiIndex.from_tuples(hier_index) hier_index Out[3]: <pre>MultiIndex(levels=[['G1', 'G2'], [1, 2, 3]],\n           labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]])</pre> In\u00a0[4]: Copied! <pre># Create a dataframe (6,2) with multi level index\ndf = pd.DataFrame(np.random.randn(6,2),index=hier_index,columns=['A','B'])\ndf\n</pre> # Create a dataframe (6,2) with multi level index df = pd.DataFrame(np.random.randn(6,2),index=hier_index,columns=['A','B']) df Out[4]: A B G1 1 -0.349997 1.372488 2 0.090160 0.121530 3 0.751559 -0.335218 G2 1 1.501890 0.835727 2 -1.542555 -1.878225 3 0.721939 -0.121186 In\u00a0[5]: Copied! <pre>#access columns as usual\ndf['A']\n</pre> #access columns as usual df['A'] Out[5]: <pre>G1  1   -0.349997\n    2    0.090160\n    3    0.751559\nG2  1    1.501890\n    2   -1.542555\n    3    0.721939\nName: A, dtype: float64</pre> In\u00a0[6]: Copied! <pre>#access rows\ndf.loc['G1']\n</pre> #access rows df.loc['G1'] Out[6]: A B 1 -0.349997 1.372488 2 0.090160 0.121530 3 0.751559 -0.335218 In\u00a0[7]: Copied! <pre>#acess a single row form inner\ndf.loc['G1'].loc[1]\n</pre> #acess a single row form inner df.loc['G1'].loc[1] Out[7]: <pre>A   -0.349997\nB    1.372488\nName: 1, dtype: float64</pre> In\u00a0[14]: Copied! <pre>#access a single cell\ndf.loc['G2'].loc[3]['B']\n</pre> #access a single cell df.loc['G2'].loc[3]['B'] Out[14]: <pre>-0.12479824997165252</pre> In\u00a0[8]: Copied! <pre>df.index.names\n</pre> df.index.names Out[8]: <pre>FrozenList([None, None])</pre> In\u00a0[9]: Copied! <pre>df.index.names = ['Group', 'Serial']\ndf\n</pre> df.index.names = ['Group', 'Serial'] df Out[9]: A B Group Serial G1 1 -0.349997 1.372488 2 0.090160 0.121530 3 0.751559 -0.335218 G2 1 1.501890 0.835727 2 -1.542555 -1.878225 3 0.721939 -0.121186 In\u00a0[10]: Copied! <pre># Get all rows with Serial 1\ndf.xs(1, level='Serial')\n</pre> # Get all rows with Serial 1 df.xs(1, level='Serial') Out[10]: A B Group G1 -0.349997 1.372488 G2 1.501890 0.835727 In\u00a0[11]: Copied! <pre># Get rows with serial 2 in group 1\ndf.xs(['G1',2])\n</pre> # Get rows with serial 2 in group 1 df.xs(['G1',2]) Out[11]: <pre>A    0.09016\nB    0.12153\nName: (G1, 2), dtype: float64</pre> In\u00a0[12]: Copied! <pre>d = {'a':[1,2,np.nan], 'b':[np.nan, 5, np.nan], 'c':[6,7,8]}\ndfna = pd.DataFrame(d)\ndfna\n</pre> d = {'a':[1,2,np.nan], 'b':[np.nan, 5, np.nan], 'c':[6,7,8]} dfna = pd.DataFrame(d) dfna Out[12]: a b c 0 1.0 NaN 6 1 2.0 5.0 7 2 NaN NaN 8 In\u00a0[13]: Copied! <pre># dropping rows with one or more na values\ndfna.dropna()\n</pre> # dropping rows with one or more na values dfna.dropna() Out[13]: a b c 1 2.0 5.0 7 In\u00a0[14]: Copied! <pre># dropping cols with one or more na values\ndfna.dropna(axis=1)\n</pre> # dropping cols with one or more na values dfna.dropna(axis=1) Out[14]: c 0 6 1 7 2 8 In\u00a0[15]: Copied! <pre># Dropping rows only if 2 or more cols have na values\ndfna.dropna(axis=0, thresh=2)\n</pre> # Dropping rows only if 2 or more cols have na values dfna.dropna(axis=0, thresh=2) Out[15]: a b c 0 1.0 NaN 6 1 2.0 5.0 7 In\u00a0[16]: Copied! <pre>dfna.fillna(value=999)\n</pre> dfna.fillna(value=999) Out[16]: a b c 0 1.0 999.0 6 1 2.0 5.0 7 2 999.0 999.0 8 In\u00a0[17]: Copied! <pre># filling with mean value of entire dataframe\ndfna.fillna(value = dfna.mean())\n</pre> # filling with mean value of entire dataframe dfna.fillna(value = dfna.mean()) Out[17]: a b c 0 1.0 5.0 6 1 2.0 5.0 7 2 1.5 5.0 8 In\u00a0[18]: Copied! <pre># fill with mean value row by row\ndfna['a'].fillna(value = dfna['a'].mean())\n</pre> # fill with mean value row by row dfna['a'].fillna(value = dfna['a'].mean()) Out[18]: <pre>0    1.0\n1    2.0\n2    1.5\nName: a, dtype: float64</pre> In\u00a0[19]: Copied! <pre>comp_data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],\n       'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],\n       'Sales':[200,120,340,124,243,350]}\n\ncomp_df = pd.DataFrame(comp_data)\ncomp_df\n</pre> comp_data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],        'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],        'Sales':[200,120,340,124,243,350]}  comp_df = pd.DataFrame(comp_data) comp_df Out[19]: Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 In\u00a0[21]: Copied! <pre># mean sales by company - automatically only applies mean on numerical columns\ncomp_df.groupby('Company').mean()\n</pre> # mean sales by company - automatically only applies mean on numerical columns comp_df.groupby('Company').mean() Out[21]: Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 In\u00a0[22]: Copied! <pre># standard deviation in sales by company\ncomp_df.groupby('Company').std()\n</pre> # standard deviation in sales by company comp_df.groupby('Company').std() Out[22]: Sales Company FB 75.660426 GOOG 56.568542 MSFT 152.735065 <p>You can run other aggregation functions like <code>mean, min, max, std, count</code> etc. Lets look at <code>describe</code> which does all of it.</p> In\u00a0[23]: Copied! <pre>comp_df.groupby('Company').describe()\n</pre> comp_df.groupby('Company').describe() Out[23]: Sales count mean std min 25% 50% 75% max Company FB 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0 GOOG 2.0 160.0 56.568542 120.0 140.00 160.0 180.00 200.0 MSFT 2.0 232.0 152.735065 124.0 178.00 232.0 286.00 340.0 In\u00a0[24]: Copied! <pre>comp_df.groupby('Company').describe().transpose()\n</pre> comp_df.groupby('Company').describe().transpose() Out[24]: Company FB GOOG MSFT Sales count 2.000000 2.000000 2.000000 mean 296.500000 160.000000 232.000000 std 75.660426 56.568542 152.735065 min 243.000000 120.000000 124.000000 25% 269.750000 140.000000 178.000000 50% 296.500000 160.000000 232.000000 75% 323.250000 180.000000 286.000000 max 350.000000 200.000000 340.000000 In\u00a0[25]: Copied! <pre>comp_df.groupby('Company').describe().index\n</pre> comp_df.groupby('Company').describe().index Out[25]: <pre>Index(['FB', 'GOOG', 'MSFT'], dtype='object', name='Company')</pre> In\u00a0[26]: Copied! <pre>df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],'B': ['B0', 'B1', 'B2', 'B3'],\n                        'C': ['C0', 'C1', 'C2', 'C3'],'D': ['D0', 'D1', 'D2', 'D3']}, index=[0, 1, 2, 3])\ndf2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'], 'B': ['B4', 'B5', 'B6', 'B7'],\n                        'C': ['C4', 'C5', 'C6', 'C7'],'D': ['D4', 'D5', 'D6', 'D7']}, index=[4, 5, 6, 7])\n</pre> df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],'B': ['B0', 'B1', 'B2', 'B3'],                         'C': ['C0', 'C1', 'C2', 'C3'],'D': ['D0', 'D1', 'D2', 'D3']}, index=[0, 1, 2, 3]) df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'], 'B': ['B4', 'B5', 'B6', 'B7'],                         'C': ['C4', 'C5', 'C6', 'C7'],'D': ['D4', 'D5', 'D6', 'D7']}, index=[4, 5, 6, 7]) In\u00a0[27]: Copied! <pre>df1\n</pre> df1 Out[27]: A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 In\u00a0[28]: Copied! <pre>df2\n</pre> df2 Out[28]: A B C D 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 In\u00a0[29]: Copied! <pre># extend along rows\npd.concat([df1, df2]) #flows well because index is sequential and colmns match\n</pre> # extend along rows pd.concat([df1, df2]) #flows well because index is sequential and colmns match Out[29]: A B C D 0 A0 B0 C0 D0 1 A1 B1 C1 D1 2 A2 B2 C2 D2 3 A3 B3 C3 D3 4 A4 B4 C4 D4 5 A5 B5 C5 D5 6 A6 B6 C6 D6 7 A7 B7 C7 D7 In\u00a0[30]: Copied! <pre>#extend along columns\npd.concat([df1, df2], axis=1) #fills NaN when index dont match\n</pre> #extend along columns pd.concat([df1, df2], axis=1) #fills NaN when index dont match Out[30]: A B C D A B C D 0 A0 B0 C0 D0 NaN NaN NaN NaN 1 A1 B1 C1 D1 NaN NaN NaN NaN 2 A2 B2 C2 D2 NaN NaN NaN NaN 3 A3 B3 C3 D3 NaN NaN NaN NaN 4 NaN NaN NaN NaN A4 B4 C4 D4 5 NaN NaN NaN NaN A5 B5 C5 D5 6 NaN NaN NaN NaN A6 B6 C6 D6 7 NaN NaN NaN NaN A7 B7 C7 D7 In\u00a0[31]: Copied! <pre>left = pd.DataFrame({'key1': ['K0', 'K1', 'K2', 'K3'],'A': ['A0', 'A1', 'A2', 'A3'],\n                        'B': ['B0', 'B1', 'B2', 'B3']})\n    \nright = pd.DataFrame({'key1': ['K0', 'K1', 'K2', 'K3'],'B': ['C0', 'C1', 'C2', 'C3'],\n                                  'C': ['D0', 'D1', 'D2', 'D3']})\nleft\n</pre> left = pd.DataFrame({'key1': ['K0', 'K1', 'K2', 'K3'],'A': ['A0', 'A1', 'A2', 'A3'],                         'B': ['B0', 'B1', 'B2', 'B3']})      right = pd.DataFrame({'key1': ['K0', 'K1', 'K2', 'K3'],'B': ['C0', 'C1', 'C2', 'C3'],                                   'C': ['D0', 'D1', 'D2', 'D3']}) left Out[31]: A B key1 0 A0 B0 K0 1 A1 B1 K1 2 A2 B2 K2 3 A3 B3 K3 In\u00a0[32]: Copied! <pre>right\n</pre> right Out[32]: B C key1 0 C0 D0 K0 1 C1 D1 K1 2 C2 D2 K2 3 C3 D3 K3 In\u00a0[33]: Copied! <pre>#merge along key1\npd.merge(left, right, how='inner', on='key1')\n</pre> #merge along key1 pd.merge(left, right, how='inner', on='key1') Out[33]: A B_x key1 B_y C 0 A0 B0 K0 C0 D0 1 A1 B1 K1 C1 D1 2 A2 B2 K2 C2 D2 3 A3 B3 K3 C3 D3 <p>When both tables have same column names that are not used for merging (<code>on</code>) then pandas appends <code>x</code> and <code>y</code> to their names to differentiate</p> In\u00a0[34]: Copied! <pre>left['key2'] = ['K0', 'K1', 'K0', 'K1']\nleft\n</pre> left['key2'] = ['K0', 'K1', 'K0', 'K1'] left Out[34]: A B key1 key2 0 A0 B0 K0 K0 1 A1 B1 K1 K1 2 A2 B2 K2 K0 3 A3 B3 K3 K1 In\u00a0[35]: Copied! <pre>right['key2'] =  ['K0', 'K0', 'K0', 'K0']\nright\n</pre> right['key2'] =  ['K0', 'K0', 'K0', 'K0'] right Out[35]: B C key1 key2 0 C0 D0 K0 K0 1 C1 D1 K1 K0 2 C2 D2 K2 K0 3 C3 D3 K3 K0 In\u00a0[36]: Copied! <pre>pd.merge(left, right, how='inner', on=['key1', 'key2'])\n</pre> pd.merge(left, right, how='inner', on=['key1', 'key2']) Out[36]: A B_x key1 key2 B_y C 0 A0 B0 K0 K0 C0 D0 1 A2 B2 K2 K0 C2 D2 <p><code>inner</code> merge will only keep the intersection, thus only 2 rows.</p> In\u00a0[37]: Copied! <pre>om = pd.merge(left, right, how='outer', on=['key1', 'key2'])\nom\n</pre> om = pd.merge(left, right, how='outer', on=['key1', 'key2']) om Out[37]: A B_x key1 key2 B_y C 0 A0 B0 K0 K0 C0 D0 1 A1 B1 K1 K1 NaN NaN 2 A2 B2 K2 K0 C2 D2 3 A3 B3 K3 K1 NaN NaN 4 NaN NaN K1 K0 C1 D1 5 NaN NaN K3 K0 C3 D3 In\u00a0[38]: Copied! <pre>om.sort_values(by=['key1', 'key2']) #now you got the merge sorted by columns.\n</pre> om.sort_values(by=['key1', 'key2']) #now you got the merge sorted by columns. Out[38]: A B_x key1 key2 B_y C 0 A0 B0 K0 K0 C0 D0 4 NaN NaN K1 K0 C1 D1 1 A1 B1 K1 K1 NaN NaN 2 A2 B2 K2 K0 C2 D2 5 NaN NaN K3 K0 C3 D3 3 A3 B3 K3 K1 NaN NaN In\u00a0[39]: Copied! <pre>pd.merge(left, right, how='right', on=['key1', 'key2']).sort_values(by='key1')\n</pre> pd.merge(left, right, how='right', on=['key1', 'key2']).sort_values(by='key1') Out[39]: A B_x key1 key2 B_y C 0 A0 B0 K0 K0 C0 D0 2 NaN NaN K1 K0 C1 D1 1 A2 B2 K2 K0 C2 D2 3 NaN NaN K3 K0 C3 D3 In\u00a0[40]: Copied! <pre>pd.merge(left, right, how='left', on=['key1', 'key2']).sort_values(by='key1')\n</pre> pd.merge(left, right, how='left', on=['key1', 'key2']).sort_values(by='key1') Out[40]: A B_x key1 key2 B_y C 0 A0 B0 K0 K0 C0 D0 1 A1 B1 K1 K1 NaN NaN 2 A2 B2 K2 K0 C2 D2 3 A3 B3 K3 K1 NaN NaN In\u00a0[41]: Copied! <pre>df_a = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n                     'B': ['B0', 'B1', 'B2']},\n                      index=['K0', 'K1', 'K2']) \ndf_b = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n                    'D': ['D0', 'D2', 'D3']},\n                      index=['K0', 'K2', 'K3'])\n\ndf_a\n</pre> df_a = pd.DataFrame({'A': ['A0', 'A1', 'A2'],                      'B': ['B0', 'B1', 'B2']},                       index=['K0', 'K1', 'K2'])  df_b = pd.DataFrame({'C': ['C0', 'C2', 'C3'],                     'D': ['D0', 'D2', 'D3']},                       index=['K0', 'K2', 'K3'])  df_a Out[41]: A B K0 A0 B0 K1 A1 B1 K2 A2 B2 In\u00a0[42]: Copied! <pre>df_b\n</pre> df_b Out[42]: C D K0 C0 D0 K2 C2 D2 K3 C3 D3 In\u00a0[43]: Copied! <pre>#join b to a, default mode = keep all rows of a and matching rows of b (left join)\ndf_a.join(df_b)\n</pre> #join b to a, default mode = keep all rows of a and matching rows of b (left join) df_a.join(df_b) Out[43]: A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 <p>Thus all rows of df_a and those in df_b. If df_b did not have that index, then NaN for values.</p> In\u00a0[44]: Copied! <pre>#join b to a\ndf_b.join(df_a)\n</pre> #join b to a df_b.join(df_a) Out[44]: C D A B K0 C0 D0 A0 B0 K2 C2 D2 A2 B2 K3 C3 D3 NaN NaN In\u00a0[45]: Copied! <pre>#outer join - union of outputs\ndf_b.join(df_a, how='outer')\n</pre> #outer join - union of outputs df_b.join(df_a, how='outer') Out[45]: C D A B K0 C0 D0 A0 B0 K1 NaN NaN A1 B1 K2 C2 D2 A2 B2 K3 C3 D3 NaN NaN"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#pandas-indices-sorting-merging","title":"Pandas - Indices, Sorting, Merging\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#navigating-multilevel-index","title":"Navigating multilevel index\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#accessing-rows-and-columns","title":"Accessing rows and columns\u00b6","text":"<p>You can use <code>loc</code> and <code>iloc</code> as a chain to access the elements. Go from outer index to inner index</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#naming-indices","title":"Naming indices\u00b6","text":"<p>Indices can have names (appear similar to column names)</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#accessing-rows-and-columns-using-cross-section","title":"Accessing rows and columns using cross section\u00b6","text":"<p>The <code>xs</code> method allows to get a cross section. The advantage is it can penetrate a multilevel index in a single step. Now that we have named the indices, we can use cross section effectively</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#missing-data","title":"Missing data\u00b6","text":"<p>You can either drop rows/cols with missing values using <code>dropna()</code> or fill those cells with values using the <code>fillna()</code> methods.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#dropna","title":"dropna\u00b6","text":"<p>Use <code>dropna(axis, thresh...)</code> where axis is 0 for rows, 1 for cols and thresh represents how many occurrences of nan before dropping happens</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#fillna","title":"fillna\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#data-aggregation","title":"Data aggregation\u00b6","text":"<p>Pandas allows sql like control on the dataframes. You can treat each DF as a table and perform sql aggregation.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#groupby","title":"groupby\u00b6","text":"<p>Format is: <code>df.groupby('col_name').aggregation()</code></p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#mean-min-max","title":"mean min max\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#describe","title":"describe\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#transpose","title":"transpose\u00b6","text":"<p>Long over due, you can tile a DF by calling the <code>transpose()</code> method.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#combining-dataframes","title":"Combining DataFrames\u00b6","text":"<p>You can concatenate, merge and join data frames.</p> <p>Lets take a look at 3 DataFrames</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#concat","title":"concat\u00b6","text":"<p><code>pd.concat([list_of_df], axis=0)</code> will extend a dataframe either along rows or columns. All DF in the list should be of same dimension.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#merge","title":"merge\u00b6","text":"<p>merge lets you do a sql merge with <code>inner, outer, right</code> and <code>left</code> joins. <code>pd.merge(left, right, how='outer', on='key')</code> where, <code>left</code> and <code>right</code> are your two DataFrames (tables) and <code>on</code> refers to the <code>foreign key</code></p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#inner-merge","title":"inner merge\u00b6","text":"<p>Inner join keeps only the intersection.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#merge-on-multiple-columns","title":"merge on multiple columns\u00b6","text":"<p>Sometimes, your foreign key is composite. Then you can merge on multiple keys by passing a list to the <code>on</code> argument. Now lets add a key2 column to both the tables.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#outer-merge","title":"outer merge\u00b6","text":"<p>Use <code>how='outer'</code> to keep the union of both the tables. pandas fills <code>NaN</code> when a cell has no values.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#sorting","title":"Sorting\u00b6","text":"<p>Use <code>DataFrame.sort_values(by=columns, inplace=False, ascending=True)</code> to sort the table.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#right-merge","title":"right merge\u00b6","text":"<p><code>how='right'</code> will keep all the rows of right table and drop the rows of left table that dont have a matching keys.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#left-merge","title":"left merge\u00b6","text":"<p><code>how='left'</code> will similarly keep all rows of left and those rows of right that has a matching foreign key.</p>"},{"location":"teaching_resources/pandas/pandas_indices_sorting_merging/#join","title":"join\u00b6","text":"<p>Joins are like merges but work on index instead of columns. Further, they are by default either <code>left</code> or <code>right</code> with <code>inner</code> as mode of joins. See example below:</p>"},{"location":"teaching_resources/pandas/pandas_unique_time_series/","title":"Pandas - Unique, IsNull, Time series","text":"In\u00a0[28]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import pandas as pd import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[2]: Copied! <pre>comp_data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],\n       'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],\n       'Sales':[200,120,340,124,243,350]}\n\ncomp_df = pd.DataFrame(comp_data)\ncomp_df\n</pre> comp_data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],        'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],        'Sales':[200,120,340,124,243,350]}  comp_df = pd.DataFrame(comp_data) comp_df Out[2]: Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 In\u00a0[3]: Copied! <pre># find unique company names\ncomp_df['Company'].unique()\n</pre> # find unique company names comp_df['Company'].unique() Out[3]: <pre>array(['GOOG', 'MSFT', 'FB'], dtype=object)</pre> In\u00a0[4]: Copied! <pre>comp_df['Company'].nunique()\n</pre> comp_df['Company'].nunique() Out[4]: <pre>3</pre> In\u00a0[5]: Copied! <pre>comp_df['Company'].value_counts()\n</pre> comp_df['Company'].value_counts() Out[5]: <pre>GOOG    2\nFB      2\nMSFT    2\nName: Company, dtype: int64</pre> In\u00a0[6]: Copied! <pre>comp_df['sq_sales'] = comp_df['Sales'].apply(lambda x:x*x)\ncomp_df\n</pre> comp_df['sq_sales'] = comp_df['Sales'].apply(lambda x:x*x) comp_df Out[6]: Company Person Sales sq_sales 0 GOOG Sam 200 40000 1 GOOG Charlie 120 14400 2 MSFT Amy 340 115600 3 MSFT Vanessa 124 15376 4 FB Carl 243 59049 5 FB Sarah 350 122500 <p>We can also define a function and call that within the <code>apply()</code> method. This can accept values of one or more columns to calculate a new column.</p> In\u00a0[9]: Copied! <pre>def cuber(row):\n    return row['Sales'] * row['sq_sales']\n\ncomp_df['cu_sales'] = comp_df.apply(cuber, axis=1) \n#note - how the function is called as an obj\n# note - how I need to set axis to 1, instead of 0 which is defualt.\ncomp_df\n</pre> def cuber(row):     return row['Sales'] * row['sq_sales']  comp_df['cu_sales'] = comp_df.apply(cuber, axis=1)  #note - how the function is called as an obj # note - how I need to set axis to 1, instead of 0 which is defualt. comp_df Out[9]: Company Person Sales sq_sales cu_sales 0 GOOG Sam 200 40000 8000000 1 GOOG Charlie 120 14400 1728000 2 MSFT Amy 340 115600 39304000 3 MSFT Vanessa 124 15376 1906624 4 FB Carl 243 59049 14348907 5 FB Sarah 350 122500 42875000 In\u00a0[10]: Copied! <pre>comp_df.sort_values('Sales')\n</pre> comp_df.sort_values('Sales') Out[10]: Company Person Sales sq_sales cu_sales 1 GOOG Charlie 120 14400 1728000 3 MSFT Vanessa 124 15376 1906624 0 GOOG Sam 200 40000 8000000 4 FB Carl 243 59049 14348907 2 MSFT Amy 340 115600 39304000 5 FB Sarah 350 122500 42875000 <p>Note how the index remains attached to the original rows.</p> In\u00a0[12]: Copied! <pre>#sorting along multiple columns\ncomp_df.sort_values(['Company','Sales'])\n</pre> #sorting along multiple columns comp_df.sort_values(['Company','Sales']) Out[12]: Company Person Sales sq_sales cu_sales 4 FB Carl 243 59049 14348907 5 FB Sarah 350 122500 42875000 1 GOOG Charlie 120 14400 1728000 0 GOOG Sam 200 40000 8000000 3 MSFT Vanessa 124 15376 1906624 2 MSFT Amy 340 115600 39304000 In\u00a0[13]: Copied! <pre>comp_df.isnull()\n</pre> comp_df.isnull() Out[13]: Company Person Sales sq_sales cu_sales 0 False False False False False 1 False False False False False 2 False False False False False 3 False False False False False 4 False False False False False 5 False False False False False In\u00a0[23]: Copied! <pre>registrant_df = pd.read_csv('./registrant.csv')\nregistrant_df.head()\n</pre> registrant_df = pd.read_csv('./registrant.csv') registrant_df.head() Out[23]: Unnamed: 0 Registration Date Country Organization Current customer? What would you like to learn? 0 0 11/08/2019 06:09 PM EST Jamaica The University of the West Indies NaN NaN 1 1 11/08/2019 06:09 PM EST Japan iLand6 Co.,Ltd. no I am interested ArcGIS. 2 2 11/08/2019 05:56 PM EST Canada Safe Software Inc yes data science workflos 3 3 11/08/2019 05:51 PM EST Canada Le Groupe GeoInfo Inc yes general information 4 4 11/08/2019 05:26 PM EST Canada Safe Software Inc. NaN NaN <p>The <code>Registration Date</code> should be of type <code>datetime</code> and the <code>Current customer?</code> should be of <code>bool</code>. However, are they?</p> In\u00a0[24]: Copied! <pre>registrant_df.dtypes\n</pre> registrant_df.dtypes Out[24]: <pre>Unnamed: 0                        int64\nRegistration Date                object\nCountry                          object\nOrganization                     object\nCurrent customer?                object\nWhat would you like to learn?    object\ndtype: object</pre> <p>Everything is a generic <code>object</code>. Let us re-read, this time knowing what their data types should be.</p> In\u00a0[25]: Copied! <pre># define a function (lambda in this case) which will convert a column to bool depending on the \n# value of the cell\nconvertor_fn = lambda x: x in ['Yes', 'yes', 'YES']\nconvertor_map = {'Current customer?': convertor_fn}\n\n# re-read data\nregistrant_df2 = pd.read_csv('./registrant.csv', \n                             parse_dates=['Registration Date'], \n                             converters = convertor_map)\n\nregistrant_df2.dtypes\n</pre> # define a function (lambda in this case) which will convert a column to bool depending on the  # value of the cell convertor_fn = lambda x: x in ['Yes', 'yes', 'YES'] convertor_map = {'Current customer?': convertor_fn}  # re-read data registrant_df2 = pd.read_csv('./registrant.csv',                               parse_dates=['Registration Date'],                               converters = convertor_map)  registrant_df2.dtypes <pre>/Users/atma6951/anaconda3/lib/python3.7/site-packages/dateutil/parser/_parser.py:1206: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n  category=UnknownTimezoneWarning)\n/Users/atma6951/anaconda3/lib/python3.7/site-packages/dateutil/parser/_parser.py:1206: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n  category=UnknownTimezoneWarning)\n</pre> Out[25]: <pre>Unnamed: 0                                int64\nRegistration Date                datetime64[ns]\nCountry                                  object\nOrganization                             object\nCurrent customer?                          bool\nWhat would you like to learn?            object\ndtype: object</pre> In\u00a0[26]: Copied! <pre>registrant_df2.set_index('Registration Date', inplace=True)\nregistrant_df2.drop(axis=1, columns=['Unnamed: 0'], inplace=True) # drop bad column\nregistrant_df2.head()\n</pre> registrant_df2.set_index('Registration Date', inplace=True) registrant_df2.drop(axis=1, columns=['Unnamed: 0'], inplace=True) # drop bad column registrant_df2.head() Out[26]: Country Organization Current customer? What would you like to learn? Registration Date 2019-11-08 18:09:00 Jamaica The University of the West Indies False NaN 2019-11-08 18:09:00 Japan iLand6 Co.,Ltd. False I am interested ArcGIS. 2019-11-08 17:56:00 Canada Safe Software Inc True data science workflos 2019-11-08 17:51:00 Canada Le Groupe GeoInfo Inc True general information 2019-11-08 17:26:00 Canada Safe Software Inc. False NaN In\u00a0[30]: Copied! <pre>registrant_df2.sort_index(inplace=True)\n</pre> registrant_df2.sort_index(inplace=True) In\u00a0[31]: Copied! <pre>registrant_df2['registration_count'] = range(1, len(registrant_df2)+1) # goes from 1 to 284\n</pre> registrant_df2['registration_count'] = range(1, len(registrant_df2)+1) # goes from 1 to 284 In\u00a0[32]: Copied! <pre>plt.figure(figsize=(10,7))\nregistrant_df2['registration_count'].plot(kind='line')\nplt.title('Number of registrants over time');\n</pre> plt.figure(figsize=(10,7)) registrant_df2['registration_count'].plot(kind='line') plt.title('Number of registrants over time');"},{"location":"teaching_resources/pandas/pandas_unique_time_series/#pandas-unique-isnull-time-series","title":"Pandas - Unique, IsNull, Time series\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_unique_time_series/#unique-find-unique-rows","title":"unique - find unique rows\u00b6","text":"<p>Find unique rows in dataset</p>"},{"location":"teaching_resources/pandas/pandas_unique_time_series/#nunique-find-number-of-unique-rows","title":"nunique - find number of unique rows\u00b6","text":"<p>More efficient than finding the unique array and finding the length of it.</p>"},{"location":"teaching_resources/pandas/pandas_unique_time_series/#value_counts-find-unique-values-and-number-of-occurrences","title":"value_counts - find unique values and number of occurrences\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_unique_time_series/#apply-batch-process-column-values","title":"apply - batch process column values\u00b6","text":"<p>Calling the apply() is similar to calling the <code>map()</code> in Python. It can apply an operation on all records of a selected column. For instance, to find the squared sales, do the following</p>"},{"location":"teaching_resources/pandas/pandas_unique_time_series/#sort_values-sorting-rows","title":"sort_values - sorting rows\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_unique_time_series/#isnull-finding-null-values-throughout-the-dataframe","title":"isnull - finding null values throughout the DataFrame\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_unique_time_series/#working-with-time-series-data","title":"Working with time series data\u00b6","text":"<p>This section explains how to specify datatypes of columns while reading data and how to define column converters to ease certain data types.</p>"},{"location":"teaching_resources/pandas/pandas_unique_time_series/#plotting-time-series","title":"Plotting time series\u00b6","text":"<p>Now that the Registration date is datetime, we can plot the number of registrants by time. But before that, we need to set it as the index.</p>"},{"location":"teaching_resources/pandas/pandas_unique_time_series/#sort-dataframe-by-time","title":"Sort dataframe by time\u00b6","text":""},{"location":"teaching_resources/pandas/pandas_unique_time_series/#add-a-counter-column-to-the-dataframe","title":"Add a counter column to the dataframe\u00b6","text":"<p>Note. It is important to count up only after sorting. Else the numbers are going to be all over the place.</p>"},{"location":"teaching_resources/plotly/plotly_geographical_plotting/","title":"Plotly - geographical plotting","text":"In\u00a0[1]: Copied! <pre>import plotly.plotly as py\nimport plotly.graph_objs as go\nimport pandas as pd\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n</pre> import plotly.plotly as py import plotly.graph_objs as go import pandas as pd from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot init_notebook_mode(connected=True) In\u00a0[2]: Copied! <pre>data = dict(type='choropleth', locations=['AZ','CA','NY'],\n           locationmode='USA-states', colorscale='Greens',\n           text=['text 1', 'text 2', 'text 3'],\n           z=[1,2,3], colorbar={'title':'Color bar title here'})\n\nlayout = dict(geo={'scope':'usa'})\n</pre> data = dict(type='choropleth', locations=['AZ','CA','NY'],            locationmode='USA-states', colorscale='Greens',            text=['text 1', 'text 2', 'text 3'],            z=[1,2,3], colorbar={'title':'Color bar title here'})  layout = dict(geo={'scope':'usa'}) In\u00a0[3]: Copied! <pre>layout\n</pre> layout Out[3]: <pre>{'geo': {'scope': 'usa'}}</pre> In\u00a0[4]: Copied! <pre>choro_map = go.Figure(data=[data], layout=layout)\nchoro_map\n</pre> choro_map = go.Figure(data=[data], layout=layout) choro_map Out[4]: <pre>{'data': [{'colorbar': {'title': 'Color bar title here'},\n   'colorscale': 'Greens',\n   'locationmode': 'USA-states',\n   'locations': ['AZ', 'CA', 'NY'],\n   'text': ['text 1', 'text 2', 'text 3'],\n   'type': 'choropleth',\n   'z': [1, 2, 3]}],\n 'layout': {'geo': {'scope': 'usa'}}}</pre> In\u00a0[5]: Copied! <pre>iplot(choro_map)\n</pre> iplot(choro_map) In\u00a0[6]: Copied! <pre>world_gdp_df = pd.read_csv('/Users/atma6951/Documents/code/pychakras/pychakras/udemy_ml_bootcamp/Python-for-Data-Visualization/Geographical Plotting/2014_World_GDP')\nworld_gdp_df.head()\n</pre> world_gdp_df = pd.read_csv('/Users/atma6951/Documents/code/pychakras/pychakras/udemy_ml_bootcamp/Python-for-Data-Visualization/Geographical Plotting/2014_World_GDP') world_gdp_df.head() Out[6]: COUNTRY GDP (BILLIONS) CODE 0 Afghanistan 21.71 AFG 1 Albania 13.40 ALB 2 Algeria 227.80 DZA 3 American Samoa 0.75 ASM 4 Andorra 4.80 AND In\u00a0[7]: Copied! <pre>data = {'type':'choropleth', 'locations':world_gdp_df['CODE'],\n       'z':world_gdp_df['GDP (BILLIONS)'], 'text':world_gdp_df['COUNTRY'],\n       'colorbar':{'title':'GDP in Billions USD'}}\n\nlayout={'title':'2014 Global GDP',\n       'geo':{'showframe':False, 'projection':{'type':'Mercator'}}}\n\nchoromap3 = go.Figure(data=[data], layout=layout)\n</pre> data = {'type':'choropleth', 'locations':world_gdp_df['CODE'],        'z':world_gdp_df['GDP (BILLIONS)'], 'text':world_gdp_df['COUNTRY'],        'colorbar':{'title':'GDP in Billions USD'}}  layout={'title':'2014 Global GDP',        'geo':{'showframe':False, 'projection':{'type':'Mercator'}}}  choromap3 = go.Figure(data=[data], layout=layout) In\u00a0[8]: Copied! <pre>iplot(choromap3)\n</pre> iplot(choromap3) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/plotly/plotly_geographical_plotting/#plotly-geographical-plotting","title":"Plotly - geographical plotting\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_geographical_plotting/#choropleth-map-of-usa","title":"Choropleth map of USA\u00b6","text":"<p>Create some data for plotting. Plotly needs this notation</p>"},{"location":"teaching_resources/plotly/plotly_geographical_plotting/#world-choropleth-maps","title":"World choropleth maps\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_getting_started/","title":"Plotly - getting started","text":"<p>Plotly is a JS based plotting library with Python bindings (similar to bokeh). Plotly also has a portal online where users can upload their charts for public use.</p> In\u00a0[2]: Copied! <pre>import plotly\n</pre> import plotly In\u00a0[3]: Copied! <pre>plotly.tools.set_credentials_file(username='atmamani',\n                                  api_key='xxx')\n</pre> plotly.tools.set_credentials_file(username='atmamani',                                   api_key='xxx') In\u00a0[4]: Copied! <pre>import plotly.plotly as py\nfrom plotly.graph_objs import *\n\ntrace0 = Scatter(\n    x=[1, 2, 3, 4],\n    y=[10, 15, 13, 17]\n)\ntrace1 = Scatter(\n    x=[1, 2, 3, 4],\n    y=[16, 5, 11, 9]\n)\ndata = Data([trace0, trace1])\n\npy.plot(data, filename = 'basic-line2')\n</pre> import plotly.plotly as py from plotly.graph_objs import *  trace0 = Scatter(     x=[1, 2, 3, 4],     y=[10, 15, 13, 17] ) trace1 = Scatter(     x=[1, 2, 3, 4],     y=[16, 5, 11, 9] ) data = Data([trace0, trace1])  py.plot(data, filename = 'basic-line2') Out[4]: <pre>'https://plot.ly/~AtmaMani/2'</pre> In\u00a0[5]: Copied! <pre>import plotly.plotly as py\nfrom plotly.graph_objs import *\n\ntrace0 = Scatter(\n    x=[1, 2, 3, 4],\n    y=[10, 15, 13, 17]\n)\ntrace1 = Scatter(\n    x=[1, 2, 3, 4],\n    y=[16, 5, 11, 9]\n)\ndata = Data([trace0, trace1])\n\npy.iplot(data, filename = 'basic-line')\n</pre> import plotly.plotly as py from plotly.graph_objs import *  trace0 = Scatter(     x=[1, 2, 3, 4],     y=[10, 15, 13, 17] ) trace1 = Scatter(     x=[1, 2, 3, 4],     y=[16, 5, 11, 9] ) data = Data([trace0, trace1])  py.iplot(data, filename = 'basic-line') <pre>High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~AtmaMani/0 or inside your plot.ly account where it is named 'basic-line'\n</pre> Out[5]: In\u00a0[5]: Copied! <pre>import plotly\nfrom plotly.graph_objs import Scatter, Layout\n\nplotly.offline.plot({\n    \"data\": [Scatter(x=[1, 2, 3, 4], y=[4, 3, 2, 1])],\n    \"layout\": Layout(title=\"hello world\")\n})\n</pre> import plotly from plotly.graph_objs import Scatter, Layout  plotly.offline.plot({     \"data\": [Scatter(x=[1, 2, 3, 4], y=[4, 3, 2, 1])],     \"layout\": Layout(title=\"hello world\") }) Out[5]: <pre>'file:///Users/atma6951/Documents/code/pychakras/pychakras/python_crash_course/temp-plot.html'</pre> In\u00a0[3]: Copied! <pre>import plotly\nfrom plotly.graph_objs import Scatter, Layout\n\n# init code\nplotly.offline.init_notebook_mode(connected=True)\n\nplotly.offline.iplot({\n    \"data\": [Scatter(x=[1, 2, 3, 4], y=[4, 3, 2, 1])],\n    \"layout\": Layout(title=\"hello world\")\n})\n</pre> import plotly from plotly.graph_objs import Scatter, Layout  # init code plotly.offline.init_notebook_mode(connected=True)  plotly.offline.iplot({     \"data\": [Scatter(x=[1, 2, 3, 4], y=[4, 3, 2, 1])],     \"layout\": Layout(title=\"hello world\") })"},{"location":"teaching_resources/plotly/plotly_getting_started/#plotly-getting-started","title":"Plotly - getting started\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_getting_started/#installation","title":"Installation\u00b6","text":"<pre><code>pip instal plotly\n</code></pre>"},{"location":"teaching_resources/plotly/plotly_getting_started/#login","title":"login\u00b6","text":"<pre><code>import plotly\nplotly.tools.set_credentials_file(username='', api_key='')\n</code></pre> <p>create your key here: https://plot.ly/settings/api</p>"},{"location":"teaching_resources/plotly/plotly_getting_started/#online-and-offline-plotting","title":"Online and Offline plotting\u00b6","text":"<p>Plotly allows you to plot both</p> <ul> <li>online - plot data is set online to your account.<ul> <li><code>plot()</code> - returns a unique url and opens it new tab</li> <li><code>iplot()</code> - when using notebook to embed the plot</li> </ul> </li> <li>offline - explicit call<ul> <li><code>offline.plot</code> - creates a new html output page</li> <li><code>offline.iplot</code> - interactive while in a notebook</li> </ul> </li> </ul>"},{"location":"teaching_resources/plotly/plotly_getting_started/#online-plotting","title":"Online plotting\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_getting_started/#plot-command","title":"<code>plot</code> command\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_getting_started/#iplot-command","title":"<code>iplot</code> command\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_getting_started/#offline-plotting","title":"Offline plotting\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_getting_started/#plot-command","title":"<code>plot</code> command\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_getting_started/#iplot-command","title":"<code>iplot</code> command\u00b6","text":"<p>You need to set initialization code, similar to matplotlib</p>"},{"location":"teaching_resources/plotly/plotly_getting_started/#building-dashboards-with-plotly-dash","title":"Building dashboards with Plotly Dash\u00b6","text":"<p>Minimal HTML and no JS. Pure Python based dashboards. More info here: https://plot.ly/dash/</p>"},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/","title":"Plotly - plotting with Pandas","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport cufflinks as cf\nfrom plotly.offline import download_plotlyjs, init_notebook_mode\nfrom plotly.offline import plot, iplot\n\n#set notebook mode\ninit_notebook_mode(connected=True)\ncf.go_offline()\n</pre> import numpy as np import pandas as pd import cufflinks as cf from plotly.offline import download_plotlyjs, init_notebook_mode from plotly.offline import plot, iplot  #set notebook mode init_notebook_mode(connected=True) cf.go_offline() <pre>IOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.\n</pre> In\u00a0[2]: Copied! <pre>df = pd.DataFrame(np.random.randn(100,4), \n                  columns='A B C D'.split(' '))\ndf.head()\n</pre> df = pd.DataFrame(np.random.randn(100,4),                    columns='A B C D'.split(' ')) df.head() Out[2]: A B C D 0 -1.539048 -0.876097 -0.236866 -0.591233 1 -0.347391 0.584317 1.223430 0.269432 2 0.342396 -1.198989 0.692799 0.451392 3 -2.244973 0.979081 -0.896566 -0.114954 4 -1.358436 1.246352 1.182089 -1.072197 In\u00a0[3]: Copied! <pre>df2 = pd.DataFrame({'category':['A','B','C'], 'values':[33,56,67]})\ndf2\n</pre> df2 = pd.DataFrame({'category':['A','B','C'], 'values':[33,56,67]}) df2 Out[3]: category values 0 A 33 1 B 56 2 C 67 In\u00a0[4]: Copied! <pre>df.iplot()\n</pre> df.iplot() In\u00a0[5]: Copied! <pre>df2.iplot(kind='bar')\n</pre> df2.iplot(kind='bar') In\u00a0[6]: Copied! <pre>df.iplot(kind='box')\n</pre> df.iplot(kind='box') In\u00a0[7]: Copied! <pre>df3 = pd.DataFrame({'x':[1,2,3,4,5],\n                   'y':[11,22,33,44,55],\n                    'z':[5,4,3,2,1]})\ndf3\n</pre> df3 = pd.DataFrame({'x':[1,2,3,4,5],                    'y':[11,22,33,44,55],                     'z':[5,4,3,2,1]}) df3 Out[7]: x y z 0 1 11 5 1 2 22 4 2 3 33 3 3 4 44 2 4 5 55 1 In\u00a0[8]: Copied! <pre>df3.iplot(kind='surface')\n</pre> df3.iplot(kind='surface') In\u00a0[9]: Copied! <pre>df.iplot(kind='hist',bins=50)\n</pre> df.iplot(kind='hist',bins=50) In\u00a0[12]: Copied! <pre>df[['A','B']].iplot(kind='spread')\n</pre> df[['A','B']].iplot(kind='spread') In\u00a0[16]: Copied! <pre>df.iplot(kind='bubble',x='A', y='B', size='C')\n</pre> df.iplot(kind='bubble',x='A', y='B', size='C') In\u00a0[17]: Copied! <pre>df.scatter_matrix()\n</pre> df.scatter_matrix()"},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#plotly-plotting-with-pandas","title":"Plotly - plotting with Pandas\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#get-cufflinks","title":"Get cufflinks\u00b6","text":"<p>Cufflinks integrates plotly with pandas to allow plotting right from pandas dataframes. Install using <code>pip</code></p> <pre><code>pip install cufflinks\n</code></pre>"},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#interactive-plotting","title":"interactive plotting\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#line-plots","title":"line plots\u00b6","text":"<p>With Plotly, you can turn on and off data values by clicking on the legend</p>"},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#bar-plot","title":"bar plot\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#box-plot","title":"box plot\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#surface-plot","title":"surface plot\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#histograms","title":"histograms\u00b6","text":""},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#spread-plots","title":"spread plots\u00b6","text":"<p>Used to show the spread in data value between two columns / variables.</p>"},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#bubble-scatter-plots","title":"bubble scatter plots\u00b6","text":"<p>same as scatter, but you can easily size the dots by another column</p>"},{"location":"teaching_resources/plotly/plotly_plotting_with_pandas/#scatter-matrix","title":"scatter matrix\u00b6","text":"<p>This is similar to seaborn's pairplot</p>"},{"location":"teaching_resources/postgis/postgis_intro/","title":"Introduction to PostGIS","text":""},{"location":"teaching_resources/postgis/postgis_intro/#set-up","title":"Set up","text":""},{"location":"teaching_resources/postgis/postgis_intro/#installation","title":"Installation","text":"<p>I am installing the all-in-one package - Postgres APP which inclues Postgres, PostGIS, PLV8. Once installed, it looks like below:</p> <p></p> <p>I am following the tutorial from here</p> <p>Note: There is an alternate download called 'OpenGeo Suite', but this is no longer in active development. It may have been acquired by boundless and now it does not resemble its original suite of products. Instead, I am installing pgadmin to admin the DB. The new pgAdmin4 is a complete rewrite. It is web based, written in Python, Qt, JS and core components in C++.</p> <p>Once you install both the Postgres database and PgAdmin4 app, start both, connect to the database server using <code>localhost:5432</code> port (which you can look up from properties of db server). PgAdmin will now list the databases. Below is an image of the pgAdmin window running SQL queries on a custom DB I created.</p> <p></p>"},{"location":"teaching_resources/postgis/postgis_intro/#sqlalchemy-set-up","title":"SQLAlchemy set up","text":"<p>I am tripping when setting up sqlalchemy with postgres server in the geopandas env. It turns out, sqlalchemy requires a package called <code>psycopg2</code> which is not installed by default. In a clean env, installing these two works well. I am trying out in the geopandas env now.</p> <p>Note: all I had to do was remove and reinstall <code>sqlalchemy</code> and <code>psycopg2</code>. Net, I had to recreate the whole package</p>"},{"location":"teaching_resources/postgis/postgis_intro/#introduction","title":"Introduction","text":"<p>PostGIS is a spatial database. It turns PostgreSQL database into a spatial database. PostGIS inherits all enterprise functionality of PostgreSQL. There are 3 aspects that make a database spatial  - spatial data types - store shapes as points ,lines, polygons. They abstract and encapsulate spatial structures such as boundary, dimension.  - spatial indexing - for efficient spatial operations  - spatial functions - exposed in SQL for querying spatial properties and relationships. Eg: answers which objects are within this particular envelope.</p> <p>True spatial databases treat spatial features as first class database objs. Spatial DBs are used not just in geospatial env, but to store data related to anatomy of human body, large-scale integrated circuits, molecular structures, EMF etc.</p> <p>More about Spatial index: Bounding boxes are used for spatial containment queries. Computing containment for polygons is very computationally intensive, hence it is performed over their respective bounding boxes. Computing this for rectangles is very simple and fast.</p> <p>More about spatial functions: 5 main categories of spatial functions - conversion of geometries - management of spatial tables, PostGIS administration - retrieval of properties and measurements of geometries - comparison of geometries, wrt spatial relationship - generation of new geometries.</p> <p>Notes: The PostGIS team outlines their reason behind choosing PostgreSQL as the foundation and not some other popular open source ORDBMS like mySQL.</p>"},{"location":"teaching_resources/postgis/postgis_intro/#history-of-postgis","title":"History of PostGIS","text":"<p>Started in May of 2001. For many years, the functionality, power and ease of use was limited. MapServer became the first app to provide visualization of PostGIS. Later, another open source initiative GOES (Geometry Engine, Open Source) was integrated with PostGIS to make it complete</p>"},{"location":"teaching_resources/postgis/postgis_intro/#tutorial","title":"Tutorial","text":"<p>Follow tutorial from postgis-intro</p> <ol> <li>To enable postGIS, run <code>CREATE EXTENSION postgis;</code>. Run by hitting <code>F5</code>. Then confirm it is installed by running <code>SELECT postgis_full_version();</code></li> <li>To import data into postGIS, the tool <code>shp2pgis</code> extension does not seem available. Hence I did the import using QGIS. You can connect to the postGIS from qGIS using the same database server, port, db name. Then install the DB Admin QGIS extension to manage postGIS. The db manager looks like below:    </li> </ol> <p>Once imported, you can query for the features from pgAdmin and also visualize the results like below: </p>"},{"location":"teaching_resources/postgis/postgis_intro/#working-with-geometries-in-postgis","title":"Working with Geometries in PostGIS","text":"<p>A special table called <code>geometry_columns</code> exists which lists all tables within postGIS that are spatial. It lists details such as what their <code>SRID</code> is, their geometry type etc.</p> <pre><code>SELECT * FROM geometry_columns\n</code></pre> <p>Spatial functions can be used to collect information about each feature (row)</p> <pre><code>SELECT name, ST_GeometryType(geom), ST_NDims(geom), ST_SRID(geom)\n  FROM sample_geometries;\n</code></pre> <p>will return</p> <pre><code>name             |    st_geometrytype    | st_ndims | st_srid\n-----------------+-----------------------+----------+---------\n Point           | ST_Point              |        2 |       0\n Polygon         | ST_Polygon            |        2 |       0\n PolygonWithHole | ST_Polygon            |        2 |       0\n Collection      | ST_GeometryCollection |        2 |       0\n Linestring      | ST_LineString         |        2 |       0\n</code></pre> <p>Note: Spatial functions in postGIS are generally prefixed with <code>ST..</code></p>"},{"location":"teaching_resources/postgis/postgis_intro/#spatial-functions-for-point-geometries","title":"Spatial functions for Point geometries","text":"<p>Thus, to get the X, Y coordinates of a point, </p> <pre><code>SELECT ST_X(geom), ST_Y(geom) FROM sample_geometries where name='Point'\n</code></pre>"},{"location":"teaching_resources/postgis/postgis_intro/#spatial-functions-for-linestring-geometries","title":"Spatial functions for Linestring geometries","text":"<ul> <li><code>ST_Length(geometry)</code> returns the length of the linestring</li> <li><code>ST_StartPoint(geometry)</code> returns the first coordinate as a point</li> <li><code>ST_EndPoint(geometry)</code> returns the last coordinate as a point</li> <li><code>ST_NPoints(geometry)</code> returns the number of coordinates in the linestring</li> </ul>"},{"location":"teaching_resources/postgis/postgis_intro/#spatial-functions-for-polygon-geometries","title":"Spatial functions for Polygon geometries","text":"<ul> <li><code>ST_Area(geometry)</code> returns the area of the polygons</li> <li><code>ST_NRings(geometry)</code> returns the number of rings (usually 1, more of there are holes)</li> <li><code>ST_ExteriorRing(geometry)</code> returns the outer ring as a linestring</li> <li><code>ST_InteriorRingN(geometry,n)</code> returns a specified interior ring as a linestring</li> <li><code>ST_Perimeter(geometry)</code> returns the length of all the rings</li> </ul>"},{"location":"teaching_resources/postgis/postgis_intro/#spatial-functions-for-geometry-collections","title":"Spatial functions for Geometry Collections","text":"<p>Geometry collections are the PolyLine, Multipoint, etc.</p> <ul> <li><code>ST_NumGeometries(geometry)</code> returns the number of parts in the collection</li> <li><code>ST_GeometryN(geometry,n)</code> returns the specified part</li> <li><code>ST_Area(geometry)</code> returns the total area of all polygonal parts</li> <li><code>ST_Length(geometry)</code> returns the total length of all linear parts</li> </ul>"},{"location":"teaching_resources/postgis/postgis_intro/#some-sql-queries","title":"Some SQL queries","text":"<pre><code>SELECT boroname, SUM(popn_white)/SUM(popn_total) AS white_pct\n  FROM nyc_census_blocks GROUP BY boroname;\n</code></pre> <pre><code>\n</code></pre>"},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/","title":"PostGIS - Using SQLAlchemy, GeoAlchemy, and GeoPandas","text":"In\u00a0[8]: Copied! <pre>import sqlalchemy as db\nimport pandas as pd\n# import geopandas as gpd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import sqlalchemy as db import pandas as pd # import geopandas as gpd  import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>engine = db.create_engine('postgresql+psycopg2://postgres:postgres@localhost:5432/nyc')\ncon = engine.connect()\n</pre> engine = db.create_engine('postgresql+psycopg2://postgres:postgres@localhost:5432/nyc') con = engine.connect() <p>View some tables in the database</p> In\u00a0[3]: Copied! <pre>metadata = db.MetaData()\nnyc_census_blocks_tb = db.Table('nyc_census_blocks', metadata, \n                                autoload=True,autoload_with=engine)\n</pre> metadata = db.MetaData() nyc_census_blocks_tb = db.Table('nyc_census_blocks', metadata,                                  autoload=True,autoload_with=engine) <pre>/Users/atma6951/anaconda3/envs/geopandasenv/lib/python3.6/site-packages/sqlalchemy/dialects/postgresql/base.py:2963: SAWarning: Did not recognize type 'geometry' of column 'geom'\n  \"Did not recognize type '%s' of column '%s'\" % (attype, name)\n</pre> In\u00a0[4]: Copied! <pre>print(nyc_census_blocks_tb.columns.keys())\n</pre> print(nyc_census_blocks_tb.columns.keys()) <pre>['id', 'geom', 'blkid', 'popn_total', 'popn_white', 'popn_black', 'popn_nativ', 'popn_asian', 'popn_other', 'boroname']\n</pre> In\u00a0[5]: Copied! <pre>query = nyc_census_blocks_tb.select()\nquery\n</pre> query = nyc_census_blocks_tb.select() query Out[5]: <pre>&lt;sqlalchemy.sql.selectable.Select at 0x118204908; Select object&gt;</pre> In\u00a0[6]: Copied! <pre>query_result = con.execute(query)\nquery_result\n</pre> query_result = con.execute(query) query_result Out[6]: <pre>&lt;sqlalchemy.engine.result.ResultProxy at 0x11826e0f0&gt;</pre> In\u00a0[7]: Copied! <pre>query_result_set = query_result.fetchall()\nlen(query_result_set)\n</pre> query_result_set = query_result.fetchall() len(query_result_set) Out[7]: <pre>38794</pre> In\u00a0[8]: Copied! <pre>query_result_set[:3]\n</pre> query_result_set[:3] Out[8]: <pre>[(1, '010600002026690000010000000103000000010000000A00000051AC161881A22141A31409CF1F2A51415F4321458DA2214100102A3F1D2A51418C34807C0BA221414E3E89F5122A51417 ... (74 characters truncated) ... 02141A00099C72F2A51412365B4789AA021419F60A7BB342A514160E3E8FA66A0214118B4C0CE402A5141EA4BF3EEC7A12141A3023D61452A514151AC161881A22141A31409CF1F2A5141', '360850009001000', 97, 51, 32, 1, 5, 8, 'Staten Island'),\n (2, '0106000020266900000100000001030000000100000007000000083B4A6F79A8214127EC57B49926514151B51BB7CEA72141B2EAD6F38A2651416F429640B9A72141449FCB1C89265141163AA64D56A72141B89E2B7C9B26514150509213EDA72141DCC9A351A826514184FA4C6017A82141B9AE24F0AB265141083B4A6F79A8214127EC57B499265141', '360850020011000', 66, 52, 2, 0, 7, 5, 'Staten Island'),\n (3, '010600002026690000010000000103000000010000000600000082DCED72969D2141563247C49E2651417C120440079D214123319BFC8626514179D4895B6A9C2141F3667FC995265141C0428AC2C29C214159EB5C75AC265141CB126202D69C214180215728B126514182DCED72969D2141563247C49E265141', '360850040001000', 62, 14, 18, 2, 25, 3, 'Staten Island')]</pre> In\u00a0[10]: Copied! <pre>nyc_census_blocks_df = pd.read_sql(query, con)\nnyc_census_blocks_df.head()\n</pre> nyc_census_blocks_df = pd.read_sql(query, con) nyc_census_blocks_df.head() Out[10]: id geom blkid popn_total popn_white popn_black popn_nativ popn_asian popn_other boroname 0 1 010600002026690000010000000103000000010000000A... 360850009001000 97 51 32 1 5 8 Staten Island 1 2 0106000020266900000100000001030000000100000007... 360850020011000 66 52 2 0 7 5 Staten Island 2 3 0106000020266900000100000001030000000100000006... 360850040001000 62 14 18 2 25 3 Staten Island 3 4 010600002026690000010000000103000000010000000A... 360850074001000 137 92 12 0 13 20 Staten Island 4 5 0106000020266900000100000001030000000100000014... 360850096011000 289 230 0 0 32 27 Staten Island In\u00a0[11]: Copied! <pre>nyc_census_blocks_gpd = gpd.read_postgis(sql = query, con = con)\nnyc_census_blocks_gpd.head()\n</pre> nyc_census_blocks_gpd = gpd.read_postgis(sql = query, con = con) nyc_census_blocks_gpd.head() Out[11]: id geom blkid popn_total popn_white popn_black popn_nativ popn_asian popn_other boroname 0 1 (POLYGON ((577856.5470479821 4499583.234929237... 360850009001000 97 51 32 1 5 8 Staten Island 1 2 (POLYGON ((578620.7173632095 4495974.817866362... 360850020011000 66 52 2 0 7 5 Staten Island 2 3 (POLYGON ((577227.2244709881 4495995.066845497... 360850040001000 62 14 18 2 25 3 Staten Island 3 4 (POLYGON ((579037.0332016965 4494421.769816227... 360850074001000 137 92 12 0 13 20 Staten Island 4 5 (POLYGON ((577652.4825280879 4494975.052285533... 360850096011000 289 230 0 0 32 27 Staten Island In\u00a0[14]: Copied! <pre>nyc_census_blocks_gpd.head(50).plot()\n</pre> nyc_census_blocks_gpd.head(50).plot() Out[14]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x126067240&gt;</pre> In\u00a0[15]: Copied! <pre>nyc_census_blocks_gpd.plot()\n</pre> nyc_census_blocks_gpd.plot() Out[15]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1260aaac8&gt;</pre> In\u00a0[17]: Copied! <pre># Get only Staten Island boroughs\n# raw_sql = SELECT * FROM nyc_census_blocks_tb WHERE boroname = 'Staten Island'\nquery_boro = db.select([nyc_census_blocks_tb]).where(nyc_census_blocks_tb.columns.boroname=='Staten Island')\ngpd.read_postgis(sql = query_boro, con = con).shape\n</pre> # Get only Staten Island boroughs # raw_sql = SELECT * FROM nyc_census_blocks_tb WHERE boroname = 'Staten Island' query_boro = db.select([nyc_census_blocks_tb]).where(nyc_census_blocks_tb.columns.boroname=='Staten Island') gpd.read_postgis(sql = query_boro, con = con).shape Out[17]: <pre>(5037, 10)</pre> In\u00a0[18]: Copied! <pre># AND clause\n# raw_sql = SELECT * FROM nyc_.. WHERE boroname='Staten Island' AND popn_white &lt; popn_black\n\nquery_boro_and = db.select([nyc_census_blocks_tb]).where(\n    db.and_(nyc_census_blocks_tb.columns.boroname=='Staten Island', \n           nyc_census_blocks_tb.columns.popn_white &lt; nyc_census_blocks_tb.columns.popn_black))\n\ngpd.read_postgis(sql=query_boro_and, con=con).plot()\n</pre> # AND clause # raw_sql = SELECT * FROM nyc_.. WHERE boroname='Staten Island' AND popn_white &lt; popn_black  query_boro_and = db.select([nyc_census_blocks_tb]).where(     db.and_(nyc_census_blocks_tb.columns.boroname=='Staten Island',             nyc_census_blocks_tb.columns.popn_white &lt; nyc_census_blocks_tb.columns.popn_black))  gpd.read_postgis(sql=query_boro_and, con=con).plot() Out[18]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1280acac8&gt;</pre> In\u00a0[28]: Copied! <pre>raw_query = \"SELECT Sum(ST_Area(geom))/4047  FROM nyc_neighborhoods  WHERE boroname ='Manhattan';\"\n# con.execute(\"SELECT Sum(ST_Area(geom))/4047  FROM nyc_neighborhoods  WHERE boroname ='Manhattan';\").fetchall()\n\npd.read_sql(raw_query, con)\n</pre> raw_query = \"SELECT Sum(ST_Area(geom))/4047  FROM nyc_neighborhoods  WHERE boroname ='Manhattan';\" # con.execute(\"SELECT Sum(ST_Area(geom))/4047  FROM nyc_neighborhoods  WHERE boroname ='Manhattan';\").fetchall()  pd.read_sql(raw_query, con) Out[28]: ?column? 0 13965.320122 In\u00a0[10]: Copied! <pre>engine = db.create_engine('postgresql+psycopg2://postgres:engineTest888@localhost:5555/zesty')\ncon = engine.connect()\n</pre> engine = db.create_engine('postgresql+psycopg2://postgres:engineTest888@localhost:5555/zesty') con = engine.connect() In\u00a0[14]: Copied! <pre>metadata = db.MetaData()\nproperties_tb = db.Table('properties', metadata, \n                                autoload=True,autoload_with=engine)\n</pre> metadata = db.MetaData() properties_tb = db.Table('properties', metadata,                                  autoload=True,autoload_with=engine) <pre>/Users/atma6951/anaconda3/envs/geopandasenv/lib/python3.6/site-packages/sqlalchemy/dialects/postgresql/base.py:2963: SAWarning: Did not recognize type 'geography' of column 'geocode_geo'\n  \"Did not recognize type '%s' of column '%s'\" % (attype, name)\n/Users/atma6951/anaconda3/envs/geopandasenv/lib/python3.6/site-packages/sqlalchemy/dialects/postgresql/base.py:2963: SAWarning: Did not recognize type 'geography' of column 'parcel_geo'\n  \"Did not recognize type '%s' of column '%s'\" % (attype, name)\n/Users/atma6951/anaconda3/envs/geopandasenv/lib/python3.6/site-packages/sqlalchemy/dialects/postgresql/base.py:2963: SAWarning: Did not recognize type 'geography' of column 'building_geo'\n  \"Did not recognize type '%s' of column '%s'\" % (attype, name)\n</pre> In\u00a0[19]: Copied! <pre>query = \"SELECT properties.image_url FROM properties WHERE properties.id='f1650f2a99824f349643ad234abff6a2'\"\nquery_result = con.execute(query).fetchall()\nquery_result\n</pre> query = \"SELECT properties.image_url FROM properties WHERE properties.id='f1650f2a99824f349643ad234abff6a2'\" query_result = con.execute(query).fetchall() query_result Out[19]: <pre>[('https://storage.googleapis.com/engineering-test/images/f1650f2a99824f349643ad234abff6a2.tif',)]</pre> In\u00a0[20]: Copied! <pre>query_result[0][0]\n</pre> query_result[0][0] Out[20]: <pre>'https://storage.googleapis.com/engineering-test/images/f1650f2a99824f349643ad234abff6a2.tif'</pre> In\u00a0[38]: Copied! <pre>import requests, shutil\nresponse = requests.get(query_result[0][0], stream=True)\n</pre> import requests, shutil response = requests.get(query_result[0][0], stream=True) In\u00a0[39]: Copied! <pre>if response.status_code == 200:\n    with open('img.tif', 'wb') as out_file:\n        shutil.copyfileobj(response.raw, out_file)\ndel response\n</pre> if response.status_code == 200:     with open('img.tif', 'wb') as out_file:         shutil.copyfileobj(response.raw, out_file) del response In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/#postgis-using-sqlalchemy-geoalchemy-and-geopandas","title":"PostGIS - Using SQLAlchemy, GeoAlchemy, and GeoPandas\u00b6","text":"<p>PostGIS is an open source spatial database. I am using the Python SQLAlchemy library to connect to and execute spatial and non-spatial queries from this database</p>"},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/#querying","title":"Querying\u00b6","text":""},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/#querying-with-pandas","title":"Querying with pandas\u00b6","text":""},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/#querying-with-geopandas","title":"Querying with geopandas\u00b6","text":""},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/#where-clauses","title":"Where clauses\u00b6","text":"<p>Note, both pandas and geopandas read the full data by design. But we can design a sql query to meet our requirements and pass them to the pandas and geopandas <code>read_sql</code>, <code>read_postgis</code> methods.</p>"},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/#where-clauses-geometry-functions","title":"Where clauses geometry functions\u00b6","text":"<p>Note, here, I am not rewriting SQL into SQLAlchemy. I apparently need to use <code>geoalchemy</code> to make use of geometry functions as SQLAlchemy knows only of standard db functions.</p> <p>I also learnt I can simply pass raw SQL query into sqlalchemy and it works. Thus, below, I am able to pass raw sql which contains the geometry functions <code>ST_Area()</code> and compute the result.</p> <p>Note: Using Geopandas to connect requires geometry column. Since aggregations don't yeild geometries, I am reading the result using pandas.</p>"},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/#connect-to-zesty-db","title":"Connect to Zesty db\u00b6","text":""},{"location":"teaching_resources/postgis/postgis_sqlalchemy_gpd/#display-image","title":"Display image\u00b6","text":""},{"location":"teaching_resources/python/conda_pkg_manager/","title":"Conda - Env and Package manager for Python","text":"<p>But you would most likely want to clone the ArcGIS Pro's Python environment so you can experiment without messing up the default one</p> <pre><code>conda create --name propy_clone --clone arcgispro-py3</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#conda-env-and-package-manager-for-python","title":"Conda - Env and Package manager for Python\u00b6","text":"<p>What is conda?</p> <ul> <li>Conda is a powerful environment and package manager.<ul> <li>What are packages?</li> <li>PyPi</li> </ul> </li> <li>Create multiple environments with different versions of Python with different sets of libraries</li> <li>easily switch between environments</li> <li>http://anaconda.org/ provides free hosting for Python libraries</li> <li>conda helps to easily search for, install libraries and also to keep them up to date</li> </ul> <p>Learn resources:</p> <ul> <li>getting started: http://conda.pydata.org/docs/test-drive.html#managing-environments</li> <li>reference doc: http://conda.pydata.org/docs/</li> </ul>"},{"location":"teaching_resources/python/conda_pkg_manager/#list-all-environments","title":"List all environments\u00b6","text":"<pre><code>conda env list</code></pre> <p>Run the above command in terminal</p>"},{"location":"teaching_resources/python/conda_pkg_manager/#create-a-new-environment","title":"Create a new environment\u00b6","text":"<pre><code>conda create --name &lt;name_of_env&gt; package=version\n\nconda create --name devsummit_precon python=3.6\n\nconda create --name data_sci_proj scipy pandas</code></pre> <p>Check if new folder was created in <code>C:\\anaconda3\\envs</code></p>"},{"location":"teaching_resources/python/conda_pkg_manager/#activating-environemnts","title":"Activating environemnts\u00b6","text":"<pre><code>activate pro_devsummit_handson</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#search-packages","title":"search packages\u00b6","text":"<pre><code>conda search &lt;package name&gt;\nconda search --channel &lt;channel name&gt; &lt;package name&gt;\n\nconda search beautifulsoup</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#get-info-about-a-package","title":"get info about a package\u00b6","text":"<pre><code>conda info &lt;package name&gt; ## gives you all dependencies\nconda info --channel esri arcgis</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#install-packages","title":"Install packages\u00b6","text":"<pre><code>conda install &lt;package name&gt;\n\nconda install pandas\nconda install requests\n\nconda instal --channel &lt;channel_name&gt; &lt;package name&gt;\nconda install --channel esri arcgis\nconda install -c esri arcgis</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#list-all-packages-in-current-active-environment","title":"List all packages in current active environment\u00b6","text":"<pre><code>conda list\nconda list nb*   # wild card search</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#remove-a-package","title":"Remove a package\u00b6","text":"<pre><code>conda remove &lt;package name&gt;</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#remove-an-environment","title":"Remove an environment\u00b6","text":"<pre><code>conda env remove --name &lt;name of env&gt;\nconda env remove --name devsummit_precon</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#advanced-steps","title":"Advanced steps\u00b6","text":""},{"location":"teaching_resources/python/conda_pkg_manager/#clone-an-existing-environment","title":"Clone an existing environment\u00b6","text":"<p>For instance, to clone the <code>root</code> environment:</p> <pre><code>conda create --name clone_of_root --clone root</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#starting-jupyter-notebook-in-a-different-directory","title":"Starting Jupyter notebook in a different directory\u00b6","text":"<p>For the most part you can <code>cd</code> to desired directory after activating your environment and start the notebook. But if you cannot do it:</p> <pre><code>jupyter-notebook --notebook-dir=D:\\code\\my_notebooks</code></pre>"},{"location":"teaching_resources/python/conda_pkg_manager/#exporting-an-environment-to-a-file","title":"Exporting an environment to a file\u00b6","text":"<p>It is useful to export your environment to a <code>environment.yml</code> file and include it in your repository. This allows replicating the project in a different computer by you or someone else.</p> <pre><code>conda env export --name your_env_name --file your_env_name.yml\n</code></pre> <p>This creates the <code>.yml</code> file with version number and source repo for all your packages. Thus, this file is specific for a particular OS and Python build version.</p> <p>To create an agnostic file, use</p> <pre><code>conda env export --name your-env-name | cut -f 1 -d '=' &gt; your-env-name.yml\n</code></pre> <p>This strips out the version numbers, enabling you to upgrade to newer versions of the packages.</p>"},{"location":"teaching_resources/python/conda_pkg_manager/#install-from-an-environment-file","title":"Install from an environment file\u00b6","text":"<p>To create a new environment from this file, run</p> <pre><code>conda env create -f your_env_file.yml\n</code></pre>"},{"location":"teaching_resources/python/mamba_pkg_manager/","title":"Mamba - Env &amp; Package manager","text":""},{"location":"teaching_resources/python/mamba_pkg_manager/#introduction","title":"Introduction","text":"<p>What is mamba: Mamba is a package and environment for Python. It is similar to Conda, but uses a C++ solver that is much faster. It can pick packages from regular Conda channels. It's syntax mirrors that of Conda quite closely.</p> <p>Installation: You can install micromamba by itself, say in a Docker image. But on regular workstations, you first install Conda, then install mamba in the base env using <code>brew</code></p> <pre><code>brew install --cask micromamba\n</code></pre> <p>Then, enable shell completion:</p> <pre><code>micromamba shell completion\nsource ~/.zshrc\n</code></pre>"},{"location":"teaching_resources/python/mamba_pkg_manager/#operations","title":"Operations","text":"<p>Creating new env:</p> <pre><code>micromamba create -n &lt;name of env&gt; &lt;pkg list&gt;\nmicromamba create -n stac jupyterlab\n</code></pre> <p>You will notice the resolution time is pretty fast and so is the speed of installs. The shell returns an output like so:</p> <pre><code>\n                                           __\n          __  ______ ___  ____ _____ ___  / /_  ____ _\n         / / / / __ `__ \\/ __ `/ __ `__ \\/ __ \\/ __ `/\n        / /_/ / / / / / / /_/ / / / / / / /_/ / /_/ /\n       / .___/_/ /_/ /_/\\__,_/_/ /_/ /_/_.___/\\__,_/\n      /_/\n\nwarning  libmamba 'root_prefix' set with default value: /Users/abharathi/micromamba\nconda-forge/osx-arm64                                4.0MB @   3.0MB/s  1.5s\nconda-forge/noarch                                  10.1MB @   3.9MB/s  2.9s\n\nTransaction\n\n  Prefix: /Users/abharathi/micromamba/envs/stac\n\n  Updating specs:\n\n   - jupyterlab\n\n\n  Package                              Version  Build               Channel                    Size\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  Install:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n  + anyio                                3.6.2  pyhd8ed1ab_0        conda-forge/noarch         85kB\n</code></pre> <p>Listing and Activating env: You can use regular <code>conda</code> to activate the env:</p> <pre><code>micromamba env list\nmicromamba activate &lt;env name&gt;\n</code></pre> <p>Installing packages Once activated, then use the <code>micromamba install &lt;pkg&gt;</code> syntax, similar to conda.</p> <pre><code>micromamba install -c &lt;channel&gt; &lt;pkg_name&gt; -y\n</code></pre>"},{"location":"teaching_resources/python/python_classes/","title":"Classes","text":"In\u00a0[6]: Copied! <pre># Define a class to hold a satellite or aerial imagery file. Its properties give information\n# such as location of the ground, area, dimensions, spatial and spectral resolution etc.\n\nclass ImageryObject:\n    _default_gsd = 5.0\n    \n    def __init__(self, file_path):\n        self._file_path = file_path\n        self._gps_location = (3,4)\n        \n    @property\n    def bands(self):\n        #count number of bands\n        count = 3\n        return count\n    \n    @property\n    def gsd(self):\n        # logic to calculate the ground sample distance\n        gsd = 10.0\n        return gsd\n    \n    @property\n    def address(self):\n        # logic to reverse geocode the self._gps_location to get address\n        # reverse geocode self._gps_location\n        address = \"123 XYZ Street\"\n        return address\n    \n    #class methods\n    def display(self):\n        #logic to display picture\n        print(\"image is displayed\")\n    \n    def shuffle_bands(self):\n        #logic to shift RGB combination\n        print(\"shifting pands\")\n        self.display()\n</pre> # Define a class to hold a satellite or aerial imagery file. Its properties give information # such as location of the ground, area, dimensions, spatial and spectral resolution etc.  class ImageryObject:     _default_gsd = 5.0          def __init__(self, file_path):         self._file_path = file_path         self._gps_location = (3,4)              @property     def bands(self):         #count number of bands         count = 3         return count          @property     def gsd(self):         # logic to calculate the ground sample distance         gsd = 10.0         return gsd          @property     def address(self):         # logic to reverse geocode the self._gps_location to get address         # reverse geocode self._gps_location         address = \"123 XYZ Street\"         return address          #class methods     def display(self):         #logic to display picture         print(\"image is displayed\")          def shuffle_bands(self):         #logic to shift RGB combination         print(\"shifting pands\")         self.display() In\u00a0[7]: Copied! <pre># class instantiation\nimg1 = ImageryObject(\"user\\img\\file.img\") #pass value to constructor\n</pre> # class instantiation img1 = ImageryObject(\"user\\img\\file.img\") #pass value to constructor In\u00a0[8]: Copied! <pre>img1.address\n</pre> img1.address Out[8]: <pre>'123 XYZ Street'</pre> In\u00a0[9]: Copied! <pre>img1._default_gsd\n</pre> img1._default_gsd Out[9]: <pre>5.0</pre> In\u00a0[10]: Copied! <pre>img1._gps_location\n</pre> img1._gps_location Out[10]: <pre>(3, 4)</pre> In\u00a0[11]: Copied! <pre>img1.shuffle_bands()\n</pre> img1.shuffle_bands() <pre>shifting pands\nimage is displayed\n</pre> In\u00a0[12]: Copied! <pre># Get help on any object. Only public methods, properties are displayed.\n# fields are private, properties are public. Class variables beginning with _ are private fields.\nhelp(img1)\n</pre> # Get help on any object. Only public methods, properties are displayed. # fields are private, properties are public. Class variables beginning with _ are private fields. help(img1) <pre>Help on ImageryObject in module __main__ object:\n\nclass ImageryObject(builtins.object)\n |  Methods defined here:\n |  \n |  __init__(self, file_path)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  display(self)\n |      #class methods\n |  \n |  shuffle_bands(self)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  address\n |  \n |  bands\n |  \n |  gsd\n\n</pre> In\u00a0[6]: Copied! <pre>class Rectangle:\n    \"Defines a rectangle\"\n    \n    def __init__(self, width, height):\n        self._width = width\n        self._height = height\n        \n    @property\n    def width(self):\n        return self._width\n    \n    @width.setter\n    def width(self, w):\n        if w &lt; 0:\n            raise ValueError(\"Width cannot be negative\")\n        else:\n            self._width = w\n    \n    @property\n    def height(self):\n        return self._height\n    \n    @height.setter\n    def height(self, h):\n        if h &lt; 0:\n            raise valueError(\"Height cannot be negative\")\n        else:\n            self._height = h\n    \n    def area(self):\n        return self._width * self._height\n    \n    def perimeter(self):\n        return 2*(self._width + self._height)\n    \n    # Customize how the object prints\n    def __str__(self):\n        return f\"Rectangle: Width: {self._width}  Height: {self._height}\"\n    \n    # Customize how the object displays in IPython kernels like notebooks\n    def _repr_png_(self):\n        import matplotlib.pyplot as plt\n        from matplotlib.patches import Rectangle\n        \n        #define Matplotlib figure and axis\n        fig, ax = plt.subplots()\n        \n        #create simple line plot\n        ax.plot([0, 10],[0, 10])\n\n\n        #add rectangle to plot\n        ax.add_patch(Rectangle((1, 1), self._width, self._height))\n\n        #display plot\n        # plt.show()\n        return ax\n    \n    # Equality checks. \n    # If you compare r1=Rectangle(10, 20) and r2 = Rectangle(10, 20), they would \n    # r1 is r2 =&gt; will return false as it should. These are 2 different objects\n    # r1 == r2 =&gt; will also return False\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            if self.width == other.width and self.height == other.height:\n                return True\n            \n        return False\n    \n    # LT, GT, LE, GE comparisons\n    def __le__(self, other):\n        if isinstance(other, self.__class__):\n            if self.area() &lt;= other.area():\n                return True\n        \n        return False\n</pre> class Rectangle:     \"Defines a rectangle\"          def __init__(self, width, height):         self._width = width         self._height = height              @property     def width(self):         return self._width          @width.setter     def width(self, w):         if w &lt; 0:             raise ValueError(\"Width cannot be negative\")         else:             self._width = w          @property     def height(self):         return self._height          @height.setter     def height(self, h):         if h &lt; 0:             raise valueError(\"Height cannot be negative\")         else:             self._height = h          def area(self):         return self._width * self._height          def perimeter(self):         return 2*(self._width + self._height)          # Customize how the object prints     def __str__(self):         return f\"Rectangle: Width: {self._width}  Height: {self._height}\"          # Customize how the object displays in IPython kernels like notebooks     def _repr_png_(self):         import matplotlib.pyplot as plt         from matplotlib.patches import Rectangle                  #define Matplotlib figure and axis         fig, ax = plt.subplots()                  #create simple line plot         ax.plot([0, 10],[0, 10])           #add rectangle to plot         ax.add_patch(Rectangle((1, 1), self._width, self._height))          #display plot         # plt.show()         return ax          # Equality checks.      # If you compare r1=Rectangle(10, 20) and r2 = Rectangle(10, 20), they would      # r1 is r2 =&gt; will return false as it should. These are 2 different objects     # r1 == r2 =&gt; will also return False     def __eq__(self, other):         if isinstance(other, self.__class__):             if self.width == other.width and self.height == other.height:                 return True                      return False          # LT, GT, LE, GE comparisons     def __le__(self, other):         if isinstance(other, self.__class__):             if self.area() &lt;= other.area():                 return True                  return False In\u00a0[7]: Copied! <pre>r1 = Rectangle(10, 20)\nprint(r1)\nr1\n</pre> r1 = Rectangle(10, 20) print(r1) r1 <pre>Rectangle: Width: 10  Height: 20\n</pre> <pre>/Users/Geodexter/opt/anaconda3/lib/python3.9/site-packages/IPython/core/formatters.py:368: FormatterWarning: image/png formatter returned invalid type &lt;class 'matplotlib.axes._subplots.AxesSubplot'&gt; (expected (&lt;class 'bytes'&gt;, &lt;class 'str'&gt;)) for object: &lt;__main__.Rectangle object at 0x7f9ac63484f0&gt;\n  warnings.warn(\n</pre> Out[7]: <pre>&lt;__main__.Rectangle at 0x7f9ac63484f0&gt;</pre> In\u00a0[8]: Copied! <pre>r1.area(), r1.perimeter()\n</pre> r1.area(), r1.perimeter() Out[8]: <pre>(200, 60)</pre> In\u00a0[9]: Copied! <pre>r2 = Rectangle(10, 20) # same as r1\nr1 is r2  # should be false as these are two diff obj, not singleton with alias\n</pre> r2 = Rectangle(10, 20) # same as r1 r1 is r2  # should be false as these are two diff obj, not singleton with alias Out[9]: <pre>False</pre> In\u00a0[10]: Copied! <pre>r1 == r2  # This should equate to True as these are the identical objects\n</pre> r1 == r2  # This should equate to True as these are the identical objects Out[10]: <pre>True</pre> In\u00a0[11]: Copied! <pre>r1 &lt;= r2\n</pre> r1 &lt;= r2 Out[11]: <pre>True</pre> In\u00a0[12]: Copied! <pre>r3 = Rectangle(30, 40)\nprint(r1 &lt;= r3)\nprint(r3 &lt;= r1)\n</pre> r3 = Rectangle(30, 40) print(r1 &lt;= r3) print(r3 &lt;= r1) <pre>True\nFalse\n</pre> In\u00a0[15]: Copied! <pre>r2 &gt;= r3  # even though ge is not implemented, Python is smart and will flip le and use it.\n</pre> r2 &gt;= r3  # even though ge is not implemented, Python is smart and will flip le and use it. Out[15]: <pre>False</pre> In\u00a0[16]: Copied! <pre># Will throw an error:\nr2 &gt; r3\n</pre> # Will throw an error: r2 &gt; r3 <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/var/folders/d8/93856prx6tj3x7vdn97z2ps40000gn/T/ipykernel_38974/128862589.py in &lt;module&gt;\n      1 # Will throw an error:\n----&gt; 2 r2 &gt; r3\n\nTypeError: '&gt;' not supported between instances of 'Rectangle' and 'Rectangle'</pre> In\u00a0[11]: Copied! <pre>print(None is None)\nprint(None == None)\n\nlupus = None\nprint(lupus is None)  # Evals to True as None is a singleton and lupus is an alias\nprint(lupus == None)\n</pre> print(None is None) print(None == None)  lupus = None print(lupus is None)  # Evals to True as None is a singleton and lupus is an alias print(lupus == None) <pre>True\nTrue\nTrue\nTrue\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/python/python_classes/#classes","title":"Classes\u00b6","text":"<p>Everything is an object in Python including native types. You define class names with camel casing. You define the constructor with special name <code>__init__()</code>. The fields (private) are denoted with <code>_variable_name</code> specification and properties are decorated with <code>@property</code> decorator.</p> <p>Fields and properties are accessed within the class using <code>self.name</code> notation. This helps differentiate a class field / property from a local variable or method argument of the same name.</p>"},{"location":"teaching_resources/python/python_classes/#a-simple-class","title":"A simple class\u00b6","text":"<pre>class MyClass:\n    _local_variables = \"value\"\n\n    def __init__(self, args):  #constructor\n        statements\n        self._local_variables = args   # assign values to fields\n\n    def func_1(self, args):\n        statements\n</pre> <p>The first argument to class methods, including the constructor is <code>self</code>, which points to the instance itself. The name <code>self</code> is convention. You can call it anything. You can instantiate as shown below:</p> <pre>obj1 = MyClass(args_defined_in_constructor)\n</pre>"},{"location":"teaching_resources/python/python_classes/#overloading-built-in-methods","title":"Overloading built-in methods\u00b6","text":"<p>In Python, there is no concept of overloading. However, you can overwrite certain built-in methods to customize the behavior as shown below:</p>"},{"location":"teaching_resources/python/python_classes/#sidebar-python-singletons","title":"Sidebar: Python Singletons\u00b6","text":"<p>Singleton is a type of class which can instantiate just 1 instance of itself. Hence there can be just one object of that class and no more. These useful for making reference counters, manager objects.</p> <p>In Python, <code>None</code>, <code>True</code>, <code>False</code> are all singleton objects. Now you can assign alias to them and call them something else, but all of those will still be alias pointing to the same single object in memory.</p> <p>Singletons are relevant for that session of the kernel. Once you restart, their address space will change.</p>"},{"location":"teaching_resources/python/python_conditional_execution/","title":"Python conditional execution","text":"In\u00a0[1]: Copied! <pre>a = 20\n\nif a &lt; 10:\n    print(\"A less than 10\")\nelse:\n    print(\"A greater than or equal to 10\")\n</pre> a = 20  if a &lt; 10:     print(\"A less than 10\") else:     print(\"A greater than or equal to 10\") <pre>A greater than or equal to 10\n</pre> In\u00a0[5]: Copied! <pre>a = 13\n\nif a &lt; 5:\n    print(\"a is less than 5\")\nelif a &lt; 10:\n    print(\"a is greater than 5 but less than 10\")\nelif a &lt; 15:\n    print(\"a is greater than 10 but less than 15\")\nelse:\n    print(\"a is greater than 15\")\n</pre> a = 13  if a &lt; 5:     print(\"a is less than 5\") elif a &lt; 10:     print(\"a is greater than 5 but less than 10\") elif a &lt; 15:     print(\"a is greater than 10 but less than 15\") else:     print(\"a is greater than 15\") <pre>a is greater than 10 but less than 15\n</pre> In\u00a0[6]: Copied! <pre>a = 16\n\nif 5 &lt; a &lt; 25:\n    print(\"A is between 5 and 25\")\n</pre> a = 16  if 5 &lt; a &lt; 25:     print(\"A is between 5 and 25\") <pre>A is between 5 and 25\n</pre> In\u00a0[8]: Copied! <pre>a=5\n'a less than 5' if a &lt; 5 else 'a greater than or equal 5'\n</pre> a=5 'a less than 5' if a &lt; 5 else 'a greater than or equal 5' Out[8]: <pre>'a greater than or equal 5'</pre> In\u00a0[11]: Copied! <pre>my_list = [0, 1, -1, 5, True, False, [], 'a', '', ' ', {'a':'vak'}, {}]\n\nfor e in my_list:\n    print(f'{e}: , True') if e else print(f'{e}: False')\n</pre> my_list = [0, 1, -1, 5, True, False, [], 'a', '', ' ', {'a':'vak'}, {}]  for e in my_list:     print(f'{e}: , True') if e else print(f'{e}: False') <pre>0: False\n1: , True\n-1: , True\n5: , True\nTrue: , True\nFalse: False\n[]: False\na: , True\n: False\n : , True\n{'a': 'vak'}: , True\n{}: False\n</pre> <p>We see that <code>0</code>, <code>False</code>, empty string, empty list, empty dict eval to <code>False</code>. While <code>1</code>, <code>-1</code>, string with space or any char all eval to <code>True</code>.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/python/python_conditional_execution/#python-conditional-execution","title":"Python conditional execution\u00b6","text":""},{"location":"teaching_resources/python/python_conditional_execution/#conditional-execution","title":"Conditional execution\u00b6","text":""},{"location":"teaching_resources/python/python_conditional_execution/#simple-if-else-block","title":"Simple If - else block:\u00b6","text":""},{"location":"teaching_resources/python/python_conditional_execution/#implementing-a-switch-block-using-elif","title":"Implementing a switch block using <code>elif</code>\u00b6","text":""},{"location":"teaching_resources/python/python_conditional_execution/#between-check","title":"Between check\u00b6","text":""},{"location":"teaching_resources/python/python_conditional_execution/#ternary-operator","title":"Ternary operator\u00b6","text":""},{"location":"teaching_resources/python/python_conditional_execution/#truthy-checks","title":"Truthy checks\u00b6","text":"<p>How do the following evaluate on a truthy check?</p>"},{"location":"teaching_resources/python/python_datatypes/","title":"Python - data types","text":"In\u00a0[1]: Copied! <pre>l1 = list()\ntype(l1)\n</pre> l1 = list() type(l1) Out[1]: <pre>list</pre> In\u00a0[2]: Copied! <pre>l2 = []\nlen(l2)\n</pre> l2 = [] len(l2) Out[2]: <pre>0</pre> In\u00a0[3]: Copied! <pre>l3 = [1,2,3,4,5,6,7,8,9]\nl3[:] #prints all\n</pre> l3 = [1,2,3,4,5,6,7,8,9] l3[:] #prints all Out[3]: <pre>[1, 2, 3, 4, 5, 6, 7, 8, 9]</pre> In\u00a0[4]: Copied! <pre>l3[0]\n</pre> l3[0] Out[4]: <pre>1</pre> In\u00a0[5]: Copied! <pre>l3[:4] #prints first 4. the : is slicing operator\n</pre> l3[:4] #prints first 4. the : is slicing operator Out[5]: <pre>[1, 2, 3, 4]</pre> In\u00a0[6]: Copied! <pre>l3[4:7] #upto 1 less than highest index\n</pre> l3[4:7] #upto 1 less than highest index Out[6]: <pre>[5, 6, 7]</pre> In\u00a0[7]: Copied! <pre>a = len(l3)\nl3[a-1] #negative index for traversing in opposite dir\n</pre> a = len(l3) l3[a-1] #negative index for traversing in opposite dir Out[7]: <pre>9</pre> In\u00a0[8]: Copied! <pre>l3[-4:] #to pick the last 4 elements\n</pre> l3[-4:] #to pick the last 4 elements Out[8]: <pre>[6, 7, 8, 9]</pre> In\u00a0[9]: Copied! <pre>l3.reverse() #happens inplace\n</pre> l3.reverse() #happens inplace In\u00a0[10]: Copied! <pre>l3\n</pre> l3 Out[10]: <pre>[9, 8, 7, 6, 5, 4, 3, 2, 1]</pre> In\u00a0[11]: Copied! <pre>l3.append(10) #to add new values\nl3\n</pre> l3.append(10) #to add new values l3 Out[11]: <pre>[9, 8, 7, 6, 5, 4, 3, 2, 1, 10]</pre> In\u00a0[12]: Copied! <pre>a1 = ['a','b','c']\nl3.append(a1)\n</pre> a1 = ['a','b','c'] l3.append(a1) In\u00a0[13]: Copied! <pre>l3[-1]\n</pre> l3[-1] Out[13]: <pre>['a', 'b', 'c']</pre> In\u00a0[14]: Copied! <pre>a1 = ['a','b','c']\nl3.extend(a1) #to splice two lists. need not be same data type\nl3\n</pre> a1 = ['a','b','c'] l3.extend(a1) #to splice two lists. need not be same data type l3 Out[14]: <pre>[9, 8, 7, 6, 5, 4, 3, 2, 1, 10, ['a', 'b', 'c'], 'a', 'b', 'c']</pre> In\u00a0[15]: Copied! <pre>lol = [[1,2,3],[4,5,6]] #lol - list of lists\nlen(lol)\n</pre> lol = [[1,2,3],[4,5,6]] #lol - list of lists len(lol) Out[15]: <pre>2</pre> In\u00a0[16]: Copied! <pre>lol[1].reverse()\nlol[1]\n</pre> lol[1].reverse() lol[1] Out[16]: <pre>[6, 5, 4]</pre> In\u00a0[17]: Copied! <pre>l3\n</pre> l3 Out[17]: <pre>[9, 8, 7, 6, 5, 4, 3, 2, 1, 10, ['a', 'b', 'c'], 'a', 'b', 'c']</pre> In\u00a0[18]: Copied! <pre>l3[-1] = 'solar fare' #modify the last element\nl3\n</pre> l3[-1] = 'solar fare' #modify the last element l3 Out[18]: <pre>[9, 8, 7, 6, 5, 4, 3, 2, 1, 10, ['a', 'b', 'c'], 'a', 'b', 'solar fare']</pre> In\u00a0[19]: Copied! <pre>#list.insert(index, object) to insert a new value\nprint(str(len(l3))) #before insertion\nl3.insert(1,'two')\nl3\n</pre> #list.insert(index, object) to insert a new value print(str(len(l3))) #before insertion l3.insert(1,'two') l3 <pre>14\n</pre> Out[19]: <pre>[9, 'two', 8, 7, 6, 5, 4, 3, 2, 1, 10, ['a', 'b', 'c'], 'a', 'b', 'solar fare']</pre> In\u00a0[20]: Copied! <pre># l3.pop(index) remove item at index and give that item\nl3.pop(-3) #remove 3rd item from last and give them\n</pre> # l3.pop(index) remove item at index and give that item l3.pop(-3) #remove 3rd item from last and give them Out[20]: <pre>'a'</pre> In\u00a0[21]: Copied! <pre>l3\n</pre> l3 Out[21]: <pre>[9, 'two', 8, 7, 6, 5, 4, 3, 2, 1, 10, ['a', 'b', 'c'], 'b', 'solar fare']</pre> In\u00a0[22]: Copied! <pre># l3.clear()  to empty a list\nlol.clear()\nlol\n</pre> # l3.clear()  to empty a list lol.clear() lol Out[22]: <pre>[]</pre> In\u00a0[1]: Copied! <pre>l3 = [9, 8, 7, 6, 5, 4, 3, 2, 1, 10, ['a', 'b', 'c'], 'a', 'b', 'c', 10,10,10]\nl3\n</pre> l3 = [9, 8, 7, 6, 5, 4, 3, 2, 1, 10, ['a', 'b', 'c'], 'a', 'b', 'c', 10,10,10] l3 Out[1]: <pre>[9, 8, 7, 6, 5, 4, 3, 2, 1, 10, ['a', 'b', 'c'], 'a', 'b', 'c', 10, 10, 10]</pre> In\u00a0[2]: Copied! <pre># l3.count(value) counts the number of occurrences of a value\nl3.count(10)\n</pre> # l3.count(value) counts the number of occurrences of a value l3.count(10) Out[2]: <pre>4</pre> In\u00a0[3]: Copied! <pre># l3.index(value, &lt;start, &lt;stop&gt;&gt;) returns the first occurrence of element\nl3.index(10)\n</pre> # l3.index(value, &gt;) returns the first occurrence of element l3.index(10) Out[3]: <pre>9</pre> <p>Find all the indices of an element</p> In\u00a0[5]: Copied! <pre># indices = [i for i, x in enumerate(my_list) if x == \"whatever\"]\n\n#find all occurrence of 10\nindices_of_10 = [i for i, x in enumerate(l3) if x == 10]\nindices_of_10\n</pre> # indices = [i for i, x in enumerate(my_list) if x == \"whatever\"]  #find all occurrence of 10 indices_of_10 = [i for i, x in enumerate(l3) if x == 10] indices_of_10 Out[5]: <pre>[9, 14, 15, 16]</pre> In\u00a0[7]: Copied! <pre>list(enumerate(l3))\n</pre> list(enumerate(l3)) Out[7]: <pre>[(0, 9),\n (1, 8),\n (2, 7),\n (3, 6),\n (4, 5),\n (5, 4),\n (6, 3),\n (7, 2),\n (8, 1),\n (9, 10),\n (10, ['a', 'b', 'c']),\n (11, 'a'),\n (12, 'b'),\n (13, 'c'),\n (14, 10),\n (15, 10),\n (16, 10)]</pre> In\u00a0[26]: Copied! <pre>d1 = dict()\nd2 = {}\nlen(d2)\n</pre> d1 = dict() d2 = {} len(d2) Out[26]: <pre>0</pre> In\u00a0[27]: Copied! <pre>d3 = {'day':'Thursday',\n     'day_of_week':5,\n     'start_of_week':'Sunday',\n     'day_of_year':123,\n     'dod':{'month_of_year':'Feb',\n           'year':2017},\n     'list1':[8,7,66]}\nlen(d3)\n</pre> d3 = {'day':'Thursday',      'day_of_week':5,      'start_of_week':'Sunday',      'day_of_year':123,      'dod':{'month_of_year':'Feb',            'year':2017},      'list1':[8,7,66]} len(d3) Out[27]: <pre>6</pre> In\u00a0[28]: Copied! <pre>d3.keys()\n</pre> d3.keys() Out[28]: <pre>dict_keys(['day', 'day_of_week', 'start_of_week', 'day_of_year', 'dod', 'list1'])</pre> In\u00a0[29]: Copied! <pre>d3['start_of_week']\n</pre> d3['start_of_week'] Out[29]: <pre>'Sunday'</pre> In\u00a0[30]: Copied! <pre>type(d3['dod'])\n</pre> type(d3['dod']) Out[30]: <pre>dict</pre> In\u00a0[31]: Copied! <pre># now that dod is a dict, get its keys\nd3['dod'].keys()\n</pre> # now that dod is a dict, get its keys d3['dod'].keys() Out[31]: <pre>dict_keys(['month_of_year', 'year'])</pre> In\u00a0[32]: Copied! <pre>d3['dod']['year']\n</pre> d3['dod']['year'] Out[32]: <pre>2017</pre> In\u00a0[33]: Copied! <pre>d3['day_of_year'] = -48\nd3\n</pre> d3['day_of_year'] = -48 d3 Out[33]: <pre>{'day': 'Thursday',\n 'day_of_week': 5,\n 'day_of_year': -48,\n 'dod': {'month_of_year': 'Feb', 'year': 2017},\n 'list1': [8, 7, 66],\n 'start_of_week': 'Sunday'}</pre> In\u00a0[46]: Copied! <pre># insert new values just by adding kvp (key value pair)\nd3['workout_of_the_week']='bungee jumpging'\nd3\n</pre> # insert new values just by adding kvp (key value pair) d3['workout_of_the_week']='bungee jumpging' d3 Out[46]: <pre>{'day': 'Thursday',\n 'day_of_week': 5,\n 'day_of_year': -48,\n 'dod': {'month_of_year': 'Feb', 'year': 2017},\n 'list1': [8, 7, 66],\n 'start_of_week': 'Sunday',\n 'workout_of_the_week': 'bungee jumpging'}</pre> In\u00a0[47]: Copied! <pre>d3['dayyy']\n</pre> d3['dayyy'] <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-47-c500fcefcb1b&gt; in &lt;module&gt;()\n----&gt; 1 d3['dayyy']\n\nKeyError: 'dayyy'</pre> In\u00a0[48]: Copied! <pre># safe way to get elements is to use get()\nd3.get('day')\n</pre> # safe way to get elements is to use get() d3.get('day') Out[48]: <pre>'Thursday'</pre> In\u00a0[49]: Copied! <pre>d3.get('dayyy') #retuns None\n</pre> d3.get('dayyy') #retuns None In\u00a0[50]: Copied! <pre># use items() to get a list of tuples of key value pairs\nd3.items()\n</pre> # use items() to get a list of tuples of key value pairs d3.items() Out[50]: <pre>dict_items([('day_of_week', 5), ('day', 'Thursday'), ('workout_of_the_week', 'bungee jumpging'), ('dod', {'year': 2017, 'month_of_year': 'Feb'}), ('list1', [8, 7, 66]), ('day_of_year', -48), ('start_of_week', 'Sunday')])</pre> In\u00a0[51]: Copied! <pre># use values() to get only the values\nd3.values()\n</pre> # use values() to get only the values d3.values() Out[51]: <pre>dict_values([5, 'Thursday', 'bungee jumpging', {'year': 2017, 'month_of_year': 'Feb'}, [8, 7, 66], -48, 'Sunday'])</pre> In\u00a0[58]: Copied! <pre>t1 = tuple()\nt2 = ()\nlen(t1)\n</pre> t1 = tuple() t2 = () len(t1) Out[58]: <pre>0</pre> In\u00a0[59]: Copied! <pre>type(t2)\n</pre> type(t2) Out[59]: <pre>tuple</pre> In\u00a0[60]: Copied! <pre>t3 = (3,4,5,'t','g','b')\nt3[0]\n</pre> t3 = (3,4,5,'t','g','b') t3[0] Out[60]: <pre>3</pre> In\u00a0[61]: Copied! <pre>#use it just like a list\nt3[-1]\n</pre> #use it just like a list t3[-1] Out[61]: <pre>'b'</pre> In\u00a0[62]: Copied! <pre>t3[0] = 'good evening'\n</pre> t3[0] = 'good evening' <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-62-8d3766e24208&gt; in &lt;module&gt;()\n----&gt; 1 t3[0] = 'good evening'\n\nTypeError: 'tuple' object does not support item assignment</pre> In\u00a0[63]: Copied! <pre>s1 = set([1,1,1,2,2,2,4,4,4,4,4,4,4,5])\ns1\n</pre> s1 = set([1,1,1,2,2,2,4,4,4,4,4,4,4,5]) s1 Out[63]: <pre>{1, 2, 4, 5}</pre> In\u00a0[64]: Copied! <pre>s2 = {1,2,2,2,2,3} \ns2\n</pre> s2 = {1,2,2,2,2,3}  s2 Out[64]: <pre>{1, 2, 3}</pre> In\u00a0[65]: Copied! <pre># works on dicts too\ns3_repeat_values = set({'k1':'v1',\n                   'k2':'v1',\n                   'k3':'v2'})\ns3_repeat_values\n</pre> # works on dicts too s3_repeat_values = set({'k1':'v1',                    'k2':'v1',                    'k3':'v2'}) s3_repeat_values Out[65]: <pre>{'k1', 'k2', 'k3'}</pre> In\u00a0[66]: Copied! <pre>type(s3_repeat_values)\n</pre> type(s3_repeat_values) Out[66]: <pre>set</pre> In\u00a0[67]: Copied! <pre># repeating keys\ns3_repeat_keys = set({'k1':'v1',\n                     'k1':'v2'})\n</pre> # repeating keys s3_repeat_keys = set({'k1':'v1',                      'k1':'v2'}) In\u00a0[68]: Copied! <pre>s3_repeat_keys\n</pre> s3_repeat_keys Out[68]: <pre>{'k1'}</pre> <p>Note. When you create a dict with duplicate keys, Python just keeps the last occurrence of the kvp. It thinks the kvp needs to be updated to the latest value</p> In\u00a0[69]: Copied! <pre>d80 = {'k1':'v1', 'k2':'v2', 'k1':'v45'} # k1 is repeated\nd80\n</pre> d80 = {'k1':'v1', 'k2':'v2', 'k1':'v45'} # k1 is repeated d80 Out[69]: <pre>{'k1': 'v45', 'k2': 'v2'}</pre>"},{"location":"teaching_resources/python/python_datatypes/#python-data-types","title":"Python - data types\u00b6","text":"<p>A set of reference materials to help you in those occassional bouts of forgetfulness.</p>"},{"location":"teaching_resources/python/python_datatypes/#python-tutorials-and-references","title":"Python tutorials and references\u00b6","text":"<p>Following are some resources to learn Python</p> <ol> <li>Article with reviews about various tutorials http://noeticforce.com/best-free-tutorials-to-learn-python-pdfs-ebooks-online-interactive</li> <li>user voted list of tutorials on quora: https://www.quora.com/What-is-the-best-online-resource-to-learn-Python</li> <li>Google's Python class https://developers.google.com/edu/python/</li> <li>https://www.learnpython.org/</li> <li>Python reference documentation https://docs.python.org/3/</li> <li>A list of Python libraries for various applications: https://github.com/vinta/awesome-python</li> </ol>"},{"location":"teaching_resources/python/python_datatypes/#python-type-system","title":"Python type system\u00b6","text":"<p>Before we get into data structures, let us talk about the type system in Python. At a high level, there are \"Numbers\", \"Collections\", \"Callables\" and \"Singletons\"</p> <p>Numbers have two categories - Integral and Non-integral numbers.</p> <ul> <li>Integral number types:<ul> <li><code>Integers</code></li> <li><code>Booleans</code></li> </ul> </li> <li>Non integral number types:<ul> <li><code>Floats</code> implemented as <code>doubles</code> in C</li> <li><code>Complex</code></li> <li><code>Decimals</code></li> <li><code>Fractions</code></li> </ul> </li> </ul> <p>Collections have three sub categories:</p> <ul> <li>Sequence types<ul> <li><code>List</code> (mutable)</li> <li><code>Tuple</code> (immutable)</li> <li><code>String</code> (immutable)</li> </ul> </li> <li>Sets<ul> <li><code>Set</code> - mutable</li> <li><code>FrozenSet</code> - immutable</li> </ul> </li> <li>Mappings<ul> <li><code>Dict</code></li> </ul> </li> </ul> <p>Callables are types that can be called for execution</p> <ul> <li>UDF or user defined functions</li> <li>generators</li> <li>classes</li> <li>instance methods</li> </ul> <p>Singletons are types that have only 1 instance within the execution space</p> <ul> <li><code>None</code></li> <li><code>NotImplemented</code></li> <li>Ellipsis operator : <code>(...)</code></li> </ul>"},{"location":"teaching_resources/python/python_datatypes/#python-naming-conventions","title":"Python naming conventions\u00b6","text":""},{"location":"teaching_resources/python/python_datatypes/#variable-identifier-names","title":"Variable / identifier names\u00b6","text":"<p>The following rules apply when choosing variable names</p> <ul> <li>Can start with <code>_</code>, <code>a-z, A-z</code></li> <li>Can be of any length and contain <code>0-9</code> in addition</li> <li>Can contain any unicode char</li> <li>Cannot be a reserved keyword in the Python language</li> </ul> <p>Special names</p> <ul> <li>Vars that start with <code>_</code> mean they are internal and not to be used by the consumer. They are private. But this is only by convention as everything is public in Python</li> <li>Further, when you run <code>from module import *</code>, vars that begin with <code>_</code> are not imported by the interpreter</li> <li>Vars that follow <code>__var_name__</code> are really reserved for Python internals. For example <code>__init__</code> is used for a class constructor. The <code>__lt__()</code> method is used to implement a custom <code>&lt;</code> operator etc. Don't invent your own <code>__var__</code> names.</li> <li>Vars that follow <code>__var_name</code> are slightly different. They are used in a specific feature called name mangling in inheritance chains.</li> </ul>"},{"location":"teaching_resources/python/python_datatypes/#pep8-conventions","title":"PEP8 conventions\u00b6","text":"<p>The list below are just conventions and not rules. Following these will improve code readability.</p> <ul> <li>Packages : short, all-lowercase names. No underscores</li> <li>Modules: short, all-lowercase names. Can have underscores</li> <li>Classes: CapWords or upper-camel case</li> <li>Functions &amp; Variables: snake_case</li> <li>Constants: UPPER_SNAKE_CASE</li> </ul>"},{"location":"teaching_resources/python/python_datatypes/#data-structures","title":"Data structures\u00b6","text":""},{"location":"teaching_resources/python/python_datatypes/#lists","title":"Lists\u00b6","text":"<pre><code>l1 = list()\nl2 = [] #both empty lists\nl3 = [1,2,3]</code></pre>"},{"location":"teaching_resources/python/python_datatypes/#list-slicing","title":"list slicing\u00b6","text":""},{"location":"teaching_resources/python/python_datatypes/#append-and-extend","title":"append and extend\u00b6","text":""},{"location":"teaching_resources/python/python_datatypes/#mutability-of-lists","title":"mutability of lists\u00b6","text":"<p>list elements are mutable and can be changed</p>"},{"location":"teaching_resources/python/python_datatypes/#lists-and-indices","title":"Lists and indices\u00b6","text":""},{"location":"teaching_resources/python/python_datatypes/#dictionaries","title":"Dictionaries\u00b6","text":"<p>Key value pairs</p> <pre><code>d1 = dict()\nd2 = {'key1':value,\n      'key2':value2}</code></pre>"},{"location":"teaching_resources/python/python_datatypes/#mutability-of-dicts","title":"mutability of dicts\u00b6","text":"<p>dicts like lists are mutable</p>"},{"location":"teaching_resources/python/python_datatypes/#dict-exploration","title":"dict exploration\u00b6","text":"<p>what happens when you inquire a key thats not present</p>"},{"location":"teaching_resources/python/python_datatypes/#tuple","title":"Tuple\u00b6","text":"<p>tuple is a immutable list</p>"},{"location":"teaching_resources/python/python_datatypes/#mutability-of-tuples","title":"mutability of tuples\u00b6","text":"<p>cannot modify tuples.</p>"},{"location":"teaching_resources/python/python_datatypes/#sets","title":"Sets\u00b6","text":"<p>set is a sequence of unique values</p> <pre><code>s1 = set(&lt;sequence&gt;)\ns2 = {}</code></pre>"},{"location":"teaching_resources/python/python_datatypes/#set-from-dictionary","title":"set from dictionary\u00b6","text":"<p>Works on dicts too. But will return a set of keys only, not values.</p>"},{"location":"teaching_resources/python/python_exception_handling/","title":"Exception handling","text":"In\u00a0[13]: Copied! <pre>try:\n    img2 = ImageryObject(\"user\\img\\file2.img\")\n    img2.display()\nexcept:\n    print(\"something bad happened\")\n</pre> try:     img2 = ImageryObject(\"user\\img\\file2.img\")     img2.display() except:     print(\"something bad happened\") <pre>image is displayed\n</pre> In\u00a0[14]: Copied! <pre>try:\n    img2 = ImageryObject(\"user\\img\\file2.img\")\n    img2.display()\nexcept:\n    print(\"something bad happened\")\nelse:\n    print(\"else block\")\nfinally:\n    print(\"finally block\")\n</pre> try:     img2 = ImageryObject(\"user\\img\\file2.img\")     img2.display() except:     print(\"something bad happened\") else:     print(\"else block\") finally:     print(\"finally block\") <pre>image is displayed\nelse block\nfinally block\n</pre> In\u00a0[15]: Copied! <pre>try:\n    img2 = ImageryObject()\n    img2.display()\nexcept:\n    print(\"something bad happened\")\nelse:\n    print(\"else block\")\nfinally:\n    print(\"finally block\")\n</pre> try:     img2 = ImageryObject()     img2.display() except:     print(\"something bad happened\") else:     print(\"else block\") finally:     print(\"finally block\") <pre>something bad happened\nfinally block\n</pre> In\u00a0[21]: Copied! <pre>try:\n    img2 = ImageryObject()\n    img2.display()\n\nexcept Exception as ex:\n    print(\"something bad happened\")\n    print(\"exactly what whent bad? : \" + str(ex))\n</pre> try:     img2 = ImageryObject()     img2.display()  except Exception as ex:     print(\"something bad happened\")     print(\"exactly what whent bad? : \" + str(ex)) <pre>something bad happened\nexactly what whent bad? : __init__() missing 1 required positional argument: 'file_path'\n</pre> In\u00a0[22]: Copied! <pre>try:\n    img2 = ImageryObject('path')\n    img2.dddisplay()\n\nexcept TypeError as terr:\n    print(\"looks like you forgot a parameter\")\nexcept Exception as ex:\n    print(\"nope, it went worng here: \" + str(ex))\n</pre> try:     img2 = ImageryObject('path')     img2.dddisplay()  except TypeError as terr:     print(\"looks like you forgot a parameter\") except Exception as ex:     print(\"nope, it went worng here: \" + str(ex)) <pre>nope, it went worng here: 'ImageryObject' object has no attribute 'dddisplay'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/python/python_exception_handling/#exception-handling","title":"Exception handling\u00b6","text":"<p>Exceptions are classes. You can define your own by inheriting from <code>Exception</code> class.</p> <pre>try:\n    statements\n\nexcept Exception_type1 as e1:\n    handling statements\n\nexcept Exception_type2 as e2:\n    specific handling statements\n\nexcept Exception as generic_ex:\n    generic handling statements\n\nelse:\n    some more statements  # executes if no exceptions were fired\n\nfinally:\n    default statements which will always be executed  # put logic such as conn close / mem release statements here\n</pre>"},{"location":"teaching_resources/python/python_functions/","title":"Python - Functions","text":"In\u00a0[1]: Copied! <pre>def func_add_numbers(num1, num2=10):\n    return (num1 + num2)  # return can be called as a statement or as a function\n</pre> def func_add_numbers(num1, num2=10):     return (num1 + num2)  # return can be called as a statement or as a function In\u00a0[2]: Copied! <pre>func_add_numbers(2)\n</pre> func_add_numbers(2) Out[2]: <pre>12</pre> In\u00a0[3]: Copied! <pre>func_add_numbers(2,34)\n</pre> func_add_numbers(2,34) Out[3]: <pre>36</pre> In\u00a0[4]: Copied! <pre>func_add_numbers()\n</pre> func_add_numbers() <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/var/folders/d8/93856prx6tj3x7vdn97z2ps40000gn/T/ipykernel_43270/1819578982.py in &lt;module&gt;\n----&gt; 1 func_add_numbers()\n\nTypeError: func_add_numbers() missing 1 required positional argument: 'num1'</pre> In\u00a0[10]: Copied! <pre>def func_add_numbers(a: int, b: int = 10) -&gt; int:\n    sum = a + b\n    return sum\n</pre> def func_add_numbers(a: int, b: int = 10) -&gt; int:     sum = a + b     return sum In\u00a0[12]: Copied! <pre>print(func_add_numbers(5, 4))\nprint(func_add_numbers(5))\nprint(func_add_numbers(5.5, 4.3))  # will still work as types are only hinted not enforced\n</pre> print(func_add_numbers(5, 4)) print(func_add_numbers(5)) print(func_add_numbers(5.5, 4.3))  # will still work as types are only hinted not enforced <pre>9\n15\n9.8\n</pre> In\u00a0[1]: Copied! <pre>def doubler(input_number):\n    return input_number*2\n</pre> def doubler(input_number):     return input_number*2 In\u00a0[2]: Copied! <pre>doubler(45)\n</pre> doubler(45) Out[2]: <pre>90</pre> In\u00a0[3]: Copied! <pre>temp_fn = lambda arg : arg*2\n</pre> temp_fn = lambda arg : arg*2 In\u00a0[4]: Copied! <pre>temp_fn(55)\n</pre> temp_fn(55) Out[4]: <pre>110</pre> In\u00a0[5]: Copied! <pre>type(temp_fn)\n</pre> type(temp_fn) Out[5]: <pre>function</pre> <p>It looks silly now, but lambdas work great with <code>map</code> and other productivity functions. You can have other methods and functions that do the heavy lifting and call them in a particular order from a lambda</p> In\u00a0[6]: Copied! <pre>l1 = [1,2,3,4,5,6,7]\n\n#to double elements in this list using list comp\nl1_double = [i*2 for i in l1]\nl1_double\n</pre> l1 = [1,2,3,4,5,6,7]  #to double elements in this list using list comp l1_double = [i*2 for i in l1] l1_double Out[6]: <pre>[2, 4, 6, 8, 10, 12, 14]</pre> In\u00a0[7]: Copied! <pre>#double using map and a function\nl1_double_2 = list(map(doubler, l1))\nl1_double_2\n</pre> #double using map and a function l1_double_2 = list(map(doubler, l1)) l1_double_2 Out[7]: <pre>[2, 4, 6, 8, 10, 12, 14]</pre> In\u00a0[15]: Copied! <pre>#double using map and a lambda function\nl1_double_3 = list(map(lambda arg:arg*2, l1))\nl1_double_3\n</pre> #double using map and a lambda function l1_double_3 = list(map(lambda arg:arg*2, l1)) l1_double_3 Out[15]: <pre>[2, 4, 6, 8, 10, 12, 14]</pre> In\u00a0[9]: Copied! <pre>#find only the odd numbers --&gt; list comp way\nl1_odd = [i for i in l1 if i%2 &gt; 0]\nl1_odd\n</pre> #find only the odd numbers --&gt; list comp way l1_odd = [i for i in l1 if i%2 &gt; 0] l1_odd Out[9]: <pre>[1, 3, 5, 7]</pre> In\u00a0[13]: Copied! <pre># find only odd numbers --&gt; filter with lambda way\nl1_odd_2 = list(filter(lambda arg:arg%2&gt;0, l1))\nl1_odd_2\n</pre> # find only odd numbers --&gt; filter with lambda way l1_odd_2 = list(filter(lambda arg:arg%2&gt;0, l1)) l1_odd_2 Out[13]: <pre>[1, 3, 5, 7]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/python/python_functions/#python-functions","title":"Python - Functions\u00b6","text":""},{"location":"teaching_resources/python/python_functions/#functions","title":"Functions\u00b6","text":"<p>Specify optional parameters in the end. Specify the default values for optional parameters with = value notation</p> <pre>def func_name(arg1, arg2=None):\n    operations\n    return value\n</pre>"},{"location":"teaching_resources/python/python_functions/#type-hinting","title":"Type hinting\u00b6","text":"<p>Starting at Python 3.7 you can provide hits to specify argument data type</p>"},{"location":"teaching_resources/python/python_functions/#productivity-hacks-for-python","title":"Productivity hacks for Python\u00b6","text":"<p>These are good to know shortcuts and methods that will reduce the need for writing explicit loops and condition checks. The comprehensions explained in cheat sheet 1 is a start, and falls under this category.</p> <p>Productivity functions</p> <ul> <li>lambda functions</li> <li>map function</li> <li>filter function</li> <li>reduce function</li> </ul>"},{"location":"teaching_resources/python/python_functions/#lambda-functions","title":"lambda functions\u00b6","text":"<p>lambdas are anonymous functions, typicaly one liner functions</p> <pre><code>lambda arg : ret_val</code></pre>"},{"location":"teaching_resources/python/python_functions/#map-function","title":"map function\u00b6","text":"<p>The <code>map</code> function will perform an operation on all elements of an input list. You can execute a function on all elements of a list without a loop, like a comprehension.</p> <pre><code>map(function, sequence)  --&gt; applies the function for each element in the sequence. The return sequence if of same length as input sequence</code></pre>"},{"location":"teaching_resources/python/python_functions/#filter-function","title":"filter function\u00b6","text":"<p><code>filter</code> function is used to filter out elements in a sequence based on a condition</p> <pre><code>filter(function, sequence)  --&gt; applies the function for each element in sequence, but the return sequence is same or smaller than input based on the condition in the `function`.\nThe function should return a bool</code></pre>"},{"location":"teaching_resources/python/python_iterations/","title":"Iteration","text":"In\u00a0[70]: Copied! <pre>list1 = [1,2,3,4,5,6,7]\nfor element in list1:\n    print(element)\n</pre> list1 = [1,2,3,4,5,6,7] for element in list1:     print(element) <pre>1\n2\n3\n4\n5\n6\n7\n</pre> In\u00a0[71]: Copied! <pre>for element in list1:\n    print(element, \" squared \", element*element)\n</pre> for element in list1:     print(element, \" squared \", element*element) <pre>1  squared  1\n2  squared  4\n3  squared  9\n4  squared  16\n5  squared  25\n6  squared  36\n7  squared  49\n</pre> In\u00a0[72]: Copied! <pre>for count in range(11,20):\n    print(str(count))\n</pre> for count in range(11,20):     print(str(count)) <pre>11\n12\n13\n14\n15\n16\n17\n18\n19\n</pre> In\u00a0[19]: Copied! <pre># Find the multiple of 7\n\nfor i in range(1,6):\n    print(i, end= \" \")\n    \n    if i % 7 == 0:\n        print(f'\\nMultiple of 7 found: {i}')\n        break\nelse:\n    print('\\nNo multiples of 7 found in this series')\n</pre> # Find the multiple of 7  for i in range(1,6):     print(i, end= \" \")          if i % 7 == 0:         print(f'\\nMultiple of 7 found: {i}')         break else:     print('\\nNo multiples of 7 found in this series') <pre>1 2 3 4 5 \nNo multiples of 7 found in this series\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[5]: Copied! <pre>a = 5\nwhile a &lt; 10:\n    print(a)\n    a += 1\n</pre> a = 5 while a &lt; 10:     print(a)     a += 1 <pre>5\n6\n7\n8\n9\n</pre> In\u00a0[6]: Copied! <pre>a = 5\n\nwhile True:  ## Do-while emulation\n    print(a)\n    a += 1\n    if a &gt;= 10:\n        break\n</pre> a = 5  while True:  ## Do-while emulation     print(a)     a += 1     if a &gt;= 10:         break <pre>5\n6\n7\n8\n9\n</pre> In\u00a0[8]: Copied! <pre># Fashion a username getter that won't give up until a valid username is input.\n\nmin_char = 2\n\nwhile True:\n    name = input(\"Enter your username: \")\n    if len(name) &gt;= min_char and name.isprintable() and name.isalpha():\n        break\n\nprint(f\"Hello {name}\")\n</pre> # Fashion a username getter that won't give up until a valid username is input.  min_char = 2  while True:     name = input(\"Enter your username: \")     if len(name) &gt;= min_char and name.isprintable() and name.isalpha():         break  print(f\"Hello {name}\") <pre>Enter your username: ./\\\nEnter your username: --0\nEnter your username: 90\nEnter your username: atma\nHello atma\n</pre> In\u00a0[13]: Copied! <pre># check if an element exists and append it to a list.\nl = [9,18,27]\nidx = 0\nval = 36\n\nwhile idx &lt; len(l):\n    if l[idx] == val:\n        break  # no need to append\n    idx += 1\n\nelse:\n    l.append(val)\n\nprint(l)\n</pre> # check if an element exists and append it to a list. l = [9,18,27] idx = 0 val = 36  while idx &lt; len(l):     if l[idx] == val:         break  # no need to append     idx += 1  else:     l.append(val)  print(l) <pre>[9, 18, 27, 36]\n</pre> In\u00a0[11]: Copied! <pre>a = 4\nwhile True:\n    a += 1\n    if a == 7:  # skip on condition\n        print(\"Skipped\")\n        continue\n    print(a)\n    \n    if a &gt; 11:\n        break\n</pre> a = 4 while True:     a += 1     if a == 7:  # skip on condition         print(\"Skipped\")         continue     print(a)          if a &gt; 11:         break      <pre>5\n6\nSkipped\n8\n9\n10\n11\n12\n</pre> In\u00a0[73]: Copied! <pre>list2_comp = [e*e for e in list1]\nlist2_comp\n</pre> list2_comp = [e*e for e in list1] list2_comp Out[73]: <pre>[1, 4, 9, 16, 25, 36, 49]</pre> In\u00a0[74]: Copied! <pre>list2_even = [e*e for e in list1 if e%2==0]\nlist2_even\n</pre> list2_even = [e*e for e in list1 if e%2==0] list2_even Out[74]: <pre>[4, 16, 36]</pre> In\u00a0[39]: Copied! <pre>d3 = {'day':'Thursday',\n     'day_of_week':5,\n     'start_of_week':'Sunday',\n     'day_of_year':123,\n     'dod':{'month_of_year':'Feb',\n           'year':2017},\n     'list1':[8,7,66]}\n</pre> d3 = {'day':'Thursday',      'day_of_week':5,      'start_of_week':'Sunday',      'day_of_year':123,      'dod':{'month_of_year':'Feb',            'year':2017},      'list1':[8,7,66]} In\u00a0[43]: Copied! <pre># get those kvp whose value is a list\n{k:v for k,v in d3.items() if type(v)==list}\n</pre> # get those kvp whose value is a list {k:v for k,v in d3.items() if type(v)==list} Out[43]: <pre>{'list1': [8, 7, 66]}</pre> <p>reverse keys and values? - works only when values are immutable types. hence filter them out</p> In\u00a0[45]: Copied! <pre>d4 = {k:v for k,v in d3.items() if type(v) not in [list, dict]}\nd4\n</pre> d4 = {k:v for k,v in d3.items() if type(v) not in [list, dict]} d4 Out[45]: <pre>{'day': 'Thursday',\n 'day_of_week': 5,\n 'day_of_year': 123,\n 'start_of_week': 'Sunday'}</pre> In\u00a0[47]: Copied! <pre># reverse keys and values\nd4_reverse = {v:k for k,v in d4.items()}\nd4_reverse\n</pre> # reverse keys and values d4_reverse = {v:k for k,v in d4.items()} d4_reverse Out[47]: <pre>{'Thursday': 'day',\n 5: 'day_of_week',\n 'Sunday': 'start_of_week',\n 123: 'day_of_year'}</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/python/python_iterations/#loops","title":"Loops\u00b6","text":""},{"location":"teaching_resources/python/python_iterations/#for-loop","title":"for loop\u00b6","text":""},{"location":"teaching_resources/python/python_iterations/#for-else-statement","title":"For-else statement\u00b6","text":"<p>The <code>else</code> block of the For-else is run if the for loop runs without encountering a break statement</p>"},{"location":"teaching_resources/python/python_iterations/#while-loop","title":"While loop\u00b6","text":"<pre>while &lt;condition&gt;:\n    statements\n</pre> <p>Runs until condition becomes false.</p>"},{"location":"teaching_resources/python/python_iterations/#do-while-loop-emulation-in-python","title":"Do While loop emulation in Python\u00b6","text":"<p>There is no do while loop, but we can jimmy one. Do-while pattern is good in scenarios where an iterable code needs to be run at-least once.</p>"},{"location":"teaching_resources/python/python_iterations/#while-else-loop","title":"While Else loop\u00b6","text":"<p>The While-Else offers an option where the Else block get executed if the While block did not encounter a break statement. This is useful to insert logic that needs to run if the while loop iterated to its entirity.</p>"},{"location":"teaching_resources/python/python_iterations/#continue-statement","title":"Continue statement\u00b6","text":"<p>Allows a loop to skip an interation when some condition is met</p>"},{"location":"teaching_resources/python/python_iterations/#comprehensions","title":"Comprehensions\u00b6","text":"<p>Comprehensions are an effective way to loop through sequences</p>"},{"location":"teaching_resources/python/python_iterations/#list-comprehension","title":"List comprehension\u00b6","text":"<pre><code>   [operation for index in sequence condition] </code></pre>"},{"location":"teaching_resources/python/python_iterations/#dictionary-comprehension","title":"Dictionary comprehension\u00b6","text":"<p>Same as list comprehension, but instead of lists, it returns a dictionary. You use {} instead of []</p> <pre><code>{key:value for key, value in dictionary if condition}</code></pre>"},{"location":"teaching_resources/python/python_language_optimizations/","title":"Python language optimizations","text":"In\u00a0[4]: Copied! <pre>a = 20\nb = c = 20\nd = 40 - 20\nprint(hex(id(a)), hex(id(b)), hex(id(c)), hex(id(d))) # should all be same\nprint(a is b)\nprint(b is c)\n</pre> a = 20 b = c = 20 d = 40 - 20 print(hex(id(a)), hex(id(b)), hex(id(c)), hex(id(d))) # should all be same print(a is b) print(b is c) <pre>0x7f871df21b90 0x7f871df21b90 0x7f871df21b90 0x7f871df21b90\nTrue\nTrue\n</pre> <p>Numbers beyond <code>256</code> are not singletons. Hence, each instance is a new object.</p> In\u00a0[7]: Copied! <pre>e = 500\nf = 1000*5\nprint(hex(id(e)),\"\\n\",hex(id(f)))\nprint(e is f)\n</pre> e = 500 f = 1000*5 print(hex(id(e)),\"\\n\",hex(id(f))) print(e is f) <pre>0x7f8722327410 \n 0x7f87223273f0\nFalse\n</pre> In\u00a0[8]: Copied! <pre>s1 = \"good_morning\"  # valid identifier string\ns2 = \"good_morning\"\nprint(hex(id(s1)), hex(id(s2)))\nprint(s1 is s2)\n</pre> s1 = \"good_morning\"  # valid identifier string s2 = \"good_morning\" print(hex(id(s1)), hex(id(s2))) print(s1 is s2) <pre>0x7f87223545b0 0x7f87223545b0\nTrue\n</pre> In\u00a0[9]: Copied! <pre>s3 = \"good morning\"  # space makes it an invalid identifier string\ns4 = \"good morning\"\nprint(hex(id(s3)), hex(id(s4)))  # diff addresses\nprint(s3 is s4)\nprint(s3 == s4)\n</pre> s3 = \"good morning\"  # space makes it an invalid identifier string s4 = \"good morning\" print(hex(id(s3)), hex(id(s4)))  # diff addresses print(s3 is s4) print(s3 == s4) <pre>0x7f8722354470 0x7f8722354070\nFalse\nTrue\n</pre> In\u00a0[10]: Copied! <pre>import sys\ns5 = sys.intern(\"good morning\")  # will add given str to globally interned strings as its new\ns6 = sys.intern(\"good morning\")  # will lookup global table and return previous result\nprint(hex(id(s5)), hex(id(s6)))  # same addresses\nprint(s5 is s6)\n</pre> import sys s5 = sys.intern(\"good morning\")  # will add given str to globally interned strings as its new s6 = sys.intern(\"good morning\")  # will lookup global table and return previous result print(hex(id(s5)), hex(id(s6)))  # same addresses print(s5 is s6) <pre>0x7f87223549f0 0x7f87223549f0\nTrue\n</pre> <p>Unless absolutely needed, you don't have to intern your objects. Do this with caution. The example below will time the difference between interned and non-interned operation.</p> In\u00a0[15]: Copied! <pre>def compare_str_using_equality(reps):\n    a = \"A long uninterned string is here\" * 250\n    b = \"A long uninterned string is here\" * 250\n    for i in range(reps):\n        if a == b:\n            pass\n</pre> def compare_str_using_equality(reps):     a = \"A long uninterned string is here\" * 250     b = \"A long uninterned string is here\" * 250     for i in range(reps):         if a == b:             pass In\u00a0[16]: Copied! <pre>def compare_str_using_identity(reps):\n    a = sys.intern(\"A long uninterned string is here\" * 250)\n    b = sys.intern(\"A long uninterned string is here\" * 250)\n    for i in range(reps):\n        if a is b:\n            pass\n</pre> def compare_str_using_identity(reps):     a = sys.intern(\"A long uninterned string is here\" * 250)     b = sys.intern(\"A long uninterned string is here\" * 250)     for i in range(reps):         if a is b:             pass In\u00a0[17]: Copied! <pre>import time\n\nstart = time.perf_counter()\ncompare_str_using_equality(10000000) # 10 million times\nend = time.perf_counter()\nprint(f'Equality: {end-start} secs')\n\nstart = time.perf_counter()\ncompare_str_using_identity(10000000) # 10 million times\nend = time.perf_counter()\nprint(f'Identity: {end-start} secs')\n</pre> import time  start = time.perf_counter() compare_str_using_equality(10000000) # 10 million times end = time.perf_counter() print(f'Equality: {end-start} secs')  start = time.perf_counter() compare_str_using_identity(10000000) # 10 million times end = time.perf_counter() print(f'Identity: {end-start} secs') <pre>Equality: 4.601066562001506 secs\nIdentity: 0.5143052139992506 secs\n</pre> In\u00a0[21]: Copied! <pre>def some_func():\n    a = 'abc'*4  # str is immutable\n    b = (1,2,3) * 5  # immutable\n    c = 34*77\n    d = 'this is a long string' * 10\n    e = [1,2,3] * 4\n</pre> def some_func():     a = 'abc'*4  # str is immutable     b = (1,2,3) * 5  # immutable     c = 34*77     d = 'this is a long string' * 10     e = [1,2,3] * 4 In\u00a0[22]: Copied! <pre>some_func.__code__.co_consts\n</pre> some_func.__code__.co_consts Out[22]: <pre>(None,\n 'abcabcabcabc',\n (1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3),\n 2618,\n 'this is a long stringthis is a long stringthis is a long stringthis is a long stringthis is a long stringthis is a long stringthis is a long stringthis is a long stringthis is a long stringthis is a long string',\n (1, 2, 3),\n 4)</pre> <p>Even thought I have not executed the function, just upon compilation, the constants are evaluated. All immutable types have been evaluated, but mutable types are not, even if they are small.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/python/python_language_optimizations/#interning","title":"Interning\u00b6","text":"<p><code>Interning</code> is the practice of reusing objects on-demand. At startup, the Python kernel will preload certain frequently used objects. These include <code>integers</code> in the range <code>-5 - 256</code>. Any time a new <code>int</code> is instantiated that falls within that range, will get the pre-assigned memory address. In other words, the ints from <code>-5 to 256</code> are <code>singletons</code>. Python does this for speed.</p>"},{"location":"teaching_resources/python/python_language_optimizations/#string-interning","title":"String interning\u00b6","text":"<p>Strings are immutable and are good candidates for being cached. Thus Python will <code>intern</code> strings that look like valid identifiers (strings that match variable name criteria) as the assumption is you might need those strings multiple times.</p>"},{"location":"teaching_resources/python/python_language_optimizations/#forced-interning","title":"Forced interning\u00b6","text":"<p>You can force the interpreter to intern strings that you think you will need multiple times. Use <code>sys.intern()</code> function. This is helpful in NLP type applications where you need to tokenize common strings like <code>a</code>, <code>the</code>, <code>when</code> etc.</p>"},{"location":"teaching_resources/python/python_language_optimizations/#python-peephole-optimizations","title":"Python - peephole optimizations\u00b6","text":"<p>In addition, Python, at compile time, it performs a number of subtler optimizations. These include:</p> <ul> <li>evaluating constant expressions (like <code>5*35</code> where all operands are constants)</li> <li>evaluating short immutable sequences (like <code>'abc'*3</code> or <code>(1,2)*4</code>) that are &lt;20 elements</li> <li>converting mutable collecitons to immutable collections if they contain constants</li> </ul> <p>We can see this in action by accessing <code>func_name.__code__.co_consts</code> property:</p>"},{"location":"teaching_resources/python/python_memory/","title":"Memory","text":"In\u00a0[105]: Copied! <pre>a = 25\nprint(id(a)) # decimal or base 10\nprint(hex(id(a))) # hex or base 8 version\n</pre> a = 25 print(id(a)) # decimal or base 10 print(hex(id(a))) # hex or base 8 version <pre>140485609004080\n0x7fc55ad21c30\n</pre> In\u00a0[106]: Copied! <pre>l1 = [3, 'baby', 5, -10]\nl2 = l1\nprint(id(l1) == id(l2))  # pass by reference\nprint(l2 == l1) # True\nprint(l2 is l1) # ? also True\nprint(hex(id(l1)))\n</pre> l1 = [3, 'baby', 5, -10] l2 = l1 print(id(l1) == id(l2))  # pass by reference print(l2 == l1) # True print(l2 is l1) # ? also True print(hex(id(l1))) <pre>True\nTrue\nTrue\n0x7fc560949ec0\n</pre> In\u00a0[55]: Copied! <pre># use sys module to count reference\nimport sys\nsys.getrefcount(l1), sys.getrefcount(l2)  # will bump both by 1, resulting in 3\n</pre> # use sys module to count reference import sys sys.getrefcount(l1), sys.getrefcount(l2)  # will bump both by 1, resulting in 3 Out[55]: <pre>(3, 3)</pre> In\u00a0[56]: Copied! <pre># use lower level ctypes\ndef lower_ref_count(var_address: int) -&gt; int:\n    import ctypes\n    return ctypes.c_long.from_address(var_address).value\n</pre> # use lower level ctypes def lower_ref_count(var_address: int) -&gt; int:     import ctypes     return ctypes.c_long.from_address(var_address).value In\u00a0[57]: Copied! <pre>lower_ref_count(id(l1)), lower_ref_count(id(l2))  # should return 2\n</pre> lower_ref_count(id(l1)), lower_ref_count(id(l2))  # should return 2 Out[57]: <pre>(2, 2)</pre> In\u00a0[58]: Copied! <pre>l3 = l2\nsys.getrefcount(l1)  # should return 4\n</pre> l3 = l2 sys.getrefcount(l1)  # should return 4 Out[58]: <pre>4</pre> <p>Now, let us free up the variables and see what happens to the reference counts</p> In\u00a0[59]: Copied! <pre>l3 = 'apple'\nprint(sys.getrefcount(l1)) # returns one less now\nprint(lower_ref_count(id(l1)))\n</pre> l3 = 'apple' print(sys.getrefcount(l1)) # returns one less now print(lower_ref_count(id(l1))) <pre>3\n2\n</pre> <p>Now, if you unset <code>l1</code> then <code>l2</code> still points to the same object in memory. So the ref count should be down to <code>1</code> not <code>0</code>.</p> In\u00a0[60]: Copied! <pre># unset the original var\nl1 = None\nprint(l2)\nprint(sys.getrefcount(l2)) # returns 2\nprint(lower_ref_count(id(l2))) # returns to 1\n</pre> # unset the original var l1 = None print(l2) print(sys.getrefcount(l2)) # returns 2 print(lower_ref_count(id(l2))) # returns to 1 <pre>[3, 'baby', 5, -10]\n2\n1\n</pre> <p>Now, if you remove all references to the obj, what happens to the ref count to that address??</p> In\u00a0[63]: Copied! <pre>l2_id = id(l2)\nl2 = 'apple'\n</pre> l2_id = id(l2) l2 = 'apple' In\u00a0[62]: Copied! <pre>print(lower_ref_count(l2_id))  # returns to 0\n</pre> print(lower_ref_count(l2_id))  # returns to 0 <pre>0\n</pre> In\u00a0[65]: Copied! <pre>print(lower_ref_count(l2_id))  # now, Python has assigned or reused that address to store something else\n</pre> print(lower_ref_count(l2_id))  # now, Python has assigned or reused that address to store something else <pre>2\n</pre> <p>In Python, we typically don't work with memory addresses or ref counts in Python. But it can be helpful to debug memory leaks and circular reference issues.</p> In\u00a0[68]: Copied! <pre>import gc\nlen(gc.get_objects())  ## returns the list of all objects gc tracks\n</pre> import gc len(gc.get_objects())  ## returns the list of all objects gc tracks Out[68]: <pre>143623</pre> <p>It is suprising how many objects the gc keeps track of.</p> In\u00a0[69]: Copied! <pre>print(gc.get_objects()[0])\n</pre> print(gc.get_objects()[0]) <pre>&lt;cell at 0x7fc5605d8fd0: module object at 0x7fc55c55c310&gt;\n</pre> In\u00a0[70]: Copied! <pre>print(gc.get_objects()[100])\n</pre> print(gc.get_objects()[100]) <pre>(&lt;class 'inspect._signature_fromstr.&lt;locals&gt;.RewriteSymbolics'&gt;, &lt;class 'ast.NodeTransformer'&gt;, &lt;class 'ast.NodeVisitor'&gt;, &lt;class 'object'&gt;)\n</pre> In\u00a0[71]: Copied! <pre>import ctypes, gc\n\ndef lower_ref_count(var_address:int) -&gt; int:\n    return ctypes.c_long.from_address(var_address).value\n\ndef object_in_gc(var_address: int) -&gt; bool:\n    \"\"\"Checks if the given mem address is in Garbage collector table\"\"\"\n    for obj in gc.get_objects():\n        if id(obj) == var_address:\n            return True\n    return False\n</pre> import ctypes, gc  def lower_ref_count(var_address:int) -&gt; int:     return ctypes.c_long.from_address(var_address).value  def object_in_gc(var_address: int) -&gt; bool:     \"\"\"Checks if the given mem address is in Garbage collector table\"\"\"     for obj in gc.get_objects():         if id(obj) == var_address:             return True     return False <p>Now let us define 2 classes that will implement the circ-ref shown above.</p> In\u00a0[88]: Copied! <pre>class A:\n    def __init__(self):\n        self.b = B(self)  # construct instance of B using an instance of A\n        print(f'A: self: {hex(id(self))}, b: {hex(id(self.b))}')\n\nclass B:\n    def __init__(self, A):\n        self.a = A  # other half of circ-ref\n        print(f\"B: self: {hex(id(self))}, a: {hex(id(self.a))}\")\n</pre> class A:     def __init__(self):         self.b = B(self)  # construct instance of B using an instance of A         print(f'A: self: {hex(id(self))}, b: {hex(id(self.b))}')  class B:     def __init__(self, A):         self.a = A  # other half of circ-ref         print(f\"B: self: {hex(id(self))}, a: {hex(id(self.a))}\") In\u00a0[89]: Copied! <pre># disable garbage collector\ngc.disable()\n</pre> # disable garbage collector gc.disable() In\u00a0[90]: Copied! <pre>my_var = A()\n</pre> my_var = A() <pre>B: self: 0x7fc560bb8670, a: 0x7fc560bb8490\nA: self: 0x7fc560bb8490, b: 0x7fc560bb8670\n</pre> In\u00a0[91]: Copied! <pre>hex(id(my_var))  # should return the same address as that of A\n</pre> hex(id(my_var))  # should return the same address as that of A Out[91]: <pre>'0x7fc560bb8490'</pre> In\u00a0[92]: Copied! <pre>print(hex(id(my_var.b)))  # should returnt he same address as that of B\nprint(hex(id(my_var)) == hex(id(my_var.b.a)))  # should eval to True\n</pre> print(hex(id(my_var.b)))  # should returnt he same address as that of B print(hex(id(my_var)) == hex(id(my_var.b.a)))  # should eval to True <pre>0x7fc560bb8670\nTrue\n</pre> <p>Now, let us store the addresses into vars</p> In\u00a0[93]: Copied! <pre>a_add = id(my_var)\nb_add = id(my_var.b)\n</pre> a_add = id(my_var) b_add = id(my_var.b) In\u00a0[94]: Copied! <pre>print(lower_ref_count(a_add)) ## should be 2  as my_var and B point to A\nprint(lower_ref_count(b_add)) ## should be 1 as only A points to B\n</pre> print(lower_ref_count(a_add)) ## should be 2  as my_var and B point to A print(lower_ref_count(b_add)) ## should be 1 as only A points to B <pre>2\n1\n</pre> <p>Next, let us check if these objs are on the GC table</p> In\u00a0[95]: Copied! <pre>print(object_in_gc(a_add))\nprint(object_in_gc(b_add))\n</pre> print(object_in_gc(a_add)) print(object_in_gc(b_add)) <pre>True\nTrue\n</pre> <p>Delete my_var and see how the GC behaves</p> In\u00a0[96]: Copied! <pre>my_var = None\nprint(lower_ref_count(a_add)) ## should be 1\nprint(lower_ref_count(b_add)) ## should be 1 all because of circ-ref\n</pre> my_var = None print(lower_ref_count(a_add)) ## should be 1 print(lower_ref_count(b_add)) ## should be 1 all because of circ-ref <pre>1\n1\n</pre> In\u00a0[97]: Copied! <pre>gc.collect()\n</pre> gc.collect() Out[97]: <pre>6946</pre> In\u00a0[98]: Copied! <pre>print(object_in_gc(a_add))\nprint(object_in_gc(b_add))  # we caught it while it was cleaning up!!!\n</pre> print(object_in_gc(a_add)) print(object_in_gc(b_add))  # we caught it while it was cleaning up!!! <pre>False\nTrue\n</pre> In\u00a0[99]: Copied! <pre>print(object_in_gc(a_add))\nprint(object_in_gc(b_add)) # cleaned up\n</pre> print(object_in_gc(a_add)) print(object_in_gc(b_add)) # cleaned up <pre>False\nFalse\n</pre> <p>Let us check the ref count of our addresses, buckle up!</p> In\u00a0[100]: Copied! <pre>print(lower_ref_count(a_add)) \nprint(lower_ref_count(b_add))\n</pre> print(lower_ref_count(a_add))  print(lower_ref_count(b_add))  <pre>1\n1\n</pre> In\u00a0[101]: Copied! <pre>print(lower_ref_count(a_add)) \nprint(lower_ref_count(b_add))\n</pre> print(lower_ref_count(a_add))  print(lower_ref_count(b_add))  <pre>1\n1\n</pre> In\u00a0[102]: Copied! <pre>print(lower_ref_count(a_add)) \nprint(lower_ref_count(b_add))\n</pre> print(lower_ref_count(a_add))  print(lower_ref_count(b_add))  <pre>1\n1\n</pre> <p>Well, this is why the ref count and garbage collection in Python is not generally handled directly by developers. It is not deterministic. Use these only to debug mem leaks!!</p> In\u00a0[104]: Copied! <pre># last time\nprint(lower_ref_count(a_add)) \nprint(lower_ref_count(b_add))\n</pre> # last time print(lower_ref_count(a_add))  print(lower_ref_count(b_add))  <pre>1\n1\n</pre> In\u00a0[109]: Copied! <pre>none_id = id(None)\ntrue_id = id(True)\nprint(none_id, true_id)\n</pre> none_id = id(None) true_id = id(True) print(none_id, true_id) <pre>4438825936 4438746400\n</pre> In\u00a0[111]: Copied! <pre>print(sys.getrefcount(none_id))\nprint(sys.getrefcount(true_id))\nprint(lower_ref_count(none_id))\nprint(lower_ref_count(true_id))\n</pre> print(sys.getrefcount(none_id)) print(sys.getrefcount(true_id)) print(lower_ref_count(none_id)) print(lower_ref_count(true_id)) <pre>2\n2\n35946\n7018\n</pre> <p>Now this is why, you need to be careful with the lower level C types ;-)</p> In\u00a0[113]: Copied! <pre>from copy import copy\nl4 = [4,5,6,7]\nl5 = copy(l4)\n\nprint(l4 == l5)\nprint(l4 is l5)  # false as it is a new copy\n</pre> from copy import copy l4 = [4,5,6,7] l5 = copy(l4)  print(l4 == l5) print(l4 is l5)  # false as it is a new copy <pre>True\nFalse\n</pre> In\u00a0[114]: Copied! <pre># lets prove it, the hex should be different\nprint(hex(id(l4)))\nprint(hex(id(l5)))\n</pre> # lets prove it, the hex should be different print(hex(id(l4))) print(hex(id(l5))) <pre>0x7fc5609e89c0\n0x7fc560bce500\n</pre>"},{"location":"teaching_resources/python/python_memory/#memory-in-python","title":"Memory in Python\u00b6","text":"<p>In Python, everything is an object and is stored in memory. This memory is the <code>heap</code>. The <code>id()</code> func will give you the address of any obj and when you pass that to <code>hex()</code>,  you can turn that base-10 address to hex notation for reading.</p> <p>When you create a variable and assign value, the value gets stored in memory first, then the address to the value is attached to the variable. Thus if you create another variable and point to the first, the address is additionally attached to the new variable, making it a pass by reference in Python.</p>"},{"location":"teaching_resources/python/python_memory/#reference-counter-in-python","title":"Reference counter in Python\u00b6","text":"<p>Python memory manager maintains a reference count for all objects in memory. You can access it via <code>sys.getrefcount()</code> function. But this has an artifact which bumps the count by 1. This is because when you run the function, it ends up accessing the object and that act shows up as another instance.</p> <p>You can subtract <code>1</code> from <code>sys.getrefcount()</code> or use a lower level function <code>ctypes.c_long.from_address(var_address).value</code> which will not have this artifact. But, this function needs the address of the object directly.</p>"},{"location":"teaching_resources/python/python_memory/#garbabe-collection","title":"Garbabe collection\u00b6","text":"<p>In Python, the <code>gc</code> module represents the garbage collector. The GC uses reference counters and a few other logic to do its work. The duty of the GC is to free up address spaces once a variable / object is no longer needed. The GC runs in the background and in general, it is not predictable when a variable will be removed once its reference count drops to 0.</p>"},{"location":"teaching_resources/python/python_memory/#circular-references","title":"Circular references\u00b6","text":"<p>To illustrate the effect of garbage collection, let us set up a complicated (and potentially bad) code that involves a circular reference.</p> <p></p> <p>The variable <code>my_var</code> points to <code>A</code>, but <code>A</code> has a property <code>var_1</code> that points to <code>B</code> and <code>B</code> has a property <code>var_2</code> which points back to <code>A</code>. If <code>my_var</code> is removed or reassigned, then the ref counts of <code>A</code> and <code>B</code> would still be <code>1</code> each because of the circular reference.</p> <p>Will these objects get ever cleaned up, since their ref counts will not drop to <code>0</code>? Yes, the garbage collector can, in general, resolve circular references and will clean them up. If you need to represent something in circ-ref, then you can customize the <code>__del__()</code> the destructor method of the objects to represent any specific things to be done upon clean up.</p> <p>However, prior to Python 3.5, the GC would fail to resolve circular references if it had a custom destructor and these would not be cleaned up, leading to memory leaks. However, in modern versions, the gc can remove objects entangled in circular references as well. Because of this, we need to manually pause the gc to illustrate the scenario below:</p>"},{"location":"teaching_resources/python/python_memory/#enable-garbage-collection","title":"Enable garbage collection\u00b6","text":""},{"location":"teaching_resources/python/python_memory/#side-bar-ref-counts-of-singletons","title":"Side bar - ref counts of singletons\u00b6","text":"<p>We know <code>None</code>, <code>True</code> etc are singletons, so what is its ref count?</p>"},{"location":"teaching_resources/python/python_memory/#side-bar-copy-function","title":"Side bar - copy function\u00b6","text":""},{"location":"teaching_resources/python/python_mutability/","title":"Mutability","text":"In\u00a0[1]: Copied! <pre>a = 10\nprint(hex(id(a)), hex(id(10)))\na = 20\nprint(hex(id(a)), hex(id(20)))\n</pre> a = 10 print(hex(id(a)), hex(id(10))) a = 20 print(hex(id(a)), hex(id(20))) <pre>0x7fd7d8521a50 0x7fd7d8521a50\n0x7fd7d8521b90 0x7fd7d8521b90\n</pre> <p>You see from above, when you reassign a value, Python first creates that object in memory, then assigns a reference to that object in memory to the variable. The original address was not updated, instead a new address was assigned. This is because integers are immutable.</p> In\u00a0[3]: Copied! <pre>a = [1,2,3]\nprint(hex(id(a)))\na.append(4)\nprint(hex(id(a)))\nprint(a)\n</pre> a = [1,2,3] print(hex(id(a))) a.append(4) print(hex(id(a))) print(a) <pre>0x7fd7dcd88cc0\n0x7fd7dcd88cc0\n[1, 2, 3, 4]\n</pre> <p>Here, the address remains the same, while the content was updated. This is mutation as lists are mutables. A new address space was not assigned. Instead, the contents in the original was updated.</p> In\u00a0[5]: Copied! <pre>a.append(10)\nfor e in a:\n    print(f'{e} @ {hex(id(e))}')\n</pre> a.append(10) for e in a:     print(f'{e} @ {hex(id(e))}') <pre>1 @ 0x7fd7d8521930\n2 @ 0x7fd7d8521950\n3 @ 0x7fd7d8521970\n4 @ 0x7fd7d8521990\n10 @ 0x7fd7d8521a50\n</pre> <p>Each element in the list is a different object at a different address. When we added <code>10</code>, the same address as before was assigned (note from earlier cell).</p> In\u00a0[30]: Copied! <pre>c = 20\nd = 20\nprint(id(c) == id(d))\nprint(hex(id(c)), hex(id(d)))\n</pre> c = 20 d = 20 print(id(c) == id(d)) print(hex(id(c)), hex(id(d))) <pre>True\n0x7fd7d8521b90 0x7fd7d8521b90\n</pre> In\u00a0[31]: Copied! <pre>print(c == d)\nprint(c is d)\n</pre> print(c == d) print(c is d) <pre>True\nTrue\n</pre> <p>Above, Python used the same address for both the objects, which is equivalent to writing <code>c = d = 20</code>. This is safe since integers are immutable.</p> In\u00a0[12]: Copied! <pre>e1 = f1 = [20,30,40]  # same address\ne2 = ['a', 'b']  # different address\nf2 = ['a', 'b']\nprint(id(e1) == id(f1))\nprint(hex(id(e1)), hex(id(f1)))\nprint(id(e2) == id(f2))\nprint(hex(id(e2)), hex(id(f2)))\n</pre> e1 = f1 = [20,30,40]  # same address e2 = ['a', 'b']  # different address f2 = ['a', 'b'] print(id(e1) == id(f1)) print(hex(id(e1)), hex(id(f1))) print(id(e2) == id(f2)) print(hex(id(e2)), hex(id(f2))) <pre>True\n0x7fd7dcd8e4c0 0x7fd7dcd8e4c0\nFalse\n0x7fd7dcd8a680 0x7fd7dcd8e980\n</pre> <p>In the code above, when you use <code>var = var2 = value</code>, then the same address is assigned to both variables. However, in the next pattern, even though the values are identical, Python assigned different addresses since lists are mutable and it makes sense to instantiate them separately in memory.</p> In\u00a0[13]: Copied! <pre>print(hex(id(e2[0])), hex(id(f2[0])))  # should be the same:\n</pre> print(hex(id(e2[0])), hex(id(f2[0])))  # should be the same: <pre>0x7fd7da0b0330 0x7fd7da0b0330\n</pre> <p>Even thought Python assigned two different lists in memory, the contents of the lists still point to the same string objects in memory as strings are immutable.</p> In\u00a0[14]: Copied! <pre>l1 = [1,2,3]\nl2 = ['a','b','c']\nt1 = (l1, l2)\n\nprint(hex(id(t1)))  # print address\n\nt1[0].append(4)     # is this a mutation? Not actually\nprint(hex(id(t1)))  # print address again and compare\n</pre> l1 = [1,2,3] l2 = ['a','b','c'] t1 = (l1, l2)  print(hex(id(t1)))  # print address  t1[0].append(4)     # is this a mutation? Not actually print(hex(id(t1)))  # print address again and compare <pre>0x7fd7dcba8cc0\n0x7fd7dcba8cc0\n</pre> <p>The tuple is immutable, however, its elements can change because, the tuple's reference to the list remains the same. The list's references got changed, but that does not break the holding tuple's immutability!</p> In\u00a0[20]: Copied! <pre>l1 = [1,2,3]\nprint(hex(id(l1)))\n\nl1.append(4)\nprint(hex(id(l1)))  # should be same as above\n\nl1 = l1 + [5]\nprint(l1)\nprint(hex(id(l1)))  # will be different than from an append operation\n</pre> l1 = [1,2,3] print(hex(id(l1)))  l1.append(4) print(hex(id(l1)))  # should be same as above  l1 = l1 + [5] print(l1) print(hex(id(l1)))  # will be different than from an append operation <pre>0x7fd7dcd8b540\n0x7fd7dcd8b540\n[1, 2, 3, 4, 5]\n0x7fd7dcd8b940\n</pre> <p>What happened here? When you run <code>l1 = l1 + [val]</code>, it evaluates the expression and assigns a new memory for the result, eventhough you ask it to update the same object.</p> In\u00a0[21]: Copied! <pre>def modify_str(s):\n    print(f'Incoming s @ : {hex(id(s))}')\n    s = s + \" world\"\n    print(f'Post OP  s @ : {hex(id(s))}')\n</pre> def modify_str(s):     print(f'Incoming s @ : {hex(id(s))}')     s = s + \" world\"     print(f'Post OP  s @ : {hex(id(s))}') In\u00a0[22]: Copied! <pre>s = 'hello'\nprint(hex(id(s)))\nmodify_str(s)\nprint(s)\nprint(hex(id(s)))\n</pre> s = 'hello' print(hex(id(s))) modify_str(s) print(s) print(hex(id(s))) <pre>0x7fd7dc7000b0\nIncoming s @ : 0x7fd7dc7000b0\nPost OP  s @ : 0x7fd7dcd826f0\nhello\n0x7fd7dc7000b0\n</pre> <p>The address of <code>s</code> inside the func is initially the same. But when an operation was ran, since strings are immutable, a new object was created. However, outside the scope of the function, the value of <code>s</code> remains the same and at the same address.</p> In\u00a0[29]: Copied! <pre>def modify_lst(l):\n    print(f'Incoming list @ : {hex(id(l))}')\n    l.append(200)\n    print(f'PostOp   list @ : {hex(id(l))}')\n\nl1 = [1,2,3]\nprint(f'Outside list @ : {hex(id(l1))}')\nmodify_lst(l1)\nprint(l1)\nprint(f'Outside list @ : {hex(id(l1))}')\n</pre> def modify_lst(l):     print(f'Incoming list @ : {hex(id(l))}')     l.append(200)     print(f'PostOp   list @ : {hex(id(l))}')  l1 = [1,2,3] print(f'Outside list @ : {hex(id(l1))}') modify_lst(l1) print(l1) print(f'Outside list @ : {hex(id(l1))}') <pre>Outside list @ : 0x7fd7de1a7f00\nIncoming list @ : 0x7fd7de1a7f00\nPostOp   list @ : 0x7fd7de1a7f00\n[1, 2, 3, 200]\nOutside list @ : 0x7fd7de1a7f00\n</pre> <p>Thus, functions can modify the value of variables outside their scope as well, since Python is pass by reference.</p>"},{"location":"teaching_resources/python/python_mutability/#what-is-mutation","title":"What is mutation?\u00b6","text":"<p>What is mutation? Is it just changing value? No. It is changing value, while retaining the same address in memory.</p>"},{"location":"teaching_resources/python/python_mutability/#mutable-and-immutable-data-types","title":"Mutable and immutable data types\u00b6","text":"<p>You can define your classes to be either mutable or immutable.</p> <p>Extending the concept from above, what happens when you assign the same value to two variables?</p>"},{"location":"teaching_resources/python/python_mutability/#immutable-collections-containing-mutable-elements","title":"Immutable collections containing mutable elements\u00b6","text":"<p>What happens when a tuple's elements are lists, can you edit the elements? How is mutability considered then?</p>"},{"location":"teaching_resources/python/python_mutability/#operations-on-mutable-objects","title":"Operations on mutable objects\u00b6","text":"<p>Not all operations on mutable objects are mutable. See below:</p>"},{"location":"teaching_resources/python/python_mutability/#function-arguments-and-mutability","title":"Function arguments and mutability\u00b6","text":"<p>What happens when a function changes or modifies one of its parameters' value? Will it change it outside the function as well since python is pass by reference?</p>"},{"location":"teaching_resources/python/python_numeric_datatypes/","title":"Numeric datatypes","text":"In\u00a0[17]: Copied! <pre>import math\nfor i in [8, 16, 32, 64, 128]:\n    val = math.pow(2, i-1)\n    print(f'Int range : {i} bits \\t: [-{val:,.0f} to {val:,.0f}]')\n</pre> import math for i in [8, 16, 32, 64, 128]:     val = math.pow(2, i-1)     print(f'Int range : {i} bits \\t: [-{val:,.0f} to {val:,.0f}]') <pre>Int range : 8 bits \t: [-128 to 128]\nInt range : 16 bits \t: [-32,768 to 32,768]\nInt range : 32 bits \t: [-2,147,483,648 to 2,147,483,648]\nInt range : 64 bits \t: [-9,223,372,036,854,775,808 to 9,223,372,036,854,775,808]\nInt range : 128 bits \t: [-170,141,183,460,469,231,731,687,303,715,884,105,728 to 170,141,183,460,469,231,731,687,303,715,884,105,728]\n</pre> <p>Integers in Python use variable number of bits depending on the size of the value stored in it. Thus, the largest number that can be stored is limited by the amount of memory available to the kernel.</p> In\u00a0[18]: Copied! <pre>import sys\nprint(sys.getsizeof(0))  # integer that uses the smallest size\nprint(sys.getsizeof(10))\nprint(sys.getsizeof(1000))\nprint(sys.getsizeof(2**1000))  # exponentially large number\nprint(sys.getsizeof(2**1000000))\n</pre> import sys print(sys.getsizeof(0))  # integer that uses the smallest size print(sys.getsizeof(10)) print(sys.getsizeof(1000)) print(sys.getsizeof(2**1000))  # exponentially large number print(sys.getsizeof(2**1000000)) <pre>24\n28\n28\n160\n133360\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/python/python_numeric_datatypes/#5-types-of-numbers-in-python","title":"5 types of numbers in Python\u00b6","text":""},{"location":"teaching_resources/python/python_numeric_datatypes/#integers-in-python","title":"Integers in Python\u00b6","text":"<p>Integers are represented in binary form internally. For example, to represent a number like <code>19</code>, the binary form is <code>10011</code>, which takes <code>5 bits</code>.</p> <p></p> <p>Thus, what is the largest unsigned integer that can be stored with <code>8 bits</code>? The answer is <code>255</code>.</p> <p></p> <p>If we want signed integers, then <code>1 bit</code> is used to represent the sign, leaving $2^{7}$ . Thus the range is now <code>[-127, 127]</code></p>"},{"location":"teaching_resources/r/r_datatypes_variables/","title":"R - datatypes, variables","text":"In\u00a0[2]: Copied! <pre>x = c(1,2,3,4,5,6) #concatenate to create a vector\nx\n</pre> x = c(1,2,3,4,5,6) #concatenate to create a vector x <ol> <li>1</li> <li>2</li> <li>3</li> <li>4</li> <li>5</li> <li>6</li> </ol> In\u00a0[4]: Copied! <pre>length(x)\n</pre> length(x)  6  In\u00a0[5]: Copied! <pre>y = c(8,9,10,11,12,14)\nx_y = x+y\nx_y\n</pre> y = c(8,9,10,11,12,14) x_y = x+y x_y <ol> <li>9</li> <li>11</li> <li>13</li> <li>15</li> <li>17</li> <li>20</li> </ol> In\u00a0[6]: Copied! <pre>ls()\n</pre> ls() <ol> <li>'x'</li> <li>'x_y'</li> <li>'y'</li> </ol> <p>remove variables using <code>rm(var_name)</code></p> In\u00a0[8]: Copied! <pre>rm('x_y')\nls()\n</pre> rm('x_y') ls() <ol> <li>'x'</li> <li>'y'</li> </ol> In\u00a0[7]: Copied! <pre>?ls\n</pre> ?ls In\u00a0[9]: Copied! <pre>m1 = matrix(data= x, nrow=2, ncol=3)\nm1\n</pre> m1 = matrix(data= x, nrow=2, ncol=3) m1 135 246 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/r/r_datatypes_variables/#r-datatypes-variables","title":"R - datatypes, variables\u00b6","text":""},{"location":"teaching_resources/r/r_datatypes_variables/#basics","title":"Basics\u00b6","text":"<p>Concatenation, length of vectors, numeric operations, matrix creation</p>"},{"location":"teaching_resources/r/r_datatypes_variables/#concatenation","title":"Concatenation\u00b6","text":""},{"location":"teaching_resources/r/r_datatypes_variables/#numeric-operations","title":"Numeric operations\u00b6","text":"<p>vector 1 + vector 2 is a element by element summation</p>"},{"location":"teaching_resources/r/r_datatypes_variables/#list-all-variables-in-memory","title":"List all variables in memory\u00b6","text":"<p>use <code>ls()</code></p>"},{"location":"teaching_resources/r/r_datatypes_variables/#getting-help","title":"Getting help\u00b6","text":"<p>use <code>?symbol</code> to get help</p>"},{"location":"teaching_resources/r/r_datatypes_variables/#build-a-matrix","title":"Build a matrix\u00b6","text":"<p>use <code>matrix(data, nrow, ncol)</code> method to build a nd matrix</p>"},{"location":"teaching_resources/rasterio/rasterio_hyperspectral/","title":"Rasterio - hyperspectral data","text":"In\u00a0[1]: Copied! <pre>import rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import rasterio import numpy as np import matplotlib.pyplot as plt %matplotlib inline <p>Read the hyperspectral image</p> In\u00a0[2]: Copied! <pre>hy_raster = rasterio.open('/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img')\nhy_raster\n</pre> hy_raster = rasterio.open('/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img') hy_raster Out[2]: <pre>&lt;open RasterReader name='/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img' mode='r'&gt;</pre> In\u00a0[3]: Copied! <pre># number of bands\nhy_raster.count\n</pre> # number of bands hy_raster.count Out[3]: <pre>224</pre> In\u00a0[4]: Copied! <pre>all_bands = hy_raster.read()\nall_bands.shape\n</pre> all_bands = hy_raster.read() all_bands.shape Out[4]: <pre>(224, 737, 718)</pre> In\u00a0[8]: Copied! <pre>def get_cell_value(raster, all_bands, x,y):\n    array_coords = raster.index(x,y)\n    n_bands = raster.count\n    \n    # get pixel values\n    values= [all_bands[i, array_coords[0], array_coords[1]] \n            for i in range(raster.count)]\n    \n    # plot values\n    plt.plot(values)\n    return values\n</pre> def get_cell_value(raster, all_bands, x,y):     array_coords = raster.index(x,y)     n_bands = raster.count          # get pixel values     values= [all_bands[i, array_coords[0], array_coords[1]]              for i in range(raster.count)]          # plot values     plt.plot(values)     return values In\u00a0[10]: Copied! <pre>cell_values1 = get_cell_value(hy_raster, all_bands, 630297,3896378)\n</pre> cell_values1 = get_cell_value(hy_raster, all_bands, 630297,3896378) In\u00a0[7]: Copied! <pre>import spectral\nfrom spectral.io import aviris\n</pre> import spectral from spectral.io import aviris In\u00a0[8]: Copied! <pre>aviris.open('/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img')\n\n# hy_raster2 = spectral.open_image('/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img')\n# hy_raster2\n</pre> aviris.open('/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img')  # hy_raster2 = spectral.open_image('/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img') # hy_raster2 <pre>\n---------------------------------------------------------------------------\nInvalidFileError                          Traceback (most recent call last)\n&lt;ipython-input-8-3fb4ea539783&gt; in &lt;module&gt;()\n----&gt; 1 aviris.open('/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img')\n      2 \n      3 # hy_raster2 = spectral.open_image('/Users/atma6951/Documents/GIS_data/Imagery/AVIRIS-mohave-clipped/mohave_clipped.img')\n      4 # hy_raster2\n\n~/anaconda3/envs/geopandasenv/lib/python3.6/site-packages/spectral/io/aviris.py in open(file, band_file)\n     76     fileSize = os.stat(p.filename)[6]\n     77     if fileSize % 275072 != 0:\n---&gt; 78         raise InvalidFileError('File size not consistent with AVIRIS format.')\n     79     p.nrows = int(fileSize / 275072)\n     80     p.byte_order = 1\n\nInvalidFileError: File size not consistent with AVIRIS format.</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/rasterio/rasterio_hyperspectral/#rasterio-hyperspectral-data","title":"Rasterio - hyperspectral data\u00b6","text":"<p>My objective is to apply spectral angle mapper classifier</p>"},{"location":"teaching_resources/rasterio/rasterio_hyperspectral/#get-cell-values","title":"Get cell values\u00b6","text":""},{"location":"teaching_resources/rasterio/rasterio_hyperspectral/#read-image-using-spectral-python-library","title":"Read image using spectral python library\u00b6","text":""},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/","title":"Rasterio - inspection, plotting raster data","text":"In\u00a0[1]: Copied! <pre>45+677\n</pre> 45+677 Out[1]: <pre>722</pre> In\u00a0[1]: Copied! <pre>import rasterio\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import rasterio import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>raster = rasterio.open(path='/Users/atma6951/Documents/GIS_data/Imagery/open-geo/Helsinki_masked_p188r018_7t20020529_z34__LV-FIN.tif')\ntype(raster)\n</pre> raster = rasterio.open(path='/Users/atma6951/Documents/GIS_data/Imagery/open-geo/Helsinki_masked_p188r018_7t20020529_z34__LV-FIN.tif') type(raster) Out[2]: <pre>rasterio._io.RasterReader</pre> <p>Let us query a few important properties to understand the image better</p> In\u00a0[3]: Copied! <pre>raster.crs\n</pre> raster.crs Out[3]: <pre>CRS({'proj': 'tmerc', 'lat_0': 0, 'lon_0': -183, 'k': 0.9996, 'x_0': 500000, 'y_0': 0, 'datum': 'WGS84', 'units': 'm', 'no_defs': True})</pre> In\u00a0[4]: Copied! <pre>raster.shape\n</pre> raster.shape Out[4]: <pre>(1439, 1288)</pre> In\u00a0[5]: Copied! <pre>raster.bounds\n</pre> raster.bounds Out[5]: <pre>BoundingBox(left=698592.0, bottom=6656859.0, right=735300.0, top=6697870.5)</pre> In\u00a0[6]: Copied! <pre>raster.count\n</pre> raster.count Out[6]: <pre>7</pre> In\u00a0[8]: Copied! <pre>raster.profile\n</pre> raster.profile Out[8]: <pre>{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': None,\n 'width': 1288,\n 'height': 1439,\n 'count': 7,\n 'crs': CRS({'proj': 'tmerc', 'lat_0': 0, 'lon_0': -183, 'k': 0.9996, 'x_0': 500000, 'y_0': 0, 'datum': 'WGS84', 'units': 'm', 'no_defs': True}),\n 'transform': (698592.0, 28.5, 0.0, 6697870.5, 0.0, -28.5),\n 'affine': Affine(28.5, 0.0, 698592.0,\n        0.0, -28.5, 6697870.5),\n 'tiled': False,\n 'interleave': 'pixel'}</pre> In\u00a0[10]: Copied! <pre>band1 = raster.read([1])\ntype(band1)\n</pre> band1 = raster.read([1]) type(band1) Out[10]: <pre>numpy.ndarray</pre> In\u00a0[11]: Copied! <pre>band1.shape, band1.ndim\n</pre> band1.shape, band1.ndim Out[11]: <pre>((1, 1439, 1288), 3)</pre> In\u00a0[19]: Copied! <pre>band1.min(), band1.max(), band1.mean()\n</pre> band1.min(), band1.max(), band1.mean() Out[19]: <pre>(0, 255, 59.63132232528628)</pre> In\u00a0[14]: Copied! <pre>band1a = raster.read([1], out_shape=(raster.profile['width'], raster.profile['height']))\nband1a.reshape(1288, 1439)\nband1a.shape\n</pre> band1a = raster.read([1], out_shape=(raster.profile['width'], raster.profile['height'])) band1a.reshape(1288, 1439) band1a.shape Out[14]: <pre>(1, 1288, 1439)</pre> In\u00a0[15]: Copied! <pre>band1a\n</pre> band1a Out[15]: <pre>array([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)</pre> In\u00a0[16]: Copied! <pre>from rasterio.plot import show\nshow(band1a)\n</pre> from rasterio.plot import show show(band1a) Out[16]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x121ca41d0&gt;</pre> <p>Making a FCC or TCC</p> In\u00a0[18]: Copied! <pre>fig, ax = plt.subplots(figsize=(8,8))\nshow(band1a, cmap='Blues', ax=ax, alpha=0.33)\nshow(raster.read(3), cmap='Greens', ax=ax, alpha=0.33)\nshow(raster.read(4), cmap=\"Reds\", ax=ax, alpha=0.33)\n</pre> fig, ax = plt.subplots(figsize=(8,8)) show(band1a, cmap='Blues', ax=ax, alpha=0.33) show(raster.read(3), cmap='Greens', ax=ax, alpha=0.33) show(raster.read(4), cmap=\"Reds\", ax=ax, alpha=0.33) Out[18]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10df7fcc0&gt;</pre> In\u00a0[45]: Copied! <pre>from rasterio.plot import show_hist\nshow_hist(source=raster, bins=50, title='Histogram of all bands', \n          histtype='stepfilled', alpha=0.5)\n</pre> from rasterio.plot import show_hist show_hist(source=raster, bins=50, title='Histogram of all bands',            histtype='stepfilled', alpha=0.5) In\u00a0[24]: Copied! <pre>all_bands = raster.read()\nall_bands.shape\n</pre> all_bands = raster.read() all_bands.shape Out[24]: <pre>(7, 1439, 1288)</pre> In\u00a0[25]: Copied! <pre>all_bands.ndim\n</pre> all_bands.ndim Out[25]: <pre>3</pre> In\u00a0[35]: Copied! <pre>x,y = 717389, 6675310\narray_coords = raster.index(x, y)\narray_coords\n</pre> x,y = 717389, 6675310 array_coords = raster.index(x, y) array_coords Out[35]: <pre>(791, 659)</pre> <p>This array coordinates is the same for all bands as the dimensions of all bands is the same. Thus you can use regular array indexing as shown below to get the pixel values:</p> In\u00a0[36]: Copied! <pre>band1[0,791,659]\n</pre> band1[0,791,659] Out[36]: <pre>83</pre> In\u00a0[38]: Copied! <pre>[all_bands[i,array_coords[0], array_coords[1]] for \n i in range(raster.count)]\n</pre> [all_bands[i,array_coords[0], array_coords[1]] for   i in range(raster.count)] Out[38]: <pre>[83, 62, 66, 32, 53, 139, 48]</pre> In\u00a0[44]: Copied! <pre>def get_cell_value(raster, all_bands, x,y):\n    array_coords = raster.index(x,y)\n    n_bands = raster.count\n    \n    # get pixel values\n    values= [all_bands[i, array_coords[0], array_coords[1]] \n            for i in range(raster.count)]\n    \n    # plot values\n    plt.plot(values)\n    return values\n</pre> def get_cell_value(raster, all_bands, x,y):     array_coords = raster.index(x,y)     n_bands = raster.count          # get pixel values     values= [all_bands[i, array_coords[0], array_coords[1]]              for i in range(raster.count)]          # plot values     plt.plot(values)     return values <p>You can use this to construct spectral profiles of various points.</p> In\u00a0[43]: Copied! <pre>get_cell_value(raster, all_bands, 702829, 6685820)\n</pre> get_cell_value(raster, all_bands, 702829, 6685820) Out[43]: <pre>[68, 50, 42, 12, 12, 116, 11]</pre> In\u00a0[45]: Copied! <pre>get_cell_value(raster, all_bands, 713891, 6688037)\n</pre> get_cell_value(raster, all_bands, 713891, 6688037) Out[45]: <pre>[69, 60, 47, 102, 87, 133, 42]</pre> <p>Comparing this with QGIS, the values and charts are consistent.</p>"},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#rasterio-inspection-plotting-raster-data","title":"Rasterio - inspection, plotting raster data\u00b6","text":"<p>The open geospatial Python ecosystem has powerful libraries for processing vector data. We have seen the likes of <code>geopandas</code>, <code>pyshp</code>, <code>shapely</code>, <code>folium</code>, <code>fiona</code>, <code>pysal</code> etc. How about for raster data processing? This notebook explores a few of them, starting with <code>rasterio</code>. My understanding so far is, in general digital image and signal processing via Python is a well established field. Thus if a library allows you to read geospatial image formats into <code>numpy</code> arrays and write the results back, you can easily accomplish your processing using standard Pythonic data analysis process of reading into <code>numpy</code> arrays, doing filtering, enhancements, analysis and presisting into the native file format of choice.</p> <p>Let us see how far this theory holds up.</p>"},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#accessing-raster-files","title":"Accessing raster files\u00b6","text":""},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#read-individual-bands","title":"Read individual bands\u00b6","text":""},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#read-into-a-specified-shape","title":"Read into a specified shape\u00b6","text":"<p>I am unable to get this into a 2D array no matter what. Could be a bug</p>"},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#preview-the-band","title":"Preview the band\u00b6","text":""},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#plotting-histograms","title":"Plotting histograms\u00b6","text":""},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#read-all-bands-into-a-3d-array","title":"Read all bands into a 3D array\u00b6","text":""},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#retrieving-pixel-values","title":"Retrieving pixel values\u00b6","text":"<p>This landast has 8 bands. Calling the <code>index()</code> method of <code>rasterio._io.RasterReader</code> with spatial coordinates, returns the translation in array indices. You can then use the regular numpy array indexing on the <code>numpy.ndarray</code> object you get as a result of reading the raster image as shown above.</p> <p>Get pixel values in band 1 at X,Y: <code>(717389, 6675310)</code></p>"},{"location":"teaching_resources/rasterio/rasterio_inspection_plotting/#retrieving-pixel-values-across-all-bands","title":"Retrieving pixel values across all bands.\u00b6","text":"<p>Let us define a function that returns the pixel values across all bands</p>"},{"location":"teaching_resources/seaborn/seaborn_categorical_plotting/","title":"Seaborn - categorical plotting","text":"In\u00a0[1]: Copied! <pre>import seaborn as sns\n%matplotlib inline\ntips = sns.load_dataset('tips')\ntips.head()\n</pre> import seaborn as sns %matplotlib inline tips = sns.load_dataset('tips') tips.head() Out[1]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 <p>ToC</p> <ul> <li>Barplot</li> <li>Countplot</li> <li>Boxplot</li> <li>Violin plot</li> <li>Strip plot</li> <li>Swarm plot</li> </ul> In\u00a0[2]: Copied! <pre>sns.barplot(x='sex', y='total_bill', data=tips)\n</pre> sns.barplot(x='sex', y='total_bill', data=tips) Out[2]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1101e1860&gt;</pre> <p>Thus the average bill for men was higher than women.</p> In\u00a0[4]: Copied! <pre>sns.countplot(x='sex', data=tips)\n</pre> sns.countplot(x='sex', data=tips) Out[4]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1127c1320&gt;</pre> In\u00a0[2]: Copied! <pre>sns.boxplot(x='time', y='total_bill', data=tips)\n</pre> sns.boxplot(x='time', y='total_bill', data=tips) Out[2]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1129ecdd8&gt;</pre> <p>We can interpret this as people spend more on dinner on average than lunch. The median is higher. Yet there is higher variability as well with the amount spent on dinner. The lowest being lower than lunch.</p> In\u00a0[6]: Copied! <pre>sns.boxplot(x='time',y='total_bill', data=tips, hue='sex')\n</pre> sns.boxplot(x='time',y='total_bill', data=tips, hue='sex') Out[6]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1150d1278&gt;</pre> In\u00a0[9]: Copied! <pre>sns.violinplot(x='time',y='total_bill', data=tips)\n</pre> sns.violinplot(x='time',y='total_bill', data=tips) Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1155731d0&gt;</pre> <p>You can see, lunch bills are tighter around the median compared to dinner. The <code>Q3</code> of dinner is long, which can be noticed in the spread of the green violin plot.</p> In\u00a0[10]: Copied! <pre>sns.violinplot(x='time', y='total_bill', data=tips, hue='sex', split=True)\n</pre> sns.violinplot(x='time', y='total_bill', data=tips, hue='sex', split=True) Out[10]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x115587fd0&gt;</pre> <p>From this plot, we assert our experience so far that women's bills are lesser than men - the width of the violin is higher on the lower end.</p> In\u00a0[12]: Copied! <pre>sns.stripplot(x='time', y='total_bill', data=tips)\n</pre> sns.stripplot(x='time', y='total_bill', data=tips) Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x11577bd68&gt;</pre> <p>To make out the data distribution, you can add some jitter to the plot. Jitter will shift the points laterally in a random manner.</p> In\u00a0[13]: Copied! <pre>sns.stripplot(x='time', y='total_bill', data=tips, jitter=True)\n</pre> sns.stripplot(x='time', y='total_bill', data=tips, jitter=True) Out[13]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x115637518&gt;</pre> In\u00a0[14]: Copied! <pre>sns.swarmplot(x='time', y='total_bill', data=tips)\n</pre> sns.swarmplot(x='time', y='total_bill', data=tips) Out[14]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x115997978&gt;</pre> <p>You can combine a violin and swarm plot to see how the KDE is calculated and smooths</p> In\u00a0[15]: Copied! <pre>sns.violinplot(x='time', y='total_bill', data=tips)\nsns.swarmplot(x='time', y='total_bill', data=tips, color='black')\n</pre> sns.violinplot(x='time', y='total_bill', data=tips) sns.swarmplot(x='time', y='total_bill', data=tips, color='black') Out[15]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1159aa278&gt;</pre>"},{"location":"teaching_resources/seaborn/seaborn_categorical_plotting/#seaborn-categorical-plotting","title":"Seaborn - categorical plotting\u00b6","text":""},{"location":"teaching_resources/seaborn/seaborn_categorical_plotting/#barplot","title":"Barplot\u00b6","text":"<p>Barplot is used to indicate some measure of central tendancy. Seaborn adds some descriptors to indicate the variance in the data. Call this with a categorical column in X and numerical column for Y</p>"},{"location":"teaching_resources/seaborn/seaborn_categorical_plotting/#countplot","title":"Countplot\u00b6","text":"<p>If you want a regular bar chart that shows the count of data, then do a <code>countplot</code></p>"},{"location":"teaching_resources/seaborn/seaborn_categorical_plotting/#boxplot","title":"Boxplot\u00b6","text":"<p>Boxplots are very common. It is used to display distribution of data as well as outliers. A boxplot splits the data into 4 <code>quantiles</code> or <code>quartiles</code>. The <code>median</code> is represented as a horizontal line with the quartile +- medain in solid shade. The end of the whiskers may represent the ends of the remaining quartiles </p> <p>If outliers are calculated, then whiskers are shorter and values greater than <code>1.5</code> times the <code>IQR</code> - Inter Quartile Range are considered outliers.</p>"},{"location":"teaching_resources/seaborn/seaborn_categorical_plotting/#violin-plot","title":"Violin plot\u00b6","text":"<p>A violin plot builds on a boxplot by showing KDE of the data distribution.</p>"},{"location":"teaching_resources/seaborn/seaborn_categorical_plotting/#stirp-plot","title":"Stirp plot\u00b6","text":"<p>Strip plot is like a scatter plot for a categorial data. You specify a categorial column for X and numeric for Y.</p>"},{"location":"teaching_resources/seaborn/seaborn_categorical_plotting/#swarm-plot","title":"Swarm plot\u00b6","text":"<p>Swram plots are a combination of violin and strip plots. It shows the real data distribution using actual point values.</p>"},{"location":"teaching_resources/seaborn/seaborn_exploring_distributions/","title":"Seaborn - exploring distributions","text":"In\u00a0[1]: Copied! <pre>import seaborn as sns\n%matplotlib inline\n</pre> import seaborn as sns %matplotlib inline In\u00a0[2]: Copied! <pre>tips = sns.load_dataset('tips')\n</pre> tips = sns.load_dataset('tips') In\u00a0[3]: Copied! <pre>tips.head(5)\n</pre> tips.head(5) Out[3]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 In\u00a0[6]: Copied! <pre>#find dist of total bills\nsns.distplot(tips['total_bill'])\n</pre> #find dist of total bills sns.distplot(tips['total_bill']) Out[6]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10eafbd68&gt;</pre> <p>It is often useful to overlay the mean and SD with the histograms, below is one way to do it.</p> In\u00a0[13]: Copied! <pre>tips.total_bill.mean()\n</pre> tips.total_bill.mean() Out[13]: <pre>19.78594262295082</pre> In\u00a0[20]: Copied! <pre>tips_mean = tips.total_bill.mean()\ntips_sd = tips.total_bill.std()\n\nax = sns.distplot(tips['total_bill'])\n\n# plot mean in black\nax.axvline(x=tips_mean, color='black', linestyle='dashed')\n\n# plot mean +- 1SD in red, dotted\nax.axvline(x=tips_mean + tips_sd, color='red', linestyle='dotted')\nax.axvline(x=tips_mean - tips_sd, color='red', linestyle='dotted')\n\n# title\nax.set_title('$\\mu = {}$ | $\\sigma = {}$'.format(round(tips_mean, 2), round(tips_sd, 2)))\n</pre> tips_mean = tips.total_bill.mean() tips_sd = tips.total_bill.std()  ax = sns.distplot(tips['total_bill'])  # plot mean in black ax.axvline(x=tips_mean, color='black', linestyle='dashed')  # plot mean +- 1SD in red, dotted ax.axvline(x=tips_mean + tips_sd, color='red', linestyle='dotted') ax.axvline(x=tips_mean - tips_sd, color='red', linestyle='dotted')  # title ax.set_title('$\\mu = {}$ | $\\sigma = {}$'.format(round(tips_mean, 2), round(tips_sd, 2))) Out[20]: <pre>Text(0.5,1,'$\\\\mu = 19.79$ | $\\\\sigma = 8.9$')</pre> <p>You can change things like <code>bin</code>, <code>kde</code> flags to customize the plot</p> In\u00a0[5]: Copied! <pre>sns.distplot(tips['total_bill'], kde=False, bins=35)\n</pre> sns.distplot(tips['total_bill'], kde=False, bins=35) Out[5]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1153353c8&gt;</pre> In\u00a0[6]: Copied! <pre>sns.jointplot(x=tips['total_bill'], y=tips['tip'])\n</pre> sns.jointplot(x=tips['total_bill'], y=tips['tip']) Out[6]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1179c1358&gt;</pre> <p>You can use the <code>kind</code> argument to change the <code>scatter</code> to <code>hex</code>, <code>reg</code> etc</p> In\u00a0[102]: Copied! <pre>jgrid = sns.jointplot(x='min_season', y='max_wind_merged', data=hurricanes_ipl,\n             kind='reg', joint_kws={'line_kws':{'color':'green'}}, height=7, space=0.5)\nj = jgrid.annotate(stats.pearsonr)\nj = jgrid.ax_joint.set_title('Does hurricane wind speed increase over time?')\n</pre> jgrid = sns.jointplot(x='min_season', y='max_wind_merged', data=hurricanes_ipl,              kind='reg', joint_kws={'line_kws':{'color':'green'}}, height=7, space=0.5) j = jgrid.annotate(stats.pearsonr) j = jgrid.ax_joint.set_title('Does hurricane wind speed increase over time?') In\u00a0[7]: Copied! <pre>sns.jointplot(x=tips['total_bill'], y=tips['tip'], kind='hex')\n</pre> sns.jointplot(x=tips['total_bill'], y=tips['tip'], kind='hex') Out[7]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x117b95c88&gt;</pre> In\u00a0[8]: Copied! <pre>sns.jointplot(x=tips['total_bill'], y=tips['tip'], kind='reg') #regression\n</pre> sns.jointplot(x=tips['total_bill'], y=tips['tip'], kind='reg') #regression Out[8]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1182819e8&gt;</pre> In\u00a0[10]: Copied! <pre>sns.pairplot(tips, hue='sex')\n</pre> sns.pairplot(tips, hue='sex') Out[10]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x11908f390&gt;</pre> In\u00a0[11]: Copied! <pre>sns.rugplot(tips['total_bill'])\n</pre> sns.rugplot(tips['total_bill']) Out[11]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1196baa58&gt;</pre>"},{"location":"teaching_resources/seaborn/seaborn_exploring_distributions/#seaborn-exploring-distributions","title":"Seaborn - exploring distributions\u00b6","text":"Seaborn is an amazing data and statistical visualization library that is built using matplotlib. It has good defaults and very easy to use."},{"location":"teaching_resources/seaborn/seaborn_exploring_distributions/#load-sample-dataset","title":"Load sample dataset\u00b6","text":"<p>Seaborn comes with a number of example dataset. Let us load the restaurant tipping dataset</p>"},{"location":"teaching_resources/seaborn/seaborn_exploring_distributions/#distribution-plots","title":"Distribution plots\u00b6","text":"<p>One of the first things we do is to find the data dist.</p>"},{"location":"teaching_resources/seaborn/seaborn_exploring_distributions/#plotting-dist-of-2-variables","title":"Plotting dist of 2 variables\u00b6","text":"<p>Seaborn can very easily attach a histogram to a scatter plot to show the data distribution</p>"},{"location":"teaching_resources/seaborn/seaborn_exploring_distributions/#annotating-correlation-coefficient-and-p-value-if-unavailable","title":"Annotating correlation coefficient and p value if unavailable\u00b6","text":"Note: In recent versions, seaborn does not print the correlation coefficient and its p-value. To get this, use annotation as shown below:"},{"location":"teaching_resources/seaborn/seaborn_exploring_distributions/#plotting-dist-of-all-variables","title":"Plotting dist of all variables\u00b6","text":"<p>You can get a quick overview of the pariwise relationships between your columns using <code>pairplot</code>. Specifying a categorical variable to <code>hue</code> argument will shade it accordingly</p>"},{"location":"teaching_resources/seaborn/seaborn_exploring_distributions/#plotting-data-frequency","title":"Plotting data frequency\u00b6","text":"<p>Histograms provide data frequency. The <code>distplot</code> gives histograms. Another way to viz this is using <code>rugplot</code>. Rug plots are similar to the trading frequency bars we see in stock ticker time series datasets.</p>"},{"location":"teaching_resources/seaborn/seaborn_grids_customization/","title":"Seaborn - grids and customization","text":"In\u00a0[4]: Copied! <pre>import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\niris= sns.load_dataset('iris')\niris.head()\n</pre> import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline iris= sns.load_dataset('iris') iris.head() Out[4]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa In\u00a0[6]: Copied! <pre>grd = sns.PairGrid(data=iris)\n\n#then you can assign what you want plotted for diagonal, above diagonal, below diagonal.\n# when mapping, pass just function pointers, dont call the function itself.\ngrd.map_diag(sns.distplot)\ngrd.map_upper(plt.scatter)\ngrd.map_lower(sns.kdeplot)\n</pre> grd = sns.PairGrid(data=iris)  #then you can assign what you want plotted for diagonal, above diagonal, below diagonal. # when mapping, pass just function pointers, dont call the function itself. grd.map_diag(sns.distplot) grd.map_upper(plt.scatter) grd.map_lower(sns.kdeplot) Out[6]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x11ea36f98&gt;</pre> In\u00a0[330]: Copied! <pre>pgrid = sns.lmplot(x='min_season', y='min_pressure_merged', \n                   col='any_basin', # the column by which you need to split - needs to be categorical\n                   data=set1, \n                   col_wrap=3, # number of columns per row\n                   sharex=False, sharey=False, # will repeat ticks, coords for each plot\n                   line_kws={'color':'green'} # symbol for regression line\n                  )\n</pre> pgrid = sns.lmplot(x='min_season', y='min_pressure_merged',                     col='any_basin', # the column by which you need to split - needs to be categorical                    data=set1,                     col_wrap=3, # number of columns per row                    sharex=False, sharey=False, # will repeat ticks, coords for each plot                    line_kws={'color':'green'} # symbol for regression line                   ) In\u00a0[7]: Copied! <pre>#load tips data\ntips = sns.load_dataset('tips')\ntips.head()\n</pre> #load tips data tips = sns.load_dataset('tips') tips.head() Out[7]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 <p>Suppose we want to visualize <code>total_bill</code> by time of day and wheather or not it was a smoker. You need to filter data out then make dist plots. YOu can do all of that in 1 step with <code>FacetGrid</code>s.</p> In\u00a0[12]: Copied! <pre>#for each unique value in `time` you get a row and \n# each unique value in `smoker` you get a col\nfg = sns.FacetGrid(data=tips, row='time', col='smoker')\n\n#now map a plot for each of the grid\nfg.map(sns.distplot, 'total_bill')\n</pre> #for each unique value in `time` you get a row and  # each unique value in `smoker` you get a col fg = sns.FacetGrid(data=tips, row='time', col='smoker')  #now map a plot for each of the grid fg.map(sns.distplot, 'total_bill') Out[12]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x11db6aa90&gt;</pre> In\u00a0[16]: Copied! <pre>sns.set_style(style='ticks') #ticks, white, dark, darkgrid, whitegrid\n#redraw the facet grid from above\nfg = sns.FacetGrid(data=tips, row='time', col='smoker')\n\n#now map a plot for each of the grid\nfg.map(sns.distplot, 'total_bill')\n</pre> sns.set_style(style='ticks') #ticks, white, dark, darkgrid, whitegrid #redraw the facet grid from above fg = sns.FacetGrid(data=tips, row='time', col='smoker')  #now map a plot for each of the grid fg.map(sns.distplot, 'total_bill') Out[16]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x11db832b0&gt;</pre> In\u00a0[24]: Copied! <pre>plt.figure(figsize=(5,5)) #generate a fig, sns will piggyback this with the plot\nsns.distplot(tips['total_bill'])\n</pre> plt.figure(figsize=(5,5)) #generate a fig, sns will piggyback this with the plot sns.distplot(tips['total_bill']) Out[24]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1204cd828&gt;</pre> In\u00a0[31]: Copied! <pre>sns.set_context(context='poster', font_scale=0.8)\n# valid contexts = paper, notebook, talk, poster - \n# with notebook being 1:1 and paper being smaller and poster being largest\n\n#draw the facet grid\nfg = sns.FacetGrid(data=tips, row='smoker', col='time')\nfg.map(sns.distplot, 'total_bill')\n</pre> sns.set_context(context='poster', font_scale=0.8) # valid contexts = paper, notebook, talk, poster -  # with notebook being 1:1 and paper being smaller and poster being largest  #draw the facet grid fg = sns.FacetGrid(data=tips, row='smoker', col='time') fg.map(sns.distplot, 'total_bill') Out[31]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x120fccd68&gt;</pre> <p>Another way to set the size is to access the <code>fig</code> handle direclty</p> In\u00a0[35]: Copied! <pre>#draw the facet grid\nfg = sns.FacetGrid(data=tips, row='smoker', col='time')\n#set the size\nfg.fig.set_size_inches(w=10, h=10)\n#plot the fig\nfg.map(sns.distplot, 'total_bill')\n</pre> #draw the facet grid fg = sns.FacetGrid(data=tips, row='smoker', col='time') #set the size fg.fig.set_size_inches(w=10, h=10) #plot the fig fg.map(sns.distplot, 'total_bill') Out[35]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x122589ef0&gt;</pre>"},{"location":"teaching_resources/seaborn/seaborn_grids_customization/#seaborn-grids-and-customization","title":"Seaborn - grids and customization\u00b6","text":""},{"location":"teaching_resources/seaborn/seaborn_grids_customization/#pairgrids","title":"Pairgrids\u00b6","text":"<p>Pairgrid is similar to <code>pairplot</code>, except, it returns back an empty grid that you can fill up with desired plots later. Refresher on pariplot</p>"},{"location":"teaching_resources/seaborn/seaborn_grids_customization/#lmplot-for-scatter-and-regression-per-category","title":"<code>lmplot()</code> for scatter and regression per category\u00b6","text":"<p>Sometimes, you need to do a <code>joinplot()</code> but split it by some categorical column. You can custom build it using <code>FacetGrid</code> shown in next section. However, seaborn provides a convenience function called <code>lmplot()</code>. Note: In previous pages, you created <code>lmplots()</code> for just 2 columns without any category.</p>"},{"location":"teaching_resources/seaborn/seaborn_grids_customization/#facetgrid","title":"FacetGrid\u00b6","text":"<p>During EDA, you want to find the distribution of data by sub-categories, sub-conditions. You can do so by building FacetGrids. As it means, you get a grid for every facet of the data.</p>"},{"location":"teaching_resources/seaborn/seaborn_grids_customization/#customizing-grids","title":"Customizing grids\u00b6","text":"<p>If you dont like the pale blue background of seaborn plots, you can modify that with <code>set_style</code>.</p> Note: Using set_style() will control the appearance for the entire notebook and all future plots"},{"location":"teaching_resources/seaborn/seaborn_grids_customization/#fig-and-font-size","title":"Fig and font size\u00b6","text":"<p>You can use matplotlib <code>figsize</code> but have to specify that as a context as well.</p>"},{"location":"teaching_resources/seaborn/seaborn_grids_customization/#using-seaborn-context","title":"Using seaborn context\u00b6","text":"<p>You can use the <code>set_context()</code> to pick sizing templates</p>"},{"location":"teaching_resources/seaborn/seaborn_matrix_regression/","title":"Seaborn - matrix and regression plots","text":"In\u00a0[1]: Copied! <pre>import seaborn as sns\n%matplotlib inline\nflights = sns.load_dataset('flights')\nflights.head()\n</pre> import seaborn as sns %matplotlib inline flights = sns.load_dataset('flights') flights.head() Out[1]: year month passengers 0 1949 January 112 1 1949 February 118 2 1949 March 132 3 1949 April 129 4 1949 May 121 In\u00a0[2]: Copied! <pre>flights.shape\n</pre> flights.shape Out[2]: <pre>(144, 3)</pre> <p>Let us pivot this flights data such that it becomes a 2D matrix. Lets make the Month as row indices</p> In\u00a0[3]: Copied! <pre>flights_pv = flights.pivot_table(index='month', columns='year', values='passengers')\nflights_pv.head()\n</pre> flights_pv = flights.pivot_table(index='month', columns='year', values='passengers') flights_pv.head() Out[3]: year 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 month January 112 115 145 171 196 204 242 284 315 340 360 417 February 118 126 150 180 196 188 233 277 301 318 342 391 March 132 141 178 193 236 235 267 317 356 362 406 419 April 129 135 163 181 235 227 269 313 348 348 396 461 May 121 125 172 183 229 234 270 318 355 363 420 472 <p>Using <code>pivot_tables</code> we have also aggregated the data by month and years.</p> <p>ToC</p> <ul> <li>Heatmap</li> <li>Cluster plot</li> <li>Regression Linear model plot</li> </ul> In\u00a0[4]: Copied! <pre>sns.heatmap(flights_pv)\n</pre> sns.heatmap(flights_pv) Out[4]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x117e0ed68&gt;</pre> <p>From the heatmap above, we see there are more passengers in summer (June, July, August) and the number of passengers increases by the year as well.</p> In\u00a0[2]: Copied! <pre>#from ml chapter, read titanic data\nimport pandas as pd\ntitanic = pd.read_csv('../udemy_ml_bootcamp/Machine Learning Sections/Logistic-Regression/titanic_train.csv')\ntitanic.head()\n</pre> #from ml chapter, read titanic data import pandas as pd titanic = pd.read_csv('../udemy_ml_bootcamp/Machine Learning Sections/Logistic-Regression/titanic_train.csv') titanic.head() Out[2]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S In\u00a0[5]: Copied! <pre>titanic.shape\n</pre> titanic.shape Out[5]: <pre>(891, 12)</pre> In\u00a0[3]: Copied! <pre>titanic.isnull().head()\n</pre> titanic.isnull().head() Out[3]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 False False False False False False False False False False True False 1 False False False False False False False False False False False False 2 False False False False False False False False False False True False 3 False False False False False False False False False False False False 4 False False False False False False False False False False True False In\u00a0[4]: Copied! <pre>sns.heatmap(titanic.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n</pre> sns.heatmap(titanic.isnull(), yticklabels=False, cbar=False, cmap='viridis') Out[4]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x115ae64a8&gt;</pre> <p>You can see <code>Age</code> and <code>Cabin</code> columns have lots of null while others have none or very few.</p> In\u00a0[5]: Copied! <pre>sns.clustermap(flights_pv)\n</pre> sns.clustermap(flights_pv) <pre>/Users/atma6951/anaconda/envs/pychakras/lib/python3.6/site-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The axisbg attribute was deprecated in version 2.0. Use facecolor instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n</pre> Out[5]: <pre>&lt;seaborn.matrix.ClusterGrid at 0x11a438470&gt;</pre> <p>Cluster map rearranges the data to show cells of similar values close by.</p> In\u00a0[6]: Copied! <pre>tips = sns.load_dataset('tips')\ntips.head()\n</pre> tips = sns.load_dataset('tips') tips.head() Out[6]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 In\u00a0[7]: Copied! <pre>#regressin total bill to the tip\nsns.lmplot(x='total_bill', y='tip', data=tips)\n</pre> #regressin total bill to the tip sns.lmplot(x='total_bill', y='tip', data=tips) Out[7]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x11aa258d0&gt;</pre> <p>You can decorate this by splitting it by sex and assigning a different color for males and females</p> In\u00a0[9]: Copied! <pre>sns.lmplot(x='total_bill', y='tip', data=tips, hue='sex')\n</pre> sns.lmplot(x='total_bill', y='tip', data=tips, hue='sex') Out[9]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x11aa4c630&gt;</pre> <p>You can bring in factors like day of week and create a regression for each day</p> In\u00a0[10]: Copied! <pre>sns.lmplot(x='total_bill', y='tip', data=tips, hue='sex', col='day')\n</pre> sns.lmplot(x='total_bill', y='tip', data=tips, hue='sex', col='day') Out[10]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x11af086a0&gt;</pre>"},{"location":"teaching_resources/seaborn/seaborn_matrix_regression/#seaborn-matrix-and-regression-plots","title":"Seaborn - matrix and regression plots\u00b6","text":""},{"location":"teaching_resources/seaborn/seaborn_matrix_regression/#heatmap","title":"Heatmap\u00b6","text":"<p>Heatmaps are a great way to represent continually variying data. However, you need to run this on a matrix kind of dataset, one where the row indexes are values themselves instead of serials.</p>"},{"location":"teaching_resources/seaborn/seaborn_matrix_regression/#heatmap-for-null-data-visualization","title":"Heatmap for null data visualization\u00b6","text":"<p>SNS Heatmap is great to view how many nulls are in your data.</p>"},{"location":"teaching_resources/seaborn/seaborn_matrix_regression/#cluster-plot","title":"Cluster plot\u00b6","text":"<p>Cluster plots are useful to auto group datasets. More of this in machine learning section</p>"},{"location":"teaching_resources/seaborn/seaborn_matrix_regression/#regression-linear-model-plot","title":"Regression linear model plot\u00b6","text":"<p>You can do regression plots in two ways. You can decorate a scatter plot to have a fit or you can make a regression plot with scatter on it. We will see the latter.</p>"},{"location":"teaching_resources/sql/sql_crud/","title":"SQL - CRUD","text":"<p>Content derived from:  - https://www.sqltutorial.org/  - https://www.sqlitetutorial.net/</p> <p>SQL consists of </p> <ul> <li>Data definition language : CREATE TABLE, ALTER TABLE, CREATE DATABASE etc.</li> <li>Data manipulation language: SELECT, INSERT, UPDATE, DELETE statements </li> <li>Data control language: GRANT USER, REVOKE USER etc.</li> </ul>"},{"location":"teaching_resources/sql/sql_crud/#sql-standards","title":"SQL Standards","text":"<p>SQL was first created in 1970. ANSI then published the first SQL Standard in 1986, the second in 1992 called SQL92 or SQL2, third in 1999 called SQL99 or SQL3. The latest is SQL:2011</p>"},{"location":"teaching_resources/sql/sql_crud/#4-basic-ops-in-a-database","title":"4 basic ops in a database:","text":"<p>CRUD - Create, Read (Select), Update, Delete (Drop). In this page, we use <code>sqlite3</code> for the database. SQLite is tiny, portable, fast, light-weight database and works on all kinds of architectures (phones, laptops, cloud, edge devices etc). To enter sqlite, you type <code>&gt; sqlite3</code>. To exit and return back to bash, you type <code>.quit</code>.</p>"},{"location":"teaching_resources/sql/sql_crud/#creating-a-table","title":"Creating a table","text":"<p>Use <code>sqlite3</code> database which is portable and tiny. To create a new database, use <code>$ sqlite3 &lt;db_name&gt;</code></p> <pre><code>(scratch) \u279c  sql-playground$ sqlite3 favorites.db\nSQLite version 3.38.3 2022-04-27 12:03:15\nEnter \".help\" for usage hints.\nsqlite&gt; \n</code></pre> <p>Importing CSV into the DB:</p> <pre><code>sqlite&gt; .mode csv\nsqlite&gt; .import ../../temp/src7/favorites/favorites.csv fav\nsqlite&gt; .schema\nCREATE TABLE IF NOT EXISTS \"fav\"(\n\"Timestamp\" TEXT, \"title\" TEXT, \"genres\" TEXT);\nsqlite&gt; \n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#opening-back-a-sqlite-db","title":"Opening back a SQLite DB","text":"<p>To open a DB back, use <code>sqlite3 &lt;db name&gt;</code></p> <pre><code>scratch) \u279c  sql-playground ls\nfavorites.db  favorites2.db\n(scratch) \u279c  sql-playground sqlite3 favorites2.db \nSQLite version 3.38.3 2022-04-27 12:03:15\nEnter \".help\" for usage hints.\nsqlite&gt;\n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#listing-db-content","title":"Listing DB content","text":"<ul> <li><code>.tables</code> to list tables in the current DB</li> <li><code>.tables &lt;pattern&gt;</code> like <code>.tables f%</code> to open all tables that begin with <code>f</code>.</li> <li><code>.schema</code> to list all table schemas: Shows create table commands for all tb within the db</li> </ul>"},{"location":"teaching_resources/sql/sql_crud/#reading-select-statements","title":"Reading - Select statements","text":"<p>General syntax: <code>SELECT &lt;columns&gt; FROM &lt;database.table&gt; WHERE &lt;condition&gt;;</code>. YOu need to terminate commands with <code>;</code> Example:</p> <pre><code>SELECT Timestamp, language FROM favorires;\n</code></pre> <p>You can perform operations on the data as you query them out. You can do <code>AVG, COUNT, DISTINCT, LOWER, MAX, MIN, UPPER...</code> etc.</p> <pre><code>sqlite&gt; SELECT DISTINCT(language) FROM favorires;\nC\nPython\nScratch\n\nsqlite&gt; SELECT COUNT(Timestamp) FROM favorires;\n1456\n\nsqlite&gt; SELECT COUNT(DISTINCT(title)) FROM fav;\n107\n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#limiting-outputs","title":"Limiting outputs","text":"<p>Use <code>LIMIT &lt;num&gt;</code> to limit what is displayed.</p> <pre><code>sqlite&gt; SELECT title FROM fav LIMIT 5;\n\"How i met your mother\"\n\"The Sopranos\"\n\"Friday Night Lights\"\n\"Family Guy\"\n\"New Girl\"\nsqlite&gt; \n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#paging-output-using-offset-and-limit","title":"Paging output using <code>OFFSET</code> and <code>LIMIT</code>","text":"<p>Use <code>LIMIT row_count OFFSET offset</code> syntax to page through the results. You can increment the offset as you page through a large result table. Is is important to <code>ORDER BY</code> when paging to avoid duplicates.</p> <pre><code>--page 2\nsqlite&gt; SELECT employee_id, first_name, last_name FROM employees ORDER BY employee_id LIMIT 5 OFFSET 5;\n\nemployee_id  first_name  last_name\n-----------  ----------  ---------\n105          David       Austin   \n106          Valli       Pataballa\n107          Diana       Lorentz  \n108          Nancy       Greenberg\n109          Daniel      Faviet   \n\n--page 3\nsqlite&gt; SELECT employee_id, first_name, last_name FROM employees ORDER BY employee_id LIMIT 5 OFFSET 10;\nemployee_id  first_name   last_name\n-----------  -----------  ---------\n110          John         Chen     \n111          Ismael       Sciarra  \n112          Jose Manuel  Urman    \n113          Luis         Popp     \n114          Den          Raphaely\n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#where-clauses","title":"Where clauses","text":"<p>Equality operations. In sql, <code>=</code> is used for comparison, not <code>==</code>. For text columns, you can use <code>LIKE</code>. YOu can also combine with some simple regex like <code>%string%</code> to indicate any chars before and after the substring.</p> <pre><code>sqlite&gt; SELECT title FROM fav WHERE title= \"office\";\nsqlite&gt; SELECT title FROM fav WHERE title LIKE  \"office\";\nOffice\nOffice\nsqlite&gt; SELECT title FROM fav WHERE title LIKE  \"%office%\";\nOffice\nOffice\n\"The Office\"\n\"The Office\"\n\"The Office\"\n...\n\"ThE OffiCE\"\n\"The Office\"\nThevoffice\n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#delete-deleting-records","title":"<code>DELETE</code> - deleting records","text":"<p>use <code>DELETE FROM &lt;table&gt; WHERE &lt;condition&gt;</code> to delete records that match the condition.</p> <pre><code>sqlite&gt; SELECT count(title) FROM fav WHERE title LIKE \"%friends%\";\n9\nsqlite&gt; DELETE FROM fav WHERE title LIKE \"%friends%\";\nsqlite&gt; SELECT count(title) FROM fav WHERE title LIKE \"%friends%\";\n0\nsqlite&gt; \n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#update-updating-records","title":"<code>UPDATE</code> - Updating records","text":"<p>Use <code>UPDATE &lt;table&gt; SET &lt;operation&gt; WHERE &lt;condition&gt;</code> syntax. Command will operate on all records that match the given where clause.</p> <pre><code>sqlite&gt; SELECT title FROM fav WHERE title = \"Thevoffice\";\nThevoffice\nsqlite&gt; UPDATE fav SET title = \"The Office\" WHERE title = \"Thevoffice\";\n\n-- Verify\nsqlite&gt; SELECT title FROM fav WHERE title = \"Thevoffice\";\nsqlite&gt; \n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#insert-inserting-records","title":"<code>INSERT</code> - Inserting records","text":"<p>Use <code>INSERT INTO &lt;table&gt; (&lt;columns&gt;) VALUES (&lt;comma sep values&gt;);</code>. Example:</p> <pre><code>sqlite&gt; INSERT INTO genres (show_id, genre) VALUES (159, \"Comedy\");\n</code></pre>"},{"location":"teaching_resources/sql/sql_crud/#utility-commands-for-sqlite","title":"Utility commands for <code>SQLite</code>","text":"<ul> <li>Getting schema of a database: <code>.schema</code></li> <li>Viewing datatypes of each column in a table: <code>pragma table_info('tb_name');</code></li> <li>Timing your queries: <code>sqlite&gt; .timer on</code> will start to return tike taken for your queries</li> <li>Printing column names for select queries - <code>.headers on</code></li> <li>Pretty print with table formatting <code>.mode column</code></li> </ul>"},{"location":"teaching_resources/sql/sql_joins/","title":"SQL - Joining tables","text":"<p>To visit:  - Simple Join  - Inner Join  - Left Join  - Right Join  - Cross Join  - Self Join  - Full outer Join</p>"},{"location":"teaching_resources/sql/sql_joins/#sub-queries","title":"Sub queries","text":"<p>You can join two tables without actually using a join. This is a primitive and simpler way and is less readable. Below we try to find shows with \"Arnold Sch..\".</p> <pre><code>--Step 1: find person ID of Arnold Sch..\nsqlite&gt; SELECT id FROM people WHERE name LIKE \"%Arnold Sch%\";\n216\n\n--Step 2: pass this to stars table to find show id\nsqlite&gt; SELECT show_id FROM stars WHERE person_id = (SELECT id FROM people WHERE name LIKE \"%Arnold Sch%\");\n200355\n1025006\n10408914\n...\n\n--Step 3: pass this to shows table to get show names\nsqlite&gt; SELECT * FROM shows WHERE id IN (SELECT show_id FROM stars WHERE person_id = (SELECT id FROM people WHERE name LIKE \"%Arnold Sch%\"));\n200355|Moviewatch|1993|68\n1025006|Regis Philbin's Lifestyles|1984|11\n1388415|Citizen Kate|2008|\n1990507|Climate One Commonwealth Club Forum|2010|2\n2963070|Years of Living Dangerously|2014|17\n4074786|Radical Body Transformations|2015|30\n4995052|Explorer|2015|42\n5290904|Talking to Hollywood with Betty Zhou|2015|5\n7423218|Objectified|2016|25\n10408914|Superhero Kindergarten|2021|14\n10922386|Chad Goes Deep|2017|32\n12591074|Hallo, wie geht's|1989|37\n14650368|The New Celebrity Apprentice|2017|7\n15545956|Action - Neu im Kino|1986|\n</code></pre> <p>I had no idea Arnold Schwarzenegger acted in so many shows, some as recent at 2021!</p> <p>In the example below, we find the employees with the second highest salary from emp db.</p> <pre><code>--limit 1 offset 1 gets the 2nd row from the result set\nsqlite&gt; SELECT employee_id, first_name, last_name, salary\n   ...&gt; FROM employees\n   ...&gt; WHERE salary = (SELECT DISTINCT salary FROM\n   ...&gt; employees ORDER BY salary DESC LIMIT 1 OFFSET 1);\nemployee_id  first_name  last_name  salary \n-----------  ----------  ---------  -------\n101          Neena       Kochhar    17000.0\n102          Lex         De Haan    17000.0\n</code></pre>"},{"location":"teaching_resources/sql/sql_joins/#simple-joins","title":"Simple JOINs","text":"<p>You can JOIN two tables using the syntax: Here <code>PK - primary key</code> and <code>FK - foreign key</code>.</p> <pre><code>-- Simple join syntax\nSELECT &lt;cols&gt; FROM &lt;tableA&gt; JOIN &lt;tableB&gt; ON &lt;how_to_join&gt;\nSELECT users.fullname, cars.mileage_info FROM users JOIN cars ON users.car_name = cars.name\n\n-- With where and order by\nSELECT &lt;cols_to_disp&gt; FROM &lt;tableA&gt; JOIN &lt;tableB&gt; ON &lt;tableA.PK&gt; = &lt;tableB.FK&gt; WHERE &lt;condition&gt; ORDER BY &lt;field&gt;;\n</code></pre> <p>You can nest multiple joins as shown below:</p> <pre><code>sqlite&gt; SELECT title FROM people JOIN stars ON people.id = stars.person_id JOIN shows ON stars.show_id = shows.id WHERE people.name LIKE \"Arnold Sch%\" ORDER BY shows.year;\n</code></pre> <p>Another style to write this join, if you know the tables being used before hand, is this:</p> <pre><code>SELECT title FROM people, stars, shows WHERE people.id = stars.person_id AND stars.show_id = shows.id AND people.name LIKE \"Arnold Sch%\" ORDER BY shows.year;\n</code></pre> <p>Notice the command <code>JOIN</code> does not show up in the case above.</p>"},{"location":"teaching_resources/sql/sql_problem_sets_1/","title":"SQL Problem sets 1","text":"<p>Derived from: </p>"},{"location":"teaching_resources/sql/sql_problem_sets_1/#problem-sets","title":"Problem sets:","text":"<ol> <li>How many shows to type 'Comedy' were directed during a leap year?: This is a simple join between <code>shows</code> and <code>genres</code> tables. Use <code>%</code> operator as a modulus.</li> </ol> <pre><code>SELECT COUNT(DISTINCT shows.id) FROM shows\nJOIN genres ON shows.id = genres.show_id\nWHERE shows.year % 4 = 0\nAND genres.genre = \"Comedy\";\n</code></pre> <ol> <li>List all actors that have acted in a show called \"Anandham\".: This requires joining 3 tables - <code>person</code>, <code>stars</code>, <code>shows</code> and getting the actor names</li> </ol> <pre><code>SELECT people.name FROM people\nJOIN stars ON people.id = stars.person_id \nJOIN shows ON stars.show_id = shows.id\nWHERE shows.title = \"Anandham\";\n</code></pre> <p>which returns:</p> <pre><code>name             \n-----------------\nSaakshi Sivaa    \nBrinda Das       \nKamalesh         \nDelhi Kumar\n...\n</code></pre> <ol> <li>Find the number of actors that acted in shows between 1970 and 1990: This requires joining the same 3 tables and counting distinct actor ids:</li> </ol> <pre><code>SELECT COUNT(DISTINCT people.id) FROM PEOPLE \nJOIN stars ON people.id  = stars.person_id \nJOIN shows ON stars.show_id = shows.id \nWHERE shows.year BETWEEN 1970 AND 1990;\n</code></pre> <p>returns</p> <pre><code>COUNT(DISTINCT people.id)\n-------------------------\n72095\n</code></pre> <ol> <li>How many actors also worked as writers?</li> </ol> <pre><code>SELECT COUNT(DISTINCT name) FROM people\nWHERE id IN \n(SELECT stars.person_id FROM stars \nJOIN writers ON stars.person_id = writers.person_id);\n</code></pre> <ol> <li>List to top 10 shows that have highest votes and highest rating</li> </ol> <pre><code>SELECT shows.title, shows.year, shows.episodes, ratings.rating, ratings.votes FROM shows \nJOIN ratings ON shows.id = ratings.show_id \nORDER BY ratings.votes DESC, ratings.rating DESC \nLIMIT 10;\n</code></pre> <p>returns</p> <pre><code>title                  year  episodes  rating  votes  \n---------------------  ----  --------  ------  -------\nGame of Thrones        2011  73        9.2     1888391\nBreaking Bad           2008  62        9.4     1596038\nStranger Things        2016  34        8.7     920887 \nThe Walking Dead       2010  177       8.2     907667 \nFriends                1994  235       8.8     905205 \nSherlock               2010  15        9.1     854590 \nThe Big Bang Theory    2007  280       8.1     754516 \nDexter                 2006  96        8.6     679125 \nHow I Met Your Mother  2005  208       8.3     635758 \nTrue Detective         2014  24        8.9     524834\n</code></pre>"},{"location":"teaching_resources/sql/sql_query_operators/","title":"SQL operators and sorting rows","text":""},{"location":"teaching_resources/sql/sql_query_operators/#optimizing-queries-using-index","title":"Optimizing queries using Index","text":"<p>You can build an index using the <code>CREATE</code> command. Syntax: <code>CREATE INDEX &lt;index name&gt; ON &lt;table&gt; (&lt;col_name&gt;);</code></p> <pre><code>sqlite&gt; CREATE INDEX \"title_index\" ON \"shows\" (\"title\");\nRun Time: real 0.151 user 0.113454 sys 0.021356\n</code></pre> <p>The database uses a tree type data structure to optimize the search. Often a B-tree data structure is used. The tree is wide horizontally and shallow in tallness. The index creation is expensive, but queries are faster.</p> <p>Schema of the IMDB training table:</p> <pre><code>sqlite&gt; .schema\nCREATE TABLE shows (\n                    id INTEGER,\n                    title TEXT NOT NULL,\n                    year NUMERIC,\n                    episodes INTEGER,\n                    PRIMARY KEY(id)\n                );\nCREATE TABLE genres (\n                    show_id INTEGER NOT NULL,\n                    genre TEXT NOT NULL,\n                    FOREIGN KEY(show_id) REFERENCES shows(id)\n                );\nCREATE TABLE stars (\n                show_id INTEGER NOT NULL,\n                person_id INTEGER NOT NULL,\n                FOREIGN KEY(show_id) REFERENCES shows(id),\n                FOREIGN KEY(person_id) REFERENCES people(id)\n            );\nCREATE TABLE writers (\n                show_id INTEGER NOT NULL,\n                person_id INTEGER NOT NULL,\n                FOREIGN KEY(show_id) REFERENCES shows(id),\n                FOREIGN KEY(person_id) REFERENCES people(id)\n            );\nCREATE TABLE ratings (\n                show_id INTEGER NOT NULL,\n                rating REAL NOT NULL,\n                votes INTEGER NOT NULL,\n                FOREIGN KEY(show_id) REFERENCES shows(id)\n            );\nCREATE TABLE people (\n                id INTEGER,\n                name TEXT NOT NULL,\n                birth NUMERIC,\n                PRIMARY KEY(id)\n            );\n</code></pre>"},{"location":"teaching_resources/sql/sql_query_operators/#operations-on-columns","title":"Operations on columns","text":"<p>When querying data using <code>SELECT</code> statement, you can perform basic arithmetic and type conversions. The results are on-the-fly and not stored.</p> <pre><code>SELECT first_name, last_name, salary, salary * 1.05 as hike_salary\n    FROM employees;\n</code></pre>"},{"location":"teaching_resources/sql/sql_query_operators/#sql-operators","title":"SQL operators","text":"<p>Operators:</p> <ul> <li><code>=</code> for equality. Note, just 1 equal sign.</li> <li><code>&lt;&gt;</code> or <code>!=</code> for non equality</li> <li><code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code> for size comparison</li> <li><code>LIKE</code> - for tolerant string comparison</li> <li><code>AND</code>, <code>OR</code> - to chain conditions</li> <li><code>BETWEEN</code> - for range values such as money, dates, times, etc.</li> <li><code>IS NULL</code> to check if a value is null. Do not use <code>= NULL</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;SELECT * FROM employees WHERE department_id=5 ORDER BY first_name;\n&gt;SELECT * FROM employees WHERE last_name LIKE \"chen\";\n\n</code></pre>"},{"location":"teaching_resources/sql/sql_query_operators/#order-by-operator","title":"<code>ORDER BY</code> operator","text":"<p>Used to sort the displayed results. This does not change the data organization in the persisted table though. The general syntax is below. The default sort order is <code>ASC</code> for ascending.</p> <pre><code>SELECT &lt;columns&gt; FROM &lt;table&gt; ORDER BY &lt;col 1&gt; &lt;ASC | DESC&gt;, &lt;col2&gt; &lt;ASC | DESC&gt;;\n--example\nSELECT * FROM employees ORDER BY hire_date ASC LIMIT 10;\nSELECT * FROM employees ORDER BY hire_date ASC, salary DESC LIMIT 10;\n</code></pre>"},{"location":"teaching_resources/sql/sql_query_operators/#distinct-operator","title":"<code>DISTINCT</code> operator","text":"<p>Used to remove duplicates from the result set. Also used to get unique values. If there is <code>1</code> column after <code>DISTINCT</code> the operator finds unique values in that column. If multiple columns are specified, the operator will use the combinations of values in all specified columns for uniqueness check.</p> <p>Note: DISTINCT operator treats all <code>NULL</code> values the same. Thus it returns just one record per NULL value.</p> <pre><code>SELECT DISTINCT &lt;col1&gt;, &lt;col2&gt; FROM &lt;table&gt;;\n\n--EXAMPLE find unique job_id\nsqlite&gt; SELECT DISTINCT job_id FROM employees ORDER BY job_id;\njob_id\n------\n1     \n2     \n3     \n4     \n....\n\n--EXAMPLE: find unique combinations of job_id and manager_id\nsqlite&gt; SELECT DISTINCT job_id, manager_id FROM employees ORDER BY job_id;\njob_id  manager_id\n------  ----------\n1       205       \n2       101       \n3       101       \n4                 \n5       100       \n6       108   \n...\n</code></pre>"},{"location":"teaching_resources/sql/sql_query_operators/#group-by-operator","title":"<code>GROUP BY</code> operator","text":"<p>Used to group records by one or more columns. This is a variant of <code>DISTINCT</code> where you can select two or more columns but remove duplicates in 1 column from the result set.</p>"},{"location":"teaching_resources/tools/docker-1/","title":"Docker - an introduction","text":""},{"location":"teaching_resources/tools/docker-1/#need-for-docker","title":"Need for Docker","text":"<p>Pre-virtualization had a number of problems  - huge cost of VMs  - slow deployment, hard to manage  - VMWare and VirtualBox provided virtualization software which improved the process  - things improved when Azure, AWS started providing VMs for hire. They provided pay-as-you-go model. Also deployment was extremely fast.</p> <p>Virtualization changed the game as they allowed to almost eliminate the virtual OS.  - all containers run on the same linux kernel. However, isolation is provided at runtime.  - containers pack up only the needed parts of the OS. Thus they are faster to deploy, scale up  - containers are fully portable</p>"},{"location":"teaching_resources/tools/docker-1/#docker-architecture","title":"Docker architecture","text":"<p>There are 2 distinct parts  - <code>docker client</code> - you interact with this layer, typically via CLI apps  - <code>docker daemon</code> - the background process which powers the containers. We don't talk to this directly, but only via the <code>docker client</code>.</p> <p>Typically, the docker client and daemon run on the same machine, but it is possible from the client to connect to a remote daemon.</p>"},{"location":"teaching_resources/tools/docker-1/#images-and-containers-registries-and-repositories","title":"Images and Containers, Registries and Repositories","text":"<p>Images are read-only templates composed by stacking up other images. They are hosted on docker registry.</p> <p>Containers is an instance of an image.</p> <p>Registry is where we store our images. You can host this yourself or use Docker's public registry called <code>DockerHub</code>. Repositories are repos within the registry where images of a particular type are stored.</p> <p>Docker Hub - is a public registry. Official repos are encouraged to be used as they are vetted by Docker and ensure they get security updates frequently. Generally, user created repos have a prefixing <code>username/repo_name/image_name</code>. Usually, <code>tags</code> are used to specify docker versions.</p>"},{"location":"teaching_resources/tools/docker-1/#docker-commands-quickstart","title":"Docker commands - quickstart","text":"<ul> <li>list all images held locally - <code>docker images</code></li> <li>run an image - <code>docker run busybox:1.23 echo \"hello world\"</code> which is of syntax <code>docker run image &lt;command to run on image&gt; &lt;argument for command&gt;</code> On running this, Docker will try to find the image locally, if not found, will download from docker hub and run. Example output below:</li> </ul> <pre><code>open-geo [master] $ docker run busybox:latest echo \"hello atma\"\nUnable to find image 'busybox:latest' locally\nlatest: Pulling from library/busybox\n697743189b6d: Pull complete \nDigest: sha256:061ca9704a714ee3e8b80523ec720c64f6209ad3f97c0ff7cb9ec7d19f15149f\nStatus: Downloaded newer image for busybox:latest\nhello atma\nopen-geo [master] $ \n</code></pre> <ul> <li>if you run the same command agin, it will run faster as there is no downloading.</li> <li>you can do other things like look at the contents of the image by calling <code>docker run busybox:latest ls /</code></li> </ul>"},{"location":"teaching_resources/tools/docker-1/#running-docker-in-interactive-mode","title":"Running docker in interactive mode","text":"<ul> <li>run the image with <code>-i</code>, <code>-t</code> commands for interactive, TTY mode.</li> <li><code>docker run -i -t busybox:latest</code>. You can exit by typing <code>exit</code></li> </ul>"},{"location":"teaching_resources/tools/docker-1/#running-in-foreground-vs-detached-modes","title":"Running in foreground vs detached modes","text":"<p>The default mode is to run in foreground. Docker will start the process in container, attaches the console to this process' std output. You terminate the process and the connection together. When running, the terminal cannot be used for other things.</p> <p>Background / detached mode on the other hand can be stared with <code>-d</code> flag. The container is killed when the root process of container terminates. You can use the terminal to do other things while the container is running.</p> <ul> <li><code>docker run -d busybox:latest sleep 100</code> will run it in background mode.</li> <li>You can list the running containers using <code>docker ps</code> command. Add the <code>-a</code> to list all containers that are stopped as well.</li> <li>By default, docker remembers both your containers and images. So you can rerun a docker container. If you do not want to retain a container, use <code>docker run --rm</code> option. The container is removed after it is run.</li> <li>to remove a container, run <code>docker rm &lt;name of container&gt;</code>.</li> <li>to run a container with a custom name, run <code>docker run --name &lt;custom_name&gt; &lt;commands&gt;</code>. Then you can verify this using <code>docker ps -a</code> command.</li> <li>When running in background mode, you cannot see the logs. Thus, you can call <code>docker logs &lt;container_name_or_id&gt;</code> to view the logs.</li> </ul>"},{"location":"teaching_resources/tools/docker-1/#docker-inspect","title":"Docker inspect","text":"<p>Use inspect to display low level information about a container or image.</p> <ul> <li>to get low level details, run <code>docker inspect &lt;container_id&gt;</code> or <code>docker inspect &lt;conatiner_name&gt;</code></li> </ul>"},{"location":"teaching_resources/tools/docker-1/#port-mapping","title":"Port mapping","text":"<p>We can expose a port on the container and map that to port on host using <code>-p</code> option. For instance: <code>docker run -it -p 8888:8080 tomcat:8.0</code> will run tomcat and map its <code>8888</code> port to the default <code>80</code> port on host.</p>"},{"location":"teaching_resources/tools/docker-1/#docker-images-in-detail","title":"Docker images in detail","text":""},{"location":"teaching_resources/tools/docker-1/#docker-image-layers","title":"Docker image layers","text":"<p>A docker image is built as a list of read-only layers. At the bottom is a base layer on which other docker images or custom apps are installed. You can see all the layers of an image using <code>docker history &lt;image name:tag&gt;</code> command.</p> <p>When a new container is instantiated, a new editable, writable (container) layer is added on top of this read-only image layers. Thus when container is deleted, all changes are lost and the image remains unchanged. Thus different containers instantiated from same image can have different data states emanating from their thin, editable layers.</p>"},{"location":"teaching_resources/tools/docker-1/#build-docker-image","title":"Build docker image","text":"<p>There are 2 ways of building an image   1. commit changes made to a container (yay!)   2. by writing a <code>Dockerfile</code>.</p>"},{"location":"teaching_resources/tools/docker-1/#build-docker-image-by-committing-a-container","title":"Build docker image by committing a container","text":"<p>Here we make changes to the container, typically running it in interactive mode. Then after exiting the container (container can be stopped at this point), we call <code>docker commit &lt;container name or id&gt; &lt;username/image_name:tag&gt;</code>. Thus in this ex for example, I did the following  - <code>docker run -it debian:jessie</code> which downloads and runs debian OS in interactive mode  - from within the container, run <code>apt-get update &amp;&amp; apt-get install -y git</code> to install GIT  - exit container and to commit, I do <code>docker commit hopeful_ellis atmamani/debian:0.1</code>  - then running <code>docker images</code> will report the <code>atmamani/debian</code> image.  - finally, running <code>docker history atmamani/debian:0.1</code> lists the changes I made. Note, it only states modification as bash, meaning it was a bash command, but not what the command is.</p>"},{"location":"teaching_resources/tools/docker-1/#build-docker-image-using-dockerfile","title":"Build docker image using <code>Dockerfile</code>","text":"<p>Dockerfile is a text based manifest. The file should not have any extension and must start with a capital <code>D</code>. Below is a sample Dockerfile</p> <pre><code>FROM debain:jessie\nRUN apt-get update\nRUN apt-get install -y git\nRUN apt-get install -y vim\n</code></pre> <p>Then build it using command <code>docker build -t &lt;username/imagename:tag&gt; &lt;buildcontext&gt;</code> The build context here needs path to the dockerfile on disk. You can also use the build context to package up additional files into the docker image. Docker daemon will create a tarball of these files, send it to the contain, unpack. So I ran <code>docker build -t atmamani/debian:frmdckfile .</code> from the folder containing the Dockerfile.</p> <p>Once built, you can inspect the history and now it reports it correctly:</p> <pre><code>docker-trials $ docker history atmamani/debian:frmdckfile\nIMAGE               CREATED              CREATED BY                                      SIZE                COMMENT\na9d62a3217ae        25 seconds ago       /bin/sh -c apt-get install -y vim               29.7MB              \na4c2ad02f957        39 seconds ago       /bin/sh -c apt-get install -y git               85MB                \ncdfcdde0ccab        About a minute ago   /bin/sh -c apt-get update                       10.3MB              \nb6ebaf83dd59        8 days ago           /bin/sh -c #(nop)  CMD [\"bash\"]                 0B                  \n&lt;missing&gt;           8 days ago           /bin/sh -c #(nop) ADD file:e044496893d9e2cbf\u2026   129MB               \ndocker-trials $ \n</code></pre>"},{"location":"teaching_resources/tools/docker-1/#optimizing-docker-images","title":"Optimizing docker images","text":"<p>One aspect is to reduce the number of layers a docker image has. Each line in the Dockerfile corresponds to a layer. Thus chaining the lines into a single line will reduce this to 1 layer. For instance, the earlier Dockerfile can be trimmed to</p> <pre><code>FROM debian:jessie\nRUN apt-get update &amp;&amp; apt-get instal -y git vim\n</code></pre>"},{"location":"teaching_resources/tools/docker-1/#baking-run-commands-into-docker-image","title":"Baking run commands into Docker image","text":"<p>We can bake commands to be run when a container starts up from an image. The preferred way is to use <code>exec</code> commands, an alternate is to specify <code>bash</code> commands. We prefix such with <code>CMD</code> keyword. If you do not specify a CMD, the default is to bring up <code>bash</code>.</p> <pre><code>FROM debian:jessie\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    python \\\n    vim\nCMD [\"echo\", \"helllo world\"]\n</code></pre> <p>When a container is created, it runs this <code>helllo world</code> with a typo. You can also modify this at runtime, by specifying <code>docker run &lt;image_name&gt; &lt;commands&gt; &lt;arguments&gt;</code> such as <code>docker run atmamani/echoer echo hello hello</code>.</p>"},{"location":"teaching_resources/tools/docker-1/#copying-files-into-the-image","title":"Copying files into the image","text":"<pre><code>FROM debian:jessie\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    python \\\n    vim\nCOPY ./abc.txt /tmp/abc.txt\n</code></pre> <p>The above will copy <code>abc.txt</code> from local dir into the <code>tmp</code> dir of the image. This will be available for all containers spun up from this image.</p>"},{"location":"teaching_resources/tools/docker-1/#pushing-images-to-docker-hub","title":"Pushing images to Docker hub","text":"<p>Some people say don't use <code>latest</code> tag, instead use a real SEMVER version. To tag an image before pushing to hub or production, you need to tag it appropriately. <code>docker tag &lt;image name&gt; &lt;reponame/newname:tag&gt;</code> Thus, <code>docker tag atmamani/debian:frmdckfile atmamani/debian:0.2</code></p> <p>Then push using <code>docker push &lt;imagename&gt;</code> as <code>docker push atmamani/debian:0.2</code></p>"},{"location":"teaching_resources/tools/docker-1/#a-hello-world-app-using-flask","title":"A Hello world app using Flask","text":"<p>Download the repo using <code>git clone -b v0.1 https://github.com/jleetutorial/dockerapp.git</code>. Then build the docker image using <code>docker build -t tut-dockerapp:v0.1 .</code></p> <p>Run the container as <code>docker run -d -p 5000:5000 tut-dockerapp:v0.1</code> this spins up the container and maps the port. You can use the web app at <code>localhost:5000</code>. If for some reason, this is not available or unknown, you can find the IP address of the docker using <code>docker-machine ls</code> or find which port is mapped using <code>docker ps</code>.</p> <p>To step into a running container, use <code>docker exec -it &lt;cont name or id&gt; &lt;command&gt;</code>, such as <code>docker exec -it keen_noyce bash</code>. The <code>-i</code> flag makes it interactive. To see the running process within the container do <code>ps axu</code> from within the <code>home</code> directory:</p> <pre><code>dockerapp [(no branch)] $ docker exec -it keen_noyce bash\nadmin@dd29aef6445a:/app$ cd ~\nadmin@dd29aef6445a:~$ ps axu\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nadmin        1  0.0  1.2 102852 25080 ?        Ss   19:33   0:00 python app.py\nadmin       17  0.1  0.1  19964  3680 pts/0    Ss   19:47   0:00 bash\nadmin       24  0.0  0.1  38380  3216 pts/0    R+   19:47   0:00 ps axu\nadmin@dd29aef6445a:~$ \n</code></pre> <p>To exit out of the container's bash, type <code>exit</code>. To stop the container running as daemon, type <code>docker stop &lt;container_name&gt;</code>.</p>"},{"location":"teaching_resources/tools/docker-1/#kvp-lookup-app-using-flask","title":"KVP lookup app using Flask","text":""},{"location":"teaching_resources/tools/docker-1/#linking-containers-using-redis","title":"Linking containers using <code>redis</code>","text":"<p><code>redis</code> is a message broker, inmemory cache, db service that is containerized. We need a redis Python client.</p>"},{"location":"teaching_resources/tools/docker-1/#linking-with-docker-compose","title":"Linking with Docker compose","text":"<p><code>docker-compose.yml</code> is a YAML file. Verison 3 is up to date. You start with a version number. Then specify the services that needs to be run. The <code>dockerapp</code> contains the main, user facing app, the <code>depends_on</code> contains all other microservices that need to be spun up and the order in which they need to be spun up. A sample looks like:</p> <pre><code>verison: '3'\nservices:\n  dockerapp:\n    build: .\n    ports:\n      - \"5000:5000\"\n    depends_on:\n      - redis\n  redis:\n    image: redis:3.2.0\n</code></pre> <p>To build and run using docker compose, run <code>docker-compose up -d</code> from dir that contains the yaml file. Once done, you can run <code>docker ps</code> to see the containers created and being run.</p> <p>To rebuild a docker compose, you may have to force it if are changing just the python stack. To force rebuild use <code>docker-compose build</code>.</p>"},{"location":"teaching_resources/tools/grep-1/","title":"Grep - an introduction","text":""},{"location":"teaching_resources/tools/grep-1/#purpose","title":"Purpose","text":"<p><code>grep</code> is a Linux/Unix shell tool that allows you to search in text files. Through piping, you can also use it to search through the output printed to screen from another command. Most of what I use grep for is that.</p>"},{"location":"teaching_resources/tools/grep-1/#syntax","title":"Syntax","text":"<p>General search syntax is </p> <pre><code>grep \"&lt;search string&gt;\" &lt;file.txt&gt;\n</code></pre> <p>Grep will return any lines that contain the full or substrings of it.</p>"},{"location":"teaching_resources/tools/grep-1/#piping","title":"Piping","text":"<p>You can pass the output of a previous command to grep using the pipe <code>|</code> operator, like so:</p> <pre><code>$ argo list -n &lt;namespace&gt; --running | grep \"fire-season-full\"\nfire-season-full-ca-nox\nfire-season-full-ca-co2\n...\n</code></pre> <p>You can pipe any number of times. You can pipe <code>grep</code> to another <code>grep</code>, which leads to some very powerful workflows.</p> <pre><code>(base) \u279c  $ history | grep \"git commit -m\" | grep \"error\"\n  859  git commit -m \"chore(test):[READY-220] fix lint errors\"\n</code></pre> <p>Here, <code>history</code> feature of zsh returns the last <code>1000</code> commands ran in terminal. We extract just the <code>git commit</code> commands. Of the result, we extract any line that spoke about an <code>error</code>.</p>"},{"location":"teaching_resources/tools/grep-1/#options","title":"Options","text":"Task Option Example Search whole string <code>-w</code> <code>grep -w \"abc123\" log.txt</code> Case insensitive search <code>-i</code> <code>grep -i \"abc123\" log.txt</code> Show line numbers <code>-n</code> <code>grep -n \"abc123\" log.txt</code> Show <code>n</code> lines before match <code>-B</code> n <code>grep -win -B 2 \"abc123\" log.txt</code> Show <code>n</code> lines after the match <code>-A</code> n <code>grep -A 2 \"abc\" log.txt</code> -------------------------------- ----------- ---------------------------------- Search all text files in a dir recursively <code>-r</code> <code>grep -r \"abc123\" ./*.txt</code> Search files, but only return number of hits, not every hit in a file <code>-c</code> <code>grep -c \"abc\" ./*.txt</code>"},{"location":"teaching_resources/tools/grep-1/#combining-options","title":"Combining options","text":"<p>Grep allows you to combine options with a single hyphen. Although this makes the command less readable, it is still a valid syntax.</p> <p>Do case insensitive search of the full search term. Show results with line numbers</p> <pre><code>$ grep -win \"&lt;search term&gt;\" file.logs\n&gt; 51:search term\n&gt; 99:search TERM\n</code></pre>"},{"location":"teaching_resources/tools/gsutil-1/","title":"<code>gsutil</code> commands","text":""},{"location":"teaching_resources/tools/gsutil-1/#introduction","title":"Introduction","text":"<p><code>gsutil</code> is a CLI tool to work with data on Google Storage Service. Using the tool, you can query public data anonymously. However, to query private data, you need to authenticate. Gsutil can inherit the auth from <code>gcloud</code> CLI.</p> <p>The general syntax to represent a bucket is:</p> <pre><code>gs://BUCKET_NAME/&lt;FOLDER_NAME&gt;/OBJECT_NAME\n</code></pre> <p>You can have any number of nested sub-folders within a bucket.</p> <p>Buckets in GCS are globally-unique, even for private buckets.</p>"},{"location":"teaching_resources/tools/gsutil-1/#commands","title":"Commands","text":"<p>Adapted from https://cloud.google.com/storage/docs/discover-object-storage-gsutil</p>"},{"location":"teaching_resources/tools/gsutil-1/#creating-a-bucket","title":"Creating a bucket","text":"<pre><code>gsutil mb -b on -l us-east1 gs://my-awesome-bucket/\n\n&gt;&gt;&gt; Creating gs://my-awesome-bucket/...\n</code></pre>"},{"location":"teaching_resources/tools/gsutil-1/#uploading-objects-to-a-bucket","title":"Uploading objects to a bucket","text":"<pre><code>gsutil cp &lt;path&gt;/&lt;file&gt; gs://&lt;bucket&gt;/folder\n</code></pre>"},{"location":"teaching_resources/tools/gsutil-1/#downloading-objects-from-a-bucket","title":"Downloading objects from a bucket","text":"<pre><code>gsutil cp gs://&lt;bucket&gt;/folder/file &lt;local dest&gt;/&lt;path&gt;\n</code></pre> <p>You can also copy content from one bucket / folder to another using the same command.</p> <pre><code>gsutil cp gs://my-awesome-bucket/kitten.png gs://my-awesome-bucket/just-a-folder/kitten3.png\n</code></pre>"},{"location":"teaching_resources/tools/gsutil-1/#list-contents-of-a-bucket","title":"List contents of a bucket","text":"<pre><code>gsutil ls gs://&lt;bucket&gt;\n\n# returns\ngs://my-awesome-bucket/kitten.png\ngs://my-awesome-bucket/just-a-folder/\n</code></pre> <p>You can use <code>-l</code> flag for details.</p>"},{"location":"teaching_resources/tools/gsutil-1/#deleting-objects","title":"Deleting objects","text":"<pre><code>gsutil rm gs://&lt;bucket&gt;/file\n</code></pre> <p>You can use the <code>-r</code> recursive flag to remove folders, and even buckets.</p>"},{"location":"teaching_resources/tools/helm-1/","title":"Helm charts","text":"<p> Nautical charts - called Helm charts where used to navigate vessels in open waters</p>"},{"location":"teaching_resources/tools/helm-1/#what-are-helm-charts","title":"What are Helm charts?","text":"<p>Helm is a package manager for Kubernetes. When deploying an application on K8s, you need to have not only the pod-spec and the deployment-spec, but also YAML files that describe the stateful-set, ConfigMap, user permissions, secrets, services etc. Helm charts package up all these commonly used YAML files as charts and shares it as standard sets. This bundle of YAML files are called Helm charts.</p> <p>Commonly used apps such as DB apps (Mongo, PostGRES), monitoring apps such as Prometheus all have commonly available Helm charts. Today there are repositories of such charts that are both public and private.</p>"},{"location":"teaching_resources/tools/helm-1/#helm-as-a-templating-engine","title":"Helm as a templating engine","text":"<p>Helm is also a templating engine. Consider an application with multiple micro-services. The spec for each of these are usually identical except for metadata. With Helm, you can define a common blueprint called a <code>template</code> or <code>tpl</code> file and associate it with one or more <code>values.yaml</code> file that contains the specific info for each service.</p> <p>For example, you may have to deploy the same service on dev, staging and prod. The spec for these clusters might be identical except for some key, but uniform differences. Thus you can have Helm charts such as the one shown below:</p> <p>deployment_tpl.yaml</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: {{.Values.name}}\nspec:\n    containers:\n    - name: {{.Values.container.name}}\n      image: {{.Values.container.image}}\n      port: {{.Values.container.port}}\n</code></pre> <p>and values for each env as shown below: dev.values.yaml</p> <pre><code>name: my-app-dev\ncontainer:\n    name: cont1\n    image: gcr.io/cont1img1\n    port: 5000\n</code></pre> <p>This ensures the deployment on each cluster is identical.</p>"},{"location":"teaching_resources/tools/helm-1/#helm-chart-structure","title":"Helm chart structure","text":"<p>A Helm chart is usually a folder with the following files:</p> <pre><code>myChart/\n    Chart.yaml      # Metadata about the chart\n    templates/      # Folder with Helm template files\n    values.yaml     # Values for the template files\n    charts/         # Folder for any chart dependencies. If the main chart depends on other charts.\n    readme.md       # optional\n    license         # optional\n</code></pre> <p>These files are deployed using the command: <code>helm install &lt;chartname&gt;</code></p>"},{"location":"teaching_resources/tools/kubectl-1/","title":"Kubectl commands","text":"<p><code>kubectl</code> is the CLI tool that interfaces with Kubernetes. It can work with either a local setup (such as with Docker desktop's K8 or Minikube) or with one or more remote clusters. Most operations on K8s will be via <code>kubectl</code>.</p>"},{"location":"teaching_resources/tools/kubectl-1/#1minikube-commands","title":"1.<code>minikube</code> Commands","text":"<p>First, we need to start the local cluster. For that, we need to install minikube.</p>"},{"location":"teaching_resources/tools/kubectl-1/#install-minikube","title":"Install <code>minikube</code>","text":"<p><code>brew</code> makes install easier, whether or not you are on a M1 mac. Run:</p> <pre><code>brew install minikube\n</code></pre> <p>followed by:</p> <pre><code>which minikube\nminikube version\n</code></pre>"},{"location":"teaching_resources/tools/kubectl-1/#starting-and-stopping-the-local-cluster","title":"Starting and stopping the local cluster","text":"<p>Minikube uses virtualbox to isolate the local set up of kubernetes. Once installed, you can run </p> <pre><code>minikube start\n</code></pre> <p>which returns the following in my case:</p> <pre><code>$ minikube start\n\ud83d\ude04  minikube v1.27.1 on Darwin 12.6 (arm64)\n\ud83c\udd95  Kubernetes 1.25.2 is now available. If you would like to upgrade, specify: --kubernetes-version=v1.25.2\n\u2728  Using the docker driver based on existing profile\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\ude9c  Pulling base image ...\n\ud83d\udd04  Restarting existing docker container for \"minikube\" ...\n\ud83d\udc33  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n    \u25aa Using image docker.io/kubernetesui/metrics-scraper:v1.0.8\n    \u25aa Using image docker.io/kubernetesui/dashboard:v2.7.0\n\ud83c\udf1f  Enabled addons: default-storageclass, storage-provisioner, dashboard\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n</code></pre> <p>Note: When minikube starts, <code>kubectl</code> will automatically bind to minikube's kubectl by modifying the <code>kubectl</code> context. Thus, running <code>k get pods</code> will only return the ones running in minikube, not your remote cluster. To talk to a different cluster, use the <code>k config use-context &lt;context_name&gt;</code> command.</p> <p>Once done, you can stop the cluster by running</p> <pre><code>minikube stop\n</code></pre>"},{"location":"teaching_resources/tools/kubectl-1/#setting-up-shpod-namespace","title":"Setting up <code>shpod</code> namespace","text":"<p>The k8s course recommends using <code>shpod</code> for a consistent shell experience. The yaml file in the link has the definition of a namespace.</p> <pre><code>k apply -f https://k8smastery.com/shpod.yaml\nk attach -n shpod -it shpod\n</code></pre> <p>The attach command follows the syntax <code>kubectl attach -n &lt;namespace&gt; --interactive --tty &lt;contianer_name&gt;</code></p> <p>Once done, the pod can be terminated using</p> <pre><code>k delete -f https://k8smastery.com/shpod.yaml\n</code></pre>"},{"location":"teaching_resources/tools/kubectl-1/#2kubectl-commands","title":"2.<code>kubectl</code> Commands","text":"<p>The <code>kubectl</code> command needs to know which cluster to talk to and how to authenticate. This information is stored in the <code>~/.kube/config</code> file. The provisioner (which can be GKE or minikube) will also provide / edit this file. The file as the IP address of the k8s server and the TLS certs for auth.</p> <p>Get contexts First, you need to know which k8s cluster you are talking to. For this run: The <code>*</code> points to the active cluster. All <code>kubectl</code> commands apply to that cluster now.</p> <pre><code>k config get-contexts\nCURRENT   NAME        CLUSTER    AUTHINFO   NAMESPACE\n          gke_dev     gke_dev    gke_dev    default\n          gke_prod    gke_prod   gke_prod   default\n          gke_stage   gke_stage  gke_stage  default\n*         minikube    minikube   minikube   default\n</code></pre> <p>Change contexts When you want to talk to a different cluster, change the cluster using the syntax <code>k config use-context &lt;contxtname&gt;</code></p> <pre><code>k config use-context gke_dev\n</code></pre> <p>The run <code>k config get-contexts</code> to confirm the switch.</p> <p>Get nodes <code>k get nodes</code> Nodes are the physical machines that run the Kubernetes cluster. </p> <pre><code>(local2) \u279c  Documents k get nodes\nNAME       STATUS   ROLES           AGE     VERSION\nminikube   Ready    control-plane   7d22h   v1.24.1\n</code></pre> <p>The <code>get</code> command is the most frequently used command. <code>get</code> can return output in a variety of formats:</p> <pre><code>k get nodes -o wide\nNAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nminikube   Ready    control-plane   97d   v1.24.1   192.168.49.2   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.10.104-linuxkit   docker://20.10.17\n</code></pre> <p>You can get return in <code>json</code> format and pipe that to a CLI tool called <code>jq</code> which can parse and extract certain info like this:</p> <pre><code> k get node -o json | jq \".items[] | {name:.metadata.name} + .status.capacity\"\n{\n  \"name\": \"minikube\",\n  \"cpu\": \"5\",\n  \"ephemeral-storage\": \"61255492Ki\",\n  \"hugepages-1Gi\": \"0\",\n  \"hugepages-2Mi\": \"0\",\n  \"hugepages-32Mi\": \"0\",\n  \"hugepages-64Ki\": \"0\",\n  \"memory\": \"8039792Ki\",\n  \"pods\": \"110\"\n}\n</code></pre> <p>Describe node <code>k describe node &lt;node_name&gt;</code> If you want to delve into the details of how the node is configured and its health, you can run <code>k describe node minikube</code>.</p> <p>Get namespaces <code>k get ns</code></p> <pre><code>(flood_ml_local2) \u279c  Documents k get ns\nNAME                   STATUS   AGE\ndefault                Active   7d22h\nkube-node-lease        Active   7d22h\nkube-public            Active   7d22h\nkube-system            Active   7d22h\nkubernetes-dashboard   Active   7d22h\n</code></pre> <p>Create a new namespace <code>k create ns &lt;ns_name&gt;</code></p> <pre><code>(base) \u279c  Documents k create ns argo-local\nnamespace/argo-local created\n\n# verify\n\n(base) \u279c  Documents k get ns             \nNAME                   STATUS   AGE\nargo-local             Active   4h54m\ndefault                Active   8d\nkube-node-lease        Active   8d\nkube-public            Active   8d\nkube-system            Active   8d\nkubernetes-dashboard   Active   8d\n(base) \u279c  Documents \n</code></pre> <p>Create argo server on k8s</p> <pre><code>(base) \u279c  Documents k apply -n argo-local -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml\ncustomresourcedefinition.apiextensions.k8s.io/clusterworkflowtemplates.argoproj.io created\ncustomresourcedefinition.apiextensions.k8s.io/cronworkflows.argoproj.io created\n....\n</code></pre> <p>Then forward the port to local machine:</p> <pre><code>(base) \u279c  Documents k -n argo-local port-forward deployment/argo-server 2746:2746\nForwarding from 127.0.0.1:2746 -&gt; 2746\nForwarding from [::1]:2746 -&gt; 2746\n\n</code></pre> <p>Argo server UI is now accessible at https://localhost:2746. You may have to agree to security warnings before accessing this page.</p> <p>Get pods <code>k get pods -n &lt;namespace&gt;</code></p> <pre><code>(base) \u279c  Documents k get pods -n argo-local\nNAME                                   READY   STATUS    RESTARTS        AGE\nargo-server-7fbf57bc87-f82wl           1/1     Running   3 (2m11s ago)   2m31s\nminio-74474c548b-6hf48                 1/1     Running   0               2m31s\npostgres-6b5944c545-tpnfb              1/1     Running   0               2m31s\nworkflow-controller-7d4bf4fd7d-x4qkk   1/1     Running   2 (2m9s ago)    2m31s\n(base) \u279c  Documents \n\n</code></pre> <p>Creating Argo workflow using <code>kubectl</code></p> <pre><code>(base) \u279c  ~ k create -n argo-local -f Documents/code/temp/wf-hello-world.yaml \nworkflow.argoproj.io/hello-world-p4wlj created\n</code></pre>"},{"location":"teaching_resources/tools/latex-1/","title":"Latex notations in Jupyter notebooks","text":"In\u00a0[\u00a0]: Copied!"},{"location":"teaching_resources/tools/latex-1/#latex-notations-in-jupyter-notebooks","title":"Latex notations in Jupyter notebooks\u00b6","text":"<p>This cheetsheet covers how to write equations and math symbols in Notebooks.</p>"},{"location":"teaching_resources/tools/latex-1/#help-docs","title":"Help docs\u00b6","text":"<ul> <li>Udacity blog</li> </ul>"},{"location":"teaching_resources/tools/latex-1/#how-to-write-tex-notations","title":"How to write tex notations\u00b6","text":""},{"location":"teaching_resources/tools/latex-1/#in-line-notations","title":"In-line notations\u00b6","text":"<p>Wrap your equation with <code>$ ... $</code></p>"},{"location":"teaching_resources/tools/latex-1/#whole-line-notations","title":"Whole line notations\u00b6","text":"<p>Wrap your segments with <code>$$ ... $$</code></p>"},{"location":"teaching_resources/tools/latex-1/#math-symbols","title":"Math symbols\u00b6","text":"<ul> <li>Write brackets {} as <code>\\{ and \\}</code></li> <li>write spaces as ~ for a single space</li> <li>write most math symbols using intuitive names. For instance, write phi as <code>$ \\phi $</code>  : $\\phi$</li> <li>to negate a symbol, prefix it with <code>\\not</code>. For instance, for not subset, write as <code>$A \\not\\subset B$</code> which results in $A \\not\\subset B$</li> <li>write fractions as <code>\\frac{x}{y}</code> to represent x over y: $\\frac{x}{y}$</li> <li>write subscript as <code>_{}</code> and superscript as <code>^{}</code>. For instance <code>$P_{23}$</code> for $P_{23}$ and <code>$P^{34}$</code> for $P^{34}$</li> <li>write new line with two \\</li> <li>write decorations prefixing teh symbol. For instance for ** x bar** write as <code>\\bar x</code>: $\\bar x$</li> </ul>"},{"location":"teaching_resources/tools/yaml_syntax/","title":"Yaml Syntax","text":""},{"location":"teaching_resources/tools/yaml_syntax/#whats-yaml","title":"What's YAML","text":"<p>Yaml originally stood for \"Yet Another Markup Language\", but some say it later changed to \"YAML Ain't Markup Language\". Nonetheless, it is used to serialize config style documents in a human readable form. Contenders for YAML are JSON, TOML, XML etc.</p> <p>Characteristics of YAML - Compact syntax with Python style indentation. - Version stable since 2009 at version <code>1.2.x</code> - Files are saved with <code>.yaml</code> or <code>.yml</code> extension - Yet this is language agnostic and can be R/W from any prog. language. - Supports common data types such as <code>int</code>, <code>binary</code>, <code>str</code>, <code>bool</code>, <code>map</code> or dictionaries, <code>lists</code> or collections or sequences and <code>object</code>s for higher level organization</p>"},{"location":"teaching_resources/tools/yaml_syntax/#yaml-syntax_1","title":"YAML syntax","text":"<p>Basics Below is a simple example with notes:</p> <pre><code># This is a comment line. Comments begin with #\nkind: config-example    # a basic kvp\nname: navigator         # example of a string value. No quotes if single word\nservices:               # example of a list\n    - user-auth         # list items start with a -\n    - add-to-cart       # indentation is important\n    - remove-from-cart\n    - checkout-cart\nservice-definitions:    # example of a list of objects\n    - app: user-auth    # list item start with a -\n      port: 8000        # int value\n      version: 1.7      # float value\n      endpoint: \"/auth\" # string with special char needs to be enclosed within quotes\n    - app: add-to-cart\n      port: 8000\n      version: 1.3\n      endpoint: [\"/add\", \"/additems\"]   # single line list representation\n</code></pre> <p>Writing objects Objects can be written using a natural syntax.</p> <pre><code>apiVersion: 1.2\nkind: pod\nmetadata:                   # object\n    name: nginx             # simple kvp\n    labels:                 # object\n        app: nginx\n        cost-center: auth\n        cluster: dev\nspec:                       # object\n    containers:             # list of objects\n        - name: nginx-container\n          image: gcr.io/abcd/nginx\n          ports:            # list\n          - containerPort: 80\n          volumeMounts:\n          - name: ngnix-vol\n        - name: sidecar-container\n</code></pre> <p>Singleline and Multiline strings</p> <ul> <li>To word-wrap a long text line, use <code>&gt;</code> operator</li> <li>To represent an actual multi-line string, use the pipe key <code>|</code></li> </ul> <pre><code># singe line string, wrapped for readability\nspec:\n    container:\n        image: gcr.io/image\n        cmd: &gt;\n            python\n            -m\n            http.server\n            -name arg1\n            --long-name arg2\n            --another-long-name arg3\n</code></pre> <p>Below is an example where a multiline string is meaningful. Here, we specify the actual Python script in the Yaml itself. Uncommon, but still one option.</p> <pre><code># multiline string\nspec:\n    script:\n      image: python:alpine3.10\n      command: [python]\n      source: |\n        import json, os\n        data_file = \"somepath/input_args.json\"\n        with open(data_file) as f:\n          run_config = json.load(f)\n        print(run_config[\"somekvp\"])\n</code></pre>"},{"location":"teaching_resources/tools/yaml_syntax/#yaml-injection","title":"Yaml injection","text":"<p>Yaml allows compile time injection of values. Tools such as Helm templating engine use this technique to insert specific objects (such as secrets, keys etc) into the Yaml at run or compile time. </p> <p>The syntax to specify injection placeholders is <code>{{.variable.name}}</code>. Below is an example of Helm templates used in a K8s workflow:</p> <pre><code>apiVersion: 1.2\nkind: Service\nmetadata:\n    name: {{.Values.service.name}}\nspec:\n    ports:\n        - protocol: http\n          port: {{.Values.service.port}}\n          node: {{.Values.app.nodeSelector}}\n</code></pre>"},{"location":"teaching_resources/xarray/xarray_intro/","title":"Xarray - an introduction","text":"<p>Purpose Xarray was created to make it easy to work with multidimensional arrays (or tensors). These n-D arrays are common in data science, machine learning and in climate science. Although it is possible to work with n-D arrays entirely in NumPy, you lack the transparency, code readability and the facility to easily apply an operation on a \"dataset\" of choice.</p> In\u00a0[34]: Copied! <pre>import numpy as np\n\n# importing as xr is by convention\nimport xarray as xr\nimport pandas as pd\n</pre> import numpy as np  # importing as xr is by convention import xarray as xr import pandas as pd In\u00a0[38]: Copied! <pre>ds = xr.tutorial.load_dataset(\"air_temperature\")\nds\n</pre> ds = xr.tutorial.load_dataset(\"air_temperature\") ds Out[38]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 2920</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>This dataset has air temperature (<code>2920</code> instances of it) for a set of <code>25</code> x <code>53</code> lat lon coordinates. The <code>lon</code>, <code>lat</code>, <code>time</code> are coordinates (nD) and <code>air</code> is a variable.</p> In\u00a0[7]: Copied! <pre>da = ds.air  # can use .notation or ds['air'] dict notation\nda\n</pre> da = ds.air  # can use .notation or ds['air'] dict notation da Out[7]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\narray([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]</li></ul> <p>To extract just the data, use</p> In\u00a0[10]: Copied! <pre>air_temp = da.data\nprint(type(air_temp))\nprint(air_temp.shape)\n</pre> air_temp = da.data print(type(air_temp)) print(air_temp.shape) <pre>&lt;class 'numpy.ndarray'&gt;\n(2920, 25, 53)\n</pre> <p>A data array may have dimensions that are also coordinates. They may also have dimensions without coordinates</p> In\u00a0[11]: Copied! <pre>da.dims\n</pre> da.dims Out[11]: <pre>('time', 'lat', 'lon')</pre> In\u00a0[12]: Copied! <pre>da.coords\n</pre> da.coords Out[12]: <pre>Coordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00</pre> In\u00a0[13]: Copied! <pre>da.attrs\n</pre> da.attrs Out[13]: <pre>{'long_name': '4xDaily Air temperature at sigma level 995',\n 'units': 'degK',\n 'precision': 2,\n 'GRIB_id': 11,\n 'GRIB_name': 'TMP',\n 'var_desc': 'Air temperature',\n 'dataset': 'NMC Reanalysis',\n 'level_desc': 'Surface',\n 'statistic': 'Individual Obs',\n 'parent_stat': 'Other',\n 'actual_range': array([185.16, 322.1 ], dtype=float32)}</pre> In\u00a0[15]: Copied! <pre># to and from Pandas\nair_temp_pd = da.to_series()\nair_temp_pd\n</pre> # to and from Pandas air_temp_pd = da.to_series() air_temp_pd Out[15]: <pre>time                 lat   lon  \n2013-01-01 00:00:00  75.0  200.0    241.199997\n                           202.5    242.500000\n                           205.0    243.500000\n                           207.5    244.000000\n                           210.0    244.099991\n                                       ...    \n2014-12-31 18:00:00  15.0  320.0    297.389984\n                           322.5    297.190002\n                           325.0    296.489990\n                           327.5    296.190002\n                           330.0    295.690002\nName: air, Length: 3869000, dtype: float32</pre> In\u00a0[16]: Copied! <pre>type(air_temp_pd)\n</pre> type(air_temp_pd) Out[16]: <pre>pandas.core.series.Series</pre> <p>Air temp has <code>3</code> indices when it is turned to a Pandas Series</p> In\u00a0[17]: Copied! <pre>da.to_dataframe()\n</pre> da.to_dataframe() Out[17]: air time lat lon 2013-01-01 00:00:00 75.0 200.0 241.199997 202.5 242.500000 205.0 243.500000 207.5 244.000000 210.0 244.099991 ... ... ... ... 2014-12-31 18:00:00 15.0 320.0 297.389984 322.5 297.190002 325.0 296.489990 327.5 296.190002 330.0 295.690002 <p>3869000 rows \u00d7 1 columns</p> In\u00a0[18]: Copied! <pre>raw_data = da.data\nprint(type(raw_data))\nprint(raw_data.shape)\n</pre> raw_data = da.data print(type(raw_data)) print(raw_data.shape) <pre>&lt;class 'numpy.ndarray'&gt;\n(2920, 25, 53)\n</pre> In\u00a0[21]: Copied! <pre>raw_data[0,0,1]\n</pre> raw_data[0,0,1] Out[21]: <pre>242.5</pre> In\u00a0[23]: Copied! <pre># For now, let us not expand each array\nxr.set_options(display_expand_data=False)\n</pre> # For now, let us not expand each array xr.set_options(display_expand_data=False) Out[23]: <pre>&lt;xarray.core.options.set_options at 0x7f9eb1e21d00&gt;</pre> In\u00a0[24]: Copied! <pre># use DataArray constructor\nda2 = xr.DataArray(raw_data, dims=('time','lat','lon'))\nda2\n</pre> # use DataArray constructor da2 = xr.DataArray(raw_data, dims=('time','lat','lon')) da2 Out[24]: <pre>&lt;xarray.DataArray (time: 2920, lat: 25, lon: 53)&gt;\n241.2 242.5 243.5 244.0 244.1 243.9 ... 297.9 297.4 297.2 296.5 296.2 295.7\nDimensions without coordinates: time, lat, lon</pre>xarray.DataArray<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (0)<ul></ul></li><li>Attributes: (0)</li></ul> <p>The coordinates is empty although the data has <code>3</code> dimensions. You can set the coordinates using another DataArray object or a numpy array. In this example, lat and long are evenly spaced.</p> In\u00a0[26]: Copied! <pre>lon_array = np.arange(start=200, stop=331, step=2.5)\nprint(lon_array.shape)\n</pre> lon_array = np.arange(start=200, stop=331, step=2.5) print(lon_array.shape) <pre>(53,)\n</pre> In\u00a0[28]: Copied! <pre>da2.coords['lon'] = lon_array\nda2\n</pre> da2.coords['lon'] = lon_array da2 Out[28]: <pre>&lt;xarray.DataArray (time: 2920, lat: 25, lon: 53)&gt;\n241.2 242.5 243.5 244.0 244.1 243.9 ... 297.9 297.4 297.2 296.5 296.2 295.7\nCoordinates:\n  * lon      (lon) float64 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\nDimensions without coordinates: time, lat</pre>xarray.DataArray<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (1)<ul><li>lon(lon)float64200.0 202.5 205.0 ... 327.5 330.0<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ])</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>Similarly, set the latitude and time coordinates</p> In\u00a0[30]: Copied! <pre>da2.coords['lat'] = np.arange(start=75, stop=14.9, step=-2.5)\nda2\n</pre> da2.coords['lat'] = np.arange(start=75, stop=14.9, step=-2.5) da2 Out[30]: <pre>&lt;xarray.DataArray (time: 2920, lat: 25, lon: 53)&gt;\n241.2 242.5 243.5 244.0 244.1 243.9 ... 297.9 297.4 297.2 296.5 296.2 295.7\nCoordinates:\n  * lon      (lon) float64 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * lat      (lat) float64 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\nDimensions without coordinates: time</pre>xarray.DataArray<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (2)<ul><li>lon(lon)float64200.0 202.5 205.0 ... 327.5 330.0<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ])</pre></li><li>lat(lat)float6475.0 72.5 70.0 ... 20.0 17.5 15.0<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ])</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>You can also assign attributes in a similar fashion</p> In\u00a0[31]: Copied! <pre>da2.attrs['some_attribute'] = 'hello'\nda2\n</pre> da2.attrs['some_attribute'] = 'hello' da2 Out[31]: <pre>&lt;xarray.DataArray (time: 2920, lat: 25, lon: 53)&gt;\n241.2 242.5 243.5 244.0 244.1 243.9 ... 297.9 297.4 297.2 296.5 296.2 295.7\nCoordinates:\n  * lon      (lon) float64 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * lat      (lat) float64 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\nDimensions without coordinates: time\nAttributes:\n    some_attribute:  hello</pre>xarray.DataArray<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (2)<ul><li>lon(lon)float64200.0 202.5 205.0 ... 327.5 330.0<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ])</pre></li><li>lat(lat)float6475.0 72.5 70.0 ... 20.0 17.5 15.0<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ])</pre></li></ul></li><li>Attributes: (1)some_attribute :hello</li></ul> In\u00a0[33]: Copied! <pre>ds2 = xr.Dataset({'air':da2, 'air2':da2})  # just pass a dict like mapping. any number of variables\nds2\n</pre> ds2 = xr.Dataset({'air':da2, 'air2':da2})  # just pass a dict like mapping. any number of variables ds2 Out[33]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lon: 53, lat: 25, time: 2920)\nCoordinates:\n  * lon      (lon) float64 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * lat      (lat) float64 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\nDimensions without coordinates: time\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\n    air2     (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lon: 53</li><li>lat: 25</li><li>time: 2920</li></ul></li><li>Coordinates: (2)<ul><li>lon(lon)float64200.0 202.5 205.0 ... 327.5 330.0<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ])</pre></li><li>lat(lat)float6475.0 72.5 70.0 ... 20.0 17.5 15.0<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ])</pre></li></ul></li><li>Data variables: (2)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7some_attribute :hello<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>air2(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7some_attribute :hello<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[37]: Copied! <pre>ds2.coords['time'] = pd.date_range(start='2013-01-01', end=\"2014-12-31 18:00\", freq=\"6H\")\nds2\n</pre> ds2.coords['time'] = pd.date_range(start='2013-01-01', end=\"2014-12-31 18:00\", freq=\"6H\") ds2 Out[37]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lon: 53, lat: 25, time: 2920)\nCoordinates:\n  * lon      (lon) float64 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * lat      (lat) float64 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\n    air2     (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lon: 53</li><li>lat: 25</li><li>time: 2920</li></ul></li><li>Coordinates: (3)<ul><li>lon(lon)float64200.0 202.5 205.0 ... 327.5 330.0<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ])</pre></li><li>lat(lat)float6475.0 72.5 70.0 ... 20.0 17.5 15.0<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ])</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (2)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7some_attribute :hello<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>air2(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7some_attribute :hello<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/xarray/xarray_intro/#xarray-an-introduction","title":"Xarray - an introduction\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_intro/#core-data-structures","title":"Core data structures\u00b6","text":"<p>Xarray has 2 core data structures that extend the core strenghts of <code>NumPy</code> and <code>Pandas</code>.</p> <p></p> <ul> <li><code>DataArray</code> - labeled n-dim array. It is a n-d generalization of <code>pandas.Series</code></li> <li><code>Dataset</code> - is a dict like container of <code>DataArray</code> aligned along any number of shared dimensions. It is similar to how <code>pandas.DataFrame</code> builds on <code>pandas.Series</code>.</li> </ul> <p>The <code>Dataset</code> object allows the user to query, extract or combine <code>DataArray</code>s over a particular dimension across all variables. This pattern quickly becomes convenient when dealing with spatio-temporal datasets.</p>"},{"location":"teaching_resources/xarray/xarray_intro/#dataset-object","title":"Dataset object\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_intro/#dataarray-object","title":"DataArray object\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_intro/#dimensions-coordinates-attributes","title":"Dimensions, coordinates, attributes\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_intro/#interop-with-pandas","title":"Interop with Pandas\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_intro/#composing-a-dataarray-and-dataset","title":"Composing a DataArray and DataSet\u00b6","text":"<p>Say you have the raw data, how do you compose a DataArray and a DataSet with them?</p>"},{"location":"teaching_resources/xarray/xarray_intro/#composing-a-dataset","title":"Composing a DataSet\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_multidim_dataset/","title":"Xarray - multidimensional science data","text":"<p>Tutorial data and code are from NASA ARSET program: https://appliedsciences.nasa.gov/join-mission/training/english/high-resolution-no2-monitoring-space-tropomi</p> In\u00a0[1]: Copied! <pre>import numpy as np\n# from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nimport sys\nfrom netCDF4 import Dataset\nfrom pprint import pprint, pp\nimport pandas as pd\n</pre> import numpy as np # from mpl_toolkits.basemap import Basemap import matplotlib.pyplot as plt import sys from netCDF4 import Dataset from pprint import pprint, pp import pandas as pd In\u00a0[2]: Copied! <pre>ls TROPOMI_PythonCodesAndData/\n</pre> ls TROPOMI_PythonCodesAndData/ <pre>S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc*\nS5P_OFFL_L2__CO_____20180816T183016_20180816T201146_04361_01_010100_20180822T174815.nc*\nS5P_RPRO_L2__CH4____20180816T182917_20180816T201245_04361_01_010202_20190101T194705.nc*\nfileList.txt*\nread_and_map_tropomi_no2_ai.py*\nread_tropomi_and_list_sds.py*\nread_tropomi_no2_ai_and_dump_ascii.py*\nread_tropomi_no2_ai_at_a_location.py*\n</pre> Screenshot of Panoply software with Aerosol Index NetCDF file opened. <p>The first step is to read this file as a <code>netCDF4.Dataset</code> class.</p> In\u00a0[3]: Copied! <pre>file_path = 'TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc'\nds = Dataset(file_path, mode='r')\ntype(ds)\n</pre> file_path = 'TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc' ds = Dataset(file_path, mode='r') type(ds) Out[3]: <pre>netCDF4._netCDF4.Dataset</pre> In\u00a0[4]: Copied! <pre>ds.groups.keys()\n</pre> ds.groups.keys() Out[4]: <pre>dict_keys(['PRODUCT', 'METADATA'])</pre> <p>Explore the different variables as a DataFrame</p> In\u00a0[5]: Copied! <pre>v = {'variables':[], 'long_name':[], 'units':[]}\nfor var in list(ds.groups['PRODUCT'].variables.keys()):\n    v['variables'].append(ds.groups['PRODUCT'].variables[var].name)\n    v['long_name'].append(ds.groups['PRODUCT'].variables[var].long_name)\n    try:\n        v['units'].append(ds.groups['PRODUCT'].variables[var].units)\n    except:\n        v['units'].append(None)\n\nvars_df = pd.DataFrame.from_dict(v)\nvars_df\n</pre> v = {'variables':[], 'long_name':[], 'units':[]} for var in list(ds.groups['PRODUCT'].variables.keys()):     v['variables'].append(ds.groups['PRODUCT'].variables[var].name)     v['long_name'].append(ds.groups['PRODUCT'].variables[var].long_name)     try:         v['units'].append(ds.groups['PRODUCT'].variables[var].units)     except:         v['units'].append(None)  vars_df = pd.DataFrame.from_dict(v) vars_df Out[5]: variables long_name units 0 scanline along-track dimension index 1 1 ground_pixel across-track dimension index 1 2 time reference time for the measurements seconds since 2010-01-01 00:00:00 3 corner pixel corner index 1 4 latitude pixel center latitude degrees_north 5 longitude pixel center longitude degrees_east 6 delta_time offset from reference start time of measurement milliseconds 7 time_utc Time of observation as ISO 8601 date-time string None 8 qa_value data quality value 1 9 aerosol_index_354_388 Aerosol index from 388 and 354 nm 1 10 aerosol_index_340_380 Aerosol index from 380 and 340 nm 1 11 aerosol_index_354_388_precision Precision of aerosol index from 388 and 354 nm 1 12 aerosol_index_340_380_precision Precision of aerosol index from 380 and 340 nm 1 In\u00a0[6]: Copied! <pre># preview contents of the variable\nds.groups['PRODUCT'].variables['aerosol_index_354_388']\n</pre> # preview contents of the variable ds.groups['PRODUCT'].variables['aerosol_index_354_388'] Out[6]: <pre>&lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 aerosol_index_354_388(time, scanline, ground_pixel)\n    units: 1\n    proposed_standard_name: ultraviolet_aerosol_index\n    comment: Aerosol index from 388 and 354 nm\n    long_name: Aerosol index from 388 and 354 nm\n    radiation_wavelength: [354. 388.]\n    coordinates: longitude latitude\n    ancillary_variables: aerosol_index_354_388_precision\n    _FillValue: 9.96921e+36\npath = /PRODUCT\nunlimited dimensions: \ncurrent shape = (1, 3245, 450)\nfilling on</pre> In\u00a0[7]: Copied! <pre>ai_data = ds.groups['PRODUCT'].variables['aerosol_index_354_388'][:]\ntype(ai_data)\n</pre> ai_data = ds.groups['PRODUCT'].variables['aerosol_index_354_388'][:] type(ai_data) Out[7]: <pre>numpy.ma.core.MaskedArray</pre> In\u00a0[8]: Copied! <pre>ai_data.shape\n</pre> ai_data.shape Out[8]: <pre>(1, 3245, 450)</pre> In\u00a0[9]: Copied! <pre>plt.imshow(ai_data[0]);\n</pre> plt.imshow(ai_data[0]); In\u00a0[28]: Copied! <pre>import xarray\n</pre> import xarray In\u00a0[31]: Copied! <pre>xr_data = xarray.open_dataset(file_path, group='PRODUCT', \n                              engine='netcdf4', decode_coords=True)\ntype(xr_data)\n</pre> xr_data = xarray.open_dataset(file_path, group='PRODUCT',                                engine='netcdf4', decode_coords=True) type(xr_data) Out[31]: <pre>xarray.core.dataset.Dataset</pre> In\u00a0[32]: Copied! <pre>xr_data\n</pre> xr_data Out[32]: <pre>&lt;xarray.Dataset&gt;\nDimensions:                          (corner: 4, ground_pixel: 450, scanline: 3245, time: 1)\nCoordinates:\n  * scanline                         (scanline) float64 0.0 1.0 ... 3.244e+03\n  * ground_pixel                     (ground_pixel) float64 0.0 1.0 ... 449.0\n  * time                             (time) datetime64[ns] 2018-08-16\n  * corner                           (corner) float64 0.0 1.0 2.0 3.0\n    latitude                         (time, scanline, ground_pixel) float32 ...\n    longitude                        (time, scanline, ground_pixel) float32 ...\nData variables:\n    delta_time                       (time, scanline) timedelta64[ns] 18:51:5...\n    time_utc                         (time, scanline) object '2018-08-16T18:5...\n    qa_value                         (time, scanline, ground_pixel) float32 ...\n    aerosol_index_354_388            (time, scanline, ground_pixel) float32 ...\n    aerosol_index_340_380            (time, scanline, ground_pixel) float32 ...\n    aerosol_index_354_388_precision  (time, scanline, ground_pixel) float32 ...\n    aerosol_index_340_380_precision  (time, scanline, ground_pixel) float32 ...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>corner: 4</li><li>ground_pixel: 450</li><li>scanline: 3245</li><li>time: 1</li></ul></li><li>Coordinates: (6)<ul><li>scanline(scanline)float640.0 1.0 2.0 ... 3.243e+03 3.244e+03units :1axis :Ylong_name :along-track dimension indexcomment :This coordinate variable defines the indices along track; index starts at 0<pre>array([0.000e+00, 1.000e+00, 2.000e+00, ..., 3.242e+03, 3.243e+03, 3.244e+03])</pre></li><li>ground_pixel(ground_pixel)float640.0 1.0 2.0 ... 447.0 448.0 449.0units :1axis :Xlong_name :across-track dimension indexcomment :This coordinate variable defines the indices across track, from west to east; index starts at 0<pre>array([  0.,   1.,   2., ..., 447., 448., 449.])</pre></li><li>time(time)datetime64[ns]2018-08-16standard_name :timeaxis :Tlong_name :reference time for the measurementscomment :The time in this variable corresponds to the time in the time_reference global attribute<pre>array(['2018-08-16T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>corner(corner)float640.0 1.0 2.0 3.0units :1long_name :pixel corner indexcomment :This coordinate variable defines the indices for the pixel corners; index starts at 0 (counter-clockwise, starting from south-western corner of the pixel in ascending part of the orbit)<pre>array([0., 1., 2., 3.])</pre></li><li>latitude(time, scanline, ground_pixel)float32...long_name :pixel center latitudeunits :degrees_northstandard_name :latitudevalid_min :-90.0valid_max :90.0bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_bounds<pre>[1460250 values with dtype=float32]</pre></li><li>longitude(time, scanline, ground_pixel)float32...long_name :pixel center longitudeunits :degrees_eaststandard_name :longitudevalid_min :-180.0valid_max :180.0bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/longitude_bounds<pre>[1460250 values with dtype=float32]</pre></li></ul></li><li>Data variables: (7)<ul><li>delta_time(time, scanline)timedelta64[ns]...long_name :offset from reference start time of measurement<pre>array([[67911027000000, 67912107000000, 67913187000000, ..., 71412302000000,\n        71413382000000, 71414462000000]], dtype='timedelta64[ns]')</pre></li><li>time_utc(time, scanline)object...long_name :Time of observation as ISO 8601 date-time string<pre>array([['2018-08-16T18:51:51.027000Z', '2018-08-16T18:51:52.107000Z',\n        '2018-08-16T18:51:53.187000Z', ..., '2018-08-16T19:50:12.301999Z',\n        '2018-08-16T19:50:13.381999Z', '2018-08-16T19:50:14.461999Z']],\n      dtype=object)</pre></li><li>qa_value(time, scanline, ground_pixel)float32...units :1valid_min :0valid_max :100long_name :data quality valuecomment :A continuous quality descriptor, varying between 0 (no data) and 1 (full quality data). Recommend to ignore data with qa_value &lt; 0.5<pre>[1460250 values with dtype=float32]</pre></li><li>aerosol_index_354_388(time, scanline, ground_pixel)float32...units :1proposed_standard_name :ultraviolet_aerosol_indexcomment :Aerosol index from 388 and 354 nmlong_name :Aerosol index from 388 and 354 nmradiation_wavelength :[354. 388.]ancillary_variables :aerosol_index_354_388_precision<pre>[1460250 values with dtype=float32]</pre></li><li>aerosol_index_340_380(time, scanline, ground_pixel)float32...units :1proposed_standard_name :ultraviolet_aerosol_indexcomment :Aerosol index from 380 and 340 nmlong_name :Aerosol index from 380 and 340 nmradiation_wavelength :[340. 380.]ancillary_variables :aerosol_index_340_380_precision<pre>[1460250 values with dtype=float32]</pre></li><li>aerosol_index_354_388_precision(time, scanline, ground_pixel)float32...units :1proposed_standard_name :ultraviolet_aerosol_index standard_errorcomment :Precision of aerosol index from 388 and 354 nmlong_name :Precision of aerosol index from 388 and 354 nmradiation_wavelength :[354. 388.]<pre>[1460250 values with dtype=float32]</pre></li><li>aerosol_index_340_380_precision(time, scanline, ground_pixel)float32...units :1proposed_standard_name :ultraviolet_aerosol_index standard_errorcomment :Precision of aerosol index from 380 and 340 nmlong_name :Precision of aerosol index from 380 and 340 nmradiation_wavelength :[340. 380.]<pre>[1460250 values with dtype=float32]</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[33]: Copied! <pre>xr_data_ai = xr_data['aerosol_index_340_380']\nprint(type(xr_data_ai))\nprint(xr_data_ai.shape)\n</pre> xr_data_ai = xr_data['aerosol_index_340_380'] print(type(xr_data_ai)) print(xr_data_ai.shape) <pre>&lt;class 'xarray.core.dataarray.DataArray'&gt;\n(1, 3245, 450)\n</pre> In\u00a0[34]: Copied! <pre>xr_data_ai[0].plot();\n</pre> xr_data_ai[0].plot(); In\u00a0[37]: Copied! <pre>(xr_data.latitude.attrs, xr_data.longitude.attrs)\n</pre> (xr_data.latitude.attrs, xr_data.longitude.attrs) Out[37]: <pre>({'long_name': 'pixel center latitude',\n  'units': 'degrees_north',\n  'standard_name': 'latitude',\n  'valid_min': -90.0,\n  'valid_max': 90.0,\n  'bounds': '/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_bounds'},\n {'long_name': 'pixel center longitude',\n  'units': 'degrees_east',\n  'standard_name': 'longitude',\n  'valid_min': -180.0,\n  'valid_max': 180.0,\n  'bounds': '/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/longitude_bounds'})</pre> In\u00a0[42]: Copied! <pre>plt.figure(figsize=(14,8))\nax = plt.axes()\n\nxr_data.aerosol_index_340_380[0].plot.pcolormesh(ax=ax, x='longitude', \n                                                 y='latitude',\n                                                 add_colorbar=True, cmap='jet');\n</pre> plt.figure(figsize=(14,8)) ax = plt.axes()  xr_data.aerosol_index_340_380[0].plot.pcolormesh(ax=ax, x='longitude',                                                   y='latitude',                                                  add_colorbar=True, cmap='jet'); In\u00a0[44]: Copied! <pre>import cartopy.crs as ccrs\nplt.figure(figsize=(14,6))\nax = plt.axes(projection = ccrs.PlateCarree())\n\nxr_data.aerosol_index_340_380[0].plot.pcolormesh(ax=ax, x='longitude', \n                                                 y='latitude',\n                                                 add_colorbar=True, cmap='jet')\n\nax.set_global()\nax.coastlines();\n</pre> import cartopy.crs as ccrs plt.figure(figsize=(14,6)) ax = plt.axes(projection = ccrs.PlateCarree())  xr_data.aerosol_index_340_380[0].plot.pcolormesh(ax=ax, x='longitude',                                                   y='latitude',                                                  add_colorbar=True, cmap='jet')  ax.set_global() ax.coastlines(); In\u00a0[47]: Copied! <pre>plt.figure(figsize=(10,10))\nax = plt.axes(projection = ccrs.Orthographic(-88,40))\n\nxr_data.aerosol_index_340_380[0].plot.pcolormesh(ax=ax, x='longitude', \n                                                 y='latitude',\n                                                 add_colorbar=True, cmap='jet',\n                                                transform=ccrs.PlateCarree())\n\nax.set_global()\nax.coastlines();\n</pre> plt.figure(figsize=(10,10)) ax = plt.axes(projection = ccrs.Orthographic(-88,40))  xr_data.aerosol_index_340_380[0].plot.pcolormesh(ax=ax, x='longitude',                                                   y='latitude',                                                  add_colorbar=True, cmap='jet',                                                 transform=ccrs.PlateCarree())  ax.set_global() ax.coastlines(); In\u00a0[51]: Copied! <pre>xr_data_rio = xr_data_ai.rio\ntype(xr_data_rio)\n</pre> xr_data_rio = xr_data_ai.rio type(xr_data_rio) Out[51]: <pre>rioxarray.rioxarray.RasterArray</pre> In\u00a0[55]: Copied! <pre>xr_data.aerosol_index_340_380.rio.to_raster('xr_test.tif')\n</pre> xr_data.aerosol_index_340_380.rio.to_raster('xr_test.tif') In\u00a0[60]: Copied! <pre>import rioxarray\nimport warnings; warnings.simplefilter('ignore')\n</pre> import rioxarray import warnings; warnings.simplefilter('ignore') In\u00a0[85]: Copied! <pre>rds = rioxarray.open_rasterio(filename = file_path, parse_coordinates=True,\n                             )\ntype(rds)\n</pre> rds = rioxarray.open_rasterio(filename = file_path, parse_coordinates=True,                              ) type(rds) Out[85]: <pre>list</pre> In\u00a0[87]: Copied! <pre>rds[0]\n</pre> rds[0] Out[87]: <pre>&lt;xarray.Dataset&gt;\nDimensions:                                   (band: 1, time: 1, x: 450, y: 3245)\nCoordinates:\n  * y                                         (y) float64 3.244e+03 ... 0.0\n  * x                                         (x) float64 0.0 1.0 ... 449.0\n  * time                                      (time) int64 272073600\n    spatial_ref                               int64 0\n  * band                                      (band) int64 1\nData variables:\n    latitude                                  (time, y, x) float32 ...\n    solar_zenith_angle                        (band, y, x) float32 ...\n    solar_azimuth_angle                       (band, y, x) float32 ...\n    viewing_zenith_angle                      (band, y, x) float32 ...\n    viewing_azimuth_angle                     (band, y, x) float32 ...\n    longitude                                 (time, y, x) float32 ...\n    geolocation_flags                         (band, y, x) uint8 ...\n    processing_quality_flags                  (band, y, x) uint32 ...\n    number_of_spectral_points_in_retrieval    (band, y, x) uint16 ...\n    scene_albedo_388                          (band, y, x) float32 ...\n    scene_albedo_388_precision                (band, y, x) float32 ...\n    reflectance_measured_354                  (band, y, x) float32 ...\n    reflectance_measured_354_precision        (band, y, x) float32 ...\n    reflectance_measured_388                  (band, y, x) float32 ...\n    reflectance_measured_388_precision        (band, y, x) float32 ...\n    reflectance_calculated_354                (band, y, x) float32 ...\n    reflectance_calculated_354_precision      (band, y, x) float32 ...\n    scene_albedo_380                          (band, y, x) float32 ...\n    scene_albedo_380_precision                (band, y, x) float32 ...\n    reflectance_measured_340                  (band, y, x) float32 ...\n    reflectance_measured_340_precision        (band, y, x) float32 ...\n    reflectance_measured_380                  (band, y, x) float32 ...\n    reflectance_measured_380_precision        (band, y, x) float32 ...\n    reflectance_calculated_340                (band, y, x) float32 ...\n    reflectance_calculated_340_precision      (band, y, x) float32 ...\n    wavelength_calibration_offset             (band, y, x) float32 ...\n    wavelength_calibration_offset_precision   (band, y, x) float32 ...\n    wavelength_calibration_stretch            (band, y, x) float32 ...\n    wavelength_calibration_stretch_precision  (band, y, x) float32 ...\n    wavelength_calibration_chi_square         (band, y, x) float32 ...\n    surface_altitude                          (band, y, x) float32 ...\n    surface_altitude_precision                (band, y, x) float32 ...\n    surface_classification                    (band, y, x) uint8 ...\n    qa_value                                  (time, y, x) uint8 ...\n    scaled_small_pixel_variance               (band, y, x) float32 ...\n    ozone_total_column                        (band, y, x) float32 ...\n    surface_pressure                          (band, y, x) float32 ...\n    aerosol_index_354_388                     (time, y, x) float32 ...\n    aerosol_index_340_380                     (time, y, x) float32 ...\n    aerosol_index_354_388_precision           (time, y, x) float32 ...\n    aerosol_index_340_380_precision           (time, y, x) float32 ...\nAttributes:\n    algo.algorithm_variant:                                            1\n    algo.n_pair:                                                       2\n    algo.pair_1.delta_wavelength:                                      2.0\n    algo.pair_1.id:                                                    TOMS_pair\n    algo.pair_1.min_wavelength:                                        1\n    algo.pair_1.number_spectral_pixels:                                7\n    algo.pair_1.wavelength_1:                                          340\n    algo.pair_1.wavelength_2:                                          380\n    algo.pair_2.delta_wavelength:                                      2.0\n    algo.pair_2.id:                                                    OMI_pair\n    algo.pair_2.min_wavelength:                                        1\n    algo.pair_2.number_spectral_pixels:                                7\n    algo.pair_2.wavelength_1:                                          354\n    algo.pair_2.wavelength_2:                                          388\n    configuration.version.algorithm:                                   1.1.0\n    configuration.version.framework:                                   1.1.0\n    input.1.band:                                                      3\n    input.1.irrType:                                                   L1B_IR...\n    input.1.type:                                                      L1B_RA...\n    input.count:                                                       1\n    output.1.band:                                                     3\n    output.1.config:                                                   cfg/pr...\n    output.1.type:                                                     L2__AE...\n    output.compressionLevel:                                           3\n    output.count:                                                      1\n    output.histogram.aerosol_index_340_380.end:                        14\n    output.histogram.aerosol_index_340_380.start:                      -6\n    output.histogram.aerosol_index_354_388.end:                        14\n    output.histogram.aerosol_index_354_388.start:                      -6\n    output.useCompression:                                             true\n    output.useFletcher32:                                              true\n    output.useShuffleFilter:                                           true\n    processing.algorithm:                                              AER_AI\n    processing.correct_surface_pressure_for_altitude:                  true\n    processing.exclude_flags:                                          429496...\n    processing.groupDem:                                               DEM_RA...\n    processing.ignore_pixel_flags:                                     False\n    processing.radiancePixelsMinError:                                 2\n    processing.radiancePixelsMinWarning:                               7\n    processing.signal_to_noise.test:                                   yes\n    processing.signal_to_noise.threshold:                              12\n    processing.signal_to_noise.window.range:                           350.0,...\n    processing.szaMax:                                                 88.0\n    processing.szaMin:                                                 0.0\n    processing.vzaMax:                                                 78.0\n    processing.vzaMin:                                                 0.0\n    qa_value.AAI_warning:                                              100.0\n    qa_value.altitude_consistency_warning:                             100.0\n    qa_value.cloud_warning:                                            100.0\n    qa_value.data_range_warning:                                       100.0\n    qa_value.deconvolution_warning:                                    100.0\n    qa_value.extrapolation_warning:                                    100.0\n    qa_value.input_spectrum_warning:                                   70.0\n    qa_value.interpolation_warning:                                    100.0\n    qa_value.low_cloud_fraction_warning:                               100.0\n    qa_value.pixel_level_input_data_missing:                           80.0\n    qa_value.signal_to_noise_ratio_warning:                            100.0\n    qa_value.snow_ice_warning:                                         100.0\n    qa_value.so2_volcanic_origin_certain_warning:                      100.0\n    qa_value.so2_volcanic_origin_likely_warning:                       100.0\n    qa_value.south_atlantic_anomaly_warning:                           100.0\n    qa_value.sun_glint_correction:                                     100.0\n    qa_value.sun_glint_warning:                                        70.0\n    qa_value.wavelength_calibration_warning:                           90.0\n    wavelength_calibration.convergence_threshold:                      1.0\n    wavelength_calibration.include_stretch:                            no\n    wavelength_calibration.initial_guess.a0:                           1.0\n    wavelength_calibration.initial_guess.a1:                           0.1\n    wavelength_calibration.initial_guess.a2:                           0.01\n    wavelength_calibration.initial_guess.ring:                         0.06\n    wavelength_calibration.initial_guess.shift:                        0.0\n    wavelength_calibration.initial_guess.stretch:                      0.0\n    wavelength_calibration.irr.include_ring:                           no\n    wavelength_calibration.irr.max_iterations:                         20\n    wavelength_calibration.irr.polynomial_order:                       2\n    wavelength_calibration.max_iterations:                             12\n    wavelength_calibration.perform_wavelength_fit:                     yes\n    wavelength_calibration.rad.include_ring:                           yes\n    wavelength_calibration.rad.polynomial_order:                       3\n    wavelength_calibration.sigma.a0:                                   1.0\n    wavelength_calibration.sigma.a1:                                   0.1\n    wavelength_calibration.sigma.ring:                                 0.06\n    wavelength_calibration.sigma.shift:                                0.07\n    wavelength_calibration.sigma.stretch:                              0.07\n    wavelength_calibration.window:                                     338.0,...\n    /METADATA/EOP_METADATA/eop:                                        metaDa...\n    gml:                                                               id=S5P...\n    objectType:                                                        gmi:MI...\n    /METADATA/EOP_METADATA/om:                                         proced...\n    File_Class:                                                        OFFL\n    File_Description:                                                  Aeroso...\n    File_Name:                                                         S5P_OF...\n    File_Type:                                                         L2__AE...\n    File_Version:                                                      1\n    Mission:                                                           S5P\n    Notes:                                                             \n    Creation_Date:                                                     UTC=20...\n    Creator:                                                           TROPNL...\n    Creator_Version:                                                   1.1.0\n    System:                                                            PDGS-OP\n    Validity_Start:                                                    UTC=20...\n    Validity_Stop:                                                     UTC=20...\n    /METADATA/ESA_METADATA/earth_explorer_header/variable_header/gmd:  lineag...\n    GranuleEnd:                                                        2018-0...\n    GranuleStart:                                                      2018-0...\n    InstrumentName:                                                    TROPOMI\n    LongitudeOfDaysideNadirEquatorCrossing:                            -86.44...\n    MissionName:                                                       Sentin...\n    MissionShortName:                                                  S5P\n    ProcessingCenter:                                                  PDGS-OP\n    ProcessingMode:                                                    Offline\n    ProcessingNode:                                                    s5p-of...\n    ProcessLevel:                                                      2\n    ProcessorVersion:                                                  1.1.0\n    ProductFormatVersion:                                              10000\n    ProductShortName:                                                  L2__AE...\n    /METADATA/ISO_METADATA/gmd:                                        langua...\n    /METADATA/ISO_METADATA/gmi:                                        acquis...\n    gmd:                                                               metada...\n    global_processing_warnings:                                        --- TE...\n    number_of_aai_filter_occurrences:                                  0\n    number_of_aai_scene_albedo_filter_occurrences:                     0\n    number_of_AAI_warning_occurrences:                                 0\n    number_of_abort_error_occurrences:                                 0\n    number_of_aerosol_boundary_error_occurrences:                      0\n    number_of_airmass_factor_error_occurrences:                        0\n    number_of_altitude_consistency_filter_occurrences:                 0\n    number_of_altitude_consistency_warning_occurrences:                0\n    number_of_altitude_roughness_filter_occurrences:                   0\n    number_of_aot_lower_boundary_convergence_error_occurrences:        0\n    number_of_assertion_error_occurrences:                             31\n    number_of_boundary_hit_error_occurrences:                          0\n    number_of_cf_viirs_nir_ifov_filter_occurrences:                    0\n    number_of_cf_viirs_nir_ofova_filter_occurrences:                   0\n    number_of_cf_viirs_nir_ofovb_filter_occurrences:                   0\n    number_of_cf_viirs_nir_ofovc_filter_occurrences:                   0\n    number_of_cf_viirs_swir_ifov_filter_occurrences:                   0\n    number_of_cf_viirs_swir_ofova_filter_occurrences:                  0\n    number_of_cf_viirs_swir_ofovb_filter_occurrences:                  0\n    number_of_cf_viirs_swir_ofovc_filter_occurrences:                  0\n    number_of_ch4_noscat_ratio_filter_occurrences:                     0\n    number_of_ch4_noscat_ratio_std_filter_occurrences:                 0\n    number_of_ch4_noscat_zero_error_occurrences:                       0\n    number_of_chi2_error_occurrences:                                  0\n    number_of_cirrus_reflectance_viirs_filter_occurrences:             0\n    number_of_cloud_error_occurrences:                                 0\n    number_of_cloud_filter_convergence_error_occurrences:              0\n    number_of_cloud_filter_occurrences:                                0\n    number_of_cloud_fraction_fresco_filter_occurrences:                0\n    number_of_cloud_fraction_viirs_filter_occurrences:                 0\n    number_of_cloud_pressure_spread_too_low_error_occurrences:         0\n    number_of_cloud_too_low_level_error_occurrences:                   0\n    number_of_cloud_warning_occurrences:                               0\n    number_of_configuration_error_occurrences:                         0\n    number_of_convergence_error_occurrences:                           0\n    number_of_coregistration_error_occurrences:                        0\n    number_of_data_range_warning_occurrences:                          0\n    number_of_deconvolution_warning_occurrences:                       0\n    number_of_dfs_error_occurrences:                                   0\n    number_of_diff_psurf_fresco_ecmwf_filter_occurrences:              0\n    number_of_diff_refl_cirrus_viirs_filter_occurrences:               0\n    number_of_extrapolation_warning_occurrences:                       0\n    number_of_failed_retrievals:                                       222068\n    number_of_generic_exception_occurrences:                           0\n    number_of_generic_range_error_occurrences:                         0\n    number_of_geographic_region_filter_occurrences:                    0\n    number_of_geolocation_error_occurrences:                           0\n    number_of_groundpixels:                                            1460250\n    number_of_ground_pixels_with_warnings:                             84516\n    number_of_h2o_noscat_ratio_filter_occurrences:                     0\n    number_of_h2o_noscat_ratio_std_filter_occurrences:                 0\n    number_of_h2o_noscat_zero_error_occurrences:                       0\n    number_of_initialization_error_occurrences:                        0\n    number_of_input_spectrum_alignment_error_occurrences:              0\n    number_of_input_spectrum_missing_occurrences:                      0\n    number_of_input_spectrum_warning_occurrences:                      0\n    number_of_interpolation_warning_occurrences:                       0\n    number_of_io_error_occurrences:                                    0\n    number_of_irradiance_missing_occurrences:                          0\n    number_of_ISRF_error_occurrences:                                  0\n    number_of_key_error_occurrences:                                   0\n    number_of_ler_range_error_occurrences:                             0\n    number_of_low_cloud_fraction_warning_occurrences:                  0\n    number_of_lut_error_occurrences:                                   0\n    number_of_lut_range_error_occurrences:                             0\n    number_of_max_iteration_convergence_error_occurrences:             0\n    number_of_max_optical_thickness_error_occurrences:                 0\n    number_of_memory_error_occurrences:                                0\n    number_of_mixed_surface_type_filter_occurrences:                   0\n    number_of_model_error_occurrences:                                 0\n    number_of_number_of_input_data_points_too_low_error_occurrences:   0\n    number_of_numerical_error_occurrences:                             0\n    number_of_ocean_filter_occurrences:                                0\n    number_of_optimal_estimation_error_occurrences:                    0\n    number_of_other_boundary_convergence_error_occurrences:            0\n    number_of_ozone_range_error_occurrences:                           0\n    number_of_pixel_level_input_data_missing_occurrences:              0\n    number_of_pixel_or_scanline_index_filter_occurrences:              0\n    number_of_processed_pixels:                                        1460250\n    number_of_profile_error_occurrences:                               0\n    number_of_psurf_fresco_stdv_filter_occurrences:                    0\n    number_of_radiance_missing_occurrences:                            0\n    number_of_radiative_transfer_error_occurrences:                    0\n    number_of_reflectance_range_error_occurrences:                     0\n    number_of_refl_cirrus_viirs_nir_filter_occurrences:                0\n    number_of_refl_cirrus_viirs_swir_filter_occurrences:               0\n    number_of_rejected_pixels_not_enough_spectrum:                     0\n    number_of_saturation_error_occurrences:                            0\n    number_of_saturation_warning_occurrences:                          0\n    number_of_signal_to_noise_ratio_error_occurrences:                 0\n    number_of_signal_to_noise_ratio_warning_occurrences:               0\n    number_of_slant_column_density_error_occurrences:                  0\n    number_of_small_pixel_radiance_std_filter_occurrences:             0\n    number_of_snow_ice_filter_occurrences:                             0\n    number_of_snow_ice_warning_occurrences:                            0\n    number_of_snr_range_error_occurrences:                             0\n    number_of_so2_volcanic_origin_certain_warning_occurrences:         0\n    number_of_so2_volcanic_origin_likely_warning_occurrences:          0\n    number_of_solar_eclipse_filter_occurrences:                        0\n    number_of_south_atlantic_anomaly_warning_occurrences:              0\n    number_of_successfully_processed_pixels:                           1238182\n    number_of_sun_glint_correction_occurrences:                        0\n    number_of_sun_glint_filter_occurrences:                            0\n    number_of_sun_glint_warning_occurrences:                           84516\n    number_of_svd_error_occurrences:                                   0\n    number_of_sza_range_error_occurrences:                             222037\n    number_of_time_range_filter_occurrences:                           0\n    number_of_vertical_column_density_error_occurrences:               0\n    number_of_vza_range_error_occurrences:                             0\n    number_of_wavelength_calibration_error_occurrences:                0\n    number_of_wavelength_calibration_warning_occurrences:              0\n    number_of_wavelength_offset_error_occurrences:                     0\n    number_of_wrong_input_type_error_occurrences:                      0\n    time_for_algorithm_initialization:                                 23.927286\n    time_for_processing:                                               212.96926\n    time_per_pixel:                                                    0.0015...\n    time_standard_deviation_per_pixel:                                 2.5657...\n    algorithm_version:                                                 1.1.0\n    build_date:                                                        2018-0...\n    cdm_data_type:                                                     Swath\n    Conventions:                                                       CF-1.7\n    cpp_compiler_flags:                                                -g -O2...\n    cpp_compiler_version:                                              g++ (G...\n    creator_email:                                                     EOSupp...\n    creator_name:                                                      The Se...\n    creator_url:                                                       http:/...\n    date_created:                                                      2018-0...\n    f90_compiler_flags:                                                -gdwar...\n    f90_compiler_version:                                              GNU Fo...\n    geolocation_grid_from_band:                                        3\n    geospatial_lat_max:                                                89.972939\n    geospatial_lat_min:                                                -86.81926\n    geospatial_lon_max:                                                -179.9...\n    geospatial_lon_min:                                                179.99924\n    history:                                                           2018-0...\n    id:                                                                S5P_OF...\n    identifier_product_doi:                                            10.527...\n    identifier_product_doi_authority:                                  http:/...\n    institution:                                                       KNMI\n    keywords:                                                          0300 A...\n    keywords_vocabulary:                                               AGU in...\n    license:                                                           No con...\n    naming_authority:                                                  nl.knmi\n    orbit:                                                             4361\n    platform:                                                          S5P\n    processing_status:                                                 Nominal\n    processor_version:                                                 1.1.0\n    product_version:                                                   1.0.0\n    project:                                                           Sentin...\n    references:                                                        http:/...\n    revision_control_identifier:                                       df649d...\n    sensor:                                                            TROPOMI\n    source:                                                            Sentin...\n    spatial_resolution:                                                7x3.5km2\n    standard_name_vocabulary:                                          NetCDF...\n    Status_MET_2D:                                                     Nominal\n    summary:                                                           TROPOM...\n    time_coverage_duration:                                            PT3503...\n    time_coverage_end:                                                 2018-0...\n    time_coverage_resolution:                                          PT1.080S\n    time_coverage_start:                                               2018-0...\n    time_reference:                                                    2018-0...\n    time_reference_days_since_1950:                                    25064\n    time_reference_julian_day:                                         2458346.5\n    time_reference_seconds_since_1970:                                 153437...\n    title:                                                             TROPOM...\n    tracking_id:                                                       0b05f3...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>band: 1</li><li>time: 1</li><li>x: 450</li><li>y: 3245</li></ul></li><li>Coordinates: (5)<ul><li>y(y)float643.244e+03 3.243e+03 ... 1.0 0.0<pre>array([3.244e+03, 3.243e+03, 3.242e+03, ..., 2.000e+00, 1.000e+00, 0.000e+00])</pre></li><li>x(x)float640.0 1.0 2.0 ... 447.0 448.0 449.0<pre>array([  0.,   1.,   2., ..., 447., 448., 449.])</pre></li><li>time(time)int64272073600<pre>array([272073600])</pre></li><li>spatial_ref()int640GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0<pre>array(0)</pre></li><li>band(band)int641<pre>array([1])</pre></li></ul></li><li>Data variables: (41)<ul><li>latitude(time, y, x)float32...bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_boundslong_name :pixel center latitudestandard_name :latitudeunits :degrees_northvalid_max :90valid_min :-90_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>solar_zenith_angle(band, y, x)float32...comment :Solar zenith angle at the ground pixel location on the reference ellipsoid. Angle is measured away from the verticalcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :solar zenith angleNETCDF_DIM_time :1standard_name :solar_zenith_angleunits :degreevalid_max :180valid_min :0_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>solar_azimuth_angle(band, y, x)float32...comment :Solar azimuth angle at the ground pixel location on the reference ellipsoid. Angle is measured clockwise from the North (East = 90, South = 180, West = 270)coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :solar azimuth angleNETCDF_DIM_time :1standard_name :solar_azimuth_angleunits :degreevalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>viewing_zenith_angle(band, y, x)float32...comment :Zenith angle of the satellite at the ground pixel location on the reference ellipsoid. Angle is measured away from the verticalcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :viewing zenith angleNETCDF_DIM_time :1standard_name :viewing_zenith_angleunits :degreevalid_max :180valid_min :0_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>viewing_azimuth_angle(band, y, x)float32...comment :Satellite azimuth angle at the ground pixel location on the reference ellipsoid. Angle is measured clockwise from the North (East = 90, South = 180, West = 270)coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :viewing azimuth angleNETCDF_DIM_time :1standard_name :viewing_azimuth_angleunits :degreevalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>longitude(time, y, x)float32...bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/longitude_boundslong_name :pixel center longitudestandard_name :longitudeunits :degrees_eastvalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>geolocation_flags(band, y, x)uint8...coordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[  0.   1.   2.   4.   8.  16. 128.]flag_meanings :no_error solar_eclipse sun_glint_possible descending night geo_boundary_crossing geolocation_errorflag_values :[  0.   1.   2.   4.   8.  16. 128.]long_name :ground pixel quality flagmax_val :254min_val :0NETCDF_DIM_time :1units :1_FillValue :255.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=uint8]</pre></li><li>processing_quality_flags(band, y, x)uint32...comment :Flags indicating conditions that affect quality of the retrieval.coordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5600000e+02  5.1200000e+02 1.0240000e+03 2.0480000e+03 4.0960000e+03 8.1920000e+03  1.6384000e+04 3.2768000e+04 6.5536000e+04 1.3107200e+05 2.6214400e+05  5.2428800e+05 1.0485760e+06 2.0971520e+06 4.1943040e+06 8.3886080e+06  1.6777216e+07 3.3554432e+07 6.7108864e+07]flag_meanings :success radiance_missing irradiance_missing input_spectrum_missing reflectance_range_error ler_range_error snr_range_error sza_range_error vza_range_error lut_range_error ozone_range_error wavelength_offset_error initialization_error memory_error assertion_error io_error numerical_error lut_error ISRF_error convergence_error cloud_filter_convergence_error max_iteration_convergence_error aot_lower_boundary_convergence_error other_boundary_convergence_error geolocation_error ch4_noscat_zero_error h2o_noscat_zero_error max_optical_thickness_error aerosol_boundary_error boundary_hit_error chi2_error svd_error dfs_error radiative_transfer_error optimal_estimation_error profile_error cloud_error model_error number_of_input_data_points_too_low_error cloud_pressure_spread_too_low_error cloud_too_low_level_error generic_range_error generic_exception input_spectrum_alignment_error abort_error wrong_input_type_error wavelength_calibration_error coregistration_error slant_column_density_error airmass_factor_error vertical_column_density_error signal_to_noise_ratio_error configuration_error key_error saturation_error solar_eclipse_filter cloud_filter altitude_consistency_filter altitude_roughness_filter sun_glint_filter mixed_surface_type_filter snow_ice_filter aai_filter cloud_fraction_fresco_filter aai_scene_albedo_filter small_pixel_radiance_std_filter cloud_fraction_viirs_filter cirrus_reflectance_viirs_filter cf_viirs_swir_ifov_filter cf_viirs_swir_ofova_filter cf_viirs_swir_ofovb_filter cf_viirs_swir_ofovc_filter cf_viirs_nir_ifov_filter cf_viirs_nir_ofova_filter cf_viirs_nir_ofovb_filter cf_viirs_nir_ofovc_filter refl_cirrus_viirs_swir_filter refl_cirrus_viirs_nir_filter diff_refl_cirrus_viirs_filter ch4_noscat_ratio_filter ch4_noscat_ratio_std_filter h2o_noscat_ratio_filter h2o_noscat_ratio_std_filter diff_psurf_fresco_ecmwf_filter psurf_fresco_stdv_filter ocean_filter time_range_filter pixel_or_scanline_index_filter geographic_region_filter input_spectrum_warning wavelength_calibration_warning extrapolation_warning sun_glint_warning south_atlantic_anomaly_warning sun_glint_correction snow_ice_warning cloud_warning AAI_warning pixel_level_input_data_missing data_range_warning low_cloud_fraction_warning altitude_consistency_warning signal_to_noise_ratio_warning deconvolution_warning so2_volcanic_origin_likely_warning so2_volcanic_origin_certain_warning interpolation_warning saturation_warningflag_values :[0.0000000e+00 1.0000000e+00 2.0000000e+00 3.0000000e+00 4.0000000e+00  5.0000000e+00 6.0000000e+00 7.0000000e+00 8.0000000e+00 9.0000000e+00  1.0000000e+01 1.1000000e+01 1.2000000e+01 1.3000000e+01 1.4000000e+01  1.5000000e+01 1.6000000e+01 1.7000000e+01 1.8000000e+01 1.9000000e+01  2.0000000e+01 2.1000000e+01 2.2000000e+01 2.3000000e+01 2.4000000e+01  2.5000000e+01 2.6000000e+01 2.7000000e+01 2.8000000e+01 2.9000000e+01  3.0000000e+01 3.1000000e+01 3.2000000e+01 3.3000000e+01 3.4000000e+01  3.5000000e+01 3.6000000e+01 3.7000000e+01 3.8000000e+01 3.9000000e+01  4.0000000e+01 4.1000000e+01 4.2000000e+01 4.3000000e+01 4.4000000e+01  4.5000000e+01 4.6000000e+01 4.7000000e+01 4.8000000e+01 4.9000000e+01  5.0000000e+01 5.1000000e+01 5.2000000e+01 5.3000000e+01 5.4000000e+01  6.4000000e+01 6.5000000e+01 6.6000000e+01 6.7000000e+01 6.8000000e+01  6.9000000e+01 7.0000000e+01 7.1000000e+01 7.2000000e+01 7.3000000e+01  7.4000000e+01 7.5000000e+01 7.6000000e+01 7.7000000e+01 7.8000000e+01  7.9000000e+01 8.0000000e+01 8.1000000e+01 8.2000000e+01 8.3000000e+01  8.4000000e+01 8.5000000e+01 8.6000000e+01 8.7000000e+01 8.8000000e+01  8.9000000e+01 9.0000000e+01 9.1000000e+01 9.2000000e+01 9.3000000e+01  9.4000000e+01 9.5000000e+01 9.6000000e+01 9.7000000e+01 2.5600000e+02  5.1200000e+02 1.0240000e+03 2.0480000e+03 4.0960000e+03 8.1920000e+03  1.6384000e+04 3.2768000e+04 6.5536000e+04 1.3107200e+05 2.6214400e+05  5.2428800e+05 1.0485760e+06 2.0971520e+06 4.1943040e+06 8.3886080e+06  1.6777216e+07 3.3554432e+07 6.7108864e+07]long_name :Processing quality flagsNETCDF_DIM_time :1_FillValue :4294967295.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=uint32]</pre></li><li>number_of_spectral_points_in_retrieval(band, y, x)uint16...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Number of spectral points used in the retrievalNETCDF_DIM_time :1_FillValue :65535.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=uint16]</pre></li><li>scene_albedo_388(band, y, x)float32...ancillary_variables :scene_albedo_388_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Scene albedo at 388 nm calculated from the top of atmosphere reflectance. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :388units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>scene_albedo_388_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the scene albedo at 388 nm calculated from the top of atmosphere reflectance and its precision. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :388units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_measured_354(band, y, x)float32...ancillary_variables :reflectance_measured_354_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_measured_354_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_measured_388(band, y, x)float32...ancillary_variables :reflectance_measured_388_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 388 nmNETCDF_DIM_time :1radiation_wavelength :388standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_measured_388_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 388 nmNETCDF_DIM_time :1radiation_wavelength :388standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_calculated_354(band, y, x)float32...ancillary_variables :reflectance_calculated_354_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Calculated top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_calculated_354_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the calculated top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>scene_albedo_380(band, y, x)float32...ancillary_variables :scene_albedo_380_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Scene albedo at 380 nm calculated from the top of atmosphere reflectance. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :380units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>scene_albedo_380_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the scene albedo at 380 nm calculated from the top of atmosphere reflectance and its precision. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :380units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_measured_340(band, y, x)float32...ancillary_variables :reflectance_measured_340_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_measured_340_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_measured_380(band, y, x)float32...ancillary_variables :reflectance_measured_380_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 380 nmNETCDF_DIM_time :1radiation_wavelength :380standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_measured_380_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 380 nmNETCDF_DIM_time :1radiation_wavelength :380standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_calculated_340(band, y, x)float32...ancillary_variables :reflectance_calculated_340_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Calculated top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>reflectance_calculated_340_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the calculated top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>wavelength_calibration_offset(band, y, x)float32...ancillary_variables :wavelength_calibration_offset_precisioncomment :True wavelength = nominal wavelength + wavelength offset + wavelength stretch * scaled wavelengthcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength offsetNETCDF_DIM_time :1units :nmwavelength_fit_window_end :390wavelength_fit_window_start :338_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>wavelength_calibration_offset_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength offset precisionNETCDF_DIM_time :1units :nm_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>wavelength_calibration_stretch(band, y, x)float32...ancillary_variables :wavelength_calibration_stretch_precisioncomment :True wavelength = nominal wavelength + wavelength offset + wavelength stretch * scaled wavelengthcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength stretchNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>wavelength_calibration_stretch_precision(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength stretch precisionNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>wavelength_calibration_chi_square(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength calibration chi squareNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>surface_altitude(band, y, x)float32...comment :The mean of the sub-pixels of the surface altitudewithin the approximate field of view, based on the GMTED2010 surface elevation databasecoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Surface altitudeNETCDF_DIM_time :1source :http://topotools.cr.usgs.gov/gmted_viewer/standard_name :surface_altitudeunits :m_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>surface_altitude_precision(band, y, x)float32...comment :The standard deviation of sub-pixels used in calculating the mean surface altitude, based on the GMTED2010 surface elevation databasecoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :surface altitude precisionNETCDF_DIM_time :1source :http://topotools.cr.usgs.gov/gmted_viewer/standard_error_multiplier :1standard_name :surface_altitude standard_errorunits :m_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>surface_classification(band, y, x)uint8...comment :Flag indicating land/water and further surface classifications for the ground pixelcoordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[  3.   3.   3.   3.   4. 249. 249. 249. 249. 249. 249. 249. 249. 249.  249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249.  249. 249. 249. 249. 249. 249. 249.]flag_meanings :land water some_water coast value_covers_majority_of_pixel water+shallow_ocean water+shallow_inland_water water+ocean_coastline-lake_shoreline water+intermittent_water water+deep_inland_water water+continental_shelf_ocean water+deep_ocean land+urban_and_built-up_land land+dryland_cropland_and_pasture land+irrigated_cropland_and_pasture land+mixed_dryland-irrigated_cropland_and_pasture land+cropland-grassland_mosaic land+cropland-woodland_mosaic land+grassland land+shrubland land+mixed_shrubland-grassland land+savanna land+deciduous_broadleaf_forest land+deciduous_needleleaf_forest land+evergreen_broadleaf_forest land+evergreen_needleleaf_forest land+mixed_forest land+herbaceous_wetland land+wooded_wetland land+barren_or_sparsely_vegetated land+herbaceous_tundra land+wooded_tundra land+mixed_tundra land+bare_ground_tundra land+snow_or_iceflag_values :[  0.   1.   2.   3.   4.   9.  17.  25.  33.  41.  49.  57.   8.  16.   24.  32.  40.  48.  56.  64.  72.  80.  88.  96. 104. 112. 120. 128.  136. 144. 152. 160. 168. 176. 184.]long_name :Land-water mask and surface classification based on a static databaseNETCDF_DIM_time :1source :USGS (https://lta.cr.usgs.gov/GLCC) and NASA SDP toolkit (http://newsroom.gsfc.nasa.gov/sdptoolkit/toolkit.html)_FillValue :255.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=uint8]</pre></li><li>qa_value(time, y, x)uint8...add_offset :0.0comment :A continuous quality descriptor, varying between 0 (no data) and 1 (full quality data). Recommend to ignore data with qa_value &lt; 0.5coordinates :longitude latitudelong_name :data quality valuescale_factor :0.009999999776482582units :1valid_max :100valid_min :0_FillValue :255.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=uint8]</pre></li><li>scaled_small_pixel_variance(band, y, x)float32...comment :The scaled variance of the reflectances of the small pixelscoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :scaled small pixel varianceNETCDF_DIM_time :1radiation_wavelength :373units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>ozone_total_column(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :total column amount of ozone from ECMWF model datamultiplication_factor_to_convert_to_DU :2241.1499multiplication_factor_to_convert_to_molecules_percm2 :6.0221409e+19NETCDF_DIM_time :1source :standard_name :atmosphere_mole_content_of_ozoneunits :mol m-2_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>surface_pressure(band, y, x)float32...coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :surface_air_pressureNETCDF_DIM_time :1source :standard_name :surface_air_pressureunits :Pa_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>aerosol_index_354_388(time, y, x)float32...ancillary_variables :aerosol_index_354_388_precisioncomment :Aerosol index from 388 and 354 nmcoordinates :longitude latitudelong_name :Aerosol index from 388 and 354 nmproposed_standard_name :ultraviolet_aerosol_indexradiation_wavelength :[354. 388.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>aerosol_index_340_380(time, y, x)float32...ancillary_variables :aerosol_index_340_380_precisioncomment :Aerosol index from 380 and 340 nmcoordinates :longitude latitudelong_name :Aerosol index from 380 and 340 nmproposed_standard_name :ultraviolet_aerosol_indexradiation_wavelength :[340. 380.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>aerosol_index_354_388_precision(time, y, x)float32...comment :Precision of aerosol index from 388 and 354 nmcoordinates :longitude latitudelong_name :Precision of aerosol index from 388 and 354 nmproposed_standard_name :ultraviolet_aerosol_index standard_errorradiation_wavelength :[354. 388.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li><li>aerosol_index_340_380_precision(time, y, x)float32...comment :Precision of aerosol index from 380 and 340 nmcoordinates :longitude latitudelong_name :Precision of aerosol index from 380 and 340 nmproposed_standard_name :ultraviolet_aerosol_index standard_errorradiation_wavelength :[340. 380.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>[1460250 values with dtype=float32]</pre></li></ul></li><li>Attributes: (287)algo.algorithm_variant :1algo.n_pair :2algo.pair_1.delta_wavelength :2.0algo.pair_1.id :TOMS_pairalgo.pair_1.min_wavelength :1algo.pair_1.number_spectral_pixels :7algo.pair_1.wavelength_1 :340algo.pair_1.wavelength_2 :380algo.pair_2.delta_wavelength :2.0algo.pair_2.id :OMI_pairalgo.pair_2.min_wavelength :1algo.pair_2.number_spectral_pixels :7algo.pair_2.wavelength_1 :354algo.pair_2.wavelength_2 :388configuration.version.algorithm :1.1.0configuration.version.framework :1.1.0input.1.band :3input.1.irrType :L1B_IR_UVNinput.1.type :L1B_RA_BD3input.count :1output.1.band :3output.1.config :cfg/product/product.AER_AI.xmloutput.1.type :L2__AER_AIoutput.compressionLevel :3output.count :1output.histogram.aerosol_index_340_380.end :14output.histogram.aerosol_index_340_380.start :-6output.histogram.aerosol_index_354_388.end :14output.histogram.aerosol_index_354_388.start :-6output.useCompression :trueoutput.useFletcher32 :trueoutput.useShuffleFilter :trueprocessing.algorithm :AER_AIprocessing.correct_surface_pressure_for_altitude :trueprocessing.exclude_flags :4294967295processing.groupDem :DEM_RADIUS_05000processing.ignore_pixel_flags :Falseprocessing.radiancePixelsMinError :2processing.radiancePixelsMinWarning :7processing.signal_to_noise.test :yesprocessing.signal_to_noise.threshold :12processing.signal_to_noise.window.range :350.0, 355.0processing.szaMax :88.0processing.szaMin :0.0processing.vzaMax :78.0processing.vzaMin :0.0qa_value.AAI_warning :100.0qa_value.altitude_consistency_warning :100.0qa_value.cloud_warning :100.0qa_value.data_range_warning :100.0qa_value.deconvolution_warning :100.0qa_value.extrapolation_warning :100.0qa_value.input_spectrum_warning :70.0qa_value.interpolation_warning :100.0qa_value.low_cloud_fraction_warning :100.0qa_value.pixel_level_input_data_missing :80.0qa_value.signal_to_noise_ratio_warning :100.0qa_value.snow_ice_warning :100.0qa_value.so2_volcanic_origin_certain_warning :100.0qa_value.so2_volcanic_origin_likely_warning :100.0qa_value.south_atlantic_anomaly_warning :100.0qa_value.sun_glint_correction :100.0qa_value.sun_glint_warning :70.0qa_value.wavelength_calibration_warning :90.0wavelength_calibration.convergence_threshold :1.0wavelength_calibration.include_stretch :nowavelength_calibration.initial_guess.a0 :1.0wavelength_calibration.initial_guess.a1 :0.1wavelength_calibration.initial_guess.a2 :0.01wavelength_calibration.initial_guess.ring :0.06wavelength_calibration.initial_guess.shift :0.0wavelength_calibration.initial_guess.stretch :0.0wavelength_calibration.irr.include_ring :nowavelength_calibration.irr.max_iterations :20wavelength_calibration.irr.polynomial_order :2wavelength_calibration.max_iterations :12wavelength_calibration.perform_wavelength_fit :yeswavelength_calibration.rad.include_ring :yeswavelength_calibration.rad.polynomial_order :3wavelength_calibration.sigma.a0 :1.0wavelength_calibration.sigma.a1 :0.1wavelength_calibration.sigma.ring :0.06wavelength_calibration.sigma.shift :0.07wavelength_calibration.sigma.stretch :0.07wavelength_calibration.window :338.0, 390.0/METADATA/EOP_METADATA/eop :metaDataProperty/NC_GLOBAL#objectType=eop:EarthObservationMetaDatagml :id=S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.IDobjectType :gmi:MI_Metadata/METADATA/EOP_METADATA/om :procedure/NC_GLOBAL#objectType=eop:EarthObservationEquipmentFile_Class :OFFLFile_Description :Aerosol index with a spatial resolution of 7x3.5km2 observed at about 13:30 local solar time from spectra measured by TROPOMIFile_Name :S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822File_Type :L2__AER_AIFile_Version :1Mission :S5PNotes :Creation_Date :UTC=2018-08-22T19:32:01Creator :TROPNLL2DPCreator_Version :1.1.0System :PDGS-OPValidity_Start :UTC=2018-08-16T18:51:51Validity_Stop :UTC=2018-08-16T19:50:14/METADATA/ESA_METADATA/earth_explorer_header/variable_header/gmd :lineage/NC_GLOBAL#objectType=gmd:LI_LineageGranuleEnd :2018-08-16T19:50:14ZGranuleStart :2018-08-16T18:51:51ZInstrumentName :TROPOMILongitudeOfDaysideNadirEquatorCrossing :-86.442253MissionName :Sentinel-5 precursorMissionShortName :S5PProcessingCenter :PDGS-OPProcessingMode :OfflineProcessingNode :s5p-off-pn37ProcessLevel :2ProcessorVersion :1.1.0ProductFormatVersion :10000ProductShortName :L2__AER_AI/METADATA/ISO_METADATA/gmd :language/NC_GLOBAL#objectType=gmd:LanguageCode/METADATA/ISO_METADATA/gmi :acquisitionInformation/NC_GLOBAL#objectType=gmi:MI_AcquisitionInformationgmd :metadataStandardVersion=ISO 19115-2:2009(E), S5P profileglobal_processing_warnings :--- TEST FLAG SET IN JOB ORDER --- number_of_aai_filter_occurrences :0number_of_aai_scene_albedo_filter_occurrences :0number_of_AAI_warning_occurrences :0number_of_abort_error_occurrences :0number_of_aerosol_boundary_error_occurrences :0number_of_airmass_factor_error_occurrences :0number_of_altitude_consistency_filter_occurrences :0number_of_altitude_consistency_warning_occurrences :0number_of_altitude_roughness_filter_occurrences :0number_of_aot_lower_boundary_convergence_error_occurrences :0number_of_assertion_error_occurrences :31number_of_boundary_hit_error_occurrences :0number_of_cf_viirs_nir_ifov_filter_occurrences :0number_of_cf_viirs_nir_ofova_filter_occurrences :0number_of_cf_viirs_nir_ofovb_filter_occurrences :0number_of_cf_viirs_nir_ofovc_filter_occurrences :0number_of_cf_viirs_swir_ifov_filter_occurrences :0number_of_cf_viirs_swir_ofova_filter_occurrences :0number_of_cf_viirs_swir_ofovb_filter_occurrences :0number_of_cf_viirs_swir_ofovc_filter_occurrences :0number_of_ch4_noscat_ratio_filter_occurrences :0number_of_ch4_noscat_ratio_std_filter_occurrences :0number_of_ch4_noscat_zero_error_occurrences :0number_of_chi2_error_occurrences :0number_of_cirrus_reflectance_viirs_filter_occurrences :0number_of_cloud_error_occurrences :0number_of_cloud_filter_convergence_error_occurrences :0number_of_cloud_filter_occurrences :0number_of_cloud_fraction_fresco_filter_occurrences :0number_of_cloud_fraction_viirs_filter_occurrences :0number_of_cloud_pressure_spread_too_low_error_occurrences :0number_of_cloud_too_low_level_error_occurrences :0number_of_cloud_warning_occurrences :0number_of_configuration_error_occurrences :0number_of_convergence_error_occurrences :0number_of_coregistration_error_occurrences :0number_of_data_range_warning_occurrences :0number_of_deconvolution_warning_occurrences :0number_of_dfs_error_occurrences :0number_of_diff_psurf_fresco_ecmwf_filter_occurrences :0number_of_diff_refl_cirrus_viirs_filter_occurrences :0number_of_extrapolation_warning_occurrences :0number_of_failed_retrievals :222068number_of_generic_exception_occurrences :0number_of_generic_range_error_occurrences :0number_of_geographic_region_filter_occurrences :0number_of_geolocation_error_occurrences :0number_of_groundpixels :1460250number_of_ground_pixels_with_warnings :84516number_of_h2o_noscat_ratio_filter_occurrences :0number_of_h2o_noscat_ratio_std_filter_occurrences :0number_of_h2o_noscat_zero_error_occurrences :0number_of_initialization_error_occurrences :0number_of_input_spectrum_alignment_error_occurrences :0number_of_input_spectrum_missing_occurrences :0number_of_input_spectrum_warning_occurrences :0number_of_interpolation_warning_occurrences :0number_of_io_error_occurrences :0number_of_irradiance_missing_occurrences :0number_of_ISRF_error_occurrences :0number_of_key_error_occurrences :0number_of_ler_range_error_occurrences :0number_of_low_cloud_fraction_warning_occurrences :0number_of_lut_error_occurrences :0number_of_lut_range_error_occurrences :0number_of_max_iteration_convergence_error_occurrences :0number_of_max_optical_thickness_error_occurrences :0number_of_memory_error_occurrences :0number_of_mixed_surface_type_filter_occurrences :0number_of_model_error_occurrences :0number_of_number_of_input_data_points_too_low_error_occurrences :0number_of_numerical_error_occurrences :0number_of_ocean_filter_occurrences :0number_of_optimal_estimation_error_occurrences :0number_of_other_boundary_convergence_error_occurrences :0number_of_ozone_range_error_occurrences :0number_of_pixel_level_input_data_missing_occurrences :0number_of_pixel_or_scanline_index_filter_occurrences :0number_of_processed_pixels :1460250number_of_profile_error_occurrences :0number_of_psurf_fresco_stdv_filter_occurrences :0number_of_radiance_missing_occurrences :0number_of_radiative_transfer_error_occurrences :0number_of_reflectance_range_error_occurrences :0number_of_refl_cirrus_viirs_nir_filter_occurrences :0number_of_refl_cirrus_viirs_swir_filter_occurrences :0number_of_rejected_pixels_not_enough_spectrum :0number_of_saturation_error_occurrences :0number_of_saturation_warning_occurrences :0number_of_signal_to_noise_ratio_error_occurrences :0number_of_signal_to_noise_ratio_warning_occurrences :0number_of_slant_column_density_error_occurrences :0number_of_small_pixel_radiance_std_filter_occurrences :0number_of_snow_ice_filter_occurrences :0number_of_snow_ice_warning_occurrences :0number_of_snr_range_error_occurrences :0number_of_so2_volcanic_origin_certain_warning_occurrences :0number_of_so2_volcanic_origin_likely_warning_occurrences :0number_of_solar_eclipse_filter_occurrences :0number_of_south_atlantic_anomaly_warning_occurrences :0number_of_successfully_processed_pixels :1238182number_of_sun_glint_correction_occurrences :0number_of_sun_glint_filter_occurrences :0number_of_sun_glint_warning_occurrences :84516number_of_svd_error_occurrences :0number_of_sza_range_error_occurrences :222037number_of_time_range_filter_occurrences :0number_of_vertical_column_density_error_occurrences :0number_of_vza_range_error_occurrences :0number_of_wavelength_calibration_error_occurrences :0number_of_wavelength_calibration_warning_occurrences :0number_of_wavelength_offset_error_occurrences :0number_of_wrong_input_type_error_occurrences :0time_for_algorithm_initialization :23.927286time_for_processing :212.96926time_per_pixel :0.001535914273508207time_standard_deviation_per_pixel :2.56573665351871e-05algorithm_version :1.1.0build_date :2018-07-04T06:04:00Zcdm_data_type :SwathConventions :CF-1.7cpp_compiler_flags :-g -O2 -fPIC -std=c++11 -W -Wall -Wno-ignored-qualifiers -Wno-write-strings -Wno-unused-variable -DTROPNLL2DPcpp_compiler_version :g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)creator_email :EOSupport@Copernicus.esa.intcreator_name :The Sentinel 5 Precursor TROPOMI Level 2 products are developed with funding from the European Space Agency (ESA), the Netherlands Space Office (NSO), the Belgian Science Policy Office, the German Aerospace Center (DLR) and the Bayerisches Staatsministerium f\u00fcr Wirtschaft und Medien, Energie und Technologie (StMWi).creator_url :http://www.tropomi.eudate_created :2018-08-22T19:32:01Zf90_compiler_flags :-gdwarf-3 -O2 -fPIC -cpp -ffpe-trap=invalid -fno-range-check -frecursive -fimplicit-none -ffree-line-length-none -DTROPNLL2DP -Wuninitialized -Wtabsf90_compiler_version :GNU Fortran (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)geolocation_grid_from_band :3geospatial_lat_max :89.972939geospatial_lat_min :-86.81926geospatial_lon_max :-179.99773geospatial_lon_min :179.99924history :2018-08-22 19:35:59 f_s5pops tropnll2dp /mnt/data1/storage_offl_l2/cache_offl_l2/WORKING-564386761/JobOrder.564386738.xmlid :S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822identifier_product_doi :10.5270/S5P-0wafvafidentifier_product_doi_authority :http://dx.doi.org/institution :KNMIkeywords :0300 Atmospheric Composition and Structure; 0305 Aerosols and Particles; 0360 Radiation, Transmission and Scattering; 3311 Clouds and Aerosols; 3360 Remote Sensingkeywords_vocabulary :AGU index terms, http://publications.agu.org/author-resource-center/index-terms/license :No conditions applynaming_authority :nl.knmiorbit :4361platform :S5Pprocessing_status :Nominalprocessor_version :1.1.0product_version :1.0.0project :Sentinel 5 precursor/TROPOMIreferences :http://www.tropomi.eu/data-products/aerosol-indexrevision_control_identifier :df649da886dbsensor :TROPOMIsource :Sentinel 5 precursor, TROPOMI, space-borne remote sensing, L2spatial_resolution :7x3.5km2standard_name_vocabulary :NetCDF Climate and Forecast Metadata Conventions Standard Name Table (v29, 08 July 2015), http://cfconventions.org/standard-names.htmlStatus_MET_2D :Nominalsummary :TROPOMI/S5P Aerosol Index 1-Orbit L2 Swath 7x3.5kmtime_coverage_duration :PT3503.000Stime_coverage_end :2018-08-16T19:50:14Ztime_coverage_resolution :PT1.080Stime_coverage_start :2018-08-16T18:51:51Ztime_reference :2018-08-16T00:00:00Ztime_reference_days_since_1950 :25064time_reference_julian_day :2458346.5time_reference_seconds_since_1970 :1534377600title :TROPOMI/S5P Aerosol Index 1-Orbit L2 Swath 7x3.5kmtracking_id :0b05f3ea-704b-4ab5-8df3-cf6e31764960</li></ul> In\u00a0[97]: Copied! <pre>r1 = rds[0]\ntype(r1)\n</pre> r1 = rds[0] type(r1) Out[97]: <pre>xarray.core.dataset.Dataset</pre> In\u00a0[99]: Copied! <pre>r1.geolocation_flags\n</pre> r1.geolocation_flags Out[99]: <pre>&lt;xarray.DataArray 'geolocation_flags' (band: 1, y: 3245, x: 450)&gt;\narray([[[12, 12, ..., 12, 12],\n        [12, 12, ..., 12, 12],\n        ...,\n        [ 8,  8, ...,  8,  8],\n        [ 8,  8, ...,  8,  8]]], dtype=uint8)\nCoordinates:\n  * y            (y) float64 3.244e+03 3.243e+03 3.242e+03 ... 2.0 1.0 0.0\n  * x            (x) float64 0.0 1.0 2.0 3.0 4.0 ... 446.0 447.0 448.0 449.0\n    spatial_ref  int64 0\n  * band         (band) int64 1\nAttributes:\n    coordinates:      /PRODUCT/longitude /PRODUCT/latitude\n    flag_masks:       [  0.   1.   2.   4.   8.  16. 128.]\n    flag_meanings:    no_error solar_eclipse sun_glint_possible descending ni...\n    flag_values:      [  0.   1.   2.   4.   8.  16. 128.]\n    long_name:        ground pixel quality flag\n    max_val:          254\n    min_val:          0\n    NETCDF_DIM_time:  1\n    units:            1\n    _FillValue:       255.0\n    scale_factor:     1.0\n    add_offset:       0.0\n    grid_mapping:     spatial_ref</pre>xarray.DataArray'geolocation_flags'<ul><li>band: 1</li><li>y: 3245</li><li>x: 450</li></ul><ul><li>12 12 12 12 12 12 12 12 12 12 12 12 12 ... 8 8 8 8 8 8 8 8 8 8 8 8 8<pre>array([[[12, 12, ..., 12, 12],\n        [12, 12, ..., 12, 12],\n        ...,\n        [ 8,  8, ...,  8,  8],\n        [ 8,  8, ...,  8,  8]]], dtype=uint8)</pre></li><li>Coordinates: (4)<ul><li>y(y)float643.244e+03 3.243e+03 ... 1.0 0.0<pre>array([3.244e+03, 3.243e+03, 3.242e+03, ..., 2.000e+00, 1.000e+00, 0.000e+00])</pre></li><li>x(x)float640.0 1.0 2.0 ... 447.0 448.0 449.0<pre>array([  0.,   1.,   2., ..., 447., 448., 449.])</pre></li><li>spatial_ref()int640GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0<pre>array(0)</pre></li><li>band(band)int641<pre>array([1])</pre></li></ul></li><li>Attributes: (13)coordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[  0.   1.   2.   4.   8.  16. 128.]flag_meanings :no_error solar_eclipse sun_glint_possible descending night geo_boundary_crossing geolocation_errorflag_values :[  0.   1.   2.   4.   8.  16. 128.]long_name :ground pixel quality flagmax_val :254min_val :0NETCDF_DIM_time :1units :1_FillValue :255.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref</li></ul> In\u00a0[101]: Copied! <pre>r1.spatial_ref\n</pre> r1.spatial_ref Out[101]: <pre>&lt;xarray.DataArray 'spatial_ref' ()&gt;\narray(0)\nCoordinates:\n    spatial_ref  int64 0\nAttributes:\n    GeoTransform:  -0.5 1.0 0.0 3244.5 0.0 -1.0</pre>xarray.DataArray'spatial_ref'<ul><li>0<pre>array(0)</pre></li><li>Coordinates: (1)<ul><li>spatial_ref()int640GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0<pre>array(0)</pre></li></ul></li><li>Attributes: (1)GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0</li></ul> In\u00a0[102]: Copied! <pre>r1.spatial_resolution\n</pre> r1.spatial_resolution Out[102]: <pre>'7x3.5km2'</pre> In\u00a0[103]: Copied! <pre>(r1.geospatial_lat_max, r1.geospatial_lat_min,\n r1.geospatial_lon_max, r1.geospatial_lon_min)\n</pre> (r1.geospatial_lat_max, r1.geospatial_lat_min,  r1.geospatial_lon_max, r1.geospatial_lon_min) Out[103]: <pre>(89.972939, -86.81926, -179.99773, 179.99924)</pre> In\u00a0[112]: Copied! <pre>r1_ai = r1['aerosol_index_340_380']\nr1_ai.plot()\n</pre> r1_ai = r1['aerosol_index_340_380'] r1_ai.plot() Out[112]: <pre>&lt;matplotlib.collections.QuadMesh at 0x1965f1520&gt;</pre> In\u00a0[113]: Copied! <pre>r1_ai.spatial_ref\n</pre> r1_ai.spatial_ref Out[113]: <pre>&lt;xarray.DataArray 'spatial_ref' ()&gt;\narray(0)\nCoordinates:\n    spatial_ref  int64 0\nAttributes:\n    GeoTransform:  -0.5 1.0 0.0 3244.5 0.0 -1.0</pre>xarray.DataArray'spatial_ref'<ul><li>0<pre>array(0)</pre></li><li>Coordinates: (1)<ul><li>spatial_ref()int640GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0<pre>array(0)</pre></li></ul></li><li>Attributes: (1)GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0</li></ul> In\u00a0[202]: Copied! <pre>#  rds[0].rio.set_spatial_dims(x_dim='/PRODUCT/longitude', y_dim='/PRODUCT/latitude')\nrds_crs_set = rds[0].rio.set_crs(4326)\nrds_crs_set\n</pre> #  rds[0].rio.set_spatial_dims(x_dim='/PRODUCT/longitude', y_dim='/PRODUCT/latitude') rds_crs_set = rds[0].rio.set_crs(4326) rds_crs_set Out[202]: <pre>&lt;xarray.Dataset&gt;\nDimensions:                                   (band: 1, time: 1, x: 450, y: 3245)\nCoordinates:\n  * y                                         (y) float64 3.244e+03 ... 0.0\n  * x                                         (x) float64 0.0 1.0 ... 449.0\n  * time                                      (time) int64 272073600\n    spatial_ref                               int64 0\n  * band                                      (band) int64 1\nData variables:\n    latitude                                  (time, y, x) float32 53.289627 ...\n    solar_zenith_angle                        (band, y, x) float32 97.89976 ....\n    solar_azimuth_angle                       (band, y, x) float32 54.418926 ...\n    viewing_zenith_angle                      (band, y, x) float32 66.529594 ...\n    viewing_azimuth_angle                     (band, y, x) float32 -60.276833...\n    longitude                                 (time, y, x) float32 119.45596 ...\n    geolocation_flags                         (band, y, x) uint8 12 12 ... 8 8\n    processing_quality_flags                  (band, y, x) uint32 7 7 7 ... 7 7\n    number_of_spectral_points_in_retrieval    (band, y, x) uint16 65535 ... 6...\n    scene_albedo_388                          (band, y, x) float32 9.96921e+3...\n    scene_albedo_388_precision                (band, y, x) float32 9.96921e+3...\n    reflectance_measured_354                  (band, y, x) float32 9.96921e+3...\n    reflectance_measured_354_precision        (band, y, x) float32 9.96921e+3...\n    reflectance_measured_388                  (band, y, x) float32 9.96921e+3...\n    reflectance_measured_388_precision        (band, y, x) float32 9.96921e+3...\n    reflectance_calculated_354                (band, y, x) float32 9.96921e+3...\n    reflectance_calculated_354_precision      (band, y, x) float32 9.96921e+3...\n    scene_albedo_380                          (band, y, x) float32 9.96921e+3...\n    scene_albedo_380_precision                (band, y, x) float32 9.96921e+3...\n    reflectance_measured_340                  (band, y, x) float32 9.96921e+3...\n    reflectance_measured_340_precision        (band, y, x) float32 9.96921e+3...\n    reflectance_measured_380                  (band, y, x) float32 9.96921e+3...\n    reflectance_measured_380_precision        (band, y, x) float32 9.96921e+3...\n    reflectance_calculated_340                (band, y, x) float32 9.96921e+3...\n    reflectance_calculated_340_precision      (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_offset             (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_offset_precision   (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_stretch            (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_stretch_precision  (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_chi_square         (band, y, x) float32 9.96921e+3...\n    surface_altitude                          (band, y, x) float32 9.96921e+3...\n    surface_altitude_precision                (band, y, x) float32 9.96921e+3...\n    surface_classification                    (band, y, x) uint8 255 255 ... 255\n    qa_value                                  (time, y, x) uint8 0 0 0 ... 0 0 0\n    scaled_small_pixel_variance               (band, y, x) float32 9.96921e+3...\n    ozone_total_column                        (band, y, x) float32 9.96921e+3...\n    surface_pressure                          (band, y, x) float32 9.96921e+3...\n    aerosol_index_354_388                     (time, y, x) float32 9.96921e+3...\n    aerosol_index_340_380                     (time, y, x) float32 9.96921e+3...\n    aerosol_index_354_388_precision           (time, y, x) float32 9.96921e+3...\n    aerosol_index_340_380_precision           (time, y, x) float32 9.96921e+3...\nAttributes:\n    algo.algorithm_variant:                                            1\n    algo.n_pair:                                                       2\n    algo.pair_1.delta_wavelength:                                      2.0\n    algo.pair_1.id:                                                    TOMS_pair\n    algo.pair_1.min_wavelength:                                        1\n    algo.pair_1.number_spectral_pixels:                                7\n    algo.pair_1.wavelength_1:                                          340\n    algo.pair_1.wavelength_2:                                          380\n    algo.pair_2.delta_wavelength:                                      2.0\n    algo.pair_2.id:                                                    OMI_pair\n    algo.pair_2.min_wavelength:                                        1\n    algo.pair_2.number_spectral_pixels:                                7\n    algo.pair_2.wavelength_1:                                          354\n    algo.pair_2.wavelength_2:                                          388\n    configuration.version.algorithm:                                   1.1.0\n    configuration.version.framework:                                   1.1.0\n    input.1.band:                                                      3\n    input.1.irrType:                                                   L1B_IR...\n    input.1.type:                                                      L1B_RA...\n    input.count:                                                       1\n    output.1.band:                                                     3\n    output.1.config:                                                   cfg/pr...\n    output.1.type:                                                     L2__AE...\n    output.compressionLevel:                                           3\n    output.count:                                                      1\n    output.histogram.aerosol_index_340_380.end:                        14\n    output.histogram.aerosol_index_340_380.start:                      -6\n    output.histogram.aerosol_index_354_388.end:                        14\n    output.histogram.aerosol_index_354_388.start:                      -6\n    output.useCompression:                                             true\n    output.useFletcher32:                                              true\n    output.useShuffleFilter:                                           true\n    processing.algorithm:                                              AER_AI\n    processing.correct_surface_pressure_for_altitude:                  true\n    processing.exclude_flags:                                          429496...\n    processing.groupDem:                                               DEM_RA...\n    processing.ignore_pixel_flags:                                     False\n    processing.radiancePixelsMinError:                                 2\n    processing.radiancePixelsMinWarning:                               7\n    processing.signal_to_noise.test:                                   yes\n    processing.signal_to_noise.threshold:                              12\n    processing.signal_to_noise.window.range:                           350.0,...\n    processing.szaMax:                                                 88.0\n    processing.szaMin:                                                 0.0\n    processing.vzaMax:                                                 78.0\n    processing.vzaMin:                                                 0.0\n    qa_value.AAI_warning:                                              100.0\n    qa_value.altitude_consistency_warning:                             100.0\n    qa_value.cloud_warning:                                            100.0\n    qa_value.data_range_warning:                                       100.0\n    qa_value.deconvolution_warning:                                    100.0\n    qa_value.extrapolation_warning:                                    100.0\n    qa_value.input_spectrum_warning:                                   70.0\n    qa_value.interpolation_warning:                                    100.0\n    qa_value.low_cloud_fraction_warning:                               100.0\n    qa_value.pixel_level_input_data_missing:                           80.0\n    qa_value.signal_to_noise_ratio_warning:                            100.0\n    qa_value.snow_ice_warning:                                         100.0\n    qa_value.so2_volcanic_origin_certain_warning:                      100.0\n    qa_value.so2_volcanic_origin_likely_warning:                       100.0\n    qa_value.south_atlantic_anomaly_warning:                           100.0\n    qa_value.sun_glint_correction:                                     100.0\n    qa_value.sun_glint_warning:                                        70.0\n    qa_value.wavelength_calibration_warning:                           90.0\n    wavelength_calibration.convergence_threshold:                      1.0\n    wavelength_calibration.include_stretch:                            no\n    wavelength_calibration.initial_guess.a0:                           1.0\n    wavelength_calibration.initial_guess.a1:                           0.1\n    wavelength_calibration.initial_guess.a2:                           0.01\n    wavelength_calibration.initial_guess.ring:                         0.06\n    wavelength_calibration.initial_guess.shift:                        0.0\n    wavelength_calibration.initial_guess.stretch:                      0.0\n    wavelength_calibration.irr.include_ring:                           no\n    wavelength_calibration.irr.max_iterations:                         20\n    wavelength_calibration.irr.polynomial_order:                       2\n    wavelength_calibration.max_iterations:                             12\n    wavelength_calibration.perform_wavelength_fit:                     yes\n    wavelength_calibration.rad.include_ring:                           yes\n    wavelength_calibration.rad.polynomial_order:                       3\n    wavelength_calibration.sigma.a0:                                   1.0\n    wavelength_calibration.sigma.a1:                                   0.1\n    wavelength_calibration.sigma.ring:                                 0.06\n    wavelength_calibration.sigma.shift:                                0.07\n    wavelength_calibration.sigma.stretch:                              0.07\n    wavelength_calibration.window:                                     338.0,...\n    /METADATA/EOP_METADATA/eop:                                        metaDa...\n    gml:                                                               id=S5P...\n    objectType:                                                        gmi:MI...\n    /METADATA/EOP_METADATA/om:                                         proced...\n    File_Class:                                                        OFFL\n    File_Description:                                                  Aeroso...\n    File_Name:                                                         S5P_OF...\n    File_Type:                                                         L2__AE...\n    File_Version:                                                      1\n    Mission:                                                           S5P\n    Notes:                                                             \n    Creation_Date:                                                     UTC=20...\n    Creator:                                                           TROPNL...\n    Creator_Version:                                                   1.1.0\n    System:                                                            PDGS-OP\n    Validity_Start:                                                    UTC=20...\n    Validity_Stop:                                                     UTC=20...\n    /METADATA/ESA_METADATA/earth_explorer_header/variable_header/gmd:  lineag...\n    GranuleEnd:                                                        2018-0...\n    GranuleStart:                                                      2018-0...\n    InstrumentName:                                                    TROPOMI\n    LongitudeOfDaysideNadirEquatorCrossing:                            -86.44...\n    MissionName:                                                       Sentin...\n    MissionShortName:                                                  S5P\n    ProcessingCenter:                                                  PDGS-OP\n    ProcessingMode:                                                    Offline\n    ProcessingNode:                                                    s5p-of...\n    ProcessLevel:                                                      2\n    ProcessorVersion:                                                  1.1.0\n    ProductFormatVersion:                                              10000\n    ProductShortName:                                                  L2__AE...\n    /METADATA/ISO_METADATA/gmd:                                        langua...\n    /METADATA/ISO_METADATA/gmi:                                        acquis...\n    gmd:                                                               metada...\n    global_processing_warnings:                                        --- TE...\n    number_of_aai_filter_occurrences:                                  0\n    number_of_aai_scene_albedo_filter_occurrences:                     0\n    number_of_AAI_warning_occurrences:                                 0\n    number_of_abort_error_occurrences:                                 0\n    number_of_aerosol_boundary_error_occurrences:                      0\n    number_of_airmass_factor_error_occurrences:                        0\n    number_of_altitude_consistency_filter_occurrences:                 0\n    number_of_altitude_consistency_warning_occurrences:                0\n    number_of_altitude_roughness_filter_occurrences:                   0\n    number_of_aot_lower_boundary_convergence_error_occurrences:        0\n    number_of_assertion_error_occurrences:                             31\n    number_of_boundary_hit_error_occurrences:                          0\n    number_of_cf_viirs_nir_ifov_filter_occurrences:                    0\n    number_of_cf_viirs_nir_ofova_filter_occurrences:                   0\n    number_of_cf_viirs_nir_ofovb_filter_occurrences:                   0\n    number_of_cf_viirs_nir_ofovc_filter_occurrences:                   0\n    number_of_cf_viirs_swir_ifov_filter_occurrences:                   0\n    number_of_cf_viirs_swir_ofova_filter_occurrences:                  0\n    number_of_cf_viirs_swir_ofovb_filter_occurrences:                  0\n    number_of_cf_viirs_swir_ofovc_filter_occurrences:                  0\n    number_of_ch4_noscat_ratio_filter_occurrences:                     0\n    number_of_ch4_noscat_ratio_std_filter_occurrences:                 0\n    number_of_ch4_noscat_zero_error_occurrences:                       0\n    number_of_chi2_error_occurrences:                                  0\n    number_of_cirrus_reflectance_viirs_filter_occurrences:             0\n    number_of_cloud_error_occurrences:                                 0\n    number_of_cloud_filter_convergence_error_occurrences:              0\n    number_of_cloud_filter_occurrences:                                0\n    number_of_cloud_fraction_fresco_filter_occurrences:                0\n    number_of_cloud_fraction_viirs_filter_occurrences:                 0\n    number_of_cloud_pressure_spread_too_low_error_occurrences:         0\n    number_of_cloud_too_low_level_error_occurrences:                   0\n    number_of_cloud_warning_occurrences:                               0\n    number_of_configuration_error_occurrences:                         0\n    number_of_convergence_error_occurrences:                           0\n    number_of_coregistration_error_occurrences:                        0\n    number_of_data_range_warning_occurrences:                          0\n    number_of_deconvolution_warning_occurrences:                       0\n    number_of_dfs_error_occurrences:                                   0\n    number_of_diff_psurf_fresco_ecmwf_filter_occurrences:              0\n    number_of_diff_refl_cirrus_viirs_filter_occurrences:               0\n    number_of_extrapolation_warning_occurrences:                       0\n    number_of_failed_retrievals:                                       222068\n    number_of_generic_exception_occurrences:                           0\n    number_of_generic_range_error_occurrences:                         0\n    number_of_geographic_region_filter_occurrences:                    0\n    number_of_geolocation_error_occurrences:                           0\n    number_of_groundpixels:                                            1460250\n    number_of_ground_pixels_with_warnings:                             84516\n    number_of_h2o_noscat_ratio_filter_occurrences:                     0\n    number_of_h2o_noscat_ratio_std_filter_occurrences:                 0\n    number_of_h2o_noscat_zero_error_occurrences:                       0\n    number_of_initialization_error_occurrences:                        0\n    number_of_input_spectrum_alignment_error_occurrences:              0\n    number_of_input_spectrum_missing_occurrences:                      0\n    number_of_input_spectrum_warning_occurrences:                      0\n    number_of_interpolation_warning_occurrences:                       0\n    number_of_io_error_occurrences:                                    0\n    number_of_irradiance_missing_occurrences:                          0\n    number_of_ISRF_error_occurrences:                                  0\n    number_of_key_error_occurrences:                                   0\n    number_of_ler_range_error_occurrences:                             0\n    number_of_low_cloud_fraction_warning_occurrences:                  0\n    number_of_lut_error_occurrences:                                   0\n    number_of_lut_range_error_occurrences:                             0\n    number_of_max_iteration_convergence_error_occurrences:             0\n    number_of_max_optical_thickness_error_occurrences:                 0\n    number_of_memory_error_occurrences:                                0\n    number_of_mixed_surface_type_filter_occurrences:                   0\n    number_of_model_error_occurrences:                                 0\n    number_of_number_of_input_data_points_too_low_error_occurrences:   0\n    number_of_numerical_error_occurrences:                             0\n    number_of_ocean_filter_occurrences:                                0\n    number_of_optimal_estimation_error_occurrences:                    0\n    number_of_other_boundary_convergence_error_occurrences:            0\n    number_of_ozone_range_error_occurrences:                           0\n    number_of_pixel_level_input_data_missing_occurrences:              0\n    number_of_pixel_or_scanline_index_filter_occurrences:              0\n    number_of_processed_pixels:                                        1460250\n    number_of_profile_error_occurrences:                               0\n    number_of_psurf_fresco_stdv_filter_occurrences:                    0\n    number_of_radiance_missing_occurrences:                            0\n    number_of_radiative_transfer_error_occurrences:                    0\n    number_of_reflectance_range_error_occurrences:                     0\n    number_of_refl_cirrus_viirs_nir_filter_occurrences:                0\n    number_of_refl_cirrus_viirs_swir_filter_occurrences:               0\n    number_of_rejected_pixels_not_enough_spectrum:                     0\n    number_of_saturation_error_occurrences:                            0\n    number_of_saturation_warning_occurrences:                          0\n    number_of_signal_to_noise_ratio_error_occurrences:                 0\n    number_of_signal_to_noise_ratio_warning_occurrences:               0\n    number_of_slant_column_density_error_occurrences:                  0\n    number_of_small_pixel_radiance_std_filter_occurrences:             0\n    number_of_snow_ice_filter_occurrences:                             0\n    number_of_snow_ice_warning_occurrences:                            0\n    number_of_snr_range_error_occurrences:                             0\n    number_of_so2_volcanic_origin_certain_warning_occurrences:         0\n    number_of_so2_volcanic_origin_likely_warning_occurrences:          0\n    number_of_solar_eclipse_filter_occurrences:                        0\n    number_of_south_atlantic_anomaly_warning_occurrences:              0\n    number_of_successfully_processed_pixels:                           1238182\n    number_of_sun_glint_correction_occurrences:                        0\n    number_of_sun_glint_filter_occurrences:                            0\n    number_of_sun_glint_warning_occurrences:                           84516\n    number_of_svd_error_occurrences:                                   0\n    number_of_sza_range_error_occurrences:                             222037\n    number_of_time_range_filter_occurrences:                           0\n    number_of_vertical_column_density_error_occurrences:               0\n    number_of_vza_range_error_occurrences:                             0\n    number_of_wavelength_calibration_error_occurrences:                0\n    number_of_wavelength_calibration_warning_occurrences:              0\n    number_of_wavelength_offset_error_occurrences:                     0\n    number_of_wrong_input_type_error_occurrences:                      0\n    time_for_algorithm_initialization:                                 23.927286\n    time_for_processing:                                               212.96926\n    time_per_pixel:                                                    0.0015...\n    time_standard_deviation_per_pixel:                                 2.5657...\n    algorithm_version:                                                 1.1.0\n    build_date:                                                        2018-0...\n    cdm_data_type:                                                     Swath\n    Conventions:                                                       CF-1.7\n    cpp_compiler_flags:                                                -g -O2...\n    cpp_compiler_version:                                              g++ (G...\n    creator_email:                                                     EOSupp...\n    creator_name:                                                      The Se...\n    creator_url:                                                       http:/...\n    date_created:                                                      2018-0...\n    f90_compiler_flags:                                                -gdwar...\n    f90_compiler_version:                                              GNU Fo...\n    geolocation_grid_from_band:                                        3\n    geospatial_lat_max:                                                89.972939\n    geospatial_lat_min:                                                -86.81926\n    geospatial_lon_max:                                                -179.9...\n    geospatial_lon_min:                                                179.99924\n    history:                                                           2018-0...\n    id:                                                                S5P_OF...\n    identifier_product_doi:                                            10.527...\n    identifier_product_doi_authority:                                  http:/...\n    institution:                                                       KNMI\n    keywords:                                                          0300 A...\n    keywords_vocabulary:                                               AGU in...\n    license:                                                           No con...\n    naming_authority:                                                  nl.knmi\n    orbit:                                                             4361\n    platform:                                                          S5P\n    processing_status:                                                 Nominal\n    processor_version:                                                 1.1.0\n    product_version:                                                   1.0.0\n    project:                                                           Sentin...\n    references:                                                        http:/...\n    revision_control_identifier:                                       df649d...\n    sensor:                                                            TROPOMI\n    source:                                                            Sentin...\n    spatial_resolution:                                                7x3.5km2\n    standard_name_vocabulary:                                          NetCDF...\n    Status_MET_2D:                                                     Nominal\n    summary:                                                           TROPOM...\n    time_coverage_duration:                                            PT3503...\n    time_coverage_end:                                                 2018-0...\n    time_coverage_resolution:                                          PT1.080S\n    time_coverage_start:                                               2018-0...\n    time_reference:                                                    2018-0...\n    time_reference_days_since_1950:                                    25064\n    time_reference_julian_day:                                         2458346.5\n    time_reference_seconds_since_1970:                                 153437...\n    title:                                                             TROPOM...\n    tracking_id:                                                       0b05f3...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>band: 1</li><li>time: 1</li><li>x: 450</li><li>y: 3245</li></ul></li><li>Coordinates: (5)<ul><li>y(y)float643.244e+03 3.243e+03 ... 1.0 0.0<pre>array([3.244e+03, 3.243e+03, 3.242e+03, ..., 2.000e+00, 1.000e+00, 0.000e+00])</pre></li><li>x(x)float640.0 1.0 2.0 ... 447.0 448.0 449.0<pre>array([  0.,   1.,   2., ..., 447., 448., 449.])</pre></li><li>time(time)int64272073600<pre>array([272073600])</pre></li><li>spatial_ref()int640GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0<pre>array(0)</pre></li><li>band(band)int641<pre>array([1])</pre></li></ul></li><li>Data variables: (41)<ul><li>latitude(time, y, x)float3253.289627 53.328514 ... -68.705986bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_boundslong_name :pixel center latitudestandard_name :latitudeunits :degrees_northvalid_max :90valid_min :-90_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 53.289627,  53.328514, ...,  60.861183,  60.871014],\n        [ 53.34224 ,  53.3812  , ...,  60.922707,  60.9325  ],\n        ...,\n        [-85.24866 , -85.31239 , ..., -68.77974 , -68.69752 ],\n        [-85.28333 , -85.34773 , ..., -68.788284, -68.705986]]], dtype=float32)</pre></li><li>solar_zenith_angle(band, y, x)float3297.89976 97.93866 ... 107.63726comment :Solar zenith angle at the ground pixel location on the reference ellipsoid. Angle is measured away from the verticalcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :solar zenith angleNETCDF_DIM_time :1standard_name :solar_zenith_angleunits :degreevalid_max :180valid_min :0_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 97.89976 ,  97.93866 , ..., 104.90762 , 104.9158  ],\n        [ 97.84073 ,  97.879616, ..., 104.848495, 104.8567  ],\n        ...,\n        [ 99.615   ,  99.634796, ..., 107.54233 , 107.57679 ],\n        [ 99.671555,  99.69138 , ..., 107.602806, 107.63726 ]]], dtype=float32)</pre></li><li>solar_azimuth_angle(band, y, x)float3254.418926 54.30675 ... -97.91921comment :Solar azimuth angle at the ground pixel location on the reference ellipsoid. Angle is measured clockwise from the North (East = 90, South = 180, West = 270)coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :solar azimuth angleNETCDF_DIM_time :1standard_name :solar_azimuth_angleunits :degreevalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 54.418926,  54.30675 , ...,  12.696914,  12.52919 ],\n        [ 54.462734,  54.35049 , ...,  12.684926,  12.516912],\n        ...,\n        [ 33.408936,  32.67658 , ..., -97.71841 , -97.7543  ],\n        [ 34.00804 ,  33.27868 , ..., -97.884   , -97.91921 ]]], dtype=float32)</pre></li><li>viewing_zenith_angle(band, y, x)float3266.529594 66.31491 ... 66.36827comment :Zenith angle of the satellite at the ground pixel location on the reference ellipsoid. Angle is measured away from the verticalcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :viewing zenith angleNETCDF_DIM_time :1standard_name :viewing_zenith_angleunits :degreevalid_max :180valid_min :0_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[66.529594, 66.31491 , ..., 65.804245, 66.015175],\n        [66.5298  , 66.31512 , ..., 65.80443 , 66.01536 ],\n        ...,\n        [66.87674 , 66.65851 , ..., 66.15328 , 66.36778 ],\n        [66.87627 , 66.65804 , ..., 66.15376 , 66.36827 ]]], dtype=float32)</pre></li><li>viewing_azimuth_angle(band, y, x)float32-60.276833 ... -166.71306comment :Satellite azimuth angle at the ground pixel location on the reference ellipsoid. Angle is measured clockwise from the North (East = 90, South = 180, West = 270)coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :viewing azimuth angleNETCDF_DIM_time :1standard_name :viewing_azimuth_angleunits :degreevalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ -60.276833,  -60.361042, ...,   94.35131 ,   94.22261 ],\n        [ -60.22006 ,  -60.304413, ...,   94.32718 ,   94.198105],\n        ...,\n        [ 138.82686 ,  138.09726 , ..., -166.50058 , -166.54396 ],\n        [ 139.448   ,  138.72131 , ..., -166.67027 , -166.71306 ]]],\n      dtype=float32)</pre></li><li>longitude(time, y, x)float32119.45596 119.32921 ... 1.9235005bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/longitude_boundslong_name :pixel center longitudestandard_name :longitudeunits :degrees_eastvalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 119.45596 ,  119.32921 , ...,   76.114655,   75.9475  ],\n        [ 119.51897 ,  119.39211 , ...,   76.110756,   75.94327 ],\n        ...,\n        [-135.85966 , -135.1101  , ...,    1.682641,    1.747748],\n        [-136.46013 , -135.71352 , ...,    1.859037,    1.923501]]],\n      dtype=float32)</pre></li><li>geolocation_flags(band, y, x)uint812 12 12 12 12 12 ... 8 8 8 8 8 8coordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[  0.   1.   2.   4.   8.  16. 128.]flag_meanings :no_error solar_eclipse sun_glint_possible descending night geo_boundary_crossing geolocation_errorflag_values :[  0.   1.   2.   4.   8.  16. 128.]long_name :ground pixel quality flagmax_val :254min_val :0NETCDF_DIM_time :1units :1_FillValue :255.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[12, 12, ..., 12, 12],\n        [12, 12, ..., 12, 12],\n        ...,\n        [ 8,  8, ...,  8,  8],\n        [ 8,  8, ...,  8,  8]]], dtype=uint8)</pre></li><li>processing_quality_flags(band, y, x)uint327 7 7 7 7 7 7 7 ... 7 7 7 7 7 7 7 7comment :Flags indicating conditions that affect quality of the retrieval.coordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5600000e+02  5.1200000e+02 1.0240000e+03 2.0480000e+03 4.0960000e+03 8.1920000e+03  1.6384000e+04 3.2768000e+04 6.5536000e+04 1.3107200e+05 2.6214400e+05  5.2428800e+05 1.0485760e+06 2.0971520e+06 4.1943040e+06 8.3886080e+06  1.6777216e+07 3.3554432e+07 6.7108864e+07]flag_meanings :success radiance_missing irradiance_missing input_spectrum_missing reflectance_range_error ler_range_error snr_range_error sza_range_error vza_range_error lut_range_error ozone_range_error wavelength_offset_error initialization_error memory_error assertion_error io_error numerical_error lut_error ISRF_error convergence_error cloud_filter_convergence_error max_iteration_convergence_error aot_lower_boundary_convergence_error other_boundary_convergence_error geolocation_error ch4_noscat_zero_error h2o_noscat_zero_error max_optical_thickness_error aerosol_boundary_error boundary_hit_error chi2_error svd_error dfs_error radiative_transfer_error optimal_estimation_error profile_error cloud_error model_error number_of_input_data_points_too_low_error cloud_pressure_spread_too_low_error cloud_too_low_level_error generic_range_error generic_exception input_spectrum_alignment_error abort_error wrong_input_type_error wavelength_calibration_error coregistration_error slant_column_density_error airmass_factor_error vertical_column_density_error signal_to_noise_ratio_error configuration_error key_error saturation_error solar_eclipse_filter cloud_filter altitude_consistency_filter altitude_roughness_filter sun_glint_filter mixed_surface_type_filter snow_ice_filter aai_filter cloud_fraction_fresco_filter aai_scene_albedo_filter small_pixel_radiance_std_filter cloud_fraction_viirs_filter cirrus_reflectance_viirs_filter cf_viirs_swir_ifov_filter cf_viirs_swir_ofova_filter cf_viirs_swir_ofovb_filter cf_viirs_swir_ofovc_filter cf_viirs_nir_ifov_filter cf_viirs_nir_ofova_filter cf_viirs_nir_ofovb_filter cf_viirs_nir_ofovc_filter refl_cirrus_viirs_swir_filter refl_cirrus_viirs_nir_filter diff_refl_cirrus_viirs_filter ch4_noscat_ratio_filter ch4_noscat_ratio_std_filter h2o_noscat_ratio_filter h2o_noscat_ratio_std_filter diff_psurf_fresco_ecmwf_filter psurf_fresco_stdv_filter ocean_filter time_range_filter pixel_or_scanline_index_filter geographic_region_filter input_spectrum_warning wavelength_calibration_warning extrapolation_warning sun_glint_warning south_atlantic_anomaly_warning sun_glint_correction snow_ice_warning cloud_warning AAI_warning pixel_level_input_data_missing data_range_warning low_cloud_fraction_warning altitude_consistency_warning signal_to_noise_ratio_warning deconvolution_warning so2_volcanic_origin_likely_warning so2_volcanic_origin_certain_warning interpolation_warning saturation_warningflag_values :[0.0000000e+00 1.0000000e+00 2.0000000e+00 3.0000000e+00 4.0000000e+00  5.0000000e+00 6.0000000e+00 7.0000000e+00 8.0000000e+00 9.0000000e+00  1.0000000e+01 1.1000000e+01 1.2000000e+01 1.3000000e+01 1.4000000e+01  1.5000000e+01 1.6000000e+01 1.7000000e+01 1.8000000e+01 1.9000000e+01  2.0000000e+01 2.1000000e+01 2.2000000e+01 2.3000000e+01 2.4000000e+01  2.5000000e+01 2.6000000e+01 2.7000000e+01 2.8000000e+01 2.9000000e+01  3.0000000e+01 3.1000000e+01 3.2000000e+01 3.3000000e+01 3.4000000e+01  3.5000000e+01 3.6000000e+01 3.7000000e+01 3.8000000e+01 3.9000000e+01  4.0000000e+01 4.1000000e+01 4.2000000e+01 4.3000000e+01 4.4000000e+01  4.5000000e+01 4.6000000e+01 4.7000000e+01 4.8000000e+01 4.9000000e+01  5.0000000e+01 5.1000000e+01 5.2000000e+01 5.3000000e+01 5.4000000e+01  6.4000000e+01 6.5000000e+01 6.6000000e+01 6.7000000e+01 6.8000000e+01  6.9000000e+01 7.0000000e+01 7.1000000e+01 7.2000000e+01 7.3000000e+01  7.4000000e+01 7.5000000e+01 7.6000000e+01 7.7000000e+01 7.8000000e+01  7.9000000e+01 8.0000000e+01 8.1000000e+01 8.2000000e+01 8.3000000e+01  8.4000000e+01 8.5000000e+01 8.6000000e+01 8.7000000e+01 8.8000000e+01  8.9000000e+01 9.0000000e+01 9.1000000e+01 9.2000000e+01 9.3000000e+01  9.4000000e+01 9.5000000e+01 9.6000000e+01 9.7000000e+01 2.5600000e+02  5.1200000e+02 1.0240000e+03 2.0480000e+03 4.0960000e+03 8.1920000e+03  1.6384000e+04 3.2768000e+04 6.5536000e+04 1.3107200e+05 2.6214400e+05  5.2428800e+05 1.0485760e+06 2.0971520e+06 4.1943040e+06 8.3886080e+06  1.6777216e+07 3.3554432e+07 6.7108864e+07]long_name :Processing quality flagsNETCDF_DIM_time :1_FillValue :4294967295.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[7, 7, ..., 7, 7],\n        [7, 7, ..., 7, 7],\n        ...,\n        [7, 7, ..., 7, 7],\n        [7, 7, ..., 7, 7]]], dtype=uint32)</pre></li><li>number_of_spectral_points_in_retrieval(band, y, x)uint1665535 65535 65535 ... 65535 65535coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Number of spectral points used in the retrievalNETCDF_DIM_time :1_FillValue :65535.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[65535, 65535, ..., 65535, 65535],\n        [65535, 65535, ..., 65535, 65535],\n        ...,\n        [65535, 65535, ..., 65535, 65535],\n        [65535, 65535, ..., 65535, 65535]]], dtype=uint16)</pre></li><li>scene_albedo_388(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :scene_albedo_388_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Scene albedo at 388 nm calculated from the top of atmosphere reflectance. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :388units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>scene_albedo_388_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the scene albedo at 388 nm calculated from the top of atmosphere reflectance and its precision. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :388units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_measured_354(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_measured_354_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_measured_354_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_measured_388(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_measured_388_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 388 nmNETCDF_DIM_time :1radiation_wavelength :388standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_measured_388_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 388 nmNETCDF_DIM_time :1radiation_wavelength :388standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_calculated_354(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_calculated_354_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Calculated top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_calculated_354_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the calculated top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>scene_albedo_380(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :scene_albedo_380_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Scene albedo at 380 nm calculated from the top of atmosphere reflectance. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :380units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>scene_albedo_380_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the scene albedo at 380 nm calculated from the top of atmosphere reflectance and its precision. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :380units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_measured_340(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_measured_340_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_measured_340_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_measured_380(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_measured_380_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 380 nmNETCDF_DIM_time :1radiation_wavelength :380standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_measured_380_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 380 nmNETCDF_DIM_time :1radiation_wavelength :380standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_calculated_340(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_calculated_340_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Calculated top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>reflectance_calculated_340_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the calculated top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>wavelength_calibration_offset(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :wavelength_calibration_offset_precisioncomment :True wavelength = nominal wavelength + wavelength offset + wavelength stretch * scaled wavelengthcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength offsetNETCDF_DIM_time :1units :nmwavelength_fit_window_end :390wavelength_fit_window_start :338_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>wavelength_calibration_offset_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength offset precisionNETCDF_DIM_time :1units :nm_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>wavelength_calibration_stretch(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :wavelength_calibration_stretch_precisioncomment :True wavelength = nominal wavelength + wavelength offset + wavelength stretch * scaled wavelengthcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength stretchNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>wavelength_calibration_stretch_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength stretch precisionNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>wavelength_calibration_chi_square(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength calibration chi squareNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>surface_altitude(band, y, x)float329.96921e+36 ... 9.96921e+36comment :The mean of the sub-pixels of the surface altitudewithin the approximate field of view, based on the GMTED2010 surface elevation databasecoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Surface altitudeNETCDF_DIM_time :1source :http://topotools.cr.usgs.gov/gmted_viewer/standard_name :surface_altitudeunits :m_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>surface_altitude_precision(band, y, x)float329.96921e+36 ... 9.96921e+36comment :The standard deviation of sub-pixels used in calculating the mean surface altitude, based on the GMTED2010 surface elevation databasecoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :surface altitude precisionNETCDF_DIM_time :1source :http://topotools.cr.usgs.gov/gmted_viewer/standard_error_multiplier :1standard_name :surface_altitude standard_errorunits :m_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>surface_classification(band, y, x)uint8255 255 255 255 ... 255 255 255 255comment :Flag indicating land/water and further surface classifications for the ground pixelcoordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[  3.   3.   3.   3.   4. 249. 249. 249. 249. 249. 249. 249. 249. 249.  249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249.  249. 249. 249. 249. 249. 249. 249.]flag_meanings :land water some_water coast value_covers_majority_of_pixel water+shallow_ocean water+shallow_inland_water water+ocean_coastline-lake_shoreline water+intermittent_water water+deep_inland_water water+continental_shelf_ocean water+deep_ocean land+urban_and_built-up_land land+dryland_cropland_and_pasture land+irrigated_cropland_and_pasture land+mixed_dryland-irrigated_cropland_and_pasture land+cropland-grassland_mosaic land+cropland-woodland_mosaic land+grassland land+shrubland land+mixed_shrubland-grassland land+savanna land+deciduous_broadleaf_forest land+deciduous_needleleaf_forest land+evergreen_broadleaf_forest land+evergreen_needleleaf_forest land+mixed_forest land+herbaceous_wetland land+wooded_wetland land+barren_or_sparsely_vegetated land+herbaceous_tundra land+wooded_tundra land+mixed_tundra land+bare_ground_tundra land+snow_or_iceflag_values :[  0.   1.   2.   3.   4.   9.  17.  25.  33.  41.  49.  57.   8.  16.   24.  32.  40.  48.  56.  64.  72.  80.  88.  96. 104. 112. 120. 128.  136. 144. 152. 160. 168. 176. 184.]long_name :Land-water mask and surface classification based on a static databaseNETCDF_DIM_time :1source :USGS (https://lta.cr.usgs.gov/GLCC) and NASA SDP toolkit (http://newsroom.gsfc.nasa.gov/sdptoolkit/toolkit.html)_FillValue :255.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[255, 255, ..., 255, 255],\n        [255, 255, ..., 255, 255],\n        ...,\n        [255, 255, ..., 255, 255],\n        [255, 255, ..., 255, 255]]], dtype=uint8)</pre></li><li>qa_value(time, y, x)uint80 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0add_offset :0.0comment :A continuous quality descriptor, varying between 0 (no data) and 1 (full quality data). Recommend to ignore data with qa_value &lt; 0.5coordinates :longitude latitudelong_name :data quality valuescale_factor :0.009999999776482582units :1valid_max :100valid_min :0_FillValue :255.0grid_mapping :spatial_ref<pre>array([[[0, 0, ..., 0, 0],\n        [0, 0, ..., 0, 0],\n        ...,\n        [0, 0, ..., 0, 0],\n        [0, 0, ..., 0, 0]]], dtype=uint8)</pre></li><li>scaled_small_pixel_variance(band, y, x)float329.96921e+36 ... 9.96921e+36comment :The scaled variance of the reflectances of the small pixelscoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :scaled small pixel varianceNETCDF_DIM_time :1radiation_wavelength :373units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>ozone_total_column(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :total column amount of ozone from ECMWF model datamultiplication_factor_to_convert_to_DU :2241.1499multiplication_factor_to_convert_to_molecules_percm2 :6.0221409e+19NETCDF_DIM_time :1source :standard_name :atmosphere_mole_content_of_ozoneunits :mol m-2_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>surface_pressure(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :surface_air_pressureNETCDF_DIM_time :1source :standard_name :surface_air_pressureunits :Pa_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>aerosol_index_354_388(time, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :aerosol_index_354_388_precisioncomment :Aerosol index from 388 and 354 nmcoordinates :longitude latitudelong_name :Aerosol index from 388 and 354 nmproposed_standard_name :ultraviolet_aerosol_indexradiation_wavelength :[354. 388.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>aerosol_index_340_380(time, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :aerosol_index_340_380_precisioncomment :Aerosol index from 380 and 340 nmcoordinates :longitude latitudelong_name :Aerosol index from 380 and 340 nmproposed_standard_name :ultraviolet_aerosol_indexradiation_wavelength :[340. 380.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>aerosol_index_354_388_precision(time, y, x)float329.96921e+36 ... 9.96921e+36comment :Precision of aerosol index from 388 and 354 nmcoordinates :longitude latitudelong_name :Precision of aerosol index from 388 and 354 nmproposed_standard_name :ultraviolet_aerosol_index standard_errorradiation_wavelength :[354. 388.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li><li>aerosol_index_340_380_precision(time, y, x)float329.96921e+36 ... 9.96921e+36comment :Precision of aerosol index from 380 and 340 nmcoordinates :longitude latitudelong_name :Precision of aerosol index from 380 and 340 nmproposed_standard_name :ultraviolet_aerosol_index standard_errorradiation_wavelength :[340. 380.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, ..., 9.96921e+36, 9.96921e+36]]],\n      dtype=float32)</pre></li></ul></li><li>Attributes: (287)algo.algorithm_variant :1algo.n_pair :2algo.pair_1.delta_wavelength :2.0algo.pair_1.id :TOMS_pairalgo.pair_1.min_wavelength :1algo.pair_1.number_spectral_pixels :7algo.pair_1.wavelength_1 :340algo.pair_1.wavelength_2 :380algo.pair_2.delta_wavelength :2.0algo.pair_2.id :OMI_pairalgo.pair_2.min_wavelength :1algo.pair_2.number_spectral_pixels :7algo.pair_2.wavelength_1 :354algo.pair_2.wavelength_2 :388configuration.version.algorithm :1.1.0configuration.version.framework :1.1.0input.1.band :3input.1.irrType :L1B_IR_UVNinput.1.type :L1B_RA_BD3input.count :1output.1.band :3output.1.config :cfg/product/product.AER_AI.xmloutput.1.type :L2__AER_AIoutput.compressionLevel :3output.count :1output.histogram.aerosol_index_340_380.end :14output.histogram.aerosol_index_340_380.start :-6output.histogram.aerosol_index_354_388.end :14output.histogram.aerosol_index_354_388.start :-6output.useCompression :trueoutput.useFletcher32 :trueoutput.useShuffleFilter :trueprocessing.algorithm :AER_AIprocessing.correct_surface_pressure_for_altitude :trueprocessing.exclude_flags :4294967295processing.groupDem :DEM_RADIUS_05000processing.ignore_pixel_flags :Falseprocessing.radiancePixelsMinError :2processing.radiancePixelsMinWarning :7processing.signal_to_noise.test :yesprocessing.signal_to_noise.threshold :12processing.signal_to_noise.window.range :350.0, 355.0processing.szaMax :88.0processing.szaMin :0.0processing.vzaMax :78.0processing.vzaMin :0.0qa_value.AAI_warning :100.0qa_value.altitude_consistency_warning :100.0qa_value.cloud_warning :100.0qa_value.data_range_warning :100.0qa_value.deconvolution_warning :100.0qa_value.extrapolation_warning :100.0qa_value.input_spectrum_warning :70.0qa_value.interpolation_warning :100.0qa_value.low_cloud_fraction_warning :100.0qa_value.pixel_level_input_data_missing :80.0qa_value.signal_to_noise_ratio_warning :100.0qa_value.snow_ice_warning :100.0qa_value.so2_volcanic_origin_certain_warning :100.0qa_value.so2_volcanic_origin_likely_warning :100.0qa_value.south_atlantic_anomaly_warning :100.0qa_value.sun_glint_correction :100.0qa_value.sun_glint_warning :70.0qa_value.wavelength_calibration_warning :90.0wavelength_calibration.convergence_threshold :1.0wavelength_calibration.include_stretch :nowavelength_calibration.initial_guess.a0 :1.0wavelength_calibration.initial_guess.a1 :0.1wavelength_calibration.initial_guess.a2 :0.01wavelength_calibration.initial_guess.ring :0.06wavelength_calibration.initial_guess.shift :0.0wavelength_calibration.initial_guess.stretch :0.0wavelength_calibration.irr.include_ring :nowavelength_calibration.irr.max_iterations :20wavelength_calibration.irr.polynomial_order :2wavelength_calibration.max_iterations :12wavelength_calibration.perform_wavelength_fit :yeswavelength_calibration.rad.include_ring :yeswavelength_calibration.rad.polynomial_order :3wavelength_calibration.sigma.a0 :1.0wavelength_calibration.sigma.a1 :0.1wavelength_calibration.sigma.ring :0.06wavelength_calibration.sigma.shift :0.07wavelength_calibration.sigma.stretch :0.07wavelength_calibration.window :338.0, 390.0/METADATA/EOP_METADATA/eop :metaDataProperty/NC_GLOBAL#objectType=eop:EarthObservationMetaDatagml :id=S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.IDobjectType :gmi:MI_Metadata/METADATA/EOP_METADATA/om :procedure/NC_GLOBAL#objectType=eop:EarthObservationEquipmentFile_Class :OFFLFile_Description :Aerosol index with a spatial resolution of 7x3.5km2 observed at about 13:30 local solar time from spectra measured by TROPOMIFile_Name :S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822File_Type :L2__AER_AIFile_Version :1Mission :S5PNotes :Creation_Date :UTC=2018-08-22T19:32:01Creator :TROPNLL2DPCreator_Version :1.1.0System :PDGS-OPValidity_Start :UTC=2018-08-16T18:51:51Validity_Stop :UTC=2018-08-16T19:50:14/METADATA/ESA_METADATA/earth_explorer_header/variable_header/gmd :lineage/NC_GLOBAL#objectType=gmd:LI_LineageGranuleEnd :2018-08-16T19:50:14ZGranuleStart :2018-08-16T18:51:51ZInstrumentName :TROPOMILongitudeOfDaysideNadirEquatorCrossing :-86.442253MissionName :Sentinel-5 precursorMissionShortName :S5PProcessingCenter :PDGS-OPProcessingMode :OfflineProcessingNode :s5p-off-pn37ProcessLevel :2ProcessorVersion :1.1.0ProductFormatVersion :10000ProductShortName :L2__AER_AI/METADATA/ISO_METADATA/gmd :language/NC_GLOBAL#objectType=gmd:LanguageCode/METADATA/ISO_METADATA/gmi :acquisitionInformation/NC_GLOBAL#objectType=gmi:MI_AcquisitionInformationgmd :metadataStandardVersion=ISO 19115-2:2009(E), S5P profileglobal_processing_warnings :--- TEST FLAG SET IN JOB ORDER --- number_of_aai_filter_occurrences :0number_of_aai_scene_albedo_filter_occurrences :0number_of_AAI_warning_occurrences :0number_of_abort_error_occurrences :0number_of_aerosol_boundary_error_occurrences :0number_of_airmass_factor_error_occurrences :0number_of_altitude_consistency_filter_occurrences :0number_of_altitude_consistency_warning_occurrences :0number_of_altitude_roughness_filter_occurrences :0number_of_aot_lower_boundary_convergence_error_occurrences :0number_of_assertion_error_occurrences :31number_of_boundary_hit_error_occurrences :0number_of_cf_viirs_nir_ifov_filter_occurrences :0number_of_cf_viirs_nir_ofova_filter_occurrences :0number_of_cf_viirs_nir_ofovb_filter_occurrences :0number_of_cf_viirs_nir_ofovc_filter_occurrences :0number_of_cf_viirs_swir_ifov_filter_occurrences :0number_of_cf_viirs_swir_ofova_filter_occurrences :0number_of_cf_viirs_swir_ofovb_filter_occurrences :0number_of_cf_viirs_swir_ofovc_filter_occurrences :0number_of_ch4_noscat_ratio_filter_occurrences :0number_of_ch4_noscat_ratio_std_filter_occurrences :0number_of_ch4_noscat_zero_error_occurrences :0number_of_chi2_error_occurrences :0number_of_cirrus_reflectance_viirs_filter_occurrences :0number_of_cloud_error_occurrences :0number_of_cloud_filter_convergence_error_occurrences :0number_of_cloud_filter_occurrences :0number_of_cloud_fraction_fresco_filter_occurrences :0number_of_cloud_fraction_viirs_filter_occurrences :0number_of_cloud_pressure_spread_too_low_error_occurrences :0number_of_cloud_too_low_level_error_occurrences :0number_of_cloud_warning_occurrences :0number_of_configuration_error_occurrences :0number_of_convergence_error_occurrences :0number_of_coregistration_error_occurrences :0number_of_data_range_warning_occurrences :0number_of_deconvolution_warning_occurrences :0number_of_dfs_error_occurrences :0number_of_diff_psurf_fresco_ecmwf_filter_occurrences :0number_of_diff_refl_cirrus_viirs_filter_occurrences :0number_of_extrapolation_warning_occurrences :0number_of_failed_retrievals :222068number_of_generic_exception_occurrences :0number_of_generic_range_error_occurrences :0number_of_geographic_region_filter_occurrences :0number_of_geolocation_error_occurrences :0number_of_groundpixels :1460250number_of_ground_pixels_with_warnings :84516number_of_h2o_noscat_ratio_filter_occurrences :0number_of_h2o_noscat_ratio_std_filter_occurrences :0number_of_h2o_noscat_zero_error_occurrences :0number_of_initialization_error_occurrences :0number_of_input_spectrum_alignment_error_occurrences :0number_of_input_spectrum_missing_occurrences :0number_of_input_spectrum_warning_occurrences :0number_of_interpolation_warning_occurrences :0number_of_io_error_occurrences :0number_of_irradiance_missing_occurrences :0number_of_ISRF_error_occurrences :0number_of_key_error_occurrences :0number_of_ler_range_error_occurrences :0number_of_low_cloud_fraction_warning_occurrences :0number_of_lut_error_occurrences :0number_of_lut_range_error_occurrences :0number_of_max_iteration_convergence_error_occurrences :0number_of_max_optical_thickness_error_occurrences :0number_of_memory_error_occurrences :0number_of_mixed_surface_type_filter_occurrences :0number_of_model_error_occurrences :0number_of_number_of_input_data_points_too_low_error_occurrences :0number_of_numerical_error_occurrences :0number_of_ocean_filter_occurrences :0number_of_optimal_estimation_error_occurrences :0number_of_other_boundary_convergence_error_occurrences :0number_of_ozone_range_error_occurrences :0number_of_pixel_level_input_data_missing_occurrences :0number_of_pixel_or_scanline_index_filter_occurrences :0number_of_processed_pixels :1460250number_of_profile_error_occurrences :0number_of_psurf_fresco_stdv_filter_occurrences :0number_of_radiance_missing_occurrences :0number_of_radiative_transfer_error_occurrences :0number_of_reflectance_range_error_occurrences :0number_of_refl_cirrus_viirs_nir_filter_occurrences :0number_of_refl_cirrus_viirs_swir_filter_occurrences :0number_of_rejected_pixels_not_enough_spectrum :0number_of_saturation_error_occurrences :0number_of_saturation_warning_occurrences :0number_of_signal_to_noise_ratio_error_occurrences :0number_of_signal_to_noise_ratio_warning_occurrences :0number_of_slant_column_density_error_occurrences :0number_of_small_pixel_radiance_std_filter_occurrences :0number_of_snow_ice_filter_occurrences :0number_of_snow_ice_warning_occurrences :0number_of_snr_range_error_occurrences :0number_of_so2_volcanic_origin_certain_warning_occurrences :0number_of_so2_volcanic_origin_likely_warning_occurrences :0number_of_solar_eclipse_filter_occurrences :0number_of_south_atlantic_anomaly_warning_occurrences :0number_of_successfully_processed_pixels :1238182number_of_sun_glint_correction_occurrences :0number_of_sun_glint_filter_occurrences :0number_of_sun_glint_warning_occurrences :84516number_of_svd_error_occurrences :0number_of_sza_range_error_occurrences :222037number_of_time_range_filter_occurrences :0number_of_vertical_column_density_error_occurrences :0number_of_vza_range_error_occurrences :0number_of_wavelength_calibration_error_occurrences :0number_of_wavelength_calibration_warning_occurrences :0number_of_wavelength_offset_error_occurrences :0number_of_wrong_input_type_error_occurrences :0time_for_algorithm_initialization :23.927286time_for_processing :212.96926time_per_pixel :0.001535914273508207time_standard_deviation_per_pixel :2.56573665351871e-05algorithm_version :1.1.0build_date :2018-07-04T06:04:00Zcdm_data_type :SwathConventions :CF-1.7cpp_compiler_flags :-g -O2 -fPIC -std=c++11 -W -Wall -Wno-ignored-qualifiers -Wno-write-strings -Wno-unused-variable -DTROPNLL2DPcpp_compiler_version :g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)creator_email :EOSupport@Copernicus.esa.intcreator_name :The Sentinel 5 Precursor TROPOMI Level 2 products are developed with funding from the European Space Agency (ESA), the Netherlands Space Office (NSO), the Belgian Science Policy Office, the German Aerospace Center (DLR) and the Bayerisches Staatsministerium f\u00fcr Wirtschaft und Medien, Energie und Technologie (StMWi).creator_url :http://www.tropomi.eudate_created :2018-08-22T19:32:01Zf90_compiler_flags :-gdwarf-3 -O2 -fPIC -cpp -ffpe-trap=invalid -fno-range-check -frecursive -fimplicit-none -ffree-line-length-none -DTROPNLL2DP -Wuninitialized -Wtabsf90_compiler_version :GNU Fortran (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)geolocation_grid_from_band :3geospatial_lat_max :89.972939geospatial_lat_min :-86.81926geospatial_lon_max :-179.99773geospatial_lon_min :179.99924history :2018-08-22 19:35:59 f_s5pops tropnll2dp /mnt/data1/storage_offl_l2/cache_offl_l2/WORKING-564386761/JobOrder.564386738.xmlid :S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822identifier_product_doi :10.5270/S5P-0wafvafidentifier_product_doi_authority :http://dx.doi.org/institution :KNMIkeywords :0300 Atmospheric Composition and Structure; 0305 Aerosols and Particles; 0360 Radiation, Transmission and Scattering; 3311 Clouds and Aerosols; 3360 Remote Sensingkeywords_vocabulary :AGU index terms, http://publications.agu.org/author-resource-center/index-terms/license :No conditions applynaming_authority :nl.knmiorbit :4361platform :S5Pprocessing_status :Nominalprocessor_version :1.1.0product_version :1.0.0project :Sentinel 5 precursor/TROPOMIreferences :http://www.tropomi.eu/data-products/aerosol-indexrevision_control_identifier :df649da886dbsensor :TROPOMIsource :Sentinel 5 precursor, TROPOMI, space-borne remote sensing, L2spatial_resolution :7x3.5km2standard_name_vocabulary :NetCDF Climate and Forecast Metadata Conventions Standard Name Table (v29, 08 July 2015), http://cfconventions.org/standard-names.htmlStatus_MET_2D :Nominalsummary :TROPOMI/S5P Aerosol Index 1-Orbit L2 Swath 7x3.5kmtime_coverage_duration :PT3503.000Stime_coverage_end :2018-08-16T19:50:14Ztime_coverage_resolution :PT1.080Stime_coverage_start :2018-08-16T18:51:51Ztime_reference :2018-08-16T00:00:00Ztime_reference_days_since_1950 :25064time_reference_julian_day :2458346.5time_reference_seconds_since_1970 :1534377600title :TROPOMI/S5P Aerosol Index 1-Orbit L2 Swath 7x3.5kmtracking_id :0b05f3ea-704b-4ab5-8df3-cf6e31764960</li></ul> In\u00a0[260]: Copied! <pre>rds_crs_set = rds_crs_set.set_coords(['longitude','latitude'])\nrds_crs_set\n</pre> rds_crs_set = rds_crs_set.set_coords(['longitude','latitude']) rds_crs_set Out[260]: <pre>&lt;xarray.Dataset&gt;\nDimensions:                                   (band: 1, time: 1, x: 450, y: 3245)\nCoordinates:\n  * y                                         (y) float64 3.244e+03 ... 0.0\n  * x                                         (x) float64 0.0 1.0 ... 449.0\n  * time                                      (time) int64 272073600\n    spatial_ref                               int64 0\n    latitude                                  (time, y, x) float32 53.289627 ...\n  * band                                      (band) int64 1\n    longitude                                 (time, y, x) float32 119.45596 ...\nData variables:\n    solar_zenith_angle                        (band, y, x) float32 97.89976 ....\n    solar_azimuth_angle                       (band, y, x) float32 54.418926 ...\n    viewing_zenith_angle                      (band, y, x) float32 66.529594 ...\n    viewing_azimuth_angle                     (band, y, x) float32 -60.276833...\n    geolocation_flags                         (band, y, x) uint8 12 12 ... 8 8\n    processing_quality_flags                  (band, y, x) uint32 7 7 7 ... 7 7\n    number_of_spectral_points_in_retrieval    (band, y, x) uint16 65535 ... 6...\n    scene_albedo_388                          (band, y, x) float32 9.96921e+3...\n    scene_albedo_388_precision                (band, y, x) float32 9.96921e+3...\n    reflectance_measured_354                  (band, y, x) float32 9.96921e+3...\n    reflectance_measured_354_precision        (band, y, x) float32 9.96921e+3...\n    reflectance_measured_388                  (band, y, x) float32 9.96921e+3...\n    reflectance_measured_388_precision        (band, y, x) float32 9.96921e+3...\n    reflectance_calculated_354                (band, y, x) float32 9.96921e+3...\n    reflectance_calculated_354_precision      (band, y, x) float32 9.96921e+3...\n    scene_albedo_380                          (band, y, x) float32 9.96921e+3...\n    scene_albedo_380_precision                (band, y, x) float32 9.96921e+3...\n    reflectance_measured_340                  (band, y, x) float32 9.96921e+3...\n    reflectance_measured_340_precision        (band, y, x) float32 9.96921e+3...\n    reflectance_measured_380                  (band, y, x) float32 9.96921e+3...\n    reflectance_measured_380_precision        (band, y, x) float32 9.96921e+3...\n    reflectance_calculated_340                (band, y, x) float32 9.96921e+3...\n    reflectance_calculated_340_precision      (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_offset             (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_offset_precision   (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_stretch            (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_stretch_precision  (band, y, x) float32 9.96921e+3...\n    wavelength_calibration_chi_square         (band, y, x) float32 9.96921e+3...\n    surface_altitude                          (band, y, x) float32 9.96921e+3...\n    surface_altitude_precision                (band, y, x) float32 9.96921e+3...\n    surface_classification                    (band, y, x) uint8 255 255 ... 255\n    qa_value                                  (time, y, x) uint8 0 0 0 ... 0 0 0\n    scaled_small_pixel_variance               (band, y, x) float32 9.96921e+3...\n    ozone_total_column                        (band, y, x) float32 9.96921e+3...\n    surface_pressure                          (band, y, x) float32 9.96921e+3...\n    aerosol_index_354_388                     (time, y, x) float32 9.96921e+3...\n    aerosol_index_340_380                     (time, y, x) float32 9.96921e+3...\n    aerosol_index_354_388_precision           (time, y, x) float32 9.96921e+3...\n    aerosol_index_340_380_precision           (time, y, x) float32 9.96921e+3...\nAttributes:\n    algo.algorithm_variant:                                            1\n    algo.n_pair:                                                       2\n    algo.pair_1.delta_wavelength:                                      2.0\n    algo.pair_1.id:                                                    TOMS_pair\n    algo.pair_1.min_wavelength:                                        1\n    algo.pair_1.number_spectral_pixels:                                7\n    algo.pair_1.wavelength_1:                                          340\n    algo.pair_1.wavelength_2:                                          380\n    algo.pair_2.delta_wavelength:                                      2.0\n    algo.pair_2.id:                                                    OMI_pair\n    algo.pair_2.min_wavelength:                                        1\n    algo.pair_2.number_spectral_pixels:                                7\n    algo.pair_2.wavelength_1:                                          354\n    algo.pair_2.wavelength_2:                                          388\n    configuration.version.algorithm:                                   1.1.0\n    configuration.version.framework:                                   1.1.0\n    input.1.band:                                                      3\n    input.1.irrType:                                                   L1B_IR...\n    input.1.type:                                                      L1B_RA...\n    input.count:                                                       1\n    output.1.band:                                                     3\n    output.1.config:                                                   cfg/pr...\n    output.1.type:                                                     L2__AE...\n    output.compressionLevel:                                           3\n    output.count:                                                      1\n    output.histogram.aerosol_index_340_380.end:                        14\n    output.histogram.aerosol_index_340_380.start:                      -6\n    output.histogram.aerosol_index_354_388.end:                        14\n    output.histogram.aerosol_index_354_388.start:                      -6\n    output.useCompression:                                             true\n    output.useFletcher32:                                              true\n    output.useShuffleFilter:                                           true\n    processing.algorithm:                                              AER_AI\n    processing.correct_surface_pressure_for_altitude:                  true\n    processing.exclude_flags:                                          429496...\n    processing.groupDem:                                               DEM_RA...\n    processing.ignore_pixel_flags:                                     False\n    processing.radiancePixelsMinError:                                 2\n    processing.radiancePixelsMinWarning:                               7\n    processing.signal_to_noise.test:                                   yes\n    processing.signal_to_noise.threshold:                              12\n    processing.signal_to_noise.window.range:                           350.0,...\n    processing.szaMax:                                                 88.0\n    processing.szaMin:                                                 0.0\n    processing.vzaMax:                                                 78.0\n    processing.vzaMin:                                                 0.0\n    qa_value.AAI_warning:                                              100.0\n    qa_value.altitude_consistency_warning:                             100.0\n    qa_value.cloud_warning:                                            100.0\n    qa_value.data_range_warning:                                       100.0\n    qa_value.deconvolution_warning:                                    100.0\n    qa_value.extrapolation_warning:                                    100.0\n    qa_value.input_spectrum_warning:                                   70.0\n    qa_value.interpolation_warning:                                    100.0\n    qa_value.low_cloud_fraction_warning:                               100.0\n    qa_value.pixel_level_input_data_missing:                           80.0\n    qa_value.signal_to_noise_ratio_warning:                            100.0\n    qa_value.snow_ice_warning:                                         100.0\n    qa_value.so2_volcanic_origin_certain_warning:                      100.0\n    qa_value.so2_volcanic_origin_likely_warning:                       100.0\n    qa_value.south_atlantic_anomaly_warning:                           100.0\n    qa_value.sun_glint_correction:                                     100.0\n    qa_value.sun_glint_warning:                                        70.0\n    qa_value.wavelength_calibration_warning:                           90.0\n    wavelength_calibration.convergence_threshold:                      1.0\n    wavelength_calibration.include_stretch:                            no\n    wavelength_calibration.initial_guess.a0:                           1.0\n    wavelength_calibration.initial_guess.a1:                           0.1\n    wavelength_calibration.initial_guess.a2:                           0.01\n    wavelength_calibration.initial_guess.ring:                         0.06\n    wavelength_calibration.initial_guess.shift:                        0.0\n    wavelength_calibration.initial_guess.stretch:                      0.0\n    wavelength_calibration.irr.include_ring:                           no\n    wavelength_calibration.irr.max_iterations:                         20\n    wavelength_calibration.irr.polynomial_order:                       2\n    wavelength_calibration.max_iterations:                             12\n    wavelength_calibration.perform_wavelength_fit:                     yes\n    wavelength_calibration.rad.include_ring:                           yes\n    wavelength_calibration.rad.polynomial_order:                       3\n    wavelength_calibration.sigma.a0:                                   1.0\n    wavelength_calibration.sigma.a1:                                   0.1\n    wavelength_calibration.sigma.ring:                                 0.06\n    wavelength_calibration.sigma.shift:                                0.07\n    wavelength_calibration.sigma.stretch:                              0.07\n    wavelength_calibration.window:                                     338.0,...\n    /METADATA/EOP_METADATA/eop:                                        metaDa...\n    gml:                                                               id=S5P...\n    objectType:                                                        gmi:MI...\n    /METADATA/EOP_METADATA/om:                                         proced...\n    File_Class:                                                        OFFL\n    File_Description:                                                  Aeroso...\n    File_Name:                                                         S5P_OF...\n    File_Type:                                                         L2__AE...\n    File_Version:                                                      1\n    Mission:                                                           S5P\n    Notes:                                                             \n    Creation_Date:                                                     UTC=20...\n    Creator:                                                           TROPNL...\n    Creator_Version:                                                   1.1.0\n    System:                                                            PDGS-OP\n    Validity_Start:                                                    UTC=20...\n    Validity_Stop:                                                     UTC=20...\n    /METADATA/ESA_METADATA/earth_explorer_header/variable_header/gmd:  lineag...\n    GranuleEnd:                                                        2018-0...\n    GranuleStart:                                                      2018-0...\n    InstrumentName:                                                    TROPOMI\n    LongitudeOfDaysideNadirEquatorCrossing:                            -86.44...\n    MissionName:                                                       Sentin...\n    MissionShortName:                                                  S5P\n    ProcessingCenter:                                                  PDGS-OP\n    ProcessingMode:                                                    Offline\n    ProcessingNode:                                                    s5p-of...\n    ProcessLevel:                                                      2\n    ProcessorVersion:                                                  1.1.0\n    ProductFormatVersion:                                              10000\n    ProductShortName:                                                  L2__AE...\n    /METADATA/ISO_METADATA/gmd:                                        langua...\n    /METADATA/ISO_METADATA/gmi:                                        acquis...\n    gmd:                                                               metada...\n    global_processing_warnings:                                        --- TE...\n    number_of_aai_filter_occurrences:                                  0\n    number_of_aai_scene_albedo_filter_occurrences:                     0\n    number_of_AAI_warning_occurrences:                                 0\n    number_of_abort_error_occurrences:                                 0\n    number_of_aerosol_boundary_error_occurrences:                      0\n    number_of_airmass_factor_error_occurrences:                        0\n    number_of_altitude_consistency_filter_occurrences:                 0\n    number_of_altitude_consistency_warning_occurrences:                0\n    number_of_altitude_roughness_filter_occurrences:                   0\n    number_of_aot_lower_boundary_convergence_error_occurrences:        0\n    number_of_assertion_error_occurrences:                             31\n    number_of_boundary_hit_error_occurrences:                          0\n    number_of_cf_viirs_nir_ifov_filter_occurrences:                    0\n    number_of_cf_viirs_nir_ofova_filter_occurrences:                   0\n    number_of_cf_viirs_nir_ofovb_filter_occurrences:                   0\n    number_of_cf_viirs_nir_ofovc_filter_occurrences:                   0\n    number_of_cf_viirs_swir_ifov_filter_occurrences:                   0\n    number_of_cf_viirs_swir_ofova_filter_occurrences:                  0\n    number_of_cf_viirs_swir_ofovb_filter_occurrences:                  0\n    number_of_cf_viirs_swir_ofovc_filter_occurrences:                  0\n    number_of_ch4_noscat_ratio_filter_occurrences:                     0\n    number_of_ch4_noscat_ratio_std_filter_occurrences:                 0\n    number_of_ch4_noscat_zero_error_occurrences:                       0\n    number_of_chi2_error_occurrences:                                  0\n    number_of_cirrus_reflectance_viirs_filter_occurrences:             0\n    number_of_cloud_error_occurrences:                                 0\n    number_of_cloud_filter_convergence_error_occurrences:              0\n    number_of_cloud_filter_occurrences:                                0\n    number_of_cloud_fraction_fresco_filter_occurrences:                0\n    number_of_cloud_fraction_viirs_filter_occurrences:                 0\n    number_of_cloud_pressure_spread_too_low_error_occurrences:         0\n    number_of_cloud_too_low_level_error_occurrences:                   0\n    number_of_cloud_warning_occurrences:                               0\n    number_of_configuration_error_occurrences:                         0\n    number_of_convergence_error_occurrences:                           0\n    number_of_coregistration_error_occurrences:                        0\n    number_of_data_range_warning_occurrences:                          0\n    number_of_deconvolution_warning_occurrences:                       0\n    number_of_dfs_error_occurrences:                                   0\n    number_of_diff_psurf_fresco_ecmwf_filter_occurrences:              0\n    number_of_diff_refl_cirrus_viirs_filter_occurrences:               0\n    number_of_extrapolation_warning_occurrences:                       0\n    number_of_failed_retrievals:                                       222068\n    number_of_generic_exception_occurrences:                           0\n    number_of_generic_range_error_occurrences:                         0\n    number_of_geographic_region_filter_occurrences:                    0\n    number_of_geolocation_error_occurrences:                           0\n    number_of_groundpixels:                                            1460250\n    number_of_ground_pixels_with_warnings:                             84516\n    number_of_h2o_noscat_ratio_filter_occurrences:                     0\n    number_of_h2o_noscat_ratio_std_filter_occurrences:                 0\n    number_of_h2o_noscat_zero_error_occurrences:                       0\n    number_of_initialization_error_occurrences:                        0\n    number_of_input_spectrum_alignment_error_occurrences:              0\n    number_of_input_spectrum_missing_occurrences:                      0\n    number_of_input_spectrum_warning_occurrences:                      0\n    number_of_interpolation_warning_occurrences:                       0\n    number_of_io_error_occurrences:                                    0\n    number_of_irradiance_missing_occurrences:                          0\n    number_of_ISRF_error_occurrences:                                  0\n    number_of_key_error_occurrences:                                   0\n    number_of_ler_range_error_occurrences:                             0\n    number_of_low_cloud_fraction_warning_occurrences:                  0\n    number_of_lut_error_occurrences:                                   0\n    number_of_lut_range_error_occurrences:                             0\n    number_of_max_iteration_convergence_error_occurrences:             0\n    number_of_max_optical_thickness_error_occurrences:                 0\n    number_of_memory_error_occurrences:                                0\n    number_of_mixed_surface_type_filter_occurrences:                   0\n    number_of_model_error_occurrences:                                 0\n    number_of_number_of_input_data_points_too_low_error_occurrences:   0\n    number_of_numerical_error_occurrences:                             0\n    number_of_ocean_filter_occurrences:                                0\n    number_of_optimal_estimation_error_occurrences:                    0\n    number_of_other_boundary_convergence_error_occurrences:            0\n    number_of_ozone_range_error_occurrences:                           0\n    number_of_pixel_level_input_data_missing_occurrences:              0\n    number_of_pixel_or_scanline_index_filter_occurrences:              0\n    number_of_processed_pixels:                                        1460250\n    number_of_profile_error_occurrences:                               0\n    number_of_psurf_fresco_stdv_filter_occurrences:                    0\n    number_of_radiance_missing_occurrences:                            0\n    number_of_radiative_transfer_error_occurrences:                    0\n    number_of_reflectance_range_error_occurrences:                     0\n    number_of_refl_cirrus_viirs_nir_filter_occurrences:                0\n    number_of_refl_cirrus_viirs_swir_filter_occurrences:               0\n    number_of_rejected_pixels_not_enough_spectrum:                     0\n    number_of_saturation_error_occurrences:                            0\n    number_of_saturation_warning_occurrences:                          0\n    number_of_signal_to_noise_ratio_error_occurrences:                 0\n    number_of_signal_to_noise_ratio_warning_occurrences:               0\n    number_of_slant_column_density_error_occurrences:                  0\n    number_of_small_pixel_radiance_std_filter_occurrences:             0\n    number_of_snow_ice_filter_occurrences:                             0\n    number_of_snow_ice_warning_occurrences:                            0\n    number_of_snr_range_error_occurrences:                             0\n    number_of_so2_volcanic_origin_certain_warning_occurrences:         0\n    number_of_so2_volcanic_origin_likely_warning_occurrences:          0\n    number_of_solar_eclipse_filter_occurrences:                        0\n    number_of_south_atlantic_anomaly_warning_occurrences:              0\n    number_of_successfully_processed_pixels:                           1238182\n    number_of_sun_glint_correction_occurrences:                        0\n    number_of_sun_glint_filter_occurrences:                            0\n    number_of_sun_glint_warning_occurrences:                           84516\n    number_of_svd_error_occurrences:                                   0\n    number_of_sza_range_error_occurrences:                             222037\n    number_of_time_range_filter_occurrences:                           0\n    number_of_vertical_column_density_error_occurrences:               0\n    number_of_vza_range_error_occurrences:                             0\n    number_of_wavelength_calibration_error_occurrences:                0\n    number_of_wavelength_calibration_warning_occurrences:              0\n    number_of_wavelength_offset_error_occurrences:                     0\n    number_of_wrong_input_type_error_occurrences:                      0\n    time_for_algorithm_initialization:                                 23.927286\n    time_for_processing:                                               212.96926\n    time_per_pixel:                                                    0.0015...\n    time_standard_deviation_per_pixel:                                 2.5657...\n    algorithm_version:                                                 1.1.0\n    build_date:                                                        2018-0...\n    cdm_data_type:                                                     Swath\n    Conventions:                                                       CF-1.7\n    cpp_compiler_flags:                                                -g -O2...\n    cpp_compiler_version:                                              g++ (G...\n    creator_email:                                                     EOSupp...\n    creator_name:                                                      The Se...\n    creator_url:                                                       http:/...\n    date_created:                                                      2018-0...\n    f90_compiler_flags:                                                -gdwar...\n    f90_compiler_version:                                              GNU Fo...\n    geolocation_grid_from_band:                                        3\n    geospatial_lat_max:                                                89.972939\n    geospatial_lat_min:                                                -86.81926\n    geospatial_lon_max:                                                -179.9...\n    geospatial_lon_min:                                                179.99924\n    history:                                                           2018-0...\n    id:                                                                S5P_OF...\n    identifier_product_doi:                                            10.527...\n    identifier_product_doi_authority:                                  http:/...\n    institution:                                                       KNMI\n    keywords:                                                          0300 A...\n    keywords_vocabulary:                                               AGU in...\n    license:                                                           No con...\n    naming_authority:                                                  nl.knmi\n    orbit:                                                             4361\n    platform:                                                          S5P\n    processing_status:                                                 Nominal\n    processor_version:                                                 1.1.0\n    product_version:                                                   1.0.0\n    project:                                                           Sentin...\n    references:                                                        http:/...\n    revision_control_identifier:                                       df649d...\n    sensor:                                                            TROPOMI\n    source:                                                            Sentin...\n    spatial_resolution:                                                7x3.5km2\n    standard_name_vocabulary:                                          NetCDF...\n    Status_MET_2D:                                                     Nominal\n    summary:                                                           TROPOM...\n    time_coverage_duration:                                            PT3503...\n    time_coverage_end:                                                 2018-0...\n    time_coverage_resolution:                                          PT1.080S\n    time_coverage_start:                                               2018-0...\n    time_reference:                                                    2018-0...\n    time_reference_days_since_1950:                                    25064\n    time_reference_julian_day:                                         2458346.5\n    time_reference_seconds_since_1970:                                 153437...\n    title:                                                             TROPOM...\n    tracking_id:                                                       0b05f3...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>band: 1</li><li>time: 1</li><li>x: 450</li><li>y: 3245</li></ul></li><li>Coordinates: (7)<ul><li>y(y)float643.244e+03 3.243e+03 ... 1.0 0.0axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([3.244e+03, 3.243e+03, 3.242e+03, ..., 2.000e+00, 1.000e+00, 0.000e+00])</pre></li><li>x(x)float640.0 1.0 2.0 ... 447.0 448.0 449.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([  0.,   1.,   2., ..., 447., 448., 449.])</pre></li><li>time(time)int64272073600<pre>array([272073600])</pre></li><li>spatial_ref()int640GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0<pre>array(0)</pre></li><li>latitude(time, y, x)float3253.289627 53.328514 ... -68.705986bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_boundslong_name :pixel center latitudestandard_name :latitudeunits :degrees_northvalid_max :90valid_min :-90_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 53.289627,  53.328514,  53.366734, ...,  60.851288,\n          60.861183,  60.871014],\n        [ 53.34224 ,  53.3812  ,  53.419495, ...,  60.912846,\n          60.922707,  60.9325  ],\n        [ 53.39479 ,  53.433826,  53.4722  , ...,  60.974384,\n          60.984215,  60.993973],\n        ...,\n        [-85.21351 , -85.27658 , -85.337944, ..., -68.85199 ,\n         -68.77097 , -68.68882 ],\n        [-85.24866 , -85.31239 , -85.37443 , ..., -68.860825,\n         -68.77974 , -68.69752 ],\n        [-85.28333 , -85.34773 , -85.41043 , ..., -68.869446,\n         -68.788284, -68.705986]]], dtype=float32)</pre></li><li>band(band)int641<pre>array([1])</pre></li><li>longitude(time, y, x)float32119.45596 119.32921 ... 1.9235005bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/longitude_boundslong_name :pixel center longitudestandard_name :longitudeunits :degrees_eastvalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 119.45596  ,  119.32921  ,  119.20397  , ...,   76.279495 ,\n           76.114655 ,   75.9475   ],\n        [ 119.51897  ,  119.39211  ,  119.266785 , ...,   76.27593  ,\n           76.110756 ,   75.94327  ],\n        [ 119.5822   ,  119.45525  ,  119.32983  , ...,   76.27239  ,\n           76.106895 ,   75.939064 ],\n        ...,\n        [-135.26692  , -134.51474  , -133.75343  , ...,    1.4409312,\n            1.5062535,    1.5720007],\n        [-135.85966  , -135.1101   , -134.3511   , ...,    1.6179553,\n            1.6826414,    1.7477475],\n        [-136.46013  , -135.71352  , -134.95714  , ...,    1.7949897,\n            1.8590373,    1.9235005]]], dtype=float32)</pre></li></ul></li><li>Data variables: (39)<ul><li>solar_zenith_angle(band, y, x)float3297.89976 97.93866 ... 107.63726comment :Solar zenith angle at the ground pixel location on the reference ellipsoid. Angle is measured away from the verticalcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :solar zenith angleNETCDF_DIM_time :1standard_name :solar_zenith_angleunits :degreevalid_max :180valid_min :0_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 97.89976 ,  97.93866 ,  97.977005, ..., 104.89952 ,\n         104.90762 , 104.9158  ],\n        [ 97.84073 ,  97.879616,  97.917946, ..., 104.84038 ,\n         104.848495, 104.8567  ],\n        [ 97.78168 ,  97.82054 ,  97.85886 , ..., 104.78124 ,\n         104.78938 , 104.79759 ],\n        ...,\n        [ 99.558426,  99.57819 ,  99.597664, ..., 107.44779 ,\n         107.48182 , 107.51628 ],\n        [ 99.615   ,  99.634796,  99.6543  , ..., 107.50831 ,\n         107.54233 , 107.57679 ],\n        [ 99.671555,  99.69138 ,  99.710915, ..., 107.56879 ,\n         107.602806, 107.63726 ]]], dtype=float32)</pre></li><li>solar_azimuth_angle(band, y, x)float3254.418926 54.30675 ... -97.91921comment :Solar azimuth angle at the ground pixel location on the reference ellipsoid. Angle is measured clockwise from the North (East = 90, South = 180, West = 270)coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :solar azimuth angleNETCDF_DIM_time :1standard_name :solar_azimuth_angleunits :degreevalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 54.418926,  54.30675 ,  54.19584 , ...,  12.862301,\n          12.696914,  12.52919 ],\n        [ 54.462734,  54.35049 ,  54.23951 , ...,  12.850599,\n          12.684926,  12.516912],\n        [ 54.50679 ,  54.394478,  54.28343 , ...,  12.83895 ,\n          12.672991,  12.504687],\n        ...,\n        [ 32.81754 ,  32.08251 ,  31.338154, ..., -97.51621 ,\n         -97.552826, -97.58942 ],\n        [ 33.408936,  32.67658 ,  31.934553, ..., -97.68247 ,\n         -97.71841 , -97.7543  ],\n        [ 34.00804 ,  33.27868 ,  32.539314, ..., -97.84875 ,\n         -97.884   , -97.91921 ]]], dtype=float32)</pre></li><li>viewing_zenith_angle(band, y, x)float3266.529594 66.31491 ... 66.36827comment :Zenith angle of the satellite at the ground pixel location on the reference ellipsoid. Angle is measured away from the verticalcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :viewing zenith angleNETCDF_DIM_time :1standard_name :viewing_zenith_angleunits :degreevalid_max :180valid_min :0_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[66.529594, 66.31491 , 66.10142 , ..., 65.59442 , 65.804245,\n         66.015175],\n        [66.5298  , 66.31512 , 66.10162 , ..., 65.594604, 65.80443 ,\n         66.01536 ],\n        [66.53011 , 66.31543 , 66.10193 , ..., 65.59467 , 65.8045  ,\n         66.01543 ],\n        ...,\n        [66.87699 , 66.65876 , 66.441795, ..., 65.93957 , 66.152885,\n         66.367386],\n        [66.87674 , 66.65851 , 66.44155 , ..., 65.939964, 66.15328 ,\n         66.36778 ],\n        [66.87627 , 66.65804 , 66.44108 , ..., 65.94044 , 66.15376 ,\n         66.36827 ]]], dtype=float32)</pre></li><li>viewing_azimuth_angle(band, y, x)float32-60.276833 ... -166.71306comment :Satellite azimuth angle at the ground pixel location on the reference ellipsoid. Angle is measured clockwise from the North (East = 90, South = 180, West = 270)coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :viewing azimuth angleNETCDF_DIM_time :1standard_name :viewing_azimuth_angleunits :degreevalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ -60.276833,  -60.361042,  -60.444096, ...,   94.477974,\n           94.35131 ,   94.22261 ],\n        [ -60.22006 ,  -60.304413,  -60.38761 , ...,   94.45422 ,\n           94.32718 ,   94.198105],\n        [ -60.163074,  -60.247574,  -60.33092 , ...,   94.43045 ,\n           94.30303 ,   94.17358 ],\n        ...,\n        [ 138.21364 ,  137.48146 ,  136.74011 , ..., -166.28754 ,\n         -166.33116 , -166.37514 ],\n        [ 138.82686 ,  138.09726 ,  137.35817 , ..., -166.45755 ,\n         -166.50058 , -166.54396 ],\n        [ 139.448   ,  138.72131 ,  137.98479 , ..., -166.62784 ,\n         -166.67027 , -166.71306 ]]], dtype=float32)</pre></li><li>geolocation_flags(band, y, x)uint812 12 12 12 12 12 ... 8 8 8 8 8 8coordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[  0.   1.   2.   4.   8.  16. 128.]flag_meanings :no_error solar_eclipse sun_glint_possible descending night geo_boundary_crossing geolocation_errorflag_values :[  0.   1.   2.   4.   8.  16. 128.]long_name :ground pixel quality flagmax_val :254min_val :0NETCDF_DIM_time :1units :1_FillValue :255.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[12, 12, 12, ..., 12, 12, 12],\n        [12, 12, 12, ..., 12, 12, 12],\n        [12, 12, 12, ..., 12, 12, 12],\n        ...,\n        [ 8,  8,  8, ...,  8,  8,  8],\n        [ 8,  8,  8, ...,  8,  8,  8],\n        [ 8,  8,  8, ...,  8,  8,  8]]], dtype=uint8)</pre></li><li>processing_quality_flags(band, y, x)uint327 7 7 7 7 7 7 7 ... 7 7 7 7 7 7 7 7comment :Flags indicating conditions that affect quality of the retrieval.coordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02  2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5500000e+02 2.5600000e+02  5.1200000e+02 1.0240000e+03 2.0480000e+03 4.0960000e+03 8.1920000e+03  1.6384000e+04 3.2768000e+04 6.5536000e+04 1.3107200e+05 2.6214400e+05  5.2428800e+05 1.0485760e+06 2.0971520e+06 4.1943040e+06 8.3886080e+06  1.6777216e+07 3.3554432e+07 6.7108864e+07]flag_meanings :success radiance_missing irradiance_missing input_spectrum_missing reflectance_range_error ler_range_error snr_range_error sza_range_error vza_range_error lut_range_error ozone_range_error wavelength_offset_error initialization_error memory_error assertion_error io_error numerical_error lut_error ISRF_error convergence_error cloud_filter_convergence_error max_iteration_convergence_error aot_lower_boundary_convergence_error other_boundary_convergence_error geolocation_error ch4_noscat_zero_error h2o_noscat_zero_error max_optical_thickness_error aerosol_boundary_error boundary_hit_error chi2_error svd_error dfs_error radiative_transfer_error optimal_estimation_error profile_error cloud_error model_error number_of_input_data_points_too_low_error cloud_pressure_spread_too_low_error cloud_too_low_level_error generic_range_error generic_exception input_spectrum_alignment_error abort_error wrong_input_type_error wavelength_calibration_error coregistration_error slant_column_density_error airmass_factor_error vertical_column_density_error signal_to_noise_ratio_error configuration_error key_error saturation_error solar_eclipse_filter cloud_filter altitude_consistency_filter altitude_roughness_filter sun_glint_filter mixed_surface_type_filter snow_ice_filter aai_filter cloud_fraction_fresco_filter aai_scene_albedo_filter small_pixel_radiance_std_filter cloud_fraction_viirs_filter cirrus_reflectance_viirs_filter cf_viirs_swir_ifov_filter cf_viirs_swir_ofova_filter cf_viirs_swir_ofovb_filter cf_viirs_swir_ofovc_filter cf_viirs_nir_ifov_filter cf_viirs_nir_ofova_filter cf_viirs_nir_ofovb_filter cf_viirs_nir_ofovc_filter refl_cirrus_viirs_swir_filter refl_cirrus_viirs_nir_filter diff_refl_cirrus_viirs_filter ch4_noscat_ratio_filter ch4_noscat_ratio_std_filter h2o_noscat_ratio_filter h2o_noscat_ratio_std_filter diff_psurf_fresco_ecmwf_filter psurf_fresco_stdv_filter ocean_filter time_range_filter pixel_or_scanline_index_filter geographic_region_filter input_spectrum_warning wavelength_calibration_warning extrapolation_warning sun_glint_warning south_atlantic_anomaly_warning sun_glint_correction snow_ice_warning cloud_warning AAI_warning pixel_level_input_data_missing data_range_warning low_cloud_fraction_warning altitude_consistency_warning signal_to_noise_ratio_warning deconvolution_warning so2_volcanic_origin_likely_warning so2_volcanic_origin_certain_warning interpolation_warning saturation_warningflag_values :[0.0000000e+00 1.0000000e+00 2.0000000e+00 3.0000000e+00 4.0000000e+00  5.0000000e+00 6.0000000e+00 7.0000000e+00 8.0000000e+00 9.0000000e+00  1.0000000e+01 1.1000000e+01 1.2000000e+01 1.3000000e+01 1.4000000e+01  1.5000000e+01 1.6000000e+01 1.7000000e+01 1.8000000e+01 1.9000000e+01  2.0000000e+01 2.1000000e+01 2.2000000e+01 2.3000000e+01 2.4000000e+01  2.5000000e+01 2.6000000e+01 2.7000000e+01 2.8000000e+01 2.9000000e+01  3.0000000e+01 3.1000000e+01 3.2000000e+01 3.3000000e+01 3.4000000e+01  3.5000000e+01 3.6000000e+01 3.7000000e+01 3.8000000e+01 3.9000000e+01  4.0000000e+01 4.1000000e+01 4.2000000e+01 4.3000000e+01 4.4000000e+01  4.5000000e+01 4.6000000e+01 4.7000000e+01 4.8000000e+01 4.9000000e+01  5.0000000e+01 5.1000000e+01 5.2000000e+01 5.3000000e+01 5.4000000e+01  6.4000000e+01 6.5000000e+01 6.6000000e+01 6.7000000e+01 6.8000000e+01  6.9000000e+01 7.0000000e+01 7.1000000e+01 7.2000000e+01 7.3000000e+01  7.4000000e+01 7.5000000e+01 7.6000000e+01 7.7000000e+01 7.8000000e+01  7.9000000e+01 8.0000000e+01 8.1000000e+01 8.2000000e+01 8.3000000e+01  8.4000000e+01 8.5000000e+01 8.6000000e+01 8.7000000e+01 8.8000000e+01  8.9000000e+01 9.0000000e+01 9.1000000e+01 9.2000000e+01 9.3000000e+01  9.4000000e+01 9.5000000e+01 9.6000000e+01 9.7000000e+01 2.5600000e+02  5.1200000e+02 1.0240000e+03 2.0480000e+03 4.0960000e+03 8.1920000e+03  1.6384000e+04 3.2768000e+04 6.5536000e+04 1.3107200e+05 2.6214400e+05  5.2428800e+05 1.0485760e+06 2.0971520e+06 4.1943040e+06 8.3886080e+06  1.6777216e+07 3.3554432e+07 6.7108864e+07]long_name :Processing quality flagsNETCDF_DIM_time :1_FillValue :4294967295.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[7, 7, 7, ..., 7, 7, 7],\n        [7, 7, 7, ..., 7, 7, 7],\n        [7, 7, 7, ..., 7, 7, 7],\n        ...,\n        [7, 7, 7, ..., 7, 7, 7],\n        [7, 7, 7, ..., 7, 7, 7],\n        [7, 7, 7, ..., 7, 7, 7]]], dtype=uint32)</pre></li><li>number_of_spectral_points_in_retrieval(band, y, x)uint1665535 65535 65535 ... 65535 65535coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Number of spectral points used in the retrievalNETCDF_DIM_time :1_FillValue :65535.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[65535, 65535, 65535, ..., 65535, 65535, 65535],\n        [65535, 65535, 65535, ..., 65535, 65535, 65535],\n        [65535, 65535, 65535, ..., 65535, 65535, 65535],\n        ...,\n        [65535, 65535, 65535, ..., 65535, 65535, 65535],\n        [65535, 65535, 65535, ..., 65535, 65535, 65535],\n        [65535, 65535, 65535, ..., 65535, 65535, 65535]]], dtype=uint16)</pre></li><li>scene_albedo_388(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :scene_albedo_388_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Scene albedo at 388 nm calculated from the top of atmosphere reflectance. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :388units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>scene_albedo_388_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the scene albedo at 388 nm calculated from the top of atmosphere reflectance and its precision. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :388units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_measured_354(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_measured_354_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_measured_354_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_measured_388(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_measured_388_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 388 nmNETCDF_DIM_time :1radiation_wavelength :388standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_measured_388_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 388 nmNETCDF_DIM_time :1radiation_wavelength :388standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_calculated_354(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_calculated_354_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Calculated top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_calculated_354_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the calculated top of atmosphere reflectance at 354 nmNETCDF_DIM_time :1radiation_wavelength :354standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>scene_albedo_380(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :scene_albedo_380_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Scene albedo at 380 nm calculated from the top of atmosphere reflectance. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :380units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>scene_albedo_380_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the scene albedo at 380 nm calculated from the top of atmosphere reflectance and its precision. For a cloud- and aerosol-free scene this is equivalent to the surface albedoNETCDF_DIM_time :1radiation_wavelength :380units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_measured_340(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_measured_340_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_measured_340_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_measured_380(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_measured_380_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Top of atmosphere reflectance at 380 nmNETCDF_DIM_time :1radiation_wavelength :380standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_measured_380_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the top of atmosphere reflectance at 380 nmNETCDF_DIM_time :1radiation_wavelength :380standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_calculated_340(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :reflectance_calculated_340_precisioncoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Calculated top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectanceunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>reflectance_calculated_340_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Precision of the calculated top of atmosphere reflectance at 340 nmNETCDF_DIM_time :1radiation_wavelength :340standard_name :toa_bidirectional_reflectance standard_errorunits :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>wavelength_calibration_offset(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :wavelength_calibration_offset_precisioncomment :True wavelength = nominal wavelength + wavelength offset + wavelength stretch * scaled wavelengthcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength offsetNETCDF_DIM_time :1units :nmwavelength_fit_window_end :390wavelength_fit_window_start :338_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>wavelength_calibration_offset_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength offset precisionNETCDF_DIM_time :1units :nm_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>wavelength_calibration_stretch(band, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :wavelength_calibration_stretch_precisioncomment :True wavelength = nominal wavelength + wavelength offset + wavelength stretch * scaled wavelengthcoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength stretchNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>wavelength_calibration_stretch_precision(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength stretch precisionNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>wavelength_calibration_chi_square(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :wavelength calibration chi squareNETCDF_DIM_time :1units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>surface_altitude(band, y, x)float329.96921e+36 ... 9.96921e+36comment :The mean of the sub-pixels of the surface altitudewithin the approximate field of view, based on the GMTED2010 surface elevation databasecoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :Surface altitudeNETCDF_DIM_time :1source :http://topotools.cr.usgs.gov/gmted_viewer/standard_name :surface_altitudeunits :m_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>surface_altitude_precision(band, y, x)float329.96921e+36 ... 9.96921e+36comment :The standard deviation of sub-pixels used in calculating the mean surface altitude, based on the GMTED2010 surface elevation databasecoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :surface altitude precisionNETCDF_DIM_time :1source :http://topotools.cr.usgs.gov/gmted_viewer/standard_error_multiplier :1standard_name :surface_altitude standard_errorunits :m_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>surface_classification(band, y, x)uint8255 255 255 255 ... 255 255 255 255comment :Flag indicating land/water and further surface classifications for the ground pixelcoordinates :/PRODUCT/longitude /PRODUCT/latitudeflag_masks :[  3.   3.   3.   3.   4. 249. 249. 249. 249. 249. 249. 249. 249. 249.  249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249. 249.  249. 249. 249. 249. 249. 249. 249.]flag_meanings :land water some_water coast value_covers_majority_of_pixel water+shallow_ocean water+shallow_inland_water water+ocean_coastline-lake_shoreline water+intermittent_water water+deep_inland_water water+continental_shelf_ocean water+deep_ocean land+urban_and_built-up_land land+dryland_cropland_and_pasture land+irrigated_cropland_and_pasture land+mixed_dryland-irrigated_cropland_and_pasture land+cropland-grassland_mosaic land+cropland-woodland_mosaic land+grassland land+shrubland land+mixed_shrubland-grassland land+savanna land+deciduous_broadleaf_forest land+deciduous_needleleaf_forest land+evergreen_broadleaf_forest land+evergreen_needleleaf_forest land+mixed_forest land+herbaceous_wetland land+wooded_wetland land+barren_or_sparsely_vegetated land+herbaceous_tundra land+wooded_tundra land+mixed_tundra land+bare_ground_tundra land+snow_or_iceflag_values :[  0.   1.   2.   3.   4.   9.  17.  25.  33.  41.  49.  57.   8.  16.   24.  32.  40.  48.  56.  64.  72.  80.  88.  96. 104. 112. 120. 128.  136. 144. 152. 160. 168. 176. 184.]long_name :Land-water mask and surface classification based on a static databaseNETCDF_DIM_time :1source :USGS (https://lta.cr.usgs.gov/GLCC) and NASA SDP toolkit (http://newsroom.gsfc.nasa.gov/sdptoolkit/toolkit.html)_FillValue :255.0scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        ...,\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255],\n        [255, 255, 255, ..., 255, 255, 255]]], dtype=uint8)</pre></li><li>qa_value(time, y, x)uint80 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0add_offset :0.0comment :A continuous quality descriptor, varying between 0 (no data) and 1 (full quality data). Recommend to ignore data with qa_value &lt; 0.5coordinates :longitude latitudelong_name :data quality valuescale_factor :0.009999999776482582units :1valid_max :100valid_min :0_FillValue :255.0grid_mapping :spatial_ref<pre>array([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)</pre></li><li>scaled_small_pixel_variance(band, y, x)float329.96921e+36 ... 9.96921e+36comment :The scaled variance of the reflectances of the small pixelscoordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :scaled small pixel varianceNETCDF_DIM_time :1radiation_wavelength :373units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>ozone_total_column(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :total column amount of ozone from ECMWF model datamultiplication_factor_to_convert_to_DU :2241.1499multiplication_factor_to_convert_to_molecules_percm2 :6.0221409e+19NETCDF_DIM_time :1source :standard_name :atmosphere_mole_content_of_ozoneunits :mol m-2_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>surface_pressure(band, y, x)float329.96921e+36 ... 9.96921e+36coordinates :/PRODUCT/longitude /PRODUCT/latitudelong_name :surface_air_pressureNETCDF_DIM_time :1source :standard_name :surface_air_pressureunits :Pa_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>aerosol_index_354_388(time, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :aerosol_index_354_388_precisioncomment :Aerosol index from 388 and 354 nmcoordinates :longitude latitudelong_name :Aerosol index from 388 and 354 nmproposed_standard_name :ultraviolet_aerosol_indexradiation_wavelength :[354. 388.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>aerosol_index_340_380(time, y, x)float329.96921e+36 ... 9.96921e+36ancillary_variables :aerosol_index_340_380_precisioncomment :Aerosol index from 380 and 340 nmcoordinates :longitude latitudelong_name :Aerosol index from 380 and 340 nmproposed_standard_name :ultraviolet_aerosol_indexradiation_wavelength :[340. 380.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>aerosol_index_354_388_precision(time, y, x)float329.96921e+36 ... 9.96921e+36comment :Precision of aerosol index from 388 and 354 nmcoordinates :longitude latitudelong_name :Precision of aerosol index from 388 and 354 nmproposed_standard_name :ultraviolet_aerosol_index standard_errorradiation_wavelength :[354. 388.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>aerosol_index_340_380_precision(time, y, x)float329.96921e+36 ... 9.96921e+36comment :Precision of aerosol index from 380 and 340 nmcoordinates :longitude latitudelong_name :Precision of aerosol index from 380 and 340 nmproposed_standard_name :ultraviolet_aerosol_index standard_errorradiation_wavelength :[340. 380.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li></ul></li><li>Attributes: (287)algo.algorithm_variant :1algo.n_pair :2algo.pair_1.delta_wavelength :2.0algo.pair_1.id :TOMS_pairalgo.pair_1.min_wavelength :1algo.pair_1.number_spectral_pixels :7algo.pair_1.wavelength_1 :340algo.pair_1.wavelength_2 :380algo.pair_2.delta_wavelength :2.0algo.pair_2.id :OMI_pairalgo.pair_2.min_wavelength :1algo.pair_2.number_spectral_pixels :7algo.pair_2.wavelength_1 :354algo.pair_2.wavelength_2 :388configuration.version.algorithm :1.1.0configuration.version.framework :1.1.0input.1.band :3input.1.irrType :L1B_IR_UVNinput.1.type :L1B_RA_BD3input.count :1output.1.band :3output.1.config :cfg/product/product.AER_AI.xmloutput.1.type :L2__AER_AIoutput.compressionLevel :3output.count :1output.histogram.aerosol_index_340_380.end :14output.histogram.aerosol_index_340_380.start :-6output.histogram.aerosol_index_354_388.end :14output.histogram.aerosol_index_354_388.start :-6output.useCompression :trueoutput.useFletcher32 :trueoutput.useShuffleFilter :trueprocessing.algorithm :AER_AIprocessing.correct_surface_pressure_for_altitude :trueprocessing.exclude_flags :4294967295processing.groupDem :DEM_RADIUS_05000processing.ignore_pixel_flags :Falseprocessing.radiancePixelsMinError :2processing.radiancePixelsMinWarning :7processing.signal_to_noise.test :yesprocessing.signal_to_noise.threshold :12processing.signal_to_noise.window.range :350.0, 355.0processing.szaMax :88.0processing.szaMin :0.0processing.vzaMax :78.0processing.vzaMin :0.0qa_value.AAI_warning :100.0qa_value.altitude_consistency_warning :100.0qa_value.cloud_warning :100.0qa_value.data_range_warning :100.0qa_value.deconvolution_warning :100.0qa_value.extrapolation_warning :100.0qa_value.input_spectrum_warning :70.0qa_value.interpolation_warning :100.0qa_value.low_cloud_fraction_warning :100.0qa_value.pixel_level_input_data_missing :80.0qa_value.signal_to_noise_ratio_warning :100.0qa_value.snow_ice_warning :100.0qa_value.so2_volcanic_origin_certain_warning :100.0qa_value.so2_volcanic_origin_likely_warning :100.0qa_value.south_atlantic_anomaly_warning :100.0qa_value.sun_glint_correction :100.0qa_value.sun_glint_warning :70.0qa_value.wavelength_calibration_warning :90.0wavelength_calibration.convergence_threshold :1.0wavelength_calibration.include_stretch :nowavelength_calibration.initial_guess.a0 :1.0wavelength_calibration.initial_guess.a1 :0.1wavelength_calibration.initial_guess.a2 :0.01wavelength_calibration.initial_guess.ring :0.06wavelength_calibration.initial_guess.shift :0.0wavelength_calibration.initial_guess.stretch :0.0wavelength_calibration.irr.include_ring :nowavelength_calibration.irr.max_iterations :20wavelength_calibration.irr.polynomial_order :2wavelength_calibration.max_iterations :12wavelength_calibration.perform_wavelength_fit :yeswavelength_calibration.rad.include_ring :yeswavelength_calibration.rad.polynomial_order :3wavelength_calibration.sigma.a0 :1.0wavelength_calibration.sigma.a1 :0.1wavelength_calibration.sigma.ring :0.06wavelength_calibration.sigma.shift :0.07wavelength_calibration.sigma.stretch :0.07wavelength_calibration.window :338.0, 390.0/METADATA/EOP_METADATA/eop :metaDataProperty/NC_GLOBAL#objectType=eop:EarthObservationMetaDatagml :id=S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.IDobjectType :gmi:MI_Metadata/METADATA/EOP_METADATA/om :procedure/NC_GLOBAL#objectType=eop:EarthObservationEquipmentFile_Class :OFFLFile_Description :Aerosol index with a spatial resolution of 7x3.5km2 observed at about 13:30 local solar time from spectra measured by TROPOMIFile_Name :S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822File_Type :L2__AER_AIFile_Version :1Mission :S5PNotes :Creation_Date :UTC=2018-08-22T19:32:01Creator :TROPNLL2DPCreator_Version :1.1.0System :PDGS-OPValidity_Start :UTC=2018-08-16T18:51:51Validity_Stop :UTC=2018-08-16T19:50:14/METADATA/ESA_METADATA/earth_explorer_header/variable_header/gmd :lineage/NC_GLOBAL#objectType=gmd:LI_LineageGranuleEnd :2018-08-16T19:50:14ZGranuleStart :2018-08-16T18:51:51ZInstrumentName :TROPOMILongitudeOfDaysideNadirEquatorCrossing :-86.442253MissionName :Sentinel-5 precursorMissionShortName :S5PProcessingCenter :PDGS-OPProcessingMode :OfflineProcessingNode :s5p-off-pn37ProcessLevel :2ProcessorVersion :1.1.0ProductFormatVersion :10000ProductShortName :L2__AER_AI/METADATA/ISO_METADATA/gmd :language/NC_GLOBAL#objectType=gmd:LanguageCode/METADATA/ISO_METADATA/gmi :acquisitionInformation/NC_GLOBAL#objectType=gmi:MI_AcquisitionInformationgmd :metadataStandardVersion=ISO 19115-2:2009(E), S5P profileglobal_processing_warnings :--- TEST FLAG SET IN JOB ORDER --- number_of_aai_filter_occurrences :0number_of_aai_scene_albedo_filter_occurrences :0number_of_AAI_warning_occurrences :0number_of_abort_error_occurrences :0number_of_aerosol_boundary_error_occurrences :0number_of_airmass_factor_error_occurrences :0number_of_altitude_consistency_filter_occurrences :0number_of_altitude_consistency_warning_occurrences :0number_of_altitude_roughness_filter_occurrences :0number_of_aot_lower_boundary_convergence_error_occurrences :0number_of_assertion_error_occurrences :31number_of_boundary_hit_error_occurrences :0number_of_cf_viirs_nir_ifov_filter_occurrences :0number_of_cf_viirs_nir_ofova_filter_occurrences :0number_of_cf_viirs_nir_ofovb_filter_occurrences :0number_of_cf_viirs_nir_ofovc_filter_occurrences :0number_of_cf_viirs_swir_ifov_filter_occurrences :0number_of_cf_viirs_swir_ofova_filter_occurrences :0number_of_cf_viirs_swir_ofovb_filter_occurrences :0number_of_cf_viirs_swir_ofovc_filter_occurrences :0number_of_ch4_noscat_ratio_filter_occurrences :0number_of_ch4_noscat_ratio_std_filter_occurrences :0number_of_ch4_noscat_zero_error_occurrences :0number_of_chi2_error_occurrences :0number_of_cirrus_reflectance_viirs_filter_occurrences :0number_of_cloud_error_occurrences :0number_of_cloud_filter_convergence_error_occurrences :0number_of_cloud_filter_occurrences :0number_of_cloud_fraction_fresco_filter_occurrences :0number_of_cloud_fraction_viirs_filter_occurrences :0number_of_cloud_pressure_spread_too_low_error_occurrences :0number_of_cloud_too_low_level_error_occurrences :0number_of_cloud_warning_occurrences :0number_of_configuration_error_occurrences :0number_of_convergence_error_occurrences :0number_of_coregistration_error_occurrences :0number_of_data_range_warning_occurrences :0number_of_deconvolution_warning_occurrences :0number_of_dfs_error_occurrences :0number_of_diff_psurf_fresco_ecmwf_filter_occurrences :0number_of_diff_refl_cirrus_viirs_filter_occurrences :0number_of_extrapolation_warning_occurrences :0number_of_failed_retrievals :222068number_of_generic_exception_occurrences :0number_of_generic_range_error_occurrences :0number_of_geographic_region_filter_occurrences :0number_of_geolocation_error_occurrences :0number_of_groundpixels :1460250number_of_ground_pixels_with_warnings :84516number_of_h2o_noscat_ratio_filter_occurrences :0number_of_h2o_noscat_ratio_std_filter_occurrences :0number_of_h2o_noscat_zero_error_occurrences :0number_of_initialization_error_occurrences :0number_of_input_spectrum_alignment_error_occurrences :0number_of_input_spectrum_missing_occurrences :0number_of_input_spectrum_warning_occurrences :0number_of_interpolation_warning_occurrences :0number_of_io_error_occurrences :0number_of_irradiance_missing_occurrences :0number_of_ISRF_error_occurrences :0number_of_key_error_occurrences :0number_of_ler_range_error_occurrences :0number_of_low_cloud_fraction_warning_occurrences :0number_of_lut_error_occurrences :0number_of_lut_range_error_occurrences :0number_of_max_iteration_convergence_error_occurrences :0number_of_max_optical_thickness_error_occurrences :0number_of_memory_error_occurrences :0number_of_mixed_surface_type_filter_occurrences :0number_of_model_error_occurrences :0number_of_number_of_input_data_points_too_low_error_occurrences :0number_of_numerical_error_occurrences :0number_of_ocean_filter_occurrences :0number_of_optimal_estimation_error_occurrences :0number_of_other_boundary_convergence_error_occurrences :0number_of_ozone_range_error_occurrences :0number_of_pixel_level_input_data_missing_occurrences :0number_of_pixel_or_scanline_index_filter_occurrences :0number_of_processed_pixels :1460250number_of_profile_error_occurrences :0number_of_psurf_fresco_stdv_filter_occurrences :0number_of_radiance_missing_occurrences :0number_of_radiative_transfer_error_occurrences :0number_of_reflectance_range_error_occurrences :0number_of_refl_cirrus_viirs_nir_filter_occurrences :0number_of_refl_cirrus_viirs_swir_filter_occurrences :0number_of_rejected_pixels_not_enough_spectrum :0number_of_saturation_error_occurrences :0number_of_saturation_warning_occurrences :0number_of_signal_to_noise_ratio_error_occurrences :0number_of_signal_to_noise_ratio_warning_occurrences :0number_of_slant_column_density_error_occurrences :0number_of_small_pixel_radiance_std_filter_occurrences :0number_of_snow_ice_filter_occurrences :0number_of_snow_ice_warning_occurrences :0number_of_snr_range_error_occurrences :0number_of_so2_volcanic_origin_certain_warning_occurrences :0number_of_so2_volcanic_origin_likely_warning_occurrences :0number_of_solar_eclipse_filter_occurrences :0number_of_south_atlantic_anomaly_warning_occurrences :0number_of_successfully_processed_pixels :1238182number_of_sun_glint_correction_occurrences :0number_of_sun_glint_filter_occurrences :0number_of_sun_glint_warning_occurrences :84516number_of_svd_error_occurrences :0number_of_sza_range_error_occurrences :222037number_of_time_range_filter_occurrences :0number_of_vertical_column_density_error_occurrences :0number_of_vza_range_error_occurrences :0number_of_wavelength_calibration_error_occurrences :0number_of_wavelength_calibration_warning_occurrences :0number_of_wavelength_offset_error_occurrences :0number_of_wrong_input_type_error_occurrences :0time_for_algorithm_initialization :23.927286time_for_processing :212.96926time_per_pixel :0.001535914273508207time_standard_deviation_per_pixel :2.56573665351871e-05algorithm_version :1.1.0build_date :2018-07-04T06:04:00Zcdm_data_type :SwathConventions :CF-1.7cpp_compiler_flags :-g -O2 -fPIC -std=c++11 -W -Wall -Wno-ignored-qualifiers -Wno-write-strings -Wno-unused-variable -DTROPNLL2DPcpp_compiler_version :g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)creator_email :EOSupport@Copernicus.esa.intcreator_name :The Sentinel 5 Precursor TROPOMI Level 2 products are developed with funding from the European Space Agency (ESA), the Netherlands Space Office (NSO), the Belgian Science Policy Office, the German Aerospace Center (DLR) and the Bayerisches Staatsministerium f\u00fcr Wirtschaft und Medien, Energie und Technologie (StMWi).creator_url :http://www.tropomi.eudate_created :2018-08-22T19:32:01Zf90_compiler_flags :-gdwarf-3 -O2 -fPIC -cpp -ffpe-trap=invalid -fno-range-check -frecursive -fimplicit-none -ffree-line-length-none -DTROPNLL2DP -Wuninitialized -Wtabsf90_compiler_version :GNU Fortran (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)geolocation_grid_from_band :3geospatial_lat_max :89.972939geospatial_lat_min :-86.81926geospatial_lon_max :-179.99773geospatial_lon_min :179.99924history :2018-08-22 19:35:59 f_s5pops tropnll2dp /mnt/data1/storage_offl_l2/cache_offl_l2/WORKING-564386761/JobOrder.564386738.xmlid :S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822identifier_product_doi :10.5270/S5P-0wafvafidentifier_product_doi_authority :http://dx.doi.org/institution :KNMIkeywords :0300 Atmospheric Composition and Structure; 0305 Aerosols and Particles; 0360 Radiation, Transmission and Scattering; 3311 Clouds and Aerosols; 3360 Remote Sensingkeywords_vocabulary :AGU index terms, http://publications.agu.org/author-resource-center/index-terms/license :No conditions applynaming_authority :nl.knmiorbit :4361platform :S5Pprocessing_status :Nominalprocessor_version :1.1.0product_version :1.0.0project :Sentinel 5 precursor/TROPOMIreferences :http://www.tropomi.eu/data-products/aerosol-indexrevision_control_identifier :df649da886dbsensor :TROPOMIsource :Sentinel 5 precursor, TROPOMI, space-borne remote sensing, L2spatial_resolution :7x3.5km2standard_name_vocabulary :NetCDF Climate and Forecast Metadata Conventions Standard Name Table (v29, 08 July 2015), http://cfconventions.org/standard-names.htmlStatus_MET_2D :Nominalsummary :TROPOMI/S5P Aerosol Index 1-Orbit L2 Swath 7x3.5kmtime_coverage_duration :PT3503.000Stime_coverage_end :2018-08-16T19:50:14Ztime_coverage_resolution :PT1.080Stime_coverage_start :2018-08-16T18:51:51Ztime_reference :2018-08-16T00:00:00Ztime_reference_days_since_1950 :25064time_reference_julian_day :2458346.5time_reference_seconds_since_1970 :1534377600title :TROPOMI/S5P Aerosol Index 1-Orbit L2 Swath 7x3.5kmtracking_id :0b05f3ea-704b-4ab5-8df3-cf6e31764960</li></ul> In\u00a0[206]: Copied! <pre>rds_crs_set = rds_crs_set.rio.set_crs(4326)\nrds_crs_set.rio.crs\n</pre> rds_crs_set = rds_crs_set.rio.set_crs(4326) rds_crs_set.rio.crs Out[206]: <pre>CRS.from_epsg(4326)</pre> In\u00a0[215]: Copied! <pre>rds_crs_set = rds_crs_set.rio.write_coordinate_system()\n</pre> rds_crs_set = rds_crs_set.rio.write_coordinate_system() In\u00a0[217]: Copied! <pre>r1_ai_crs_set = rds_crs_set['aerosol_index_340_380']\nr1_ai_crs_set\n</pre> r1_ai_crs_set = rds_crs_set['aerosol_index_340_380'] r1_ai_crs_set Out[217]: <pre>&lt;xarray.DataArray 'aerosol_index_340_380' (time: 1, y: 3245, x: 450)&gt;\narray([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)\nCoordinates:\n  * y            (y) float64 3.244e+03 3.243e+03 3.242e+03 ... 2.0 1.0 0.0\n  * x            (x) float64 0.0 1.0 2.0 3.0 4.0 ... 446.0 447.0 448.0 449.0\n  * time         (time) int64 272073600\n    spatial_ref  int64 0\n    latitude     (time, y, x) float32 53.289627 53.328514 ... -68.705986\n    longitude    (time, y, x) float32 119.45596 119.32921 ... 1.9235005\nAttributes:\n    ancillary_variables:     aerosol_index_340_380_precision\n    comment:                 Aerosol index from 380 and 340 nm\n    coordinates:             longitude latitude\n    long_name:               Aerosol index from 380 and 340 nm\n    proposed_standard_name:  ultraviolet_aerosol_index\n    radiation_wavelength:    [340. 380.]\n    units:                   1\n    _FillValue:              9.969209968386869e+36\n    scale_factor:            1.0\n    add_offset:              0.0\n    grid_mapping:            spatial_ref</pre>xarray.DataArray'aerosol_index_340_380'<ul><li>time: 1</li><li>y: 3245</li><li>x: 450</li></ul><ul><li>9.96921e+36 9.96921e+36 9.96921e+36 ... 9.96921e+36 9.96921e+36<pre>array([[[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        ...,\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36],\n        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n         9.96921e+36, 9.96921e+36]]], dtype=float32)</pre></li><li>Coordinates: (6)<ul><li>y(y)float643.244e+03 3.243e+03 ... 1.0 0.0axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([3.244e+03, 3.243e+03, 3.242e+03, ..., 2.000e+00, 1.000e+00, 0.000e+00])</pre></li><li>x(x)float640.0 1.0 2.0 ... 447.0 448.0 449.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([  0.,   1.,   2., ..., 447., 448., 449.])</pre></li><li>time(time)int64272073600<pre>array([272073600])</pre></li><li>spatial_ref()int640GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0<pre>array(0)</pre></li><li>latitude(time, y, x)float3253.289627 53.328514 ... -68.705986bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_boundslong_name :pixel center latitudestandard_name :latitudeunits :degrees_northvalid_max :90valid_min :-90_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 53.289627,  53.328514,  53.366734, ...,  60.851288,\n          60.861183,  60.871014],\n        [ 53.34224 ,  53.3812  ,  53.419495, ...,  60.912846,\n          60.922707,  60.9325  ],\n        [ 53.39479 ,  53.433826,  53.4722  , ...,  60.974384,\n          60.984215,  60.993973],\n        ...,\n        [-85.21351 , -85.27658 , -85.337944, ..., -68.85199 ,\n         -68.77097 , -68.68882 ],\n        [-85.24866 , -85.31239 , -85.37443 , ..., -68.860825,\n         -68.77974 , -68.69752 ],\n        [-85.28333 , -85.34773 , -85.41043 , ..., -68.869446,\n         -68.788284, -68.705986]]], dtype=float32)</pre></li><li>longitude(time, y, x)float32119.45596 119.32921 ... 1.9235005bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/longitude_boundslong_name :pixel center longitudestandard_name :longitudeunits :degrees_eastvalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([[[ 119.45596  ,  119.32921  ,  119.20397  , ...,   76.279495 ,\n           76.114655 ,   75.9475   ],\n        [ 119.51897  ,  119.39211  ,  119.266785 , ...,   76.27593  ,\n           76.110756 ,   75.94327  ],\n        [ 119.5822   ,  119.45525  ,  119.32983  , ...,   76.27239  ,\n           76.106895 ,   75.939064 ],\n        ...,\n        [-135.26692  , -134.51474  , -133.75343  , ...,    1.4409312,\n            1.5062535,    1.5720007],\n        [-135.85966  , -135.1101   , -134.3511   , ...,    1.6179553,\n            1.6826414,    1.7477475],\n        [-136.46013  , -135.71352  , -134.95714  , ...,    1.7949897,\n            1.8590373,    1.9235005]]], dtype=float32)</pre></li></ul></li><li>Attributes: (11)ancillary_variables :aerosol_index_340_380_precisioncomment :Aerosol index from 380 and 340 nmcoordinates :longitude latitudelong_name :Aerosol index from 380 and 340 nmproposed_standard_name :ultraviolet_aerosol_indexradiation_wavelength :[340. 380.]units :1_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref</li></ul> In\u00a0[221]: Copied! <pre>r1_ai_crs_set = r1_ai_crs_set.rio.set_crs(4326, True)\n</pre> r1_ai_crs_set = r1_ai_crs_set.rio.set_crs(4326, True) In\u00a0[223]: Copied! <pre>r1_ai_crs_set = r1_ai_crs_set.rio.write_coordinate_system()\n</pre> r1_ai_crs_set = r1_ai_crs_set.rio.write_coordinate_system() In\u00a0[249]: Copied! <pre>r1_ai_crs_set.longitude.attrs\n</pre> r1_ai_crs_set.longitude.attrs Out[249]: <pre>{'bounds': '/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/longitude_bounds',\n 'long_name': 'pixel center longitude',\n 'standard_name': 'longitude',\n 'units': 'degrees_east',\n 'valid_max': 180,\n 'valid_min': -180,\n '_FillValue': 9.969209968386869e+36,\n 'scale_factor': 1.0,\n 'add_offset': 0.0,\n 'grid_mapping': 'spatial_ref'}</pre> In\u00a0[258]: Copied! <pre>r1_ai_crs_set.latitude[0][0]\n</pre> r1_ai_crs_set.latitude[0][0] Out[258]: <pre>&lt;xarray.DataArray 'latitude' (x: 450)&gt;\narray([53.289627, 53.328514, 53.366734, 53.404312, 53.44126 , 53.477608,\n       53.51336 , 53.54854 , 53.58316 , 53.61724 , 53.65079 , 53.68383 ,\n       53.716366, 53.748417, 53.77999 , 53.811104, 53.841766, 53.87199 ,\n       53.901783, 53.93116 , 53.96013 , 53.9887  , 54.03083 , 54.085697,\n       54.13913 , 54.1912  , 54.24196 , 54.29147 , 54.339783, 54.386948,\n       54.433014, 54.478027, 54.522026, 54.565052, 54.607143, 54.648335,\n       54.68866 , 54.728153, 54.76684 , 54.80476 , 54.84193 , 54.87838 ,\n       54.91413 , 54.949215, 54.983654, 55.01746 , 55.050663, 55.083282,\n       55.115334, 55.146835, 55.177803, 55.20826 , 55.238216, 55.26769 ,\n       55.29669 , 55.32524 , 55.353348, 55.381027, 55.40829 , 55.435154,\n       55.46162 , 55.48771 , 55.513424, 55.53878 , 55.563786, 55.58845 ,\n       55.612785, 55.636795, 55.660496, 55.683887, 55.70698 , 55.729786,\n       55.752308, 55.774555, 55.79653 , 55.81825 , 55.839714, 55.860928,\n       55.881897, 55.902634, 55.923138, 55.943417, 55.963474, 55.98332 ,\n       56.002953, 56.022385, 56.041615, 56.06065 , 56.079494, 56.098152,\n       56.116627, 56.134926, 56.15305 , 56.171   , 56.18879 , 56.206413,\n       56.223877, 56.241188, 56.258347, 56.275352, 56.292213, 56.308933,\n       56.325516, 56.341957, 56.358265, 56.374443, 56.39049 , 56.406418,\n       56.42222 , 56.437897, 56.45346 , 56.468906, 56.484238, 56.49946 ,\n       56.514572, 56.52958 , 56.54448 , 56.559277, 56.573975, 56.588577,\n...\n       59.069664, 59.082832, 59.09606 , 59.109356, 59.12271 , 59.136127,\n       59.149612, 59.163162, 59.17678 , 59.190468, 59.20422 , 59.218044,\n       59.231937, 59.2459  , 59.259937, 59.274048, 59.28823 , 59.30249 ,\n       59.316826, 59.331238, 59.345726, 59.360294, 59.37494 , 59.389668,\n       59.40448 , 59.419373, 59.434345, 59.449406, 59.464554, 59.479782,\n       59.495102, 59.51051 , 59.52601 , 59.541595, 59.557274, 59.573044,\n       59.588905, 59.604862, 59.620914, 59.637066, 59.65331 , 59.66965 ,\n       59.686092, 59.702637, 59.719276, 59.73602 , 59.752865, 59.769814,\n       59.786865, 59.80402 , 59.82128 , 59.83865 , 59.856125, 59.873703,\n       59.891396, 59.90919 , 59.9271  , 59.945114, 59.96324 , 59.98148 ,\n       59.999825, 60.018284, 60.036854, 60.05553 , 60.07432 , 60.093224,\n       60.112236, 60.13136 , 60.15059 , 60.16993 , 60.189377, 60.20893 ,\n       60.228592, 60.24836 , 60.268227, 60.288197, 60.308266, 60.32843 ,\n       60.348686, 60.369038, 60.389473, 60.409996, 60.430595, 60.451267,\n       60.47201 , 60.49282 , 60.513687, 60.534603, 60.555557, 60.57655 ,\n       60.597565, 60.61859 , 60.63962 , 60.66064 , 60.676384, 60.68687 ,\n       60.697346, 60.70781 , 60.718254, 60.728683, 60.739086, 60.74947 ,\n       60.759827, 60.770153, 60.780445, 60.790703, 60.800922, 60.8111  ,\n       60.821228, 60.831303, 60.841328, 60.851288, 60.861183, 60.871014],\n      dtype=float32)\nCoordinates:\n    y            float64 3.244e+03\n  * x            (x) float64 0.0 1.0 2.0 3.0 4.0 ... 446.0 447.0 448.0 449.0\n    time         int64 272073600\n    spatial_ref  int64 0\n    latitude     (x) float32 53.289627 53.328514 ... 60.861183 60.871014\n    longitude    (x) float32 119.45596 119.32921 119.20397 ... 76.114655 75.9475\nAttributes:\n    bounds:         /PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_bounds\n    long_name:      pixel center latitude\n    standard_name:  latitude\n    units:          degrees_north\n    valid_max:      90\n    valid_min:      -90\n    _FillValue:     9.969209968386869e+36\n    scale_factor:   1.0\n    add_offset:     0.0\n    grid_mapping:   spatial_ref</pre>xarray.DataArray'latitude'<ul><li>x: 450</li></ul><ul><li>53.289627 53.328514 53.366734 ... 60.851288 60.861183 60.871014<pre>array([53.289627, 53.328514, 53.366734, 53.404312, 53.44126 , 53.477608,\n       53.51336 , 53.54854 , 53.58316 , 53.61724 , 53.65079 , 53.68383 ,\n       53.716366, 53.748417, 53.77999 , 53.811104, 53.841766, 53.87199 ,\n       53.901783, 53.93116 , 53.96013 , 53.9887  , 54.03083 , 54.085697,\n       54.13913 , 54.1912  , 54.24196 , 54.29147 , 54.339783, 54.386948,\n       54.433014, 54.478027, 54.522026, 54.565052, 54.607143, 54.648335,\n       54.68866 , 54.728153, 54.76684 , 54.80476 , 54.84193 , 54.87838 ,\n       54.91413 , 54.949215, 54.983654, 55.01746 , 55.050663, 55.083282,\n       55.115334, 55.146835, 55.177803, 55.20826 , 55.238216, 55.26769 ,\n       55.29669 , 55.32524 , 55.353348, 55.381027, 55.40829 , 55.435154,\n       55.46162 , 55.48771 , 55.513424, 55.53878 , 55.563786, 55.58845 ,\n       55.612785, 55.636795, 55.660496, 55.683887, 55.70698 , 55.729786,\n       55.752308, 55.774555, 55.79653 , 55.81825 , 55.839714, 55.860928,\n       55.881897, 55.902634, 55.923138, 55.943417, 55.963474, 55.98332 ,\n       56.002953, 56.022385, 56.041615, 56.06065 , 56.079494, 56.098152,\n       56.116627, 56.134926, 56.15305 , 56.171   , 56.18879 , 56.206413,\n       56.223877, 56.241188, 56.258347, 56.275352, 56.292213, 56.308933,\n       56.325516, 56.341957, 56.358265, 56.374443, 56.39049 , 56.406418,\n       56.42222 , 56.437897, 56.45346 , 56.468906, 56.484238, 56.49946 ,\n       56.514572, 56.52958 , 56.54448 , 56.559277, 56.573975, 56.588577,\n...\n       59.069664, 59.082832, 59.09606 , 59.109356, 59.12271 , 59.136127,\n       59.149612, 59.163162, 59.17678 , 59.190468, 59.20422 , 59.218044,\n       59.231937, 59.2459  , 59.259937, 59.274048, 59.28823 , 59.30249 ,\n       59.316826, 59.331238, 59.345726, 59.360294, 59.37494 , 59.389668,\n       59.40448 , 59.419373, 59.434345, 59.449406, 59.464554, 59.479782,\n       59.495102, 59.51051 , 59.52601 , 59.541595, 59.557274, 59.573044,\n       59.588905, 59.604862, 59.620914, 59.637066, 59.65331 , 59.66965 ,\n       59.686092, 59.702637, 59.719276, 59.73602 , 59.752865, 59.769814,\n       59.786865, 59.80402 , 59.82128 , 59.83865 , 59.856125, 59.873703,\n       59.891396, 59.90919 , 59.9271  , 59.945114, 59.96324 , 59.98148 ,\n       59.999825, 60.018284, 60.036854, 60.05553 , 60.07432 , 60.093224,\n       60.112236, 60.13136 , 60.15059 , 60.16993 , 60.189377, 60.20893 ,\n       60.228592, 60.24836 , 60.268227, 60.288197, 60.308266, 60.32843 ,\n       60.348686, 60.369038, 60.389473, 60.409996, 60.430595, 60.451267,\n       60.47201 , 60.49282 , 60.513687, 60.534603, 60.555557, 60.57655 ,\n       60.597565, 60.61859 , 60.63962 , 60.66064 , 60.676384, 60.68687 ,\n       60.697346, 60.70781 , 60.718254, 60.728683, 60.739086, 60.74947 ,\n       60.759827, 60.770153, 60.780445, 60.790703, 60.800922, 60.8111  ,\n       60.821228, 60.831303, 60.841328, 60.851288, 60.861183, 60.871014],\n      dtype=float32)</pre></li><li>Coordinates: (6)<ul><li>y()float643.244e+03axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_north<pre>array(3244.)</pre></li><li>x(x)float640.0 1.0 2.0 ... 447.0 448.0 449.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([  0.,   1.,   2., ..., 447., 448., 449.])</pre></li><li>time()int64272073600<pre>array(272073600)</pre></li><li>spatial_ref()int640GeoTransform :-0.5 1.0 0.0 3244.5 0.0 -1.0<pre>array(0)</pre></li><li>latitude(x)float3253.289627 53.328514 ... 60.871014bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_boundslong_name :pixel center latitudestandard_name :latitudeunits :degrees_northvalid_max :90valid_min :-90_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([53.289627, 53.328514, 53.366734, 53.404312, 53.44126 , 53.477608,\n       53.51336 , 53.54854 , 53.58316 , 53.61724 , 53.65079 , 53.68383 ,\n       53.716366, 53.748417, 53.77999 , 53.811104, 53.841766, 53.87199 ,\n       53.901783, 53.93116 , 53.96013 , 53.9887  , 54.03083 , 54.085697,\n       54.13913 , 54.1912  , 54.24196 , 54.29147 , 54.339783, 54.386948,\n       54.433014, 54.478027, 54.522026, 54.565052, 54.607143, 54.648335,\n       54.68866 , 54.728153, 54.76684 , 54.80476 , 54.84193 , 54.87838 ,\n       54.91413 , 54.949215, 54.983654, 55.01746 , 55.050663, 55.083282,\n       55.115334, 55.146835, 55.177803, 55.20826 , 55.238216, 55.26769 ,\n       55.29669 , 55.32524 , 55.353348, 55.381027, 55.40829 , 55.435154,\n       55.46162 , 55.48771 , 55.513424, 55.53878 , 55.563786, 55.58845 ,\n       55.612785, 55.636795, 55.660496, 55.683887, 55.70698 , 55.729786,\n       55.752308, 55.774555, 55.79653 , 55.81825 , 55.839714, 55.860928,\n       55.881897, 55.902634, 55.923138, 55.943417, 55.963474, 55.98332 ,\n       56.002953, 56.022385, 56.041615, 56.06065 , 56.079494, 56.098152,\n       56.116627, 56.134926, 56.15305 , 56.171   , 56.18879 , 56.206413,\n       56.223877, 56.241188, 56.258347, 56.275352, 56.292213, 56.308933,\n       56.325516, 56.341957, 56.358265, 56.374443, 56.39049 , 56.406418,\n       56.42222 , 56.437897, 56.45346 , 56.468906, 56.484238, 56.49946 ,\n       56.514572, 56.52958 , 56.54448 , 56.559277, 56.573975, 56.588577,\n...\n       59.069664, 59.082832, 59.09606 , 59.109356, 59.12271 , 59.136127,\n       59.149612, 59.163162, 59.17678 , 59.190468, 59.20422 , 59.218044,\n       59.231937, 59.2459  , 59.259937, 59.274048, 59.28823 , 59.30249 ,\n       59.316826, 59.331238, 59.345726, 59.360294, 59.37494 , 59.389668,\n       59.40448 , 59.419373, 59.434345, 59.449406, 59.464554, 59.479782,\n       59.495102, 59.51051 , 59.52601 , 59.541595, 59.557274, 59.573044,\n       59.588905, 59.604862, 59.620914, 59.637066, 59.65331 , 59.66965 ,\n       59.686092, 59.702637, 59.719276, 59.73602 , 59.752865, 59.769814,\n       59.786865, 59.80402 , 59.82128 , 59.83865 , 59.856125, 59.873703,\n       59.891396, 59.90919 , 59.9271  , 59.945114, 59.96324 , 59.98148 ,\n       59.999825, 60.018284, 60.036854, 60.05553 , 60.07432 , 60.093224,\n       60.112236, 60.13136 , 60.15059 , 60.16993 , 60.189377, 60.20893 ,\n       60.228592, 60.24836 , 60.268227, 60.288197, 60.308266, 60.32843 ,\n       60.348686, 60.369038, 60.389473, 60.409996, 60.430595, 60.451267,\n       60.47201 , 60.49282 , 60.513687, 60.534603, 60.555557, 60.57655 ,\n       60.597565, 60.61859 , 60.63962 , 60.66064 , 60.676384, 60.68687 ,\n       60.697346, 60.70781 , 60.718254, 60.728683, 60.739086, 60.74947 ,\n       60.759827, 60.770153, 60.780445, 60.790703, 60.800922, 60.8111  ,\n       60.821228, 60.831303, 60.841328, 60.851288, 60.861183, 60.871014],\n      dtype=float32)</pre></li><li>longitude(x)float32119.45596 119.32921 ... 75.9475bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/longitude_boundslong_name :pixel center longitudestandard_name :longitudeunits :degrees_eastvalid_max :180valid_min :-180_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref<pre>array([119.45596 , 119.32921 , 119.20397 , 119.080215, 118.9579  ,\n       118.837   , 118.717476, 118.5993  , 118.48243 , 118.36685 ,\n       118.252525, 118.13943 , 118.027534, 117.91682 , 117.80725 ,\n       117.69881 , 117.59147 , 117.485214, 117.38001 , 117.275856,\n       117.17271 , 117.07056 , 116.91917 , 116.72061 , 116.52572 ,\n       116.33434 , 116.14636 , 115.961655, 115.780106, 115.601616,\n       115.42607 , 115.25338 , 115.08345 , 114.91619 , 114.75152 ,\n       114.58936 , 114.42963 , 114.272255, 114.11717 , 113.96432 ,\n       113.813614, 113.66501 , 113.51843 , 113.37385 , 113.231186,\n       113.0904  , 112.95143 , 112.81425 , 112.67879 , 112.54501 ,\n       112.41288 , 112.28235 , 112.15338 , 112.02593 , 111.89996 ,\n       111.77545 , 111.65234 , 111.53062 , 111.41024 , 111.29118 ,\n       111.1734  , 111.056885, 110.94159 , 110.82749 , 110.71457 ,\n       110.60279 , 110.492134, 110.382576, 110.27409 , 110.16666 ,\n       110.06024 , 109.95484 , 109.85042 , 109.74696 , 109.64445 ,\n       109.542854, 109.44217 , 109.34238 , 109.24345 , 109.14536 ,\n       109.04812 , 108.95169 , 108.856064, 108.76122 , 108.667145,\n       108.57383 , 108.481255, 108.3894  , 108.29826 , 108.20782 ,\n       108.118065, 108.02898 , 107.94055 , 107.852776, 107.76563 ,\n       107.67911 , 107.5932  , 107.50789 , 107.42317 , 107.33903 ,\n...\n        90.14031 ,  90.04494 ,  89.94877 ,  89.85178 ,  89.75397 ,\n        89.655304,  89.555786,  89.455376,  89.35407 ,  89.25185 ,\n        89.14868 ,  89.04456 ,  88.93946 ,  88.83337 ,  88.72624 ,\n        88.61809 ,  88.50886 ,  88.398544,  88.28712 ,  88.17455 ,\n        88.06083 ,  87.945915,  87.82979 ,  87.71241 ,  87.593765,\n        87.47382 ,  87.35255 ,  87.229904,  87.105865,  86.9804  ,\n        86.85348 ,  86.72505 ,  86.595085,  86.463554,  86.33041 ,\n        86.1956  ,  86.059105,  85.92087 ,  85.78085 ,  85.639   ,\n        85.49528 ,  85.34962 ,  85.20198 ,  85.052315,  84.90055 ,\n        84.74664 ,  84.59053 ,  84.43214 ,  84.271416,  84.10829 ,\n        83.94269 ,  83.77454 ,  83.603775,  83.4303  ,  83.254036,\n        83.074905,  82.89281 ,  82.70765 ,  82.51933 ,  82.32776 ,\n        82.13281 ,  81.93439 ,  81.73236 ,  81.5266  ,  81.316986,\n        81.10338 ,  80.88563 ,  80.66358 ,  80.43708 ,  80.20595 ,\n        79.97002 ,  79.72909 ,  79.48296 ,  79.23144 ,  78.97427 ,\n        78.77756 ,  78.64453 ,  78.50995 ,  78.373795,  78.236015,\n        78.09658 ,  77.95546 ,  77.812614,  77.66799 ,  77.52156 ,\n        77.37328 ,  77.2231  ,  77.07098 ,  76.91686 ,  76.76071 ,\n        76.60247 ,  76.442085,  76.279495,  76.114655,  75.9475  ],\n      dtype=float32)</pre></li></ul></li><li>Attributes: (10)bounds :/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/latitude_boundslong_name :pixel center latitudestandard_name :latitudeunits :degrees_northvalid_max :90valid_min :-90_FillValue :9.969209968386869e+36scale_factor :1.0add_offset :0.0grid_mapping :spatial_ref</li></ul> In\u00a0[250]: Copied! <pre>r1_ai_crs_set.rio.to_raster('ai_crs_try1_rio.tif')\n</pre> r1_ai_crs_set.rio.to_raster('ai_crs_try1_rio.tif') In\u00a0[114]: Copied! <pre>import rasterio\n</pre> import rasterio In\u00a0[172]: Copied! <pre>base_file_handle = rasterio.open(file_path)\nbase_file_handle.subdatasets[:5]\n</pre> base_file_handle = rasterio.open(file_path) base_file_handle.subdatasets[:5] Out[172]: <pre>['netcdf:TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc:/PRODUCT/latitude',\n 'netcdf:TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc:/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/satellite_latitude',\n 'netcdf:TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc:/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/satellite_longitude',\n 'netcdf:TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc:/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/satellite_altitude',\n 'netcdf:TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc:/PRODUCT/SUPPORT_DATA/GEOLOCATIONS/satellite_orbit_phase']</pre> In\u00a0[173]: Copied! <pre>r2 = rasterio.open('netcdf:./'+file_path+\":/PRODUCT/aerosol_index_340_380\")\ntype(r2)\n</pre> r2 = rasterio.open('netcdf:./'+file_path+\":/PRODUCT/aerosol_index_340_380\") type(r2) Out[173]: <pre>rasterio.io.DatasetReader</pre> In\u00a0[174]: Copied! <pre>ai_data = r2.read()\ntype(ai_data)\n</pre> ai_data = r2.read() type(ai_data) Out[174]: <pre>numpy.ndarray</pre> In\u00a0[175]: Copied! <pre>ai_data.shape\n</pre> ai_data.shape Out[175]: <pre>(1, 3245, 450)</pre> In\u00a0[176]: Copied! <pre>plt.imshow(ai_data[0])\n</pre> plt.imshow(ai_data[0]) Out[176]: <pre>&lt;matplotlib.image.AxesImage at 0x196e8b8b0&gt;</pre> In\u00a0[3]: Copied! <pre>!gdal_translate NETCDF:\"TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc\":/PRODUCT/aerosol_index_340_380 try1.tif\n</pre> !gdal_translate NETCDF:\"TROPOMI_PythonCodesAndData/S5P_OFFL_L2__AER_AI_20180816T183016_20180816T201146_04361_01_010100_20180822T174822.nc\":/PRODUCT/aerosol_index_340_380 try1.tif <pre>Input file size is 450, 3245\nWarning 1: Metadata exceeding 32000 bytes cannot be written into GeoTIFF. Transferred to PAM instead.\n0...10...20...30...40...50...60...70...80...90...100 - done.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#xarray-multidimensional-science-data","title":"Xarray - multidimensional science data\u00b6","text":"<p>This notebook reads aerosol index and Tropospheric $NO_{2}$ Concentration from Sentinel-5P TROPOMI data using Opengeo Tools</p>"},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#explore-netcdf-file-for-its-contents","title":"Explore NetCDF file for its contents\u00b6","text":"<p>The NetCDF file is like a folder with multiple sub-folders and files within it. Folders are called as <code>groups</code> and files within it are called as <code>variables</code>. NASA supplies a cross-platform app called Panoply which gives you a UI to query and visualize NetCDF files. Below is a screenshot of Panoply reading the Aerosol Index file.</p>"},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#read-aerosol-index-354-388-nm-into-memory","title":"Read Aerosol Index 354 - 388 nm into memory\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#reading-using-xarray","title":"Reading using <code>xarray</code>\u00b6","text":"<p>See https://github.com/acgeospatial/Sentinel-5P/blob/master/sentinel5p_xarray_blog.ipynb</p>"},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#plot-using-matplotlib","title":"Plot using matplotlib\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#plotting-using-cartopy","title":"Plotting using <code>cartopy</code>\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#reading-using-rioxarray","title":"Reading using <code>rioxarray</code>\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#reading-using-rasterio","title":"Reading using rasterio\u00b6","text":""},{"location":"teaching_resources/xarray/xarray_multidim_dataset/#convert-to-geotiff-using-gdal","title":"Convert to GeoTIFF using GDAL\u00b6","text":""},{"location":"blog/category/parallel-processing/","title":"parallel processing","text":""},{"location":"blog/category/python/","title":"python","text":""},{"location":"blog/category/threads/","title":"threads","text":""},{"location":"blog/category/processes/","title":"processes","text":""},{"location":"blog/category/serverless/","title":"serverless","text":""},{"location":"blog/category/aws/","title":"aws","text":""},{"location":"blog/category/cloud/","title":"cloud","text":""},{"location":"blog/category/arcgis/","title":"arcgis","text":""},{"location":"blog/category/data-science/","title":"data-science","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/page/5/","title":"Blog","text":""},{"location":"blog/page/6/","title":"Blog","text":""},{"location":"blog/page/7/","title":"Blog","text":""}]}