{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Atma's blog","text":"<p>I am an avid geospatial scientist / engineer with backgrounds in software development, GIS, thermal &amp; hyperspectral remote sensing and several other areas of geospatial tech. In the recent years, I have been working on using Python to teach math and science and at the intersection of statistics, data science and geospatial technologies.</p> <p>In this personal website, you can find some of my past projects, cheatsheets I developed while learning something new and the articles and talks I have delivered.</p>"},{"location":"sql-1/","title":"SQL Introduction","text":"<p>Content derived from:  - https://www.sqltutorial.org/  - https://www.sqlitetutorial.net/</p> <p>SQL consists of </p> <ul> <li>Data definition language : CREATE TABLE, ALTER TABLE, CREATE DATABASE etc.</li> <li>Data manipulation language: SELECT, INSERT, UPDATE, DELETE statements </li> <li>Data control language: GRANT USER, REVOKE USER etc.</li> </ul>"},{"location":"sql-1/#sql-standards","title":"SQL Standards","text":"<p>SQL was first created in 1970. ANSI then published the first SQL Standard in 1986, the second in 1992 called SQL92 or SQL2, third in 1999 called SQL99 or SQL3. The latest is SQL:2011</p>"},{"location":"sql-1/#4-basic-ops-in-a-database","title":"4 basic ops in a database:","text":"<p>CRUD - Create, Read (Select), Update, Delete (Drop). In this page, we use <code>sqlite3</code> for the database. SQLite is tiny, portable, fast, light-weight database and works on all kinds of architectures (phones, laptops, cloud, edge devices etc). To enter sqlite, you type <code>&gt; sqlite3</code>. To exit and return back to bash, you type <code>.quit</code>.</p>"},{"location":"sql-1/#creating-a-table","title":"Creating a table","text":"<p>Use <code>sqlite3</code> database which is portable and tiny. To create a new database, use <code>$ sqlite3 &lt;db_name&gt;</code></p> <pre><code>(scratch) \u279c  sql-playground$ sqlite3 favorites.db\nSQLite version 3.38.3 2022-04-27 12:03:15\nEnter \".help\" for usage hints.\nsqlite&gt; \n</code></pre> <p>Importing CSV into the DB:</p> <pre><code>sqlite&gt; .mode csv\nsqlite&gt; .import ../../temp/src7/favorites/favorites.csv fav\nsqlite&gt; .schema\nCREATE TABLE IF NOT EXISTS \"fav\"(\n\"Timestamp\" TEXT, \"title\" TEXT, \"genres\" TEXT);\nsqlite&gt; \n</code></pre>"},{"location":"sql-1/#opening-back-a-sqlite-db","title":"Opening back a SQLite DB","text":"<p>To open a DB back, use <code>sqlite3 &lt;db name&gt;</code></p> <pre><code>scratch) \u279c  sql-playground ls\nfavorites.db  favorites2.db\n(scratch) \u279c  sql-playground sqlite3 favorites2.db \nSQLite version 3.38.3 2022-04-27 12:03:15\nEnter \".help\" for usage hints.\nsqlite&gt;\n</code></pre>"},{"location":"sql-1/#listing-db-content","title":"Listing DB content","text":"<ul> <li><code>.tables</code> to list tables in the current DB</li> <li><code>.tables &lt;pattern&gt;</code> like <code>.tables f%</code> to open all tables that begin with <code>f</code>.</li> <li><code>.schema</code> to list all table schemas: Shows create table commands for all tb within the db</li> </ul>"},{"location":"sql-1/#reading-select-statements","title":"Reading - Select statements","text":"<p>General syntax: <code>SELECT &lt;columns&gt; FROM &lt;database.table&gt; WHERE &lt;condition&gt;;</code>. YOu need to terminate commands with <code>;</code> Example:</p> <pre><code>SELECT Timestamp, language FROM favorires;\n</code></pre> <p>You can perform operations on the data as you query them out. You can do <code>AVG, COUNT, DISTINCT, LOWER, MAX, MIN, UPPER...</code> etc.</p> <pre><code>sqlite&gt; SELECT DISTINCT(language) FROM favorires;\nC\nPython\nScratch\n\nsqlite&gt; SELECT COUNT(Timestamp) FROM favorires;\n1456\n\nsqlite&gt; SELECT COUNT(DISTINCT(title)) FROM fav;\n107\n</code></pre>"},{"location":"sql-1/#limiting-outputs","title":"Limiting outputs","text":"<p>Use <code>LIMIT &lt;num&gt;</code> to limit what is displayed.</p> <pre><code>sqlite&gt; SELECT title FROM fav LIMIT 5;\n\"How i met your mother\"\n\"The Sopranos\"\n\"Friday Night Lights\"\n\"Family Guy\"\n\"New Girl\"\nsqlite&gt; \n</code></pre>"},{"location":"sql-1/#paging-output-using-offset-and-limit","title":"Paging output using <code>OFFSET</code> and <code>LIMIT</code>","text":"<p>Use <code>LIMIT row_count OFFSET offset</code> syntax to page through the results. You can increment the offset as you page through a large result table. Is is important to <code>ORDER BY</code> when paging to avoid duplicates.</p> <pre><code>--page 2\nsqlite&gt; SELECT employee_id, first_name, last_name FROM employees ORDER BY employee_id LIMIT 5 OFFSET 5;\n\nemployee_id  first_name  last_name\n-----------  ----------  ---------\n105          David       Austin   \n106          Valli       Pataballa\n107          Diana       Lorentz  \n108          Nancy       Greenberg\n109          Daniel      Faviet   \n\n--page 3\nsqlite&gt; SELECT employee_id, first_name, last_name FROM employees ORDER BY employee_id LIMIT 5 OFFSET 10;\nemployee_id  first_name   last_name\n-----------  -----------  ---------\n110          John         Chen     \n111          Ismael       Sciarra  \n112          Jose Manuel  Urman    \n113          Luis         Popp     \n114          Den          Raphaely\n</code></pre>"},{"location":"sql-1/#where-clauses","title":"Where clauses","text":"<p>Equality operations. In sql, <code>=</code> is used for comparison, not <code>==</code>. For text columns, you can use <code>LIKE</code>. YOu can also combine with some simple regex like <code>%string%</code> to indicate any chars before and after the substring.</p> <pre><code>sqlite&gt; SELECT title FROM fav WHERE title= \"office\";\nsqlite&gt; SELECT title FROM fav WHERE title LIKE  \"office\";\nOffice\nOffice\nsqlite&gt; SELECT title FROM fav WHERE title LIKE  \"%office%\";\nOffice\nOffice\n\"The Office\"\n\"The Office\"\n\"The Office\"\n...\n\"ThE OffiCE\"\n\"The Office\"\nThevoffice\n</code></pre>"},{"location":"sql-1/#delete-deleting-records","title":"<code>DELETE</code> - deleting records","text":"<p>use <code>DELETE FROM &lt;table&gt; WHERE &lt;condition&gt;</code> to delete records that match the condition.</p> <pre><code>sqlite&gt; SELECT count(title) FROM fav WHERE title LIKE \"%friends%\";\n9\nsqlite&gt; DELETE FROM fav WHERE title LIKE \"%friends%\";\nsqlite&gt; SELECT count(title) FROM fav WHERE title LIKE \"%friends%\";\n0\nsqlite&gt; \n</code></pre>"},{"location":"sql-1/#update-updating-records","title":"<code>UPDATE</code> - Updating records","text":"<p>Use <code>UPDATE &lt;table&gt; SET &lt;operation&gt; WHERE &lt;condition&gt;</code> syntax. Command will operate on all records that match the given where clause.</p> <pre><code>sqlite&gt; SELECT title FROM fav WHERE title = \"Thevoffice\";\nThevoffice\nsqlite&gt; UPDATE fav SET title = \"The Office\" WHERE title = \"Thevoffice\";\n\n-- Verify\nsqlite&gt; SELECT title FROM fav WHERE title = \"Thevoffice\";\nsqlite&gt; \n</code></pre>"},{"location":"sql-1/#insert-inserting-records","title":"<code>INSERT</code> - Inserting records","text":"<p>Use <code>INSERT INTO &lt;table&gt; (&lt;columns&gt;) VALUES (&lt;comma sep values&gt;);</code>. Example:</p> <pre><code>sqlite&gt; INSERT INTO genres (show_id, genre) VALUES (159, \"Comedy\");\n</code></pre>"},{"location":"sql-1/#utility-commands-for-sqlite","title":"Utility commands for <code>SQLite</code>","text":"<ul> <li>Getting schema of a database: <code>.schema</code></li> <li>Viewing datatypes of each column in a table: <code>pragma table_info('tb_name');</code></li> <li>Timing your queries: <code>sqlite&gt; .timer on</code> will start to return tike taken for your queries</li> <li>Printing column names for select queries - <code>.headers on</code></li> <li>Pretty print with table formatting <code>.mode column</code></li> </ul>"},{"location":"apps/","title":"Web apps","text":""},{"location":"apps/#full-stack-python-web-apps","title":"Full stack Python web apps","text":"<p>Python never ceases to impress me. On one end of the spectrum, you can use it to build simulations, process images and signals and perform data science. On the other end, you can quickly and easily build a full stack web app that is complete with a front-end, REST API and a backend, all in Python. Below are some of my full-stack Python based web apps.</p> <p>Serverless APIs:</p> <ul> <li>Embed maps into your photos</li> <li>Use Deep Learning to predict if a picture is Donut or Bagel</li> <li>Create a map of all incoming web requests to a Lambda function</li> <li>Visualize Google Pluscodes for USA</li> </ul> <p>Learning resources</p> <ul> <li>Before you get started, you need to know a bit about RESTful APIs. My article on the design principles behind RESTful APIs may be helpful</li> <li>Getting started with building RESTful APIs in Python with Flask</li> <li>Then take a look at this example fullstack Flask based Python web app that is published on Heroku.</li> <li>If publishing on AWS Lambda is your preferred route, then take a look at this FastAPI to Lambda function application.</li> </ul>"},{"location":"apps/#hazard-monitoring-apps","title":"Hazard monitoring apps","text":"<ul> <li>SoCal active fire map</li> <li>SoCal active earthquakes</li> <li>USA wild fire hazard potential</li> </ul>"},{"location":"apps/#maps-and-apps-that-i-love","title":"Maps and Apps that I love","text":"<p>For the love of geography, below is a set of maps and apps (mostly built by someone else) that encourage spatial thinking</p> <ul> <li> <p>Urban observatory. A fantastic app to visually compare up to 3 different cities. The app presents 3 map windows (for 3 cities) and locks the map scale, allowing you to observe the size (and magnitude) of those cities. Load up cities in the same country are across the world to see if Los Angeles is really larger than Mumbai and New York. Settle that argument you had in the bar, forever.</p> </li> <li> <p>The living land. An Esri produced story map journeying through the changes mankind has made to the land surface of this planet. With beautiful charts, maps, animations, drone footage this story provides a compelling and an unbiased view of the livable land and how mankind has put it to use.</p> </li> </ul>"},{"location":"apps/firemap/","title":"Up-to-date forest fire information","text":"<p>Data for this map is sourced from FireWhat.com's data and map on ArcGIS.com.</p> <p>See this map in full screen here</p> iFrames are not supported on this page."},{"location":"apps/flask-notes/","title":"Flask notes","text":"<p>post to AWS using zappa.</p>"},{"location":"apps/flask-notes/#server-stack","title":"Server stack","text":"<ul> <li>Web server - load balancing, SLL endpoint, eg: nginx, apache</li> <li>http server - forwards dynamic content requests, servers dynamic content, eg: uWSGI, Gunicorn</li> <li>app server - communicates with back-end res like db, process dynamic queries. eg: Flask, Django, CherryPy</li> </ul> <p>Ending a REST route with a trailing slash will support whether or not the request comes with a trailing slash.</p> <p>When accepting a request with parameters from a user, ensure you scrub the inputs before passing it into a db or such. This is because people can inject sql and can damage or hack the backend.</p> <p>Like <code>templates</code>, <code>static</code> is a reserved and known folder into which you can store pics and files. Flask will just serve it out.</p> <p>Use <code>from flask import url_for</code> and use it when you need to dynamically compose an internal URL dynamically.</p> <p>When accepting a POST endoing, use <code>request.form['password']</code> instead of <code>request.args.get('param')</code>.</p> <p>Use <code>redirect(&lt;new_url_here&gt;, 302)</code> to redirect.</p>"},{"location":"apps/flask-notes/#login","title":"Login","text":"<p><code>pip install Flask-Login</code> library is a simple one. This library is aware of sessions and can manage that. When storing passwords, use password hashes.</p> <pre><code>from werkzeug.security import generate_password_hash\nsecret_pass = generate_password_hash('123')\n</code></pre> <p>You will compare the hash of user password with a dynamic hash when user presents it.</p>"},{"location":"apps/flask-notes/#users","title":"Users","text":"<p><code>flask_login.UserMixin</code> find what a Mixin is. Find if subclass is the right term.</p>"},{"location":"apps/flask-notes/#zappa","title":"Zappa","text":"<p>Zappa makes is very easy to push to AWS. It will zip up the app, push to S3 bucket, deploy via AWS Lambda, create AWS API Gateway endpoint, adds an AWS Cloudwatch rule that keeps Lambda 'warm'. The cloudwatch will keep the lambda running by pinging it every 4 mins or so.</p>"},{"location":"apps/flask-notes/#resources","title":"Resources","text":"<p>Corey Schafer's 15 part Flask on YouTube.</p>"},{"location":"apps/fullstack-python-webapp-1/","title":"Building and publishing a fullstack Python webapp on Heroku","text":"<p>Thermos repo Get this project from the thermos GitHub repo</p> <p>Experiments with a Python's Flask based web server. </p> <p>This tiny project shows how to build RESTful APIs with Flask. You can define endpoints and the code that needs to be executed when invoked. You can send messages via the REST API and act on it in your Python code.</p>"},{"location":"apps/fullstack-python-webapp-1/#quickstart","title":"Quickstart","text":""},{"location":"apps/fullstack-python-webapp-1/#creating-your-dev-env","title":"Creating your dev env","text":"<p>To run, first clone this repo and enter its folder in terminal. Then clone  the dev env using the <code>requirements.txt</code> file. To do so first create a conda env by running:</p> <pre><code>conda create env --name flask2 python\n</code></pre> <p>then install the dependencies by running:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"apps/fullstack-python-webapp-1/#running-this-project","title":"Running this project","text":"<p>All endpoints are defined in a single <code>app.py</code> which is what should be run by the web server. After you activate and install the dependencies, from the terminal run:</p> <pre><code>python -m app\n</code></pre> <p>which will start the web server (usually at http://localhost:5000)and give you an address to hit. There is a simple HTML frontpage that you can use as UI or you can interact with the REST endpoints programmatically. You can also hit the production version at https://atma-thermos.herokuapp.com</p>"},{"location":"apps/fullstack-python-webapp-1/#architecture-of-this-project","title":"Architecture of this project","text":"<p>The goal of this project is to demonstrate building web apps using Python. It demonstrates using <code>Flask</code> to design and implement RESTful APIs, <code>arcgis</code> for geocoding, <code>NASA APIS</code> to call out to an external process, <code>HTML templates</code> to demo injecting content dynamically into web pages and finally <code>sqlalchemy</code> to perform CRUD ops on a database and expose those operations via a REST API.</p>"},{"location":"apps/fullstack-python-webapp-1/#1-apppy","title":"1. <code>app.py</code>","text":"<p>This is the main file. The web server runs this file. All REST endpoints and handlers for each are defined here. For simple operations, the business logic is defined right in the handler. For others, it is abstracted away in separate Python files that are imported.</p> <p>The <code>/</code> route defines what happens when the landing page of the app is called. If you send a GET request, then a HTML page is displayed. This page is defined in the <code>templates/index.html</code>. As you can see from the code, I am able to use Python to inquire the IP address, machine name, system path etc and pass that to the HTML page. The advantage of this is, which ever server runs this app, the details will be populated at run time. This pattern comes in handy later.</p> <p>The <code>eyeFromAbove</code> resource is probably the most complex. When called via a GET, it returns a HTML page with some UI elements. Users can enter values and hit submit which will send a POST call to the same endpoint. When it receives a POST call, it performs the actual operation of geocoding the address and calling out to NASA and Google Earth Engine apps to get a Landsat satellite image of the coordinates. Once it downloads this image (into <code>\\eye_in_sky_queries</code>), it returns the user a HTML page which dynamically displays this satellite image.</p>"},{"location":"apps/fullstack-python-webapp-1/#2-executorpy-and-geocode_toolpy","title":"2. <code>executor.py</code> and <code>geocode_tool.py</code>","text":"<p>These files abstract the logic needed for generating unique random numbers and geocoding addresses to get coordinate information. The <code>geocode_tool.py</code> uses <code>arcgis</code> package for this.</p>"},{"location":"apps/fullstack-python-webapp-1/#3-address_db_modelpy-and-dbopspy","title":"3. <code>address_db_model.py</code> and <code>dbops.py</code>","text":"<p>This part deals with database operations. The model file creates a <code>sqlite</code> db on disk, declares a table and its schema. The rest is handled in the <code>dbops.py</code> file. This file has logic to Create, Read, Update, Delete (CRUD) records. The db file is called <code>addresses.db</code> and the table is called <code>gAddress</code>. Neither of these is required to talk to the DB because <code>sqlalchemy</code> abstracts DB interaction. Thus, the workflow once DB is created is to create a <code>session</code> that connects to this db with a little bit of boilerplate code. Thereafter, I pass around the <code>session</code> variable whenever I need to perform CRUD ops. Only the <code>app.py</code> file can create the session. All other files will use this session to work with the db.</p> <p>The <code>app.py</code> exposes an <code>/addresses</code> endpoint which accepts GET, POST, PUT, DELETE and an index URL endpoint. Ops like geocoding will automatically write a record to the table. In addition, users can call the addresses endpoint via POST to push a bunch of addresses into the db. GET will read all entries. PUT will update and DELETE will remove individual records.</p>"},{"location":"apps/fullstack-python-webapp-1/#4-unittests","title":"4. <code>\\unittests</code>","text":"<p>A critical component of this repo is the unittests. Written using Python's built-in <code>unittest</code> module, these test each endpoint making REST calls. You can run the tests against development as well as production, just change the <code>base_address</code> property.</p> <p>I cannot emphasize the importance of tests enough. I practically learnt about FLASK and how to read arguments vs form data by debugging my failing tests. It was thrilling to attach debugger to both my server and my client code and see calls made and handled.</p>"},{"location":"apps/fullstack-python-webapp-1/#publishing-to-heroku","title":"Publishing to Heroku","text":"<p>These are the general steps</p> <ol> <li>Install all dependencies using <code>pip</code> as conda support is not matured yet.</li> <li>Freeze dependencies using <code>pip freeze -r requirements.txt</code></li> <li>Create a <code>Procfile</code> with contents <code>web: gunicorn app:app</code>. Here we switch from Flask server to Gunicorn which is a light weight, but production ready server.</li> <li>Install Heroku CLI, create an account, make keys, login to client.</li> <li>Create Heroku app as <code>heroku apps:create atma-thermos</code>. Note: you need to name it differently as the name I chose is taken by me.</li> <li>Test locally using <code>heroku local web</code>.</li> <li>Commit your changes. Push to both remotes.</li> </ol> <pre><code>git push origin master\ngit push heroku master\n</code></pre> <p>The last statement of pushing to heroku triggers the actual job of building your app on serverside. Heroku will unpack this repo, set up the env using requirements.txt, then execute the Procfile instructions. This will lead to your app running at the designated address it gave you when you ran the <code>create</code> command.</p> <p>Look at the appendix at the bottom to see the full terminal build output.</p>"},{"location":"apps/fullstack-python-webapp-1/#mistakes","title":"Mistakes","text":"<ol> <li>When authoring the requirements, I forgot to mention that <code>arcgis</code> should be a lite install. This resulted in heroku trying to install the full package. But it looks like it had no problems :)</li> </ol>"},{"location":"apps/fullstack-python-webapp-1/#appendix-build-outputs","title":"Appendix - build outputs","text":"<pre><code>(flask2) thermos (master) $ git push origin master\nEnumerating objects: 15, done.\nCounting objects: 100% (15/15), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (10/10), done.\nWriting objects: 100% (11/11), 171.85 KiB | 10.11 MiB/s, done.\nTotal 11 (delta 5), reused 0 (delta 0)\nremote: Resolving deltas: 100% (5/5), completed with 3 local objects.\nTo github.com:AtmaMani/thermos.git\n   f6304ec..ef5ee7f  master -&gt; master\n(flask2) thermos (master) $ git remote -v\nheroku  https://git.heroku.com/atma-thermos.git (fetch)\nheroku  https://git.heroku.com/atma-thermos.git (push)\norigin  git@github.com:AtmaMani/thermos.git (fetch)\norigin  git@github.com:AtmaMani/thermos.git (push)\n(flask2) thermos (master)* $ git push heroku master\nEnumerating objects: 59, done.\nCounting objects: 100% (59/59), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (52/52), done.\nWriting objects: 100% (59/59), 784.82 KiB | 23.78 MiB/s, done.\nTotal 59 (delta 18), reused 12 (delta 2)\nremote: Compressing source files... done.\nremote: Building source:\nremote: \nremote: -----&gt; Python app detected\nremote: -----&gt; Installing python-3.6.9\nremote: -----&gt; Installing pip\nremote: -----&gt; Installing SQLite3\nremote: -----&gt; Installing requirements with pip\nremote:        Collecting arcgis==1.7.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ef/ae/6d3f73ed54b243584bc9b0b888f7b0751e255737482a1989345747f4d81b/arcgis-1.7.0.tar.gz (1.7MB)\nremote:        Collecting certifi==2019.9.11 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 2))\nremote:          Downloading https://files.pythonhosted.org/packages/18/b0/8146a4f8dd402f60744fa380bc73ca47303cccf8b9190fd16a827281eac2/certifi-2019.9.11-py2.py3-none-any.whl (154kB)\nremote:        Collecting chardet==3.0.4 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 3))\nremote:          Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\nremote:        Collecting Click==7.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 4))\nremote:          Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\nremote:        Collecting Flask==1.1.1 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 5))\nremote:          Downloading https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl (94kB)\nremote:        Collecting gunicorn==19.9.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 6))\nremote:          Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\nremote:        Collecting idna==2.8 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 7)\nremote:          Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\nremote:        Collecting itsdangerous==1.1.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 8))\nremote:          Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\nremote:        Collecting Jinja2==2.10.3 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 9))\nremote:          Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl (125kB)\nremote:        Collecting MarkupSafe==1.1.1 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 10))\nremote:          Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\nremote:        Collecting pytz==2019.3 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 11))\nremote:          Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\nremote:        Collecting requests==2.22.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 12))\nremote:          Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\nremote:        Collecting six==1.12.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 13))\nremote:          Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nremote:        Collecting SQLAlchemy==1.3.10 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 14))\nremote:          Downloading https://files.pythonhosted.org/packages/14/0e/487f7fc1e432cec50d2678f94e4133f2b9e9356e35bacc30d73e8cb831fc/SQLAlchemy-1.3.10.tar.gz (6.0MB)\nremote:        Collecting urllib3==1.25.6 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 15))\nremote:          Downloading https://files.pythonhosted.org/packages/e0/da/55f51ea951e1b7c63a579c09dd7db825bb730ec1fe9c0180fc77bfb31448/urllib3-1.25.6-py2.py3-none-any.whl (125kB)\nremote:        Collecting Werkzeug==0.16.0 (from -r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 16))\nremote:          Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\nremote:        Collecting ipywidgets&gt;=7 (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/56/a0/dbcf5881bb2f51e8db678211907f16ea0a182b232c591a6d6f276985ca95/ipywidgets-7.5.1-py2.py3-none-any.whl (121kB)\nremote:        Collecting widgetsnbextension&gt;=3 (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/6c/7b/7ac231c20d2d33c445eaacf8a433f4e22c60677eb9776c7c5262d7ddee2d/widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2MB)\nremote:        Collecting pandas&gt;=0.23 (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\nremote:        Collecting numpy (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/0e/46/ae6773894f7eacf53308086287897ec568eac9768918d913d5b9d366c5db/numpy-1.17.3-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\nremote:        Collecting matplotlib (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\nremote:        Collecting keyring (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/b1/08/ad1ae7262c8146bee3be360cc766d0261037a90b44872b080a53aaed4e84/keyring-19.2.0-py2.py3-none-any.whl\nremote:        Collecting jupyterlab (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/56/96/5629fec605f0d320f3857241e84704e533ee54eb2aa87c0b69c34bbcc3f2/jupyterlab-1.2.1-py2.py3-none-any.whl (6.4MB)\nremote:        Collecting pyshp&lt;2,&gt;=1.2.11 (from arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/c8/ac/8e9adb8e0dadabbb3b708d38a83119ee42115d9f8fd88858261f5bec50f0/pyshp-1.2.12.tar.gz (193kB)\nremote:        Collecting traitlets&gt;=4.3.1 (from ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl (75kB)\nremote:        Collecting ipython&gt;=4.0.0; python_version &gt;= \"3.3\" (from ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/81/2e/59cdacea6476a4c21b7c090a91250ffbcd085900f5eb9f4e4d68dd2ee4e3/ipython-7.9.0-py3-none-any.whl (775kB)\nremote:        Collecting ipykernel&gt;=4.5.1 (from ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/e1/92/8fec943b5b81078399f969f00557804d884c96fcd0bc296e81a2ed4fd270/ipykernel-5.1.3-py3-none-any.whl (116kB)\nremote:        Collecting nbformat&gt;=4.2.0 (from ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/da/27/9a654d2b6cc1eaa517d1c5a4405166c7f6d72f04f6e7eea41855fe808a46/nbformat-4.4.0-py2.py3-none-any.whl (155kB)\nremote:        Collecting notebook&gt;=4.4.1 (from widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f5/69/d2ffaf7efc20ce47469187e3a41e6e03e17b45de5a6559f4e7ab3eace5e1/notebook-6.0.2-py3-none-any.whl (9.7MB)\nremote:        Collecting python-dateutil&gt;=2.6.1 (from pandas&gt;=0.23-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\nremote:        Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 (from matplotlib-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/a3/c4/7828cf9e71ce8fbd43c1e502f3fdd0498f069fcf9d1c268205ce278ae201/pyparsing-2.4.4-py2.py3-none-any.whl (67kB)\nremote:        Collecting kiwisolver&gt;=1.0.1 (from matplotlib-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\nremote:        Collecting cycler&gt;=0.10 (from matplotlib-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\nremote:        Collecting entrypoints (from keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ac/c6/44694103f8c221443ee6b0041f69e2740d89a25641e62fb4f2ee568f2f9c/entrypoints-0.3-py2.py3-none-any.whl\nremote:        Collecting secretstorage; sys_platform == \"linux\" (from keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/82/59/cb226752e20d83598d7fdcabd7819570b0329a61db07cfbdd21b2ef546e3/SecretStorage-3.1.1-py3-none-any.whl\nremote:        Collecting jupyterlab-server~=1.0.0 (from jupyterlab-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/78/98/5b87b9d38176bd98f23b58a8fb730e5124618d68571a011abbd38ad4a842/jupyterlab_server-1.0.6-py3-none-any.whl\nremote:        Collecting tornado!=6.0.0,!=6.0.1,!=6.0.2 (from jupyterlab-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/30/78/2d2823598496127b21423baffaa186b668f73cd91887fcef78b6eade136b/tornado-6.0.3.tar.gz (482kB)\nremote:        Collecting decorator (from traitlets&gt;=4.3.1-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/8f/b7/f329cfdc75f3d28d12c65980e4469e2fa373f1953f5df6e370e84ea2e875/decorator-4.4.1-py2.py3-none-any.whl\nremote:        Collecting ipython-genutils (from traitlets&gt;=4.3.1-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\nremote:        Collecting pygments (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/5c/73/1dfa428150e3ccb0fa3e68db406e5be48698f2a979ccbcec795f28f44048/Pygments-2.4.2-py2.py3-none-any.whl (883kB)\nremote:        Collecting backcall (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/84/71/c8ca4f5bb1e08401b916c68003acf0a0655df935d74d93bf3f3364b310e0/backcall-0.1.0.tar.gz\nremote:        Collecting jedi&gt;=0.10 (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/55/54/da994f359e4e7da4776a200e76dbc85ba5fc319eefc22e33d55296d95a1d/jedi-0.15.1-py2.py3-none-any.whl (1.0MB)\nremote:        Collecting prompt-toolkit&lt;2.1.0,&gt;=2.0.0 (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\nremote:        Collecting pickleshare (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\nremote:        Collecting pexpect; sys_platform != \"win32\" (from ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/0e/3e/377007e3f36ec42f1b84ec322ee12141a9e10d808312e5738f52f80a232c/pexpect-4.7.0-py2.py3-none-any.whl (58kB)\nremote:        Collecting jupyter-client (from ipykernel&gt;=4.5.1-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/13/81/fe0eee1bcf949851a120254b1f530ae1e01bdde2d3ab9710c6ff81525061/jupyter_client-5.3.4-py2.py3-none-any.whl (92kB)\nremote:        Collecting jsonschema!=2.5.0,&gt;=2.4 (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ce/6c/888d7c3c1fce3974c88a01a6bc553528c99d3586e098eee23e8383dd11c3/jsonschema-3.1.1-py2.py3-none-any.whl (56kB)\nremote:        Collecting jupyter-core (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/fb/82/86437f661875e30682e99d04c13ba6c216f86f5f6ca6ef212d3ee8b6ca11/jupyter_core-4.6.1-py2.py3-none-any.whl (82kB)\nremote:        Collecting Send2Trash (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/49/46/c3dc27481d1cc57b9385aff41c474ceb7714f7935b1247194adae45db714/Send2Trash-1.5.0-py3-none-any.whl\nremote:        Collecting pyzmq&gt;=17 (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/75/89/6f0ea51ffa9c2c00c0ab0460f137b16a5ab5b47e3b060c5b1fc9ca425836/pyzmq-18.1.0-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\nremote:        Collecting nbconvert (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/79/6c/05a569e9f703d18aacb89b7ad6075b404e8a4afde2c26b73ca77bb644b14/nbconvert-5.6.1-py2.py3-none-any.whl (455kB)\nremote:        Collecting terminado&gt;=0.8.1 (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/a7/56/80ea7fa66565fa75ae21ce0c16bc90067530e5d15e48854afcc86585a391/terminado-0.8.2-py2.py3-none-any.whl\nremote:        Collecting prometheus-client (from notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/b3/23/41a5a24b502d35a4ad50a5bb7202a5e1d9a0364d0c12f56db3dbf7aca76d/prometheus_client-0.7.1.tar.gz\nremote:        Collecting cryptography (from secretstorage; sys_platform == \"linux\"-&gt;keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/45/73/d18a8884de8bffdcda475728008b5b13be7fbef40a2acc81a0d5d524175d/cryptography-2.8-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\nremote:        Collecting jeepney (from secretstorage; sys_platform == \"linux\"-&gt;keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/0a/4c/ef880713a6c6d628869596703167eab2edf8e0ec2d870d1089dcb0901b81/jeepney-0.4.1-py3-none-any.whl (60kB)\nremote:        Collecting json5 (from jupyterlab-server~=1.0.0-&gt;jupyterlab-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/30/44/062543d4a6718f99d82e5ecf9140dbdee8a03122f2c34fbd0b0609891707/json5-0.8.5-py2.py3-none-any.whl\nremote:        Collecting parso&gt;=0.5.0 (from jedi&gt;=0.10-&gt;ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/a3/bd/bf4e5bd01d79906e5b945a7af033154da49fd2b0d5b5c705a21330323305/parso-0.5.1-py2.py3-none-any.whl (95kB)\nremote:        Collecting wcwidth (from prompt-toolkit&lt;2.1.0,&gt;=2.0.0-&gt;ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/7e/9f/526a6947247599b084ee5232e4f9190a38f398d7300d866af3ab571a5bfe/wcwidth-0.1.7-py2.py3-none-any.whl\nremote:        Collecting ptyprocess&gt;=0.5 (from pexpect; sys_platform != \"win32\"-&gt;ipython&gt;=4.0.0; python_version &gt;= \"3.3\"-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\nremote:        Collecting importlib-metadata (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f6/d2/40b3fa882147719744e6aa50ac39cf7a22a913cbcba86a0371176c425a3b/importlib_metadata-0.23-py2.py3-none-any.whl\nremote:        Collecting pyrsistent&gt;=0.14.0 (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/30/86/53a88c0a57698fa228db29a4000c28f4124823010388cb7042fe6e2be8dd/pyrsistent-0.15.5.tar.gz (107kB)\nremote:        Collecting attrs&gt;=17.4.0 (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\nremote:        Collecting pandocfilters&gt;=1.4.1 (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/4c/ea/236e2584af67bb6df960832731a6e5325fd4441de001767da328c33368ce/pandocfilters-1.4.2.tar.gz\nremote:        Collecting mistune&lt;2,&gt;=0.8.1 (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/09/ec/4b43dae793655b7d8a25f76119624350b4d65eb663459eb9603d7f1f0345/mistune-0.8.4-py2.py3-none-any.whl\nremote:        Collecting bleach (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/ab/05/27e1466475e816d3001efb6e0a85a819be17411420494a1e602c36f8299d/bleach-3.1.0-py2.py3-none-any.whl (157kB)\nremote:        Collecting testpath (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/1b/9e/1a170feaa54f22aeb5a5d16c9015e82234275a3c8ab630b552493f9cb8a9/testpath-0.4.4-py2.py3-none-any.whl (163kB)\nremote:        Collecting defusedxml (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/06/74/9b387472866358ebc08732de3da6dc48e44b0aacd2ddaa5cb85ab7e986a2/defusedxml-0.6.0-py2.py3-none-any.whl\nremote:        Collecting cffi!=1.11.3,&gt;=1.8 (from cryptography-&gt;secretstorage; sys_platform == \"linux\"-&gt;keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/49/72/0d42f94fe94afa8030350c26e9d787219f3f008ec9bf6b86c66532b29236/cffi-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (397kB)\nremote:        Collecting zipp&gt;=0.5 (from importlib-metadata-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/74/3d/1ee25a26411ba0401b43c6376d2316a71addcc72ef8690b101b4ea56d76a/zipp-0.6.0-py2.py3-none-any.whl\nremote:        Collecting webencodings (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension&gt;=3-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\nremote:        Collecting pycparser (from cffi!=1.11.3,&gt;=1.8-&gt;cryptography-&gt;secretstorage; sys_platform == \"linux\"-&gt;keyring-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\nremote:        Collecting more-itertools (from zipp&gt;=0.5-&gt;importlib-metadata-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7-&gt;arcgis==1.7.0-&gt;-r /tmp/build_771006ccbb0075f30cf8bfceed6597e5/requirements.txt (line 1))\nremote:          Downloading https://files.pythonhosted.org/packages/45/dc/3241eef99eb45f1def35cf93af35d1cf9ef4c0991792583b8f33ea41b092/more_itertools-7.2.0-py3-none-any.whl (57kB)\nremote:        Installing collected packages: six, decorator, ipython-genutils, traitlets, pygments, backcall, parso, jedi, wcwidth, prompt-toolkit, pickleshare, ptyprocess, pexpect, ipython, tornado, jupyter-core, python-dateutil, pyzmq, jupyter-client, ipykernel, more-itertools, zipp, importlib-metadata, pyrsistent, attrs, jsonschema, nbformat, MarkupSafe, Jinja2, Send2Trash, entrypoints, pandocfilters, mistune, webencodings, bleach, testpath, defusedxml, nbconvert, terminado, prometheus-client, notebook, widgetsnbextension, ipywidgets, numpy, pytz, pandas, pyparsing, kiwisolver, cycler, matplotlib, pycparser, cffi, cryptography, jeepney, secretstorage, keyring, json5, jupyterlab-server, jupyterlab, pyshp, arcgis, certifi, chardet, Click, Werkzeug, itsdangerous, Flask, gunicorn, idna, urllib3, requests, SQLAlchemy\nremote:          Running setup.py install for backcall: started\nremote:            Running setup.py install for backcall: finished with status 'done'\nremote:          Running setup.py install for tornado: started\nremote:            Running setup.py install for tornado: finished with status 'done'\nremote:          Running setup.py install for pyrsistent: started\nremote:            Running setup.py install for pyrsistent: finished with status 'done'\nremote:          Running setup.py install for pandocfilters: started\nremote:            Running setup.py install for pandocfilters: finished with status 'done'\nremote:          Running setup.py install for prometheus-client: started\nremote:            Running setup.py install for prometheus-client: finished with status 'done'\nremote:          Running setup.py install for pycparser: started\nremote:            Running setup.py install for pycparser: finished with status 'done'\nremote:          Running setup.py install for pyshp: started\nremote:            Running setup.py install for pyshp: finished with status 'done'\nremote:          Running setup.py install for arcgis: started\nremote:            Running setup.py install for arcgis: finished with status 'done'\nremote:          Running setup.py install for SQLAlchemy: started\nremote:            Running setup.py install for SQLAlchemy: finished with status 'done'\nremote:        Successfully installed Click-7.0 Flask-1.1.1 Jinja2-2.10.3 MarkupSafe-1.1.1 SQLAlchemy-1.3.10 Send2Trash-1.5.0 Werkzeug-0.16.0 arcgis-1.7.0 attrs-19.3.0 backcall-0.1.0 bleach-3.1.0 certifi-2019.9.11 cffi-1.13.2 chardet-3.0.4 cryptography-2.8 cycler-0.10.0 decorator-4.4.1 defusedxml-0.6.0 entrypoints-0.3 gunicorn-19.9.0 idna-2.8 importlib-metadata-0.23 ipykernel-5.1.3 ipython-7.9.0 ipython-genutils-0.2.0 ipywidgets-7.5.1 itsdangerous-1.1.0 jedi-0.15.1 jeepney-0.4.1 json5-0.8.5 jsonschema-3.1.1 jupyter-client-5.3.4 jupyter-core-4.6.1 jupyterlab-1.2.1 jupyterlab-server-1.0.6 keyring-19.2.0 kiwisolver-1.1.0 matplotlib-3.1.1 mistune-0.8.4 more-itertools-7.2.0 nbconvert-5.6.1 nbformat-4.4.0 notebook-6.0.2 numpy-1.17.3 pandas-0.25.3 pandocfilters-1.4.2 parso-0.5.1 pexpect-4.7.0 pickleshare-0.7.5 prometheus-client-0.7.1 prompt-toolkit-2.0.10 ptyprocess-0.6.0 pycparser-2.19 pygments-2.4.2 pyparsing-2.4.4 pyrsistent-0.15.5 pyshp-1.2.12 python-dateutil-2.8.1 pytz-2019.3 pyzmq-18.1.0 requests-2.22.0 secretstorage-3.1.1 six-1.12.0 terminado-0.8.2 testpath-0.4.4 tornado-6.0.3 traitlets-4.3.3 urllib3-1.25.6 wcwidth-0.1.7 webencodings-0.5.1 widgetsnbextension-3.5.1 zipp-0.6.0\nremote: \nremote: -----&gt; Discovering process types\nremote:        Procfile declares types -&gt; web\nremote: \nremote: -----&gt; Compressing...\nremote:        Done: 131.7M\nremote: -----&gt; Launching...\nremote:        Released v3\nremote:        https://atma-thermos.herokuapp.com/ deployed to Heroku\nremote: \nremote: Verifying deploy... done.\nTo https://git.heroku.com/atma-thermos.git\n * [new branch]      master -&gt; master\n</code></pre>"},{"location":"apps/quakemap/","title":"Live earthquake map from USGS","text":"<p>If you cannot view the map on this page, click here to open it in full screen.</p>"},{"location":"apps/usa-pluscodes/","title":"USA Pluscode 4 map","text":""},{"location":"apps/usa-pluscodes/#pluscode4-for-usa","title":"Pluscode4 for USA","text":"<p>Go fullscreen here</p> iFrames are not supported on this page."},{"location":"apps/usa-pluscodes/#pluscode4-symbolized-by-number-of-man-made-structures","title":"Pluscode4 symbolized by number of man-made structures","text":"<p>Go fullscreen here</p> iFrames are not supported on this page."},{"location":"apps/wildfire-hazard/","title":"What is my wildfire hazard score?","text":"<p>The map below shows the wildfire hazard potential (or score) for any community. The layer is out of the Esri LivingAtlas of the world. If you would like to know more about this resource, read this blog and this story map.</p> <p>\"A score of 5 is very high risk and a score between 0-1 is likely non-burnable area such as water or asphalt. People that live in areas of moderate to high risk are more likely to be affected by wildfire events including evacuations, higher home insurance, and bad air quality due to smoke.\"</p> <p>View the map in full screen here</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2017/10/26/a-whole-new-way-to-experience-gis/","title":"A whole new way to experience GIS","text":"<p>The ArcGIS API for Python is easy to learn and extremely useful for data scientists, GIS administrators, and GIS analysts. One of the features that makes this API so powerful is its integration with Jupyter Notebook. Jupyter Notebook is a web-based integrated development environment (IDE) for executing Python code. Unlike other traditional IDEs that are designed for developers, Jupyter Notebook provides a simple and easy-to-use interface that encourages the Read-Eval-Print Loop (REPL) process that is central to learning how to code in Python... Read more here</p>"},{"location":"blog/2010/01/26/articles-on-remote-sensing/","title":"Articles on Remote Sensing","text":"<p>Below are some blogs, tutorials that I authored on remote sensing while I worked as a scientist at the Indian Space Research Org.</p> <ul> <li>Shadow within shadow</li> <li>Programming: A tool read and analyze HDF5 data</li> <li>Essentials of image processing</li> <li>Programming: A look at Optimum Index Factor algorithm to choose best bands for visualization</li> <li>Microwave remote sensing concepts</li> </ul>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/","title":"AWS Lambda Functions - a Quick Start guide","text":"<p>Lambda functions from AWS sit at the heart of its serverless compute services. It lets you run code without you as a developer having to procure, host, maintain and secure servers on the cloud. All you need to worry about is just the code and the business logic. This guide will help you get started creating simple lambda functions.</p> <p>AWS promotes several services under its serverless platform as shown below: </p> <p>To create a lambda function, you need not learn a new language. You can code in one of many supported languages, including Python, Java, Node.JS etc. Your code can perform traditional compute as well as use AWS libraries to talk to the rest of AWS platform services. Scaling (compute scaling, network scaling, IO throughput scaling) happens automatically based on demand. System logs are written to AWS Cloudwatch service. As with any serverless compute service on AWS, you don't pay for idle time. Before we create a new function, we need to clear some concepts:</p> <ul> <li>AWS Lambda Functions - a Quick Start guide<ul> <li>Concepts<ul> <li>Lambda function permissions</li> <li>Lambda event sources</li> <li>Lifecycle of a lambda function</li> <li>Architecture of a lambda function</li> <li>Principles of a good lambda function</li> <li>Lambda configurations</li> </ul> </li> <li>Authoring lambda functions</li> </ul> </li> <li>Lambda - advanced concepts<ul> <li>Runtime, Handler, Memory, Timeouts</li> <li>Lambda layers<ul> <li>Building a deployment package for a layer</li> <li>Sharing layers</li> </ul> </li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#concepts","title":"Concepts","text":""},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-function-permissions","title":"Lambda function permissions","text":"<p>There are two types of permissions - a. what is allowed to invoke the function, and b. what the function is allowed to do (or talk to).</p> <p></p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-event-sources","title":"Lambda event sources","text":"<p>Events are the triggers that cause a lambda function to run. For instance, a file being stored in S3, a cloud watch event etc. are event sources. Broadly, these triggers can be classified into two types: a. Push events (where the source is external to the fn) and b. Poll events (where the lambda will poll a service on a set interval).</p> <p></p> <p>Push events can be <code>synchronous</code> or <code>asynchronous</code>. Sync events expect an immediate response, while async can do not. Async are suited for batch processing.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lifecycle-of-a-lambda-function","title":"Lifecycle of a lambda function","text":"<p>From the AWS training site:  1. When a function is first invoked, an execution environment is launched and bootstrapped. Once the environment is bootstrapped, your function code executes. Then, Lambda freezes the execution environment, expecting additional invocations.</p> <ol> <li> <p>If another invocation request for the function is made while the environment is in this state, that request goes through a warm start. With a warm start, the available frozen container is thawed and immediately begins code execution without going through the bootstrap process.</p> </li> <li> <p>This thaw and freeze cycle continues as long as requests continue to come in consistently. But if the environment becomes idle for too long, the execution environment is recycled.</p> </li> <li> <p>A subsequent request starts the lifecycle over, requiring the environment to be launched and bootstrapped. This is a cold start.</p> </li> </ol> <p></p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#architecture-of-a-lambda-function","title":"Architecture of a lambda function","text":"<p>This is a simple lambda function that returns the current date:</p> <pre><code>import json\nfrom datetime import datetime\n\nnow_date = datetime.now().strftime('%y-%m-%d')\n\ndef lambda_handler(event, context):\n    # TODO implement\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda! Now the time is: ' + now_date)\n    }\n</code></pre> <p>The filename is <code>lambda_function.py</code> and the invocation is to <code>lambda_function.lambda_handler</code>.</p> <p>At the core of a lambda function is the <code>handler(event, context)</code> method. The <code>event</code> object can either be AWS generated obj (when AWS services invoke) or custom user-defined obj. The <code>context</code> obj provides information about the current execution, such as remaining time etc.</p> <p>You author lambda functions in 3 ways:</p> <ol> <li>use the Lambda Management Console web app, which is based off the Cloud9 web IDE service.</li> <li>Upload code package after you author it using your IDE of choice</li> <li>Upload code package to a S3 bucket and give Lambda the url to the code.</li> </ol> <p>Uploading to S3 bucket might be suitable if your package is &gt;10 MB in size, or if the code is part of a CICD pipeline.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#principles-of-a-good-lambda-function","title":"Principles of a good lambda function","text":"<p>AWS Lambda developer guide has more information on best practices. Below is the gist:</p> <ul> <li>Functions in your code should be modular, testable and stateless. Separate out business logic from the handler function. If you can identify two separate tasks in your function, break them down and create two separate functions if that's possible. This makes it modular, just like a microservice.</li> <li>To benefit from warm start, store data locally in <code>/tmp</code> directory. But don't assume it is always available.</li> <li>To write data permanently, use DynamoDB service which is serverless and has millisecond latency.</li> <li>To take advantage of CloudWatch, use Python's <code>logging</code> module. Simple print statements also would do and they are captured in CloudWatch.</li> <li>To pass sensitive information, use environment variables.</li> <li>While recursion might be elegant in regular programming, it can lead to uncontrolled behavior in lambda functions. So, avoid them.</li> </ul>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-configurations","title":"Lambda configurations","text":"<p>Lambda functions are billed for memory and duration of execution. The default memory is <code>128 MB</code> and you can request up to <code>2 GB</code>. When you request for a larger size, you get proportionally higher compute power and also a higher cost rate. Services are billed for memory + execution duration (the max of duration is set as timeout limit). Default timeout is <code>3 sec</code> and the current max is <code>15 min</code>. Thus, you need to optimize for cost of higher memory and cost of exec duration. Sometimes, a higher memory might end up costing less, because it finishes in shorter duration. This comes down to profiling your function and understanding how you can speed it up.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#authoring-lambda-functions","title":"Authoring lambda functions","text":"<p>To create a lambda function, create an AWS account (which needs to be different from your Amazon shopping account). If you are a new user, you get <code>12</code> months of free service in addition to certain services that have an always free tier. Checkout https://aws.amazon.com/lambda/pricing/ to understand pricing. In my experience, they first <code>50,000</code> executions are practically free, so it is safe to experiment with.</p> <p>Steps:</p> <ol> <li> <p>From AWS console, search for and click on 'Lambda'. This opens the Lambda console. From the menu on the left, click on 'Function' which brings you to the layout shown below: </p> </li> <li> <p>Click on 'Create Function'. You can choose from one of 3 options, for now, choose 'Author from scratch'. Give a name to the function <code>myFirstFn</code> or such. Choose runtime as <code>python3.7</code> or similar. Choose a permission, the default setting will do for now.</p> </li> <li> <p>In the Lambda console, you have different sections. The 'Designer' lets you choose layers which are Python library layers, more of this later. For now you can choose a Trigger as <code>API gateway</code>. Triggers define what will invoke your function. I don't have a destination in this example, but examples are calling another 'lambda' or some other AWS service. There aren't as many destinations as there are for triggers.  If you click on the 'API Gateway', you get a URL that is public facing. AWS has set up a web server, a server framework, exposed it to the internet, generated a route that is specific for this function and given you in that URL.</p> </li> <li> <p>The Function code section gives you the Cloud9 based IDE. It feels similar to JupyterLab interface and lets you develop Python code. By convention, the lambda service looks for a file by name <code>lambda_function.py</code> and a function called <code>lambda_handler(event, context)</code> within it for invocation. You can follow the convention, which the console stubs out for you, or change this in the Basic settings section. The <code>return</code> statement should return an object that is web friendly - such as JSON or HTML. Any <code>print</code> or <code>logging</code> module statements are captured in CloudWatch logs and linked for you for debugging. </p> </li> <li> <p>Hit Save and Test to ensure it runs as you expect. This is it. You have created the 'hello-world' of lambda functions and its running.</p> </li> </ol>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-advanced-concepts","title":"Lambda - advanced concepts","text":""},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#runtime-handler-memory-timeouts","title":"Runtime, Handler, Memory, Timeouts","text":"<p>Hit the 'Edit' button on 'Basic settings' to customize as shown below: </p> <p>The 'Handler' part allows you to specify your <code>main.py</code> file and the function within to invoke. This is helpful if you deviate from the lambda convention of what you call the handler function.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#lambda-layers","title":"Lambda layers","text":"<p>The Amazon Machine Image for <code>python3.7</code> Lambda comes with bare Python and <code>boto</code> library. Nothing else. Any library that your app needs (such as <code>requests</code> or <code>flask</code>) has to be bundled in your code. If you have more than a few functions that have similar dependency stack, you are essentially repeating the same packages in each function. Besides, the dependency package for my apps can easily exceed the <code>10MB</code> upload limit and the lambda console would not let me live edit my files using the Cloud9 console. A solution to all of this is to create a lambda layer which contains all your dependencies. </p> <p>Once you publish a layer, you can link to it from your function and just like that, <code>import</code> your dependencies. Lambda service will take care of laying it and wiring up your <code>Python</code> path. To create a layer, from the lambda menu on the left, choose 'Layers' to enter the layers console. </p> <p>Click on 'Create Layer', to give it a name, upload your files as a .zip file or from a S3 bucket. Choose runtimes. You can choose more than one.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#building-a-deployment-package-for-a-layer","title":"Building a deployment package for a layer","text":"<p>The most important thing to remember here is to build your deployment package in an environment that is identical to the lambda service itself. This implies, building the package in a Linux OS (better if you get the same Docker image lambda uses). After several attempts, using <code>pipenv</code> instead of <code>conda</code> and <code>pip3</code> to install packages worked best for me.</p> <p><code>pipenv</code> allows you to create a folder as your deployment environment. Then, <code>pip3 install &lt;pkg name&gt; --target .</code> allows you to install the packages in the current directory. Go ahead and install all dependencies. Then  bundle them up as a .zip file and you got yourself a deployment package.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#sharing-layers","title":"Sharing layers","text":"<p>Per the AWS doc, you can share a layer you create with other users. All they need is the <code>arn</code> number. However only those functions deployed in your AWS Region can use the layer. If you want it universally available, you may have to publish it in all regions.</p> <p>Just like a typical software library, you can version a layer. This makes it great to keep layers updated as newer versions are released.</p>"},{"location":"blog/2020/06/02/aws-lambda-functions-quickstart/#conclusion","title":"Conclusion","text":"<p>This was a quickstart guide teaching what lambda functions are and how to get started with them. Next up, Azure functions.</p>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/","title":"AWS Lambda Functions - authoring with Docker images","text":"<p>At re:Invent 2020, AWS announced support for authoring, shipping and deploying the popular serverless Lambda services via Docker images. Further, they allow images up to <code>10GB</code> in size. As multiple authorities noted, this is a game changer, particularly for the scientific Python community as this would allow us to author machine learning and even deep learning inferencing functions using AWS Lambdas. This blog takes a quick look at authoring a \"hello-world\" style lambda using Docker.</p> <p>To understand how Docker works or how to build Docker images, checkout my Docker 101 cheatsheet. Having said that, you do not need to know much about Docker or containerization for simple functions.</p>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#the-lambda-runtime-images","title":"The Lambda runtime images","text":"<p>AWS provides the runtime images for different languages, including Python. The images are shared at <code>DockerHub: amazon/aws-lambda-provided</code> and <code>ECR Public: public.ecr.aws/lambda/provided</code>. As expected, the base image is a flavor of linux called Amazon Linux.</p>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#the-ric-and-the-rie","title":"The RIC and the RIE","text":"<p>The Python Docker images come with two important components pre-loaded. The RIC - Runtime Interface Client provides the interface between Lambda (infrastructure) &amp; the function code. Think about the invoker from API gateway that runs your handler function. The RIE - Runtime Interface Emulator is a tool that allows you to test the code locally.</p>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#high-level-workflow","title":"High-level workflow","text":"<p>The steps involved in publishing a Lambda function using Docker images will look like below</p> <ol> <li>Download base images in your language of choice</li> <li>Package your code + dependencies + resource files into the image using Docker CLI</li> <li>Push the image into AWS ECR - AWS Elastic Container Registry. Note: You are not charged storage for these images.</li> <li>Create a function, choose 'Container Image' and the rest should be similar</li> </ol>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#gotchas","title":"Gotchas","text":"<ul> <li>Image max size is <code>10 GB</code></li> <li>Application code should run on a read-only file system. Only the <code>/tmp</code> dir is writable with <code>512 MB</code> storage</li> <li>The container is instantiated with a user with least set of permissions. Ensure app code conforms with this.</li> <li>Container settings</li> </ul> <pre><code>ENTRYPOINT \u2013 Specifies the absolute path to the entry point of the application.\nCMD \u2013 Specifies parameters that you want to pass in with ENTRYPOINT.\nWORKDIR \u2013 Specifies the absolute path to the working directory.\nENV \u2013 Specifies an environment variable for the Lambda function. \n</code></pre>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#steps","title":"Steps","text":"<ul> <li>Install Docker for desktop and AWS CLI</li> <li>Create your regular Lambda function into a folder called <code>app</code>. Store your <code>app.py</code>, <code>requirements.txt</code> files into it. See sample below:</li> </ul> <p>app.py:</p> <pre><code>try:\n    import json\n    import sys\n    import requests\n    print(\"All imports ok ...\")\nexcept Exception as e:\n    print(\"Error Imports : {} \".format(e))\n\n\ndef lambda_handler(event, context):\n\n    print(\"Hello AWS!\")\n    print(\"event = {}\".format(event))\n    return {\n        'statusCode': 200,\n        'message': 'hello from docker lambda'\n    }\n</code></pre> <p>requirements.txt:</p> <pre><code>requests\n</code></pre> <ul> <li>Create a docker file as below:</li> </ul> <pre><code>FROM public.ecr.aws/lambda/python:3.8\n\n# Copy function code\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\n# Install the function's deps\nCOPY requirements.txt  .\nRUN pip3 install -r requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n\n# set the CMD to handler\nCMD [ \"app.lambda_handler\" ]\n</code></pre> <ul> <li>Build Docker image using syntax <code>docker build -t &lt;username/imagename:tag&gt; &lt;build context&gt;</code>. For example: <code>docker build -t atmamani/aws-lambda-docker:v1 .</code>. This prints an output like below:</li> </ul> <pre><code>[+] Building 20.7s (9/9) FINISHED                                                                                                 \n =&gt; [internal] load build definition from Dockerfile                                                                         0.0s\n =&gt; =&gt; transferring dockerfile: 44B                                                                                          0.0s\n =&gt; [internal] load .dockerignore                                                                                            0.0s\n =&gt; =&gt; transferring context: 2B                                                                                              0.0s\n =&gt; [internal] load metadata for public.ecr.aws/lambda/python:3.8                                                            0.4s\n =&gt; [internal] load build context                                                                                            0.0s\n =&gt; =&gt; transferring context: 81B                                                                                             0.0s\n =&gt; [1/4] FROM public.ecr.aws/lambda/python:3.8@sha256:44c0de45aa1927eecd519ad48faa27fe3318717160b5f7560475d12abea7b427     18.0s\n =&gt; =&gt; resolve public.ecr.aws/lambda/python:3.8@sha256:44c0de45aa1927eecd519ad48faa27fe3318717160b5f7560475d12abea7b427      0.0s\n =&gt; =&gt; sha256:20666df3e004b212ed97db3bb9bc7cc0251d842f8b672f1722dc55c0d5f45367 74.99kB / 74.99kB                             0.8s\n =&gt; =&gt; sha256:f7ad8276137021bb0b0cbd6826ec86b0f04c78367e486f3c1728e389d1d400bc 415B / 415B                                   0.8s\n =&gt; =&gt; sha256:44c0de45aa1927eecd519ad48faa27fe3318717160b5f7560475d12abea7b427 1.58kB / 1.58kB                               0.0s\n =&gt; =&gt; sha256:80342c69b467fef0607f90fd75c631fb351da1049b71faaaf98c8c1ce859c7b2 3.00kB / 3.00kB                               0.0s\n =&gt; =&gt; sha256:af4b61775d4cf0f587278cfbc2cbbc9afc3480f5fed5a455a9289c2c387c7187 100.74MB / 100.74MB                           7.5s\n =&gt; =&gt; sha256:bb2e44738d79a6afd429e2a103f081ad47783de7a6f7305664aee8a2e9e3976b 3.32MB / 3.32MB                               2.4s\n =&gt; =&gt; sha256:891c7fcaabb6a1090e28f4d967f1e423aa7cfd1217267b7e9bfe6636cd7b08ef 54.77MB / 54.77MB                            10.0s\n =&gt; =&gt; sha256:c1ae0f49cb65052f0d9674682af2b3bbe09c34fa37004930ef2e8019b76fa2d5 16.10MB / 16.10MB                             6.6s\n =&gt; =&gt; extracting sha256:af4b61775d4cf0f587278cfbc2cbbc9afc3480f5fed5a455a9289c2c387c7187                                    4.9s\n =&gt; =&gt; extracting sha256:20666df3e004b212ed97db3bb9bc7cc0251d842f8b672f1722dc55c0d5f45367                                    0.0s\n =&gt; =&gt; extracting sha256:f7ad8276137021bb0b0cbd6826ec86b0f04c78367e486f3c1728e389d1d400bc                                    0.0s\n =&gt; =&gt; extracting sha256:bb2e44738d79a6afd429e2a103f081ad47783de7a6f7305664aee8a2e9e3976b                                    0.1s\n =&gt; =&gt; extracting sha256:891c7fcaabb6a1090e28f4d967f1e423aa7cfd1217267b7e9bfe6636cd7b08ef                                    3.0s\n =&gt; =&gt; extracting sha256:c1ae0f49cb65052f0d9674682af2b3bbe09c34fa37004930ef2e8019b76fa2d5                                    1.5s\n =&gt; [2/4] COPY app.py /var/task                                                                                              0.1s\n =&gt; [3/4] COPY requirements.txt  .                                                                                           0.0s\n =&gt; [4/4] RUN pip3 install -r requirements.txt --target \"/var/task\"                                                          2.0s\n =&gt; exporting to image                                                                                                       0.1s\n =&gt; =&gt; exporting layers                                                                                                      0.1s\n =&gt; =&gt; writing image sha256:ac4783629d43f0d1f2c641133419bf47b52531b7535bc2ad25dca73ab126f4ce                                 0.0s \n =&gt; =&gt; naming to docker.io/atmamani/aws-lambda-docker:v1                                                                     0.0s\n</code></pre> <ul> <li>Run the container locally using the command:</li> </ul> <pre><code>(base) \u279c  app git:(master) docker run -p 9000:8080 atmamani/aws-lambda-docker:v1\n</code></pre> <ul> <li>You can test by posting <code>curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'</code></li> <li>If you notice any errors, you can edit the files and rebuild the image and update the tag. THen you can retest the application.</li> <li>Authenticate the AWS CLI by following this help.</li> <li>Pass the auth from AWS CLI to Docker CLI, so Docker can tag and later push the images. Docker CLI normally pushes to the default DockerHub registry. However, for Lambda to work, you need to push to Amazon ECR registry. In ECR, I used the web UI to make a repository called <code>ml2web</code>.</li> <li>Run below, where you replace <code>--region us-west-2</code> with your AWS region. You also replace <code>--profile &lt;aws_profile_name&gt;</code> with your AWS CLI profile name. If you are using just 1 account for all (don't do this), then you can skip this argument. The Docker username needs to be <code>AWS</code> always. The URL like string is the name of your ECR registry.</li> </ul> <pre><code>sudo aws ecr get-login-password --region us-west-2 --profile &lt;aws_profile_name&gt; | docker login --username AWS --password-stdin &lt;your_ECR_registry_name&gt;.dkr.ecr.us-west-2.amazonaws.com\n</code></pre> <p>This threw an error saying permission denied, however it still works for me.</p> <ul> <li> <p>Then tag your image using the command <code>docker tag atmamani/aws-lambda-docker:v2 &lt;your_ecr_name&gt;.dkr.ecr.us-west-2.amazonaws.com/ml2web:v2</code></p> </li> <li> <p>Then push to ECR using the command: <code>docker push &lt;your_ECR_name&gt;.dkr.ecr.us-west-2.amazonaws.com/ml2web:v2</code></p> </li> </ul> <pre><code>The push refers to repository [your_ecr_name.dkr.ecr.us-west-2.amazonaws.com/ml2web]\n6b5cd20ea590: Pushed \naef77c8c9312: Pushed \n623b1ac50c4e: Pushed \na1f8e0568112: Pushed \nbcf453d1de13: Pushed \nf6ae2f36d5d7: Pushed \n5959c8f9752b: Pushed \n3e5452c20c48: Pushed \n9c4b6b04eac3: Pushed \nv2: digest: sha256:abcdefgh_some_sha size: 2206\n</code></pre> <ul> <li>Finally, use the Lambda web UI to create a new function. The only difference is, you choose 'container image' in the radio and provide the URI to the ECR registry. You can then browse for your image and tag and choose it.</li> </ul>"},{"location":"blog/2021/09/20/aws-lambda-functions-with-docker-images/#sources","title":"Sources","text":"<ul> <li>Runtime images</li> <li>Using container images with lambda</li> <li>Announcement blog</li> <li>Creating images using AWS base images</li> </ul>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/","title":"Building AWS Lambda Functions with SAM CLI","text":"<p>The SAM (Serverless Application Model) allows you to build complex and production ready lambda functions that are performant yet light-weight. If you are new to serverless computing or lambda functions, checkout my quick start guide. This article helps you with getting started with SAM CLI.</p> <p>The SAM CLI provides you a development environment that is identical to lambda execution environment. This is possible via a Docker image. Thus before installing SAM, you need a few prerequisites met:</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#installing-sam-cli","title":"Installing SAM CLI","text":"<p>The prerequisites are</p> <ul> <li>Docker desktop</li> <li>Shared drive space between host and Docker</li> <li>Homebrew</li> </ul> <p>The SAM installation guide walks you through the steps involved for various platforms. After installation, you need to configure it with your AWS account info as explained here</p> <pre><code>&gt;&gt; sam --version\nSAM CLI, version 1.31.0\n</code></pre> <p>When installing SAM using <code>brew</code>, I noticed the install is global and not restricted to a particular <code>conda</code> env.</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#setting-up-a-sam-app","title":"Setting up a SAM APP","text":"<p>SAM makes it easy to get started with guided CLI set ups and deploys. To start a project, run <code>sam init</code> and answer the questions as shown below:</p> <pre><code>(aws_lambda_default) \u279c  sam-try git:(master) sam init\nWhich template source would you like to use?\n    1 - AWS Quick Start Templates\n    2 - Custom Template Location\nChoice: 1\nWhat package type would you like to use?\n    1 - Zip (artifact is a zip uploaded to S3)  \n    2 - Image (artifact is an image uploaded to an ECR image repository)\nPackage type: 1\n\nWhich runtime would you like to use?\n    1 - nodejs14.x\n    2 - python3.9\n    3 - ruby2.7\n    4 - go1.x\n    5 - java11\n    6 - dotnetcore3.1\n    7 - nodejs12.x\n    8 - nodejs10.x\n    9 - python3.8\n    10 - python3.7\n    11 - python3.6\n    12 - python2.7\n    13 - ruby2.5\n    14 - java8.al2\n    15 - java8\n    16 - dotnetcore2.1\nRuntime: 9\n\nProject name [sam-app]: try-sam-app\n\nCloning from https://github.com/aws/aws-sam-cli-app-templates\n\nAWS quick start application templates:\n    1 - Hello World Example\n    2 - EventBridge Hello World\n    3 - EventBridge App from scratch (100+ Event Schemas)\n    4 - Step Functions Sample App (Stock Trader)\n    5 - Elastic File System Sample App\nTemplate selection: 1\n\n    -----------------------\n    Generating application:\n    -----------------------\n    Name: try-sam-app\n    Runtime: python3.8\n    Dependency Manager: pip\n    Application Template: hello-world\n    Output Directory: .\n\n    Next steps can be found in the README file at ./try-sam-app/README.md\n</code></pre> <p>This creates a folder by the name you provided for the project name during the interactive setup. The structure of the project looks like below:</p> <pre><code>(aws_lambda_default) \u279c  sam-try git:(master) \u2717 exa -T try-sam-app -R -I venv\ntry-sam-app\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 events\n\u2502  \u2514\u2500\u2500 event.json\n\u251c\u2500\u2500 hello_world\n\u2502  \u251c\u2500\u2500 __init__.py\n\u2502  \u251c\u2500\u2500 app.py             # The main app which has the lambda_handler fn\n\u2502  \u2514\u2500\u2500 requirements.txt   # pip install pkgs\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 template.yaml         # cloud formation template pre populated\n\u2514\u2500\u2500 tests\n   \u251c\u2500\u2500 __init__.py\n   \u251c\u2500\u2500 integration\n   \u2502  \u251c\u2500\u2500 __init__.py\n   \u2502  \u2514\u2500\u2500 test_api_gateway.py\n   \u251c\u2500\u2500 requirements.txt\n   \u2514\u2500\u2500 unit\n      \u251c\u2500\u2500 __init__.py\n      \u2514\u2500\u2500 test_handler.py\n</code></pre> <p>You can now edit the files from here. For more info on this, skip to the SAM spec model topic.</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#build-sam-app","title":"Build SAM app","text":"<p>The app can be built by calling <code>sam build</code> from inside the app dir (which has the <code>template.json</code> file.)</p> <pre><code>(aws_lambda_default) \u279c  try-sam-app git:(master) \u2717 sam build\nBuilding codeuri: ~/.../try-sam-app/hello_world runtime: python3.8 metadata: {} functions: ['HelloWorldFunction']\nRunning PythonPipBuilder:ResolveDependencies\nRunning PythonPipBuilder:CopySource\n\nBuild Succeeded\n\nBuilt Artifacts  : .aws-sam/build\nBuilt Template   : .aws-sam/build/template.yaml\n\nCommands you can use next\n=========================\n[*] Invoke Function: sam local invoke\n[*] Deploy: sam deploy --guided\n</code></pre>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#testing-app-locally","title":"Testing App locally","text":"<p>There are 2 ways to test the app locally. One is the simpler <code>invoke</code> command which builds the app in a Docker image, calls the handler (based on what's defined in <code>events.json</code>), reports the output and exits. It looks like below:</p> <pre><code>(aws_lambda_default) \u279c  try-sam-app git:(master) \u2717 sam local invoke\nInvoking app.lambda_handler (python3.8)\nImage was not found.\nRemoving rapid images for repo public.ecr.aws/sam/emulation-python3.8\nBuilding image...................\nSkip pulling image and use local one: public.ecr.aws/sam/emulation-python3.8:rapid-1.31.0.\n\nMounting ~/.../try-sam-app/.aws-sam/build/HelloWorldFunction as /var/task:ro,delegated inside runtime container\nSTART RequestId: e027b54f-86ea-4a31-83ed-ce09d83b9053 Version: $LATEST\nEND RequestId: e027b54f-86ea-4a31-83ed-ce09d83b9053\nREPORT RequestId: e027b54f-86ea-4a31-83ed-ce09d83b9053  Init Duration: 0.11 ms  Duration: 98.97 ms  Billed Duration: 100 ms Memory Size: 128 MB Max Memory Used: 128 MB \n{\"statusCode\": 200, \"body\": \"{\\\"message\\\": \\\"hello world\\\"}\"}%                                                                                                                                            (aws_lambda_default) \u279c  try-sam-app git:(master) \u2717 \n</code></pre> <p>The second method is to use the app interactively: Run <code>sam local start-api</code></p> <pre><code>&gt;&gt; sam local start-api\nMounting HelloWorldFunction at http://127.0.0.1:3000/hello [GET]\nYou can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template\n2021-09-27 16:12:15  * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)\n\n&gt;&gt; curl http://127.0.0.1:3000/hello\n{\"message\": \"hello world\"}%\n</code></pre> <p>Now, if you go to http://127.0.0.1:3000/hello, you get back the default \"hello world\" message.</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#deploying-to-production","title":"Deploying to production","text":"<p>Now that we have verified the local build works (using <code>sam local start-api</code>), we can use SAM to deploy the function to production. This is achieved by calling <code>sam deploy</code>. The first time this is called, SAM recommends calling it using the guided approach -&gt; <code>sam deploy --guided</code>. This takes you through a series of questions which helps set up the deployment or cloud formation stack. Once deployment is complete, it returns with a publicly invocable URL that you can distribute to your users.</p> <p>Often, you might find that some set up which works in the local dev environment breaks in production. You might end up patching the function multiple times. During such cases, you can simply call <code>sam deploy</code> or better, <code>sam deploy --no-confirm-changeset</code> and let SAM build, configure the changesets and deploy them to production. The function is restarted and will get the updates.</p>"},{"location":"blog/2020/06/18/aws-lambda-functions-with-sam-cli/#sam-specification-the-model","title":"SAM specification (the model)","text":"<p>The model is specified in a template while in YAML format as shown below:</p> <pre><code>Transform: AWS::Serverless-2016-10-31\n\nGlobals:\n  set of globals\n  Defines whether product is a Function, API, SimpleTable, HttpApi\n\nDescription:\n  String\n\nMetadata:\n  template metadata\n\nParameters:\n  set of parameters # values to pass to the template at runtime\n\nMappings:\n  set of mappings # kvp to pass based on conditions\n\nConditions:\n  set of conditions # conditions, logic statements\n\nResources:\n  set of resources # stack resources such as EC2 instance or S3 bucket info or details of the lambda runtime\n\nOutputs:\n  set of outputs # describe the values returned when you view the stack's properties\n</code></pre> <p>Only the <code>Transform</code> and <code>Resources</code> is required. Since this template is derived from AWS CloudFormation template, there are some overlaps. For example, below is the template created by the hello world application</p> <pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: &gt;\n  try-sam-app\n\n  Sample SAM Template for try-sam-app\n\nGlobals:\n  Function:\n    Timeout: 3\n\nResources:\n  HelloWorldFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: hello_world/\n      Handler: app.lambda_handler\n      Runtime: python3.7\n      Events:\n        HelloWorld:\n          Type: Api\n          Properties:\n            Path: /hello\n            Method: get\n\nOutputs:\n  HelloWorldApi:\n    Description: \"API Gateway endpoint URL for Prod stage for Hello World function\"\n    Value: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\"\n  HelloWorldFunction:\n    Description: \"Hello World Lambda Function ARN\"\n    Value: !GetAtt HelloWorldFunction.Arn\n  HelloWorldFunctionIamRole:\n    Description: \"Implicit IAM Role created for Hello World function\"\n    Value: !GetAtt HelloWorldFunctionRole.Arn\n</code></pre> <p>This blog is a quick overview of authoring, testing and deploying as simple Python based Lambda function using the SAM CLI.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/","title":"Building data science projects using Azure-ML stack","text":"<p>This wiki covers the steps involved in building a data science project using Azure Machine Learning Workbench product. This also covers the steps involved in productionizing the model as a web service and accessing it over HTTP using its REST API.</p> <p>Azure ML consists of two major parts  - Azure ML portal  - Azure ML Workbench</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#azure-ml-workbench","title":"Azure ML Workbench","text":"<p>Azure ML provides a cloud infrastructure to turn your machine learning projects into production code with the scalability and availability of its cloud infrastructure. As of this blog, Azure itself does not have ML as Service, but it allows you to turn your projects into one.</p> <p>The Workbench is a desktop application + Python libraries with which you build your ML experiments, taking it from data cleaning, model building to refinement. It provides frameworks to </p> <ul> <li>access datasets</li> <li>build models with defined inputs and outputs</li> <li>refine models by tuning hyperparameters and export model parameters and learning artifacts</li> </ul> <p>The Workbench has a run history that allows you to visualize the run results as a time-series and helps you visualize the effects of each run.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#runtime-environments","title":"Runtime environments","text":"<p>The Workbench supports at-least 3 different runtimes.</p> <ul> <li><code>local</code> - This is your local anaconda Python environment (or R env)</li> <li><code>docker-python</code> - This encapsulates the <code>local</code> environment as a Docker container and runs it with this container. This container could run locally, on host OS, or can run on a remote machine as well (such as a VM or HDInsights cluster).</li> <li><code>docker-pyspark</code> - similar, but is useful for distributed computation using Spark.</li> </ul>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#deployment-environments","title":"Deployment environments","text":"<p>The output from Workbench is your Docker image. Azure ML helps you to deploy this image on Azure Container Services and get a <code>REST</code> API for predictions and inference during production.</p> <p>The ML model consists of   - model file (or dir of such files)  - Python file implementing model scoring function  - Conda dependency file (.yml)  - runtime environment file  - schema file for REST API parameters  - manifest file (auto generated) for building Docker image</p> <p></p> <p>Azure ML Model Management console allows your register and track your models like a version control system.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#azure-ml-portal","title":"Azure ML portal","text":"<p>Azure dashboard contains all Azure sub products, including the ML services. Sign in with free Outlook account or Office 365 account and you get <code>$200</code> in free credits.</p> <p>In the console, search for Machine learning experimentation (which is in preview at the time of this article). This service / dashboard provides an environment for the rest of this article.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#walk-through","title":"Walk through","text":"<p>An important process is to write the Python code (in scripts and notebooks) by using <code>azureml</code> Python library for data access, logging and printing. This is the hook for the Workbench UX to interpret model refinement and display them in the dashboard.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#initialization","title":"Initialization","text":"<ol> <li>Log into the ML portal, create a new experiment. Help image</li> <li>Install Workbench. Note: You need macOS, Win 10 or higher for Docker to run. Then sign into the workbench using the same account. Your experimentation account shows up on Workbench. Help image</li> <li>Create a new project, pick from a template if necessary. Help image</li> </ol>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#model-development","title":"Model development","text":"<p>Your Azure ML projects are primarily <code>git</code> repositories. When you open an existing template project, the <code>home</code> tab renders the <code>readme.md</code> file.</p> <ol> <li>Add datasets - Click on the data tab and plus sign. Use wizard to add data sources. If data is tabular, the Workbench attempts to display it as a table and provide some preliminary statistics.</li> </ol> <p>ML Workbench wants to keep a record of all your EDA and data wrangling steps, to aid reproducibility. Hence it stores a <code>.dsource</code> to record how data is input and a <code>.dprep</code> to record the transformations performed on the data.</p> <ol> <li> <p>A <code>.dsource</code> file gets created that contains connections to your dataset and how your file is read.</p> </li> <li> <p>Any data preparation performed by using the built-in prepare tool is stored in <code>.dprep</code> file.</p> </li> </ol> <p>At any time, you can do the same actions in Python, or turn the UX actions into Python code by right clicking the <code>.dprep</code> or <code>.dsource</code> file and generating code file. The sample code that reads from <code>.dsource</code> and returns a pandas DataFrame is below. Script proceeds to run data preparation using <code>.dprep</code> file.</p> <pre><code>from azureml.dataprep import datasource\nfrom azureml.dataprep import package\n\ndf = datasource.load_datasource('iris.dsource')\n# df.head(10)\n\n# column-prep.dprep is my data prep file name.\ndf = package.run('column-prep.dprep', dataflow_idx=0)\ndf.head(10)\n</code></pre> <ol> <li>Serialize your model and its weights using <code>pickle</code> library</li> </ol> <pre><code>with open('./outputs/model.pkl', 'wb') as mh:\n    pickle.dump(classifier, mh)\n</code></pre> <ol> <li> <p>Save your plots as <code>.png</code> files into the <code>./outputs</code> folder. The Workbench picks this up automatically. For each run, the Workbench stores the outputs within the run numbered folder. From the run dashboard, you can download a file later for any run.</p> </li> <li> <p>Choose runtime as <code>docker-python</code>. The Docker base image is specified in the <code>aml_config/docker.compute</code> and run configurations, Python dependencies in <code>docker-python.runconfig</code> files. Workbench first builds a Docker image from base image, installs dependencies and then runs the file.</p> </li> </ol>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#operationalization","title":"Operationalization","text":"<p>Once model is developed, you need to create a schema file that contains the inputs and output of the prediction service. This service is a Python file that reads the pickled model file, accepts inputs, performs predictions and returns the results.</p> <p>The <code>azureml</code> Python library has methods such as <code>azureml.api.realtime.services.generate_schema()</code> to generate schema. To specify the data types, use <code>azureml.api.schema.dataTypes.DataTypes</code> and <code>azureml.api.schema.sampleDefinition.SampleDefinition</code> to specify a sample input. Documentation on these is very thin and is left to user experimentation.</p>"},{"location":"blog/2018/02/26/building-data-science-projects-using-azure-ml-stack/#notes","title":"Notes","text":"<ul> <li>It appears that using Workbench UX to read and clean data into DataFrames is optional. You can as well do all in Python in standard way and create a DF from a .csv file.</li> <li>You accept script parameters as command line arguments. The Workbench UX provides a generic args text box into which you can type the values when running from UX.</li> <li>The text you want persisted in the logs should be sent to Azure ML logger</li> </ul> <pre><code>from azureml.logging import get_azureml_logger\nrun_logger = get_azureml_logger()\n\nrun_logger.log('text')\n</code></pre> <ul> <li> <p>Write your scripts such that there is <code>1</code> main Python file which in turn calls other files that are necessary.</p> </li> <li> <p><code>control_log</code> file contains the running log of the job with details injected by Workbench</p> </li> <li><code>driver_log</code> file contains the print statements</li> </ul> <p>Sources  - Azure ML help</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/","title":"Building RESTful APIs with Flask in Python","text":"<p>This article demonstrates how to quickly build a RESTful API in Python using the Flask library. To know about RESTful APIs, read the article on Design principles behind RESTful APIs.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#boiler-plate","title":"Boiler plate","text":"<p>There is very little boiler plate necessary when defining a flask app. Something as limited as:</p> <pre><code>from flask import Flask\nfrom datetime\n\n# define a variable to hold you app\napp = Flask(__name__)\n\n# define your resource endpoints\napp.route('/')\ndef index_page():\n    return \"Hello world\"\n\napp.route('/time', methods=['GET'])\ndef get_time():\n    return str(datetime.datetime.now())\n\n# server the app when this file is run\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>In the code above, you have defined 2 endpoints - a root <code>/</code> landing page and a <code>/time</code> endpoint. You can run this app from terminal as <code>python -m &lt;filename&gt;</code> which will start the webserver and give you an IP address to go or call.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#passing-arguments","title":"Passing arguments","text":"<p>When calling an endpoint, users can send the server some information. This info can be sent via <code>GET</code> or <code>POST</code> calls. By following HTTP verbs, if user wants to collect info back, they should send it via <code>GET</code>, else if the call is to perform some operation on the server side, then use <code>POST</code>, <code>PUT</code>, <code>DELETE</code>.</p> <pre><code>from flask import request # used to parse payload\napp.route('/hello')\ndef welcome_message():\n    \"\"\"\n    Called as /hello?name='value'\n    \"\"\"\n    # if user sends payload to variable name, get it. Else empty string\n    name = request.get('name', '') \n    return f'Welcome {name}'\n</code></pre> <p>Note, sometimes your web app needs to make calls to other resources on the web. For this you can use the <code>requests</code> library. The <code>request</code> module of Flask used in the snippet is not to be confused with the <code>requests</code> library.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#rendering-html-pages","title":"Rendering HTML pages","text":"<p>Flask allows you to define a <code>templates</code> directory and put HTML pages into it. In these HTML pages, you can define variables and pass values to be displayed when rendering the HTML pages. This totally expands your possibilities without having to write a line of JS code.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;Thermos - building webapis with flask&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;p&gt;\n    A little bit about the machine running this site:\n\n    &lt;h4&gt;OS type: {{os_type}}; OS name: {{os_name}}&lt;/h4&gt;\n&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Put the above HTML page (saved as <code>index.html</code>) into a <code>templates</code> directory. Then you can render this page as</p> <pre><code>from flask import render_template\napp.route('/welcomePage')\ndef welcome_page():\n\n    return render_template('index.html',\n                            'os_type' = sys.platform,\n                            'os_name' = os.name)\n</code></pre>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#getting-user-input-via-html-pages","title":"Getting user input via HTML pages","text":"<p>Not only are HTML pages good to display content on a UI, they are good to collect user input for processing. The snippet below defines a resource that when called via <code>GET</code>, will show an HTML page with form controls. If accessed via <code>POST</code>, it performs the actual processing defined by the resource.</p> <pre><code>@app.route('/eyeFromAbove', methods=['GET', 'POST'])\ndef eye_from_above():\n    \"\"\"\n    If GET request - Displays a html page with box to enter address\n    if POST request - renders output html with image from satellite\n    :return:\n    \"\"\"\n    if request.method == 'POST':\n        # User could have used the web UI or called the endpoint headless\n\n        # If user used the web UI, then address comes as form data\n        if request.form is not None:\n            address = request.form.get('address', None)\n            date = request.form.get('date', None)\n        else:\n            # request data comes via args\n            address = request.args.get('address', None)\n            date = request.args.get('date', None)\n        if address:\n            geocode_dict = geocode_address_executor(address)\n            lon = geocode_dict['location']['x']\n            lat = geocode_dict['location']['y']\n\n        else: # when no address is loaded\n            flash('No address specified')\n            return redirect(request.url)\n\n        base_url = 'https://api.nasa.gov/planetary/earth/imagery'\n        # for some reason, NASA server rejects the GET request if I send data over payload\n        if date:\n            full_url = f'{base_url}/?lat={lat}&amp;lon={lon}&amp;date={date}&amp;cloud_score=False&amp;api_key={key}'\n        else:\n            full_url = f'{base_url}/?lat={lat}&amp;lon={lon}&amp;cloud_score=False&amp;api_key={key}'\n\n\n        # construct the query and get download url\n        # resp = requests.get(base_url, params)\n        resp = requests.get(full_url)\n\n        if resp.status_code == 200:\n            resp_dict = json.loads(resp.text)\n        else:\n            return json.dumps({'error':resp.text})\n\n        # Download the image from Google Earth Engine API.\n        img_resp = requests.get(resp_dict['url'])\n        if img_resp.status_code == 200:\n            img_filename = address.replace(' ','_').replace('-','').replace('.','').replace('*','').replace(',','')\n            with open(f'eye_in_sky_queries/{img_filename}.png', 'wb') as img_handle:\n                img_handle.write(img_resp.content)\n        else:\n            return json.dumps({'error':img_resp.text})\n\n        # render the HTML page\n        return render_template('eye_from_above.html',\n                               media_type='image',\n                               media_url = os.path.join('/','eye_in_sky_queries',img_filename+'.png'),\n                               img_date = resp_dict['date'],\n                               img_id = resp_dict['id'],\n                               img_dataset = resp_dict['resource']['dataset'])\n\n    elif request.method == 'GET':\n        # case when page is loaded on browser\n        return '''\n            &lt;!doctype html&gt;\n            &lt;title&gt;Enter address to view&lt;/title&gt;\n            &lt;h1&gt;Enter the address to get image of&lt;/h1&gt;\n            &lt;form method=post enctype=multipart/form-data&gt;\n              &lt;input type=text name=address value=address&gt;\n              &lt;input type=text name=date value='2013-12-17'&gt;\n              &lt;input type=submit value=Submit&gt;\n            &lt;/form&gt;\n            '''\n\n# this resource is needed to render images from disk. Needed for the template HTML pages\n@app.route('/eye_in_sky_queries/&lt;filename&gt;')\ndef uploaded_file(filename):\n    return send_from_directory('eye_in_sky_queries',\n                               filename)\n</code></pre> <p>Now, to display the image, we need this part in the template HTML file</p> <pre><code>&lt;body&gt;\n    {% if media_type == 'image' %}\n        &lt;img src={{media_url}} class=\"img-fluid\" alt=\"Responsive image\"&gt;\n&lt;/body&gt;\n</code></pre>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#database-crud-operations","title":"Database CRUD operations","text":"<p>One of the popular use cases of RESTful web services is, to perform operations on backend database via the web. In the snippet below, I am using <code>sqlalchemy</code> library to  create an in-memory <code>sqlite</code> database and perform CRUD - Create, Read, Update, Delete operations on it.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#sqlalchemy-in-2-minutes","title":"<code>sqlalchemy</code> in 2 minutes","text":"<p>Sqlalchemy library is an ORM (Object Relatinal Mapper) which allows representing database elements as Python objects. In addition, it allows you to connect to a DB and perform CRUD ops in addition to others.</p> <p>Connecting to a DB. In this case a sqllite db is created. If you want the db in-memory, then specify <code>memory</code> as location.</p> <pre><code>from sqlalchemy import create_engine\nengine = create_engine('sqlite:///filename.db')\n</code></pre> <p>The next step is to establish a session to this newly created db.</p> <pre><code>from sqlalchemy.orm import sessionmaker\nDBSession = sessionmaker(bind=engine)\nmy_session = DBSession()\n</code></pre> <p>The next step is a little boiler plate and can be seen in many examples using sqlalchemy.</p> <pre><code>from sqlalchemy.ext.declarative import declarative_base\nBase = declarative_base()\n</code></pre> <p>Next step is to create a Model - this represents the table, columns, rows as Python objects.</p> <pre><code>from sqlalchemy import column, Integer, Numeric, String\n\nclass Puppy(Base): #must inherit from declarative base\n    __tablename__ = 'puppy'\n\n    puppy_id = Column(Integer, primary_key=True)\n    puppy_name = Column(String[30])\n    # ... other columns\n</code></pre> <p>Finnaly, we need to create the table define above on the db.</p> <pre><code>Base.metadata.create_all(engine)\n</code></pre>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#inserting-rows","title":"Inserting rows","text":"<p>Create an instance of the class you created that inherited the <code>declarative_base</code>.</p> <pre><code>puppy1 = Puppy(puppy_name = 'nemo') #pass the columns info to the constructor\n\n# insert row\nsession.add(puppy1)\nsession.commit()\n</code></pre> <p>Since, we did not pass <code>puppy_id</code>, the Primary key, the session knows this is a new row and it has to be created. Else, to edit an existing row, you still do an <code>add</code> but have the primary key specified.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#user-facing-crud-api","title":"User facing CRUD API:","text":"<p>Below is an example endpoint that can perform all 4 CRUD operations:</p> <pre><code>@app.route('/addresses/&lt;int:id&gt;', methods=['GET', 'PUT', 'DELETE'])\ndef address_id_handler(id):\n    \"\"\"\n    GET - called as /addresses/25\n    PUT - called to update as /addresses/25?address='abc'&amp;lat=25&amp;lon=89\n    DELETE - called as /addresses/25\n    :param id:\n    :return:\n    \"\"\"\n    if request.method == 'GET':\n        return jsonify(read_address(session, address_id=id))\n\n    elif request.method == 'PUT':\n        address = request.form.get('address','dummy')\n        lat = request.form.get('lat',0.1)\n        lon = request.form.get('lon',0.1)\n        update_address(session, address_id=id, search_string=address, lat=lat, lon=lon)\n        return jsonify({'success': True})\n\n    elif request.method == 'DELETE':\n        delete_address(session, id)\n</code></pre> <p>If you called <code>/addresses/&lt;id&gt;</code> with an existing <code>id</code> via <code>GET</code>, you perform a Read operation. Calling with a new <code>id</code> via <code>PUT</code> will do a Create operation. However, calling with an existing <code>id</code> via <code>PUT</code> will do an Update operation. Finally, <code>DELETE</code> will Delete that record from the DB.</p>"},{"location":"blog/2019/10/20/building-restful-apis-with-flask-in-python/#conclusion","title":"Conclusion","text":"<p>To view the full app in action, checkout the apps section section. To see its source code, see this GitHub repo.</p>"},{"location":"blog/2018/12/04/coding-standards-for-jupyter-notebooks/","title":"Coding Standards for Jupyter Notebooks","text":"<p>Jupyter Notebook has become incredibly popular among data scientists and general users of Python and R. While the Jupyter framework is liberal and lets you be creative, it would benefit you, your team, and your readers if you define a structure and follow it. Based on my experience as developer evangelist and the author of public-facing notebooks for the last three years, I share in this article the patterns I recommend for writing data science samples using Jupyter Notebook. Read more here</p>"},{"location":"blog/2015/03/15/demystifying-python-jargon-datascience/","title":"Demystifying Python jargon found in Data Science","text":"<p>For someone trying to start out using Python for data analysis, the diversity of terminology can be daunting. This article lists out some of the popular terms you hear going around, groups and explains them in a context.. Read more here</p>"},{"location":"blog/2021/10/12/deploying-deep-learning-inference-as-serverless-functions/","title":"Donut or Not! - Deploying Deep Learning Inference as Serverless Functions","text":"<p>There is a common myth that to perform deep learning, one needs high compute, GPU enabled devices. While this is true to a degree, when training deep learning models, it is often just as possible to perform inference using simple CPU based architectures in compute and memory constrained environments - such as serverless functions. This blog takes you through my journey of deploying a simple donut vs bagel vs vada classifier as a AWS Lambda function! </p>"},{"location":"blog/2021/10/12/deploying-deep-learning-inference-as-serverless-functions/#checkout-the-function-at-bitlydonutornot","title":"Checkout the function at bit.ly/donutornot.","text":"<p>At a high-level, this application consists of a deep learning model that was trained using Fastai using a <code>resnet32</code> backbone and exported as a PyTorch model. The model is invoked by a REST API defined using FastAPI library. The whole service is packaged into a Docker image and pushed up to Amazon's AWS ECR container registry. This is then used by a AWS Lambda service to create a function and exposes it using an AWS API gateway, which gives a URL to the service. The HTML front-end is created using Jinja2 templating, something that is common and well established when using micro-frameworks like Flask or FastAPI (as in this case).</p> <p>The picture below shows the technology stack that powers this app.</p> <p></p>"},{"location":"blog/2021/10/12/deploying-deep-learning-inference-as-serverless-functions/#how-to-guide","title":"How to guide","text":"<p>Surprisingly, the training aspects of this model was relatively simpler and predictable. However, it took a lot of reading, trial and error and experimentation to get the serverless application running. The GitHub repo at https://github.com/AtmaMani/donut_or_not has all the set up and deployment information. So, if that's what you are looking for, please head there.</p>"},{"location":"blog/2021/10/12/deploying-deep-learning-inference-as-serverless-functions/#things-i-learnt","title":"Things I learnt","text":"<p>I wanted to write this blog to share my experience creating this app and also take stock of the current state of things when it comes to serverless deep learning. So, here are some observations.</p> <ul> <li>This project was possible because AWS announced support for authoring Lambda functions using Docker images. Not just any images, but they provided the exact same images they use behind the scenes. AWS also expanded the image size limit to <code>10GB</code>. Since the base images are slim to begin with, this large size allowance permits developers to install most of the libs needed for performing data science.</li> <li>AWS SAM CLI makes it possible to create a template suitable for machine learning. While it gives us a framework, I found this to be bare-bones and not sufficiently furnished for someone to get started quickly, particularly for someone who is not a cloud engineer.</li> <li>It took me multiple attempts to figure out how to get the dependencies successfully install on the image. I finally settled with an approach of specifying certain deps in the <code>requirements.txt</code> and certain others to install directly via <code>Docker RUN</code> command. This seems approach seems fragile to me.</li> <li>Once the dependencies were installed, it took several attempts to get the FastAPI service run via Docker. I had problems understanding how to codify my endpoints in SAM's <code>template.yml</code> file. Since my app allows users to upload a JPG (a binary file), I had to read through several StackExchange comments and SAM's GitHub issues to figure out how to specify the MIME type. It appeared quirky and I would have liked this supported out-of-the-box, as it is with Heroku.</li> <li>Once the FastAPI service started running, I ran into a few permission denial issues. Apparently, Lambda runs the container with a user with minimal privileges, unlike the local runtime emulator. Thus, I ended up with new errors that crop up in production, but not in local dev env.</li> </ul> <p>However, this was a great learning experience and now I have a template to follow, if I were to deploy additional models to production. I have a method to perform deep learning inference via a low-cost, low-maintenance, on-demand manner using serverless cloud infrastructure.</p> <p>I hope this blog and GitHub repo is useful for others who are attempting to do the same.</p>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/","title":"Design principles behind RESTful APIs","text":"<p>RESTful stands for \"Representational State Transfer\". Consider this as a concept and a pattern of building client-server APIs. I have been building Python APIs that consume some popular RESTful APIs for the past 5+ years. This article outlines the aspects of a thoughtful and well-designed REST API. Some of the aspects here are from the perspective of a consumer, not the maker of RESTful APIs.</p>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#what-makes-a-web-api-restful","title":"What makes a web API RESTful?","text":"<ul> <li>Presence of a strong separation between client and server</li> <li>Server should not remember previous actions of the client, ergo, be what people call \"stateless\".</li> <li>Each request from client should be treated independently and as if it is the first request. This goes with the previous point about server being stateless. Thus, the onus of sending context is upon the client, not the server to remember.</li> <li>However, when designing anything for a user, you need to make it as simple as possible. Thus, servers can exchange authentication and context for <code>tokens</code> with the client. This allows for a stateless design, but gives users a stateful experience (such as a shopping website remembering what user added to shopping cart) to the client / user.</li> <li>Responses from server can be marked as cacheable or non-cacheable. This way, client does not have to talk to server for the same requests.</li> <li>Server should provide a uniform interface, no matter what the client is(mobile vs desktop vs another server).</li> </ul> <p>Some advanced features of RESTful APIs</p> <ul> <li>If the logic is complex, you can implement a layered system where clients interact with say, 'Server A'. Then, Server A would interact with many other servers to fulfill the request and give results back to client. Client does not have to know how to talk to the rest of the servers to accomplish this.</li> <li>code on demand: Server might optionally send executable code (like JS) to run on client to fulfil a request.</li> </ul>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#http-request-structure","title":"HTTP Request structure","text":"<p>Next, let us talk about what gets exchanged between the client and the server. A request from client to server consists of 3 parts:</p> <ol> <li>header - which contains:<ol> <li>A request line. This line contains a HTTP verb, a URI and HTTP version number. An example syntax of request line is <code>GET /index.html HTTP 1.1</code></li> <li>optional request headers which appear as kvp. For instance <code>Accept: image/gif, image/jpeg, */*</code>, <code>Accept-Language: en-us</code>.</li> </ol> </li> <li>blank line</li> <li>body (optional) - body can contain any additional information, such as auth info etc.<ol> <li>for instance <code>puppyId=12345&amp;name=Fido+Lava</code></li> </ol> </li> </ol>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#http-response-structure","title":"HTTP Response structure","text":"<p>Similar to a request, the response from server to client consists of 3 same parts</p> <ol> <li>a header, which contains<ol> <li>status line which has the status code, HTTP version</li> <li>optional response headers</li> </ol> </li> <li>blank line</li> <li>body (optional). The body contains what the client asked for. For instance a mp3 file, image file or an html page.</li> </ol>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#making-restful-apis-elegant","title":"Making RESTful APIs elegant","text":"<p>The rules listed above are just the skeleton. Although not strictly needed, when APIs use the the following design patterns, they become easy for the end user to predict and understand the architecture of the backend system they are working with.</p> <p>URI design</p> <ol> <li>URI should take name of resource, not the action to take on them. For instance, <code>/tickets/4</code> is good, while <code>getTicketInfo/4</code> is not. Remember, URIs should be nouns, not verbs.</li> <li>URI should be a plural form for each resource name. For instance, <code>/puppies</code>, <code>/tickets</code>. Depending on the resource needed for the backend, calling the plural form of a resource (<code>/items/</code>) can list all the items, or give a summary info.</li> <li>Use HTTP verbs to indicate action. The verbs are <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, <code>HEAD</code>, <code>OPTIONS</code>, <code>TRACE</code>, <code>CONNECT</code>.<ol> <li><code>HEAD</code> and <code>GET</code> can be called by web crawlers and search engines, so ensure these resources can only get information and not modify.</li> <li><code>GET</code>, <code>POST</code> resources typically give information to client</li> <li><code>PUT</code>, <code>DELETE</code> resources can add, update, delete resources on the server. Thus we cover the <code>CRUD</code> operations using HTTP verbs.</li> </ol> </li> <li>Use HTTP status codes such as <code>404</code>, <code>401</code> along with error messages appropriately.</li> <li>Expose child resources progressively. For instance, if <code>/items/</code> does not expose all the items and only provides a summary or count, then expose a <code>/items/search</code> for client to search the backend database. Finally, ensure <code>/search/</code> accepts well known SQL syntax queries, don't make a custom one.</li> <li>Give the benefit of doubt to the client. Build the REST handler to be fault tolerant, error tolerant by building the validation logic on backend, and not on the client side.</li> </ol> <p>API versioning</p> <ol> <li>Versioning allows to maintain backward compatibility when you build a permanent web API.</li> <li>You can either add version in the URI, as <code>/puppies/v2/resource</code> or in the header of the call.</li> </ol>"},{"location":"blog/2019/10/15/design-principles-behind-restful-apis/#conclusion","title":"Conclusion","text":"<p>To learn more about RESTful APIs, go   - restfulapi.net  - Udacity course on Flask</p> <p>To look at an example of building RESTful APIs, go to my blog article Building RESTful APIs with Flask in Python.</p>"},{"location":"blog/2017/07/20/devops-today/","title":"DevOps Today","text":"<p>As software development matures as an industry, there arose a need to identify and a group a set of tasks and folks, typically in back office, that are required to keep the rest of the software development shop running. These are the folks that keep the servers spinning, repositories and build process going, plumb and connect the various automation tools into a giant contraption. The industry came around to classify the various roles as <code>DevOps</code> short for Developers in Operations.</p> <p>This blog is a short compendium of tools used by DevOps today (early 2017).</p>"},{"location":"blog/2017/07/20/devops-today/#version-control-systems","title":"Version control systems","text":"<p> Called VCS in short form, this is where the sourcecode comes to live. Many popular VCS exist today, but you are more likely to have heard about <code>git</code> or <code>github</code> which are arguably the most popular today. VCS mark every change made to the source code and record who changed it, why and how each changes are linked to one another in a time line. In a large shop, about 50 or so developers can edit the same file yet stay sane and not overwrite or trample on other's work.</p> <p>As the product reaches an important milestone, the team would pull the source code into a build process, convert it into a finished product so the sales team can sell it to customers. But today, in the world of agile software development the philosophy is to build often, ship often and get feedback and implement it immediately thereby reducing the risk of catastrophic failure or going irrelevant. Thus, the shipping process which was once in two years, now has to be performed every day! To enable this the DevOps use two of the following tools in abundance.</p>"},{"location":"blog/2017/07/20/devops-today/#containerization","title":"Containerization","text":"<p>Traditionally software was sold in CD-ROMs which customers would buy and install in their computer. The software was as much a physical entity as it got to be. When the user installed, the software made tangible changes to the customer's operating systems as it copied numerous files into different places and took anywhere from an hour to several. This process was slow, error prone and made it very hard to build software for different operating systems (Windows vs MacOS vs Linux vs Solaris). As the internet and cloud computing came about, server software was installed on computers running on the cloud which the customers accessed as a service, hence the name <code>Saas</code> Software as a Service. With SaaS when the demand rose, the same software had to be installed on additional machines to meet the demand, then removed when the surge dropped. This process of mustering additional machines had to be performed in a matter of seconds if the company wanted to give a seamless experience to the customers and the traditional sense of shipping software as an installable file quickly became impossible.</p> <p>The industry tried various solutions for this problem, one of which was to pre-create additional computers with the necessary software installed and have them turn on only when demand rose. But this meant a number of your assets are sitting idle for the most part and used sub-optimally. To combat this problem, a concept of virtualization was invented. In virtualization, the software and all the necessary dependencies including the operating system are compressed into an <code>image</code> file. When the demand spiked, any computer hardware can be quickly programmed to power up that image and serve the software. This was way faster than installing the full software and is used widely even today. </p> <p>As the containerization technology matured, the DevOps engineers found ways to shed the weight of the image. They invented a leaner method where they hand pick only the necessary parts of the operating sytem and the dependencies and the actual software and put them into something called <code>containers</code>. Containers turned out to be much smaller than images and they were able to power up and run much faster than virtual machines. Further, containers would run the same no matter the flavor of the host computer's architecture, which meant, software engineers can build software for linux and have them sold to a customer which is primarily a Windows shop.</p>"},{"location":"blog/2017/07/20/devops-today/#docker","title":"Docker","text":"<p>.. image:: /images/docker_logo.png</p> <p><code>Docker</code> is one of the most popular containerization technologies. A <code>docker repository</code> contains a bunch of such container images that a software sells. The <code>docker registry</code> is a place where such repositories are hosted so customers can search for, buy and use. The <code>docker hub</code> and more recently <code>docker store</code> are public websites which host registries.</p> <p>When the customer wants to use a software shipped as a <code>docker image</code>, he/she has to install a <code>docker engine</code> which will act as a host to power up the <code>docker images</code> into <code>containers</code>. Increasingly customers identify a powerful computer which they designate for docker engine so it can power up a number of containers which the rest of the company can use as a service. In such cases they use <code>docker swarm</code> which is a cluster of docker engines running in <code>swarm mode</code>.</p> <p>Some companies might lease a cloud from <code>docker cloud</code> and run their containers on hardware leased from docker company while other larger companies can by <code>docker datacenter</code> software so they can power up an entire datacenter using docker based technology.</p>"},{"location":"blog/2017/07/20/devops-today/#continuous-integration","title":"Continuous integration","text":"<p> At the turn of this decade (2011), the software industry embraced a new concept of reducing the gap between software build process and customer involvement. In the old water fall model, the build process had definite stages of planning, constructions, testing and deployment. Often with the pace at which technology evolved, the software being built went out of date before it got delivered. To solve this problem, the industry adopted an <code>agile</code> framework where the build cycles are reduced to a short 3 week period and at the end of it, a fully build software was shipped to the customer. The customer would use, provide feedback which the development team would work on and in another 3 weeks, send out an increment.</p> <p>This model works well, but it also means some tasks which were performed once a year such as build, test, etc. has to be performed every day and in some cases every hour. Thus the devops engineers devised a procedure where at every time a change is made to the source code (or at set intervals) the entire source code gets compiled, built into a finished product, run tests against it and pushed into the deployment phase. This is called <code>continuous integration</code>. To orchestrate this complex process a number of tools are available in the market, some of which are <code>Jenkins</code>, <code>CircleCI</code>, <code>TravisCI</code>, <code>TeamCity</code> etc.</p>"},{"location":"blog/2017/07/20/devops-today/#chef","title":"Chef","text":"<p>Continuous integration solved the problem of keeping the agile software development in a loop. However, when the business model is to sell software as a service, devop engineers had to also automate the process of updating the service with the latest software, something I call <code>agile deployment</code>. To meet these needs, devop engineers need an all overseeing automation platform which can orchestrate the entire process of <code>agile development</code> and <code>agile deployment</code>. <code>Chef</code> is one such product that is popular in the industry today. Though a set of <code>cookbooks</code>, engineers can automate the creation and deployment of containers that result from the continuous integration framework.</p>"},{"location":"blog/2017/07/20/devops-today/#conclusion","title":"Conclusion","text":"<p>Thus this blog outlined some of the popular tools in the devops tool belt. By no means this is a complete or exhaustive list and I have grossly generalized a lot of the processes involved. However the idea of software development in today's world is portrayed to the best accuracy as possible.</p>"},{"location":"blog/2021/02/04/embedding-maps-in-photos/","title":"Where was this photo taken? A Python based web-app to extract location information and embed maps in photos","text":"<p>This is a draft, to be completed soon. Python full-stack web app created for Esri Dev Summit 2021.</p>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/","title":"A Gentle Introduction to the AWS Platform","text":""},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#aws-platform-from-30k-view","title":"AWS platform from 30k view","text":"<p>The AWS platform is an offering from Amazon and by far is considered the industry leader in this segment. It is available in 190 countries around the world and is used by hundreds of thousands of business and governmental bodies around the world.</p>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#regions-and-availability-zones","title":"Regions and availability zones","text":"<p>An AWS region is a physical location in the world, where there can be multiple availability zones. Availability zones consist of one or more discrete data centers, each with redundant power, networking, comms, housed in separate facilities. There are over <code>60</code> availability zones within over <code>20</code> regions around the world.</p>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#components-of-the-platform","title":"Components of the platform","text":"<p>The sheer number of services available from the AWS platform can be quite dizzying. I made the schematic below to refer to the most commonly used services by data scientists:</p> <p></p> <p>For convenience, the services are grouped under a set of broad categories such as analytics, application integration, database, compute, IoT... etc.</p>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#analytics","title":"Analytics","text":"<p>Below are some popular services under the analytics bucket</p> <ol> <li>Athena - interactive query service that makes it easy to analyze data in S3 using SQL</li> <li>EMR - managed Hadoop framework that makes it easy to process vast amounts of data across dynamically scalable EC2 instances.</li> <li>CloudSearch - a search solution for your website or app</li> <li>Elasticsearch - search service</li> <li>Kinesis - analyze real-time and streaming data</li> <li>Redshift - fast, scalable data warehouse with 10x performance using ML, massively parallel query execution, columnar storage on high performance disk.</li> <li>QuickSight - cloud-powered BI service</li> <li>Data Pipeline - process and move data b/w different AWS compute and storage services</li> <li>Glue - fully managed ETL service</li> <li>Lake Formation - service to set up a secure data lake in days.</li> </ol>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#compute","title":"Compute","text":"<ol> <li>EC2 - Elastic Compute Cloud - the service that changed it all. Allows you to pay for only what you consume. 3 types of instances: On-demand (pay as you go), Reserved (dedicated resources at 70% discount) and Spot (utilize unused capacity at 80% discount)</li> <li>Elastic cloud registry - Docker container registry</li> <li>Elastic container service - container orchestration service</li> <li>Elastic container service for Kubernetes</li> <li>Lightsail - VPC with AWS</li> <li>Batch - batch compute jobs on AWS</li> <li>Beanstalk - deploy scalable web apps developed with Python, .NET, PHP, Node.js etc</li> <li>Fargate - compute engine for AWS that allows you to run containers without having to manage servers or clusters.</li> <li>Lambda - run code without provisioning servers. No charge when code is not running.</li> <li>Outposts - bring native AWS services, infrastructure, operating models to any data center / on-premises facility.</li> </ol>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#database","title":"Database","text":"<ol> <li>Aurora - MySQL, PostgreSQL compatible relational database engine.</li> <li>DynamoDB - key-value and document database that delivers single-digit millisecond performance at scale.</li> <li>Neptune - graph database.</li> </ol>"},{"location":"blog/2020/05/05/gentle-introduction-aws-platform/#machine-learning","title":"Machine Learning","text":"<ol> <li>SageMaker - fully managed platform for quickly building, training, deploying ML models at scale.</li> <li>SageMaker Ground Truth - service offers access to public and private human labelers and provides them with built-in workflows and interfaces to build training data. Lowers cost by <code>70%</code>. A model in parallel keeps learning from human labelers and can offer labeling service.</li> <li>Comprehend - NLP service to find insights in text.</li> <li>Lex - build conversational interfaces into any app. It is the same service that powers Alexa.</li> <li>Polly - text to lifelike speech. Supports <code>47</code> voices in <code>24</code> languages.</li> <li>Rekognition - image analysis service that can detect objects, scenes, faces, compare faces, visual search etc.</li> <li>Translate - translation service</li> <li>Transcribe - automatic speech recognition (ASR). Allows you to transcribe files in S3 and add a textual search of audio / conversation files.</li> <li>Elastic inference - attach low-cost GPU powered inference to EC2 and SageMaker instances.</li> </ol>"},{"location":"blog/2019/11/12/gentle-introduction-azure-platform/","title":"A Gentle Introduction to the Azure platform","text":"<p>Azure from Microsoft is one of the few popular cloud platforms out in the market. This article lays out a field guide to the vastness of this platform. If you would like to know about cloud computing in general, checkout the blog titled A gentle introduction to cloud computing.</p> <p>In this section, let us take a closer look at the Azure platform and the services it provides. Azure provides over a 100 services which it groups into <code>8</code> categories:</p> <ol> <li>compute services: VMs, containers, serverless, kubernetes, Azure batch, Azure functions</li> <li>cloud storage: disk attached to VMs, file shares, DBs, blob storage, queue storage, SQL and noSQL DBs</li> <li>networking: private networks to on-prem</li> <li>app hosting: run webapps on Windows or Linux, and a marketplace of other services</li> <li>AI: cognitive services - Azure ML service, ML studio</li> <li>IoT: integrate sensors into your cloud, build dashboards etc</li> <li>integration: logic apps, service bus, workflow orchestration</li> <li>security: identity services, intelligence monitoring</li> </ol> <p>Look at the platform help doc under the picture for a table.  Image credit: Azure platform help</p>"},{"location":"blog/2019/11/12/gentle-introduction-azure-platform/#account-types","title":"Account types","text":"<p>The first <code>12</code> months of account creation gives us free access. After this, it is a pay-as-you-go model. However there are <code>25</code> services that will always be free.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/","title":"A Gentle Introduction to Cloud Computing","text":"<p>Computing mindset for the 2010s</p> <p>Cloud computing took off in the decade of 2010s. Up until then, when people wanted to run an application, they had to buy computers, databases, switches, network, domains, software, hire IT staff to deploy and maintain anything on the internet. This is similar to learning everything about electricity before you can learn to turn on and off the power switch. Cloud computing changed all of this and allowed developers to build things on the internet without having to worry about hardware and networking.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#what-is-cloud-computing","title":"What is cloud computing?","text":"<p>Cloud computing is renting of resources like storage, CPU, GPU from a server farm. You only pay for what you use - time or storage.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#3-compute-patterns","title":"3 compute patterns","text":"<p>We can group compute patterns on the cloud to 3 categories:</p> <ul> <li>Virtual Machines - where you rent VMs of certain guest OS and manage all the software, OS updates, driver updates etc on it.</li> <li>Containers - similar to VMs, but lack a separate guest OS. All the app logic and data is bundled into an image that is run directly on the host OS</li> <li>Serverless - this is just code, not even images. You write your app to constitute a bunch of functions which are called on demand.</li> </ul> <p></p> <p>Unlike VMs and containers, when using serverless, you only pay for function execution time. In VMs and containers, you pay for run time of container and VM, even if they are idle.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#characteristics-of-cloud-compute","title":"Characteristics of cloud compute","text":"<p>Cloud computing is cost effective as there is no up-front infrastructure cost, no fixed rent, no separate electricity or utility bills for your servers.</p> <p>It is scalable on demand. You can vertically scale by switching to a host machine with more RAM and CPUs or horizontally scale by distributing load to many servers and scaling out.</p> <p>Scaling can be automatic or manual and can be based off a threshold such as CPU load or traffic per second. Thus, the cloud provider can add more resources during peak times and remove unused resources during down time. This nature of scalling is called being elastic.</p> <p>Cloud gives the promise of being current - with respect to security patches, OS updates, hardware improvements etc and reliable - as redundancy is built-in by replicating / backing up periodically. This makes it fault tolerant and recoverable from disasters.</p> <p>All of these happen automatically, in the background, without any interruption to your apps or to the users consuming your apps. Further, cloud makes your app go global as it can replicate content across multiple data centers, allowing for minimal network lag when accessed from across the globe.</p> <p>On top of all this, cloud is secure as they have some of the best firewalls, anti viruses and dedicated team working to thwart attacks. They are also physically secure as they are remote in secure facilities.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#cost-savings-with-cloud-compute","title":"Cost savings with cloud compute","text":"<p>Capital expenditures, such as server costs, storage costs, network costs, backup and DR costs, utility costs are called as <code>CapEx</code> costs. These are typically encountered in the traditional datacenter model and mostly are up-front, but can become recurring as demand changes and technologies shift over time.</p> <p>Operational costs such as building custom features, scaling costs, licensing costs etc are called as <code>OpEx</code> costs.</p> <p>The graphic below illustrates how demand and costs change over time. By moving to the cloud, companies can limit both these costs, while maintaining superior experience for the end users.</p> <p></p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#cloud-deployment-patterns","title":"Cloud deployment patterns","text":"<ol> <li>Public cloud: most common, most simple, everything including hardware, storage, execution is managed by provider. Hardware is typically shared across multiple cloud customers (<code>multitenant</code>). Primary advantage is cost.</li> <li>Private cloud: next common - could be in your premises or actually on the cloud itself (as in <code>VPC</code>). You become the maintainer of the cloud and provision to internal customers. Advantages include highest data ownership, compliance with certain government regulations etc.</li> <li>Hybrid cloud: is a mix of both. Typically, the public cloud contains as much public elements as possible and the private contains the most secure and sensitive information / systems.</li> </ol>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#3-types-of-cloud-services","title":"3 types of cloud services","text":"<ol> <li>IaaS: Infrastructure as a Service: you have complete control over the software stack installed on the servers. The machines / hardware is still managed by the cloud provider, but you manage the host OS, updates, load balancing etc. Think of IaaS as renting hardware. If you want a machine that is dedicated to you, but not necessarily on your premises, then this is the best. The cost is typically pay-as-you-go.</li> <li>PaaS: Platform as a Service: is somewhere in the middle, it provides the entire cloud provider platform to manage the full life-cycle of the product you build, from development to test to deployment.</li> <li>SaaS: Software as a Service: is typically what you build and sell. However, you can subscribe services from the cloud provider as well - for instance AI services such as vision and audio transcribing, email services, storage services etc.</li> </ol> <p>As you thought, each type sits on top of the pervious level, thereby abstracting some level of complexity. The graphic below explains this better.</p> <p></p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#popular-cloud-providers","title":"Popular cloud providers","text":"<p>As of this blog, there are a number of cloud providers out there - Azure, AWS, Rackspace, Linode, Heroku, GCC etc. However, AWS from Amazon and Azure from Microsoft are industry leaders and many other cloud providers actually build on top of these providers. To know more about these, checkout my other blogs on this topic.</p>"},{"location":"blog/2019/12/11/gentle-introduction-cloud-computing/#references","title":"References","text":"<ol> <li>Azure - principles of cloud computing</li> </ol>"},{"location":"blog/2017/10/27/harness-the-power-of-gis-python-api/","title":"Harness the Power of GIS with the ArcGIS API for Python","text":"<p>Link: https://www.esri.com/about/newsroom/arcuser/harness-the-power-of-gis-with-the-arcgis-api-for-python/?rmedium=esri_com_redirects01&amp;rsource=/esri-news/arcuser/fall-2017/harness-the-power-of-gis-arcgis-api-for-python</p> <p>The ArcGIS API for Python is a new Python library for working with maps and geospatial data that is powered by Web GIS, whether online or on-premises. It is a Pythonic API that uses Python best practices in its design and employs standard Python constructs and data structures with clean, readable idioms... Read more here</p>"},{"location":"blog/2018/10/26/house-hunting-the-datascientist-way/","title":"House hunting\u200a\u2014\u200athe data scientist way","text":"<p>At some point in time, each of us would have went through the process of either renting or buying a house. Whether we realize or not, a lot of factors we consider important are heavily influenced by location. In this article, we apply the data wrangling capabilities of scientific Python ecosystem and geospatial data visualization &amp; analysis capabilities of the ArcGIS platform to build a model that will help shortlist good properties (houses). Read more here</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/","title":"How many snakes do you need? - An introduction to concurrency and parallelism in Python","text":""},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#performance-matters","title":"Performance matters","text":"<p>At some point, every Python developer wonders if it's their program that is slow, or Python that is slow. In most cases, it is their program itself. Although Python gets a bad rap for being slower than compiled languages like C, C++, developers can utilize concurrency and parallelism to see significant gains.</p> <p>So, what's the difference between concurrency and parallelism?</p> <ul> <li>Concurrency is the virtue of tasks not holding up one another and letting the program to progress. Think of blocking vs non-blocking.</li> <li>Parallelism is the practice of breaking a large task into smaller subtasks that can run in parallel, at the same time.</li> </ul> <p>Both concurrency and parallelism share implementation aspects. Concurrency is more about how a program is structured vs parallelism is doing things in parallel. Thus, concurrency is not inherently parallel, but doing things at the same time (by switching context from one thread to another). In general, UI design cares more about concurrency where as big data and scientific data processing cares more about parallelism.</p> <p>It is also relevant to discuss another paradigm in computing called asynchronous (vs synchronous) processing. A program is set to be asynchronous, if does not expect the caller to wait until it finishes execution. Programs accomplish this by providing a <code>jobid</code> for each task which the caller can poll at intervals to know if the task finished or not. Thus, the caller can submit a job to the async program, proceed to do other stuff and then use the result when it really needs it. Thus, even though a calling program may not implement parallelism, it can now run tasks concurrently by using async programming model.</p> <p>Another way to differentiate concurrency from parallelism is to simply say, parallelism involves threads in different processors, which allows them to execute at the same time, whereas concurrency is multiple threads on the same process, which although are non-blocking, they do not strictly execute all at the same time. However, these threads still individually make progress since the process will cycle through them.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#threads-vs-processes","title":"Threads vs Processes","text":"<p>A process is an instance of your program that is being executed. It has <code>3</code> elements - the code, the data (variables) used by the code and the state of the process (execution context). Each process has its own address space and don't typically talk to each other.</p> <p>Each process can have one or more threads. Threads are lightweight as they share the address space with other threads within the same process and is not treated as a separate entity by the host OS.</p> <p>Typically, operating systems cycle through threads in a round-robin fashion. The thread that gets to execute is called the main thread. But how long will it be the main thread? In Python, a thread will execute until</p> <ul> <li>it is finished</li> <li>until it is waiting for some IO</li> <li>starts a sleep</li> <li>has been running for <code>15ms</code></li> </ul> <p>after which, the OS will switch to another in its queue before returning back to it.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#threading","title":"Threading","text":"<p>A thread in Python can be created using the <code>Thread</code> class from the <code>threading</code> module. A thread can be in one of 3 states.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#thread-states","title":"Thread states","text":"<p>When a thread is created, it just exists, remains in an un-started state. Once you call the <code>start()</code> method, it goes to one of 3 states</p> <ul> <li>Runnable - a state where the processor can execute it</li> <li>Running - currently being the <code>active</code> thread</li> <li>Waiting - where it is blocked by another process or thread. The <code>join()</code> method allows you to represent this dependency.</li> </ul> <p>The scheduler moves the thread through these states. The thread remains alive until its <code>run()</code> method terminates, upon which, it becomes dead. The graphic below (from Hunt J. (2019)) explains these states quite well.</p> <p></p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#instantiating-the-thread-class","title":"Instantiating the Thread class","text":"<p>The constructor for Thread looks like below</p> <pre><code>class threading.Thread(group=None, target=None, \n                      name=None, args=(), kwargs={},\n                      daemon=None)\n</code></pre> <p><code>group</code> is reserved, primarily to be used when <code>ThreadGroup</code> class is implemented. <code>target</code> accepts a callable object (like a method). The <code>run()</code> method of the Thread object will call that that object. <code>name</code> is the name of the thread and by convention takes the form <code>Thread-N</code> where N is a number. <code>args</code> collects the arguments to pass to the callable object and <code>kwargs</code> does the same but with named arguments.</p> <p>So, a simple example could be:</p> <pre><code>from threading import Thread\n\ndef simple_worker(greet='hello'):\n    print(greet)\n\nt1 = Thread(target=simple_worker)\nprint(t1.is_alive())  # prints False\nt1.start()  # prints hello\nprint(t1.native_id)   # prints the thread ID.\n</code></pre> <p>Typically, when you execute in a separate thread, you will notice the kernel's main becomes free even if the thread you spawned is still running. For example see this:</p> <pre><code>from time import sleep\n\ndef worker():\n    for i in range(0,10):\n        print('.', end='', flush=True)\n        sleep(1)\n\nprint('Starting')\n\nt2 = Thread(target=worker, name='t2')\nt2.start()\nprint('\\nDone')\n</code></pre> <p>will print</p> <pre><code>Starting\n.\nDone\n.......\n</code></pre> <p>See <code>Done</code> got printed before all the dots got printed, which means the execution proceeded right along with the next steps. The worker did not block the main thread.</p> <p>You can make one thread wait for another using the <code>join()</code> method. If you add a <code>t2.join()</code> after <code>t2.start()</code>, it would wait for the worker to finish and then print <code>Done</code>.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#inspecting-threads","title":"Inspecting threads","text":"<p><code>threading.enumerate()</code> will print all the threads that are running:</p> <pre><code>[&lt;_MainThread(MainThread, started 4515802560)&gt;,\n &lt;Thread(Thread-2, started daemon 123145471606784)&gt;,\n &lt;Heartbeat(Thread-3, started daemon 123145488396288)&gt;,\n &lt;HistorySavingThread(IPythonHistorySavingThread, started 123145506258944)&gt;,\n &lt;ParentPollerUnix(Thread-1, started daemon 123145523585024)&gt;\n &lt;Thread(t2, started 123145540374528)&gt;]  ## -&gt; my thread\n</code></pre> <p>and <code>threading.current_thread()</code> will print the currently active thread:</p> <pre><code>&lt;_MainThread(MainThread, started 4515802560)&gt;\n</code></pre>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#multiple-threads","title":"Multiple threads","text":"<p>You can start multiple threads for the same worker function. Each thread will invoke the function once, but will retain its own local heap space as can be seen below:</p> <pre><code>from time import sleep\nimport random\n\ndef worker():\n    mark = ['a','b','c','d','e','f','g']\n    prefix=random.choice(mark)  # will choose a prefix at random\n\n    for i in range(0,10):\n        print(prefix+str(i), end=\" \", flush=True)\n        sleep(1)\n\nprint('Starting')\n\nta = Thread(target=worker, name='ta')\ntb = Thread(target=worker, name='tb')\ntc = Thread(target=worker, name='tc')\nta.start()\ntb.start()\ntc.start()\nprint('\\nDone')\n</code></pre> <p>will print</p> <pre><code>Starting\na0 d0e0  \nDone\na1d1e1   a2d2e2   e3d3a3   e4d4a4   e5 d5a5  e6a6d6   a7e7d7   e8d8a8   e9d9a9\n</code></pre> <p>In this case, <code>a</code>,<code>d</code>,<code>e</code> are the prefixes each thread chose. As you see in the print, the threads don't go off in sequence as the prefix arrive mixed in the prints. The same thread never prints in succession, which would mean as soon as the sleep is encountered, the main thread switches to the other threads and so on until each thread has to eventually finish the sleep time.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#establishing-dependency","title":"Establishing dependency","text":"<p>The operating system reserves the right to schedule your threads. Thus as a programmer, you do not have the ability to deterministically know when your threads will start and finish. However, in real life, you need certain workers to finish before you can proceed to the next step. You establish this using <code>join()</code> methods off <code>Thread</code> objects. See example below:</p> <pre><code>\"\"\" Two threads cooking soup \"\"\"\nimport threading\nimport time\n\nclass ChefOlivia(threading.Thread):\n    def __init__(self):\n        super().__init__()\n    def run(self):\n        print('Olivia started &amp; waiting for sausage to thaw...')\n        time.sleep(3)\n        print('Olivia is done cutting sausage.')\n\n# main thread\nif __name__ == '__main__':\n    print(\"Barron started &amp; requesting Olivia's help.\")\n    olivia = ChefOlivia()\n    print('  Olivia alive?:', olivia.is_alive())\n\n    print('Barron tells Olivia to start.')\n    olivia.start()\n    print('  Olivia alive?:', olivia.is_alive())\n\n    print('Barron continues cooking soup.')\n    time.sleep(0.5)\n    print('  Olivia alive?:', olivia.is_alive())\n\n    print('Barron patiently waits for Olivia to finish and join...')\n    olivia.join()\n    print('  Olivia alive?:', olivia.is_alive())\n\n    print('Barron and Olivia are both done!')\n</code></pre> <p>which prints</p> <pre><code>Barron started &amp; requesting Olivia's help.\n  Olivia alive?: False\nBarron tells Olivia to start.\nOlivia started &amp; waiting for sausage to thaw...\n  Olivia alive?: True\nBarron continues cooking soup.\n  Olivia alive?: True\nBarron patiently waits for Olivia to finish and join...\nOlivia is done cutting sausage.\n  Olivia alive?: False\n</code></pre> <p>In the example above, <code>Olivia</code> is the worker thread, on which the main thread waits before proceeding to a certain step. The example also shows how to inherit a <code>Thread</code> class. When doing this, you only override two methods - the constructor and <code>run()</code> which the scheduler will call once the thread is in Runnable state.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#daemon-threads","title":"Daemon threads","text":"<p>Normally, when a program spawns child threads, the program needs to wait until the child completes (that is, if you don't already have a dependency established using a <code>join()</code>). This may not always be ideal and you might want to kill off the child when the main program exits. You can establish this behavior using <code>daemon</code> threads. Daemon threads are created by passing that parameter to the constructor or by setting the <code>daemon</code> property to <code>True</code>. Once started, you cannot change a normal thread to a daemon thread.</p> <p>In the example below, if you did not set <code>daemon=True</code>, the child thread would run forever causing the program to never terminate.</p> <pre><code>import threading\nimport time\n\ndef kitchen_cleaner():\n    while True:\n        print('Olivia cleaned the kitchen.')\n        time.sleep(1)\n\nif __name__ == '__main__':\n    olivia = threading.Thread(target=kitchen_cleaner, daemon=True)\n    olivia.start()\n\n    print('Barron is cooking...')\n    time.sleep(0.6)\n    print('Barron is cooking...')\n    time.sleep(0.6)\n    print('Barron is done!')\n</code></pre> <p>which prints</p> <pre><code>Olivia cleaned the kitchen.\nBarron is cooking...\nBarron is cooking...\nOlivia cleaned the kitchen.\nBarron is done!\n</code></pre> <p>The daemon thread is quit abruptly when the main thread terminates. Thus you should be careful what kinds of ops are relegated to a daemon thread. Good options are using daemon for heartbeat, garbage collection, license checks etc.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#multiprocessing","title":"Multiprocessing","text":"<p>Multiprocessing is running jobs concurrently on multiple processors or cores. This allows developers to truly use modern compute hardware, allowing tasks to run truly in parallel. This mode of computing is useful in data analytics, image processing, animation and gaming.</p> <p>Similar to threading, Python provides a <code>multiprocessing</code> module and a <code>Process</code> class. This can be used to run a callable object such as a function in a separate process. Dependency between processes can be expressed using <code>join()</code> methods. Processes created this way are directly managed by the operating system. Processes are much more heavier and take resources to spin up compared to threads. The advantage though is the ability to exploit multiple cores. </p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#instantiating-a-process","title":"Instantiating a <code>Process</code>","text":"<p>Constructing a process looks like below:</p> <pre><code>class multiprocess.Process(group=None, target=None, name=None,\n                            args=(), kwargs={}, daemon=None)\n</code></pre> <p><code>group</code> is reserved as in threading and is to be used along with the threading API. <code>target</code> accepts a callable object, which will be invoked by the <code>run()</code> instance method. <code>name</code> is the process name, <code>args</code> accepts a tuple to pass to the called function and <code>kwargs</code> does the same with named args. <code>daemon</code> represents whether the process needs to be run as a background daemon.</p> <p>The <code>Process</code> class provides useful methods and properties</p> <ul> <li><code>start()</code> which arranges for the <code>run()</code> to be started in a separate process</li> <li><code>join([timeout in sec])</code> to join the current process with another. Current is blocked until timeout or the blocking process ends.</li> <li><code>is_alive()</code></li> <li><code>name</code> - the process's name. Has no semantics, can be useful for debugging</li> <li><code>daemon</code> - bool flag</li> <li><code>pid</code> - process ID</li> <li><code>exitcode</code> - <code>None</code> if process has not terminated or value</li> <li><code>terminate()</code> - to terminate a process</li> <li><code>kill()</code> - same as terminate, slight differences based on OS.</li> <li><code>close()</code> - releases all resources used by the process. Raises <code>ValueError</code> if the process is still running.</li> </ul>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#using-a-pool","title":"Using a Pool","text":"<p>Since creating processes are expensive, one option is to reuse processes within a given application. The <code>Pool</code> class represents a pool of worker processes and has methods that allow tasks to be offloaded to these worker processes. A Pool can be created as below:</p> <pre><code>class multiprocessing.pool.Pool(processes=None, initializer=None,\n                                initargs=(), maxtasksperchild=None,\n                                context=None)\n</code></pre> <p>where <code>processes</code> is the number of workers to use. Default is <code>os.cpu_count()</code>. <code>initializer(*initargs)</code> is used to represent the method to call and its arguments. <code>maxtasksperchild</code> is the number of tasks a worker can complete. If None, the worker will live as long as the pool.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#poolmap-pattern","title":"<code>Pool.map()</code> pattern","text":"<p>An example of using the Pool is shown below:</p> <pre><code>from multiprocessing import Pool\n\ndef worker(x):\n    print('In worker with: ',x)\n    sleep(2)\n    return x*x\n\ndef main():\n    with Pool(processes=4) as pool:\n        print(pool.map(worker, [0,1,2,3,4,5]))\n\n\nif __name__=='__main__':\n    main()\n\n# output\nIn worker with:  1\nIn worker with:  2\nIn worker with:  0\nIn worker with:  3\nIn worker with:  4\nIn worker with:  5\n[0, 1, 4, 9, 16, 25]\n</code></pre> <p>Best practice is to close the <code>Pool</code> object after use, so the <code>with as</code> statement is used in the example above. In the example above, only <code>4</code> processes were created, but <code>6</code> tasks were given. In this case, excess tasks should wait until running processes finish. The <code>map()</code> function returns an iterable, List in this case. Notice: the output order matches the input order despite things running in parallel.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#poolimap_unordered-pattern","title":"<code>Pool.imap_unordered()</code> pattern","text":"<p>If output order does not matter, then you can use <code>imap_unordered()</code> function instead, which gives you a performance improvement. The above program can be modified as shown below:</p> <pre><code>def main():\n    with Pool(processes=4) as pool:\n        for res in pool.imap_unordered(worker, [0,1,2,3,4,5]):\n            print(res)\n\n# output:\nIn worker with:  0\nIn worker with:  1\nIn worker with:  2\nIn worker with:  3\nIn worker with:  4\nIn worker with:  5\n0\n9\n1\n4\n16\n25\n</code></pre> <p>Notice the mismatch between input numbers and their squares.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#poolapply_asyc-pattern","title":"<code>Pool.apply_asyc()</code> pattern","text":"<p>The <code>apply_async()</code> method allows for tasks to be executed asynchronously. This way, the main process can spawn off processes in a pool and continue to progress. The pool will munch the data. Results can be collected through a callback function or by using a blocking <code>get()</code> method.</p> <p>Example using blocking <code>get()</code> method:</p> <pre><code>from multiprocessing import Pool\n\ndef collect_results(result):\n    print('In collect results: ', result)\n\n\ndef worker(x):\n    print('In worker: ',x)\n    sleep(2)\n    return x*x\n\n\ndef main():\n    with Pool(processes=2) as pool:\n        # blocking workflow:\n        res = pool.apply_async(worker, [2])\n        print('Blocking result: ' + res.get(timeout=5))\n\nif __name__ == '__main__':\n    main()\n\n# output:\nIn worker:  2\nBlocking result: 4\n</code></pre>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#conclusion","title":"Conclusion","text":"<p>This concludes the introduction to concurrency in Python. While this article explains the concepts of threads and processes with examples, it is no way close to explaining how to build a program to use these concepts. That is for another part of this article. In the subsequent parts of the article, we will see the need for locks, barriers, synchronization. We will also look into concurrency patterns such as <code>futures</code> and libraries such as <code>asyncio</code> and review their applications.</p>"},{"location":"blog/2021/03/03/how-many-snakes-python-concurrency-1/#references","title":"References","text":"<ul> <li>Hunt J. (2019) Introduction to Concurrency and Parallelism. In: Advanced Guide to Python 3 Programming. Undergraduate Topics in Computer Science. Springer, Cham. https://doi.org/10.1007/978-3-030-25943-3_29</li> </ul>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/","title":"How many snakes do you need? - Parallel computing architectures","text":"<p>The part 1 of this blog series introduced a quick start to working with threads and processes in Python. This article covers some concepts in parallel computing.</p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#parallel-processing-architectures","title":"Parallel processing architectures","text":"<p>A commonly used system to classify processor architecture is Flynn's Taxonomy. This distinguishes <code>4</code> classes of computer architecture based on <code>2</code> factors: the number of concurrent instruction streams and number of data streams.</p> <p></p> <ul> <li>SISD: Single Instruction Single Data is the simplest of architectures, which operates sequentially using a single processor.</li> <li>SIMD: Single Instruction Multiple Data architecture is a parallel computer with multiple processing units. Each processor execute the same instructions but on different data blocks. </li> </ul> <p></p> <p>This type of architecture is suited for programs that perform the same set of operations on large data sets (like image processing). Most GPUs operate on SIMD. Here, each processor is executing the same instruction, at the same time.</p> <ul> <li>MISD: Multiple Instructions Single Data is one where multiple processes operate on the same data element. This type does not make much sense and is not very common.</li> </ul> <p></p> <ul> <li>MIMD: Multiple Instructions Multiple Data, is one where multiple processors can work on different sets of instructions on different pieces of data.</li> </ul> <p></p> <p>This type of architecture is the most common and is seen everywhere from multi-core phones &amp; computers, supercomputers etc.</p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#mimd-architecture","title":"MIMD architecture","text":"<p>The MIMD is further subdivided into SPMD and MPMD models.</p> <ul> <li>SPMD: Single Program Multiple Data looks a lot like SIMD, however, each processor is executing a copy of the same program but asynchronously and different tasks of the program. SPMD is the most common stye of parallel programming.</li> </ul> <p></p> <ul> <li>MPMD: Multiple Programs, Multiple Data is when processors are each executing separate programs on different pieces of data. MPMD usually involves an orchestrator node and multiple worker nodes. MPMD is not as common as SPMD but suitable for programs that exhibit functional decomposition ability.</li> </ul> <p></p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#shared-vs-distributed-memory","title":"Shared vs Distributed memory","text":"<p>In a parallel computing architecture, the access to data could fall under one of two categories - shared vs distributed. In a shared model, all processors have access to a shared pool of memory. If one of the processes changes some data, all other processes see the change. The shared model is further subdivided into categories: Uniform Memory Access (UMA) and Non-Uniform Memory Access (NUMA)</p> <p>There are many types of UMA architectures, but the most common is the Symmetric Multiprocessing System (SMP). In a SMP, each processor is connected to a single shared memory using a system bus. In modern computers, each processor has its own cache which is a smaller, faster piece of memory that only it can see. </p> <p></p> <p>Processors use cache to store data it frequently works with. However, when a process updates a value in the cache, that needs to be flown to the shared memory before another process reads that value. This phenomena is called cache coherency and is handled by the processor hardware.</p> <p>The other type of shared memory architecture is the NUMA which is made by connecting multiple SMP together over a system bus. The access to memory is non-uniform as some processors can have access quicker than others as it takes longer to read data over a bus.</p> <p></p> <p>Shared memory architectures are easier to program with, but they don't always scale well. Adding more processors to the system adds more memory but introduces the complexity of cache coherency and puts a strain on the system bus. Further, the programmer needs to synchronize memory access.</p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#distributed-memory-architecture","title":"Distributed memory architecture","text":"<p>In a distributed memory architecture, each processor has its own memory with its own address space. Thus a global address space does not exist. The processors are connected over a network (like ethernet) and if a process makes a change to the data, it is not seen by other processes unless the programmer explicitly defines how data is to be communicated between nodes.</p> <p></p> <p>The advantage of dist. memory arch is that it is easily scalable. This structure makes it cost-effective to use commodity hardware to build large distributed systems.</p>"},{"location":"blog/2021/03/29/how-many-snakes-python-concurrency-2/#references","title":"References","text":"<ul> <li>Python - Parallel and concurrent programming part 1</li> </ul>"},{"location":"blog/2021/03/30/how-many-snakes-python-concurrency-3/","title":"How many snakes do you need? - Challenges in parallel computing","text":"<p>Concurrency and parallelism as seen earlier can speed up your Python application. However, when not careful, they can crop up a bunch of specific bugs and challenges. We will cover some of those in this article.</p>"},{"location":"blog/2021/03/30/how-many-snakes-python-concurrency-3/#data-race","title":"Data race","text":"<p>A common problem with concurrency is when multiple workers attempt to read the same location in memory and when at-least one of them is attempting to change the value.</p> <pre><code>import threading\n\ngarlic_count = 0\n\ndef shopper():\n    global garlic_count\n    print('Current garlic count: ', garlic_count)\n    for i in range(10_000_000):  # 10 million\n        garlic_count += 1\n\nif __name__ == '__main__':\n    barron = threading.Thread(target=shopper)\n    olivia = threading.Thread(target=shopper)\n    barron.start()\n    olivia.start()\n    barron.join()\n    olivia.join()\n    print('We should buy', garlic_count, 'garlic.')\n</code></pre> <p>The script above spawns <code>2</code> worker threads that increment the garlic count by <code>10</code> million. Ideally, they each are supposed to increment <code>10</code> million, resulting in a final count of <code>20</code> million. However, when you run it, you end up with this:</p> <pre><code>Current garlic count:  0\nCurrent garlic count:  83186\nWe should buy 13724492 garlic.\n</code></pre> <p>which is a classic case of a data race.</p>"},{"location":"blog/2021/03/30/how-many-snakes-python-concurrency-3/#mutex-mutual-exclusion","title":"Mutex (Mutual Exclusion)","text":"<p>A Critical Section is the part of your code where multiple threads share the same data in memory. To prevent data corruption, the task of updating a critical section needs to be done uninterrupted - meaning as an atomic transaction. This ensures all other threads would read the updated value of the critical section and not stale values.</p> <p>In programming, this is accomplished using Mutex or Mutual Exclusion. A mutex is a lock and only the thread possessing it will have access to update a shared resource / critical section. Only after the thread in possession releases a mutex, another thread can gain possession.</p> <p>Acquiring of a mutex (the lock) is atomic, meaning it appears as an indivisible action to other threads even thought it may internally involve multiple steps. Thus no other thread can interrupt the thread acquiring the lock.</p> <p>To rectify the previous code, we add create a mutex object using <code>threading.Lock()</code>. Then we call the <code>acquire()</code> and <code>release()</code> methods on the Mutex object appropriately when editing a shared resource. The snippet below rectifies the shopping cart code we had earlier.</p> <pre><code>import threading\n\ngarlic_count = 0\npencil = threading.Lock() # create a Mutex\n\ndef shopper(pencil):\n    global garlic_count\n    pencil.acquire()\n    for i in range(10_000_000):\n        garlic_count += 1\n    pencil.release()\n\nif __name__ == '__main__':\n    barron = threading.Thread(target=shopper, kwargs={'pencil':pencil})\n    olivia = threading.Thread(target=shopper, kwargs={'pencil':pencil})\n    barron.start()\n    olivia.start()\n    barron.join()\n    olivia.join()\n    print('We should buy', garlic_count, 'garlic.')\n</code></pre> <p>This has spawn two threads as earlier, but the results prints <code>20,000,000</code> correctly:</p> <pre><code>We should buy 20000000 garlic.\n</code></pre>"},{"location":"blog/2021/03/30/how-many-snakes-python-concurrency-3/#locks-and-reentrant-mutex","title":"Locks and Reentrant Mutex","text":"<p>What happens when a thread locks a resource, does not unlock and tries to lock again? Well, a thread cannot lock a resource if it is already locked, so it just ends up waiting for its turn to lock, which will never come. This situation is called a deadlock.</p> <p>Deadlocks pose a major limitation when writing recursive code. A reentrant mutex solves this problem by allowing a thread to lock a resource multiple times. However, the same thread should unlock the resource the corresponding number of times to properly release it.</p> <p>In Python, you create a reentrant lock using <code>threading.RLock()</code> API.  One difference between <code>Lock()</code> and <code>RLock()</code>, when a resource is locked using <code>Lock()</code>, any thread can release it. However, only the thread that locked the reentrant lock can unlock it and it needs to do as many number of times it locked originally.</p>"},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/","title":"Imagery in ArcGIS ecosystem","text":"<p>ArcGIS apps give you access to work imagery data from a variety of file formats. The goal is to unify the differences in image characteristics (spatial, spectral, temportal resolutions), file formats (local - different types of image formats, mosaic and web). However, it is useful to understand the basics. This page does not teach you remote sensing or spatial analysis, it just gives you a roadmap to navigate the software.</p>"},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/#file-formats","title":"File formats","text":"<p>ArcGIS with the help of GDAL system, supports over 400 file formats. ArcGIS Pro extends this support by giving you templates for common imagery providers and satellite/aerial platforms. Thus you can access imagery from  - local file on disk (.tiff, .img, .fgdb raster etc)  - local file from mosaic dataset (a collection of images)  - from an Image service (consider this as a mosaic dataset served over HTTP).</p>"},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/#arcgis-imagery-products-and-terms","title":"ArcGIS Imagery products and terms","text":""},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/#mosaic-dataset","title":"Mosaic dataset","text":"<p>You can create a MD inside your fgdb, then add rasters to it. During this process, you can choose templates to adapt it for different types of satellite images. During this step, you will build <code>pyramids</code> which build tiles for different scales for display purposes and you will build <code>overviews</code> which or low resolution snapshots of the images for viewing them at full extent.</p> <p>A MD consists of   - boundary - extent of all images (union)  - footprint - extent of individual images  - image - images with data</p> <p>A MD is like a catalog of all relevant images. MD is highly scalable, you can dynamically change the mosaic order. </p>"},{"location":"blog/2018/03/30/imagery-in-arcgis-ecosystem/#raster-products","title":"Raster Products","text":"<p>This is a virtual product. For well known satellite images, Pro will dynamically create a new file in the <code>catalog</code>`project` pane that appears to be a composite of all the bands. This is virtual, but can be used like a regular file on disk within Pro. If you expand, you will notice some commonly used band combinations and specific names for those (like cloud, water, vegetation, soil cover etc.)</p>"},{"location":"blog/2007/01/26/my-old-blog-posts/","title":"My old blog posts","text":"<p>These are some blogs I wrote when blogging used to be a thing.</p> <ul> <li>Fun: Why Kochadiiyaan is not an anachronism</li> <li>Finance: Nuances in buying a new car</li> <li>Geospatial: Satellite remote sensing at the consumer end</li> <li>Empirical modeling: Correct until proven otherwise</li> <li>Finance: Nuances in buying a used car</li> <li>Productivity: New age tools for the researcher</li> <li>Travel: Above the abode of clouds</li> <li>Fiction: Are you the martian the world is looking for?</li> <li>Global food crisis - the urban animal's work of art</li> <li>Travel: A Bangalore fly-by</li> <li>Ideas: Solar spheres</li> <li>Ideas: Interlinking of Indian rivers</li> <li>Ideas: Color of sound</li> </ul>"},{"location":"blog/2019/05/31/open-geospatial-world/","title":"The state of the Open Geospatial World in 2019","text":"<p>This article represents my takeaways from FOSS4GNA 2019 conference. FOSS4G - free and open source software for geospatial industry is part of the larger OSGeo organization.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#executive-summary","title":"Executive summary","text":""},{"location":"blog/2019/05/31/open-geospatial-world/#big-data","title":"Big data","text":"<p>Managing big data is the talk of the town. A number of players are solving different aspects of geospatial big data problem. - Element84 is working on Cumulus with NASA for ETL of RS data on AWS cloud. - GeoTrellis is working on scalable raster analysis on Apache Spark. - GeoMesa for vector data on top of Accumulo, Hbase, Cassandra db etc. - PanGeo is working on a hosted notebook server solution for analyzing big geospatial raster datasets. - OmniSci core is a high GPU database query engine. - Google Earth Engine has been revived and being maintained.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#imagery-analysis","title":"Imagery analysis","text":"<p>It was surprising to see raster and satellite imagery analysis be spoken at forefront. It was common understanding that we are about to witness an explosion in availability of Earth Observation (EO) images (via dove, cube sats, drone images, NASA and other international space agency satellites). And a number of products were focussed on solving the data analysis challenge such as GeoTrellis, PanGeo, Google Earth Engine.</p> <p>Efficient tiling engines - GeoTrellis, other providers build tiles off raster outputs and serve that. The idea is serving WMTS is less CPU intensive than WMS (which is dynamic)</p> <p>NASA appears to be a big customer of FOSS4G on the cloud. Companies like Element84, Pangeo seem to be born out of NASA's transition to cloud.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#data-discoverability","title":"Data discoverability","text":"<p>The amount of data collected is projected to grow exponentially. However, today, different vendors use custom or sometimes proprietary metadata files, making indexing and searching difficult. Thus, STAC - SpatioTemporal Asset Catalog is a common specification created to describe a range of geospatial datasets (Imagery, SAR, Point Clouds, Scientific data, FMV etc.).</p> <p>Even though STAC is relatively new (release in April 2018), a number of APIs, applications have already adopted this standard. For instance, sat-api of sat utils project from Developmentseed was used as a premier example of embracing STAC</p> <p>NASA's transition (specifically the NASA ACCESS 2017 grant) appears to drive creation of new standards like STAC. New cloud-native data formats such as Cloud optimized GeoTIFF, Zarr (Z arryas for nD data similar to netCDF) are being worked upon.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#other-observations","title":"Other observations","text":"<p>Even though the FOSS ecosystem appears fragmented, a number of libraries effectively self-organize and reuse one another. For instance, intake (which specializes in data discovery, loading, management, dissemination)  uses Xarray which uses rasterio wich again uses GDAL for loading rasters.</p> <p>Python is everywhere. Most scientific analysis presentations used Python APIs to some extent.</p> <p>FOSS is struggling to maintain funding and a number of core projects have a very tiny team that's building them.</p> <p>While the above is about what was seen at FOSS4G, it is also important to note what was missing. There was little mention about native smartphone applications. I could not find a framework that allowed platform agnostic (or not) development of runtime applications. There wasn't an equivalent to ArcGIS Runtime stack. My colleague noted the absence of the term <code>Data Science</code>. While this crowd is certainly technical and does advanced machine learning, they don't to use or associate with the term Data Science.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#points-to-ponder","title":"Points to ponder","text":"<ol> <li>Should ArcGIS Image Server evaluate Spark stack for distributed raster processing (like it's existing GeoAnalytics stack)?</li> <li>A participant noted that ArcGIS Online needs an easy, one step imagery publishing functionality. The barrier to analyzing raster is to purchase and set up ArcGIS Desktop or ArcGIS Enterprise. Further, the AGS JS API cannot display and work with local rasters. These shortcomings raise the barrier when users need to analyze their own imagery datasets. FOSS on the other hand has identified this shortcoming and products like GeoTiff.io were created which allows you to quickly analyze your local rasters right in your browser.</li> </ol>"},{"location":"blog/2019/05/31/open-geospatial-world/#products-seen-in-the-wild-at-foss4gna","title":"Products seen in the wild at FOSS4GNA","text":""},{"location":"blog/2019/05/31/open-geospatial-world/#cesium-gis","title":"Cesium GIS:","text":"<p>|  | Ceasium is primarily for 3D GIS. Products include     - ion - cloud platform     - 3D tiling pipeline     - SDK     - 3D Content - imagery, 3D Tiles etc.  |</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#crunchy-data","title":"Crunchy data","text":"<p>|  | Crunchy is enterprise postgreSQL leader. They got postgres for cloud etc. As is, this is not a FOSS4G company. Just that Paul Ramsey works there. |</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#geosurgeio-no-website-yet","title":"[Geosurge.io] - no website yet.","text":"<p>|  | Works on http://app.geotiff.io/ a UX for analyzing raster data on the browser using JS, client side computations |</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#omnisci","title":"Omnisci","text":"<p> Omnisci has GPU based high speed sql queries. Their chief evangelist said \"Speed is so high, that we don't worry about spaital indexing\". Go to https://www.omnisci.com/demos/ and https://omnisci.cloud/accounts/login/?next=/ to spin up a trial omnisci cloud for you.</p> <ul> <li>Omnisci's core is FOSS, but higher stacks are SAAS, something similar to carto.com.</li> <li>Omnisci Immerse is similar to Insights for ArcGIS product.</li> <li>They talk about \"VAST data\" (volume, agility, spatio temporal data) and their platform is uniquely positioned to analyze that.</li> <li>From a climate science talk: A typical data analysis workflow is to use omnisci (cloud) for cleaning big data in the cloud. Then download to disk for Jupyter Notebooks and Python libraries like seaborn, geopandas, xarray to visualize, explore and analyze.</li> <li>Below is an architecture diagram of Omnisci cloud:</li> <li></li> </ul>"},{"location":"blog/2019/05/31/open-geospatial-world/#google-earth-engine-oss-server","title":"Google Earth Engine OSS server:","text":"<p> GEE is not dead, it is being maintained at http://www.opengee.org/. The talk showed how to build tilecache for local datasets, load on GEE. But the speaker ran into many issues and it looked cumbersome to work with.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#geotrellis","title":"GeoTrellis","text":"<p> is a Scala Lib to work with raster data using Apache spark. It can do fast IO, map algebra, R2V, V2R conversions. It can render outputs to PNGs. It can perform dynamic computations as well as batch processing. The diagram below shows their overall architecture: </p> <p>Some recent additions allow GeoTrellis to build vector tiles off vector data, work with point cloud data, streaming data and they are working to enable GeoTrellis speak to GeoServer.</p> <p>Geotrellis had a lot of contributors and speakers from Azavea and the theme of this talk was 'cloud native GIS apps' as opposed to  Service Oriented Architecture (SOA) of traditional GIS Server apps.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#geotrellisrasterframes","title":"Geotrellis/RasterFrames","text":"<p> is spark DF for raster data. It allows to perform spatiotemporal queries, map algebra ops on rasters along side Spark ML algorithms. </p> <p>The PyRasterFrames provides Python bindings for Geotrellis scala API. PyRF is based on PySpark API. Their MAML (map algebra modeling language) was shown off significantly.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#azavea","title":"Azavea","text":"<p> appears to sponsor most of GeoTrellis work, their demo servers and speakers at this conference. Most of Azavea's talks were packed.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#azavearastervision","title":"Azavea/rasterVision","text":"<p> framework is something to be noted. This API builds on top of Tensorflow and supports workflows such as classification, object detection and semantic segmentation of satellite images. </p> <p>Raster Vision's goal seems to be well defined. The why? article clearly outlines how Azavea pictures this library as a plumber and how it benefits in creating a repeatable and deployable deep learning pipeline.</p> <p>In these ways, raster vision is aligned with the 'learn' module of the ArcGIS API for Python.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#geomesa","title":"GeoMesa","text":"<p> is for large-scale spatial querying, analytics on distributed systems. SpatioTemportal indexing on top of Accumulo, Hbase, Cassandra db etc for vector data.</p> <p>I did not collect more information about this product. I left that to my expert colleagues from database teams.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#aws","title":"AWS","text":"<p> Arguably the elephant in the room. AWS was one of the biggest sponsors of this conference and its Geo Data lead Joe Flasher did a pretty good job of being a technical evangelist and also supporting a lot of his customers (including Esri) in a number of other talks.</p> <p>Why is AWS interested in FOSS4G? Simply because its customers are. NASA and DigitalGlobe are two of its biggest customers and AWS is pretty vested in supporting them.</p> <p>Joe spoke about https://registry.opendata.aws/ which contains all open data including spatial. Some subsets from the site are</p> <ul> <li>https://registry.opendata.aws/tag/satellite-imagery/</li> <li>https://registry.opendata.aws/tag/geospatial/</li> </ul> <p>The landing page https://aws.amazon.com/earth/ is geared toward its efforts in the geospatial sector. This page features a number of talks on this topic from customers of AWS. (Esri's Peter Becker's talk from reInvent is featured here).</p> <p></p> <p>He spoke about AWS Snowball edge which is a portable AWS cloud that gets shipped out to disaster response centers for their local VPC. During such calamities, (for customers like element84), they start a data checkout into the snowball edge. STAC is the protocol for how data is laid out.</p> <p></p> <p>AWS is also building a ground station network for satellite data reception. Thus straight from satellite into AWS infrastructure. This has been functional since 2018 Nov.</p> <p> AWS and a few other companies sponsor the https://spacenet.ai/ contains ML challenges to be solved in the GeoAI community.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#here-technologies","title":"Here technologies","text":"<p> showed off their geospatial platform. It looked good, but I didn't go into the details. Their emphasis however, seemed to be around managing mapping data from and for autonomous vehicles.</p>"},{"location":"blog/2019/05/31/open-geospatial-world/#speaker-slide-decks-and-other-resources","title":"Speaker slide decks and other resources","text":"<p>There isn't a formal place where the decks are shared. These are the ones I manage to source from Twitter and other similar sources  - Paul Ramsey's keynote on FOSS lifecycle  - Gretchen's blog post  - My slide deck on 'Let's take the machines house hunting'</p> <p>Resources:  - GIS Carpentry This teaches EDA using R. The root page for all geosaptial data carpentry is here.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/","title":"Practical scrum for agile teams","text":"<p>In this blog, I introduce the concept of scrumming, which is used widely by agile software development teams. I draw upon my introspections of practicing scrum for 3 years.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#table-of-contents","title":"Table of contents","text":"<ul> <li>Misconceptions</li> <li>What is scrum?<ul> <li>History of scrum</li> <li>Core tenets of scrum</li> </ul> </li> <li>The scrum framework<ul> <li>4 formal scrum events</li> <li>Roles in a scrum<ul> <li>Scrum team</li> <li>Product Owner \\(PO\\)<ul> <li>PO responsibilities</li> </ul> </li> <li>Development team</li> <li>Scrum Master \\(SM\\)</li> </ul> </li> <li>Management tools and metrics in scrum<ul> <li>Product backlog</li> <li>User stories \\(PBIs\\)</li> <li>Tasks</li> <li>Epics</li> <li>PBI estimates</li> <li>Sprint velocity</li> <li>The burn down chart</li> </ul> </li> </ul> </li> <li>Using scrum to plan your projects</li> <li>Lessons learned</li> </ul>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#misconceptions","title":"Misconceptions","text":"<p>What scrum is not? - not just a software development process. It is independent of industry, in fact can be applied even in your household.</p> <p>Scrum is not all about the stand-up meetings - although that is sometimes trademark of teams that follow scrum. The stand up meetings is a means to improve communication and camaraderie through regular in-person or video conference meetings.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#what-is-scrum","title":"What is scrum?","text":"<p>It's a framework for teams that want to be self-organizing, agile and adapt quickly to changing requirements and deliver in short manageable time line.</p> <p>It is surprising to observe even how considerably large projects can be broken down into smaller pieces and delivered in half or quarter of the projected time and expense by adopting scrum methodology.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#history-of-scrum","title":"History of scrum","text":"<p>Founded on empirical process control theory or empiricism, scrum believes knowledge comes from experience and emphasizes on making decisions based on what is known. Scrum employs iterative, incremental approach to improve predictability and control risk. There are 3 important principles of empirical process control theory:  - Transparency - a common metric to measure progress and a shared definition of done  - Inspection - regular and frequent inspection of progress is required. But inspection should never get in the way of work or stop or modify the project.  - Adaptation - if inspection leads to a discovery of something in deviation then an adjustment should be made at the earliest.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#core-tenets-of-scrum","title":"Core tenets of scrum","text":"<p>Let us look at a few process management techniques that take a systems approach to understand where scrum fits in:  * Lean - The goal of lean is simple: minimize the time between customer request and fulfillment by continually improving and reducing non-value adding work.  * Agile - Uses business value as primary measure of progress. Reduces risk through continuous delivery  * Scrum - a wrapper around agile and is a process to practice agile delivery / development. Scrum is industry and technology agnostic.</p> <p>In this light, it is relevant to discuss about the key points of the Agile manifesto: An agile process values:  - Individual and interactions over processes and tools. Teams are self-organizing and self-solving all the way. Teams are self-contained as they figure out what to do and also how to do.  - Working software over detailed doc explaining the eccentricities and caveats while using the product. Customers pay for value, which should be provided by the main product and not by a bunch of explanations about the product. Scrum lays emphasis on developing a 'definition of done' and arriving at a consensus with the customer / stakeholder. This <code>done</code> definition could be such that   - Customer collaboration over contract negotiation. Most successful businesses attribute their success to one thing - listening and providing what the customer wants. This can be achieved only involving the customer / stakeholder not only in the beginning but also at regular intervals during the development life cycle.  - Responding to change (agile) over following a process. In today's world with increased access to real-time information in all fronts, companies can continue to remain on top only by embracing change and adapting to it. This means, as your customers constantly adapt to change, so do you. By delivering working software in increments and but periodically collaborating with the customer, scrum helps you to be agile to these changes. As needs change, your development team can change trajectory without much losses or overheads. </p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#the-scrum-framework","title":"The scrum framework","text":"<p>Now, we are getting into the details of scrum. Let us visit some concepts about scrum that will help us better understand how to adopt it:</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#4-formal-scrum-events","title":"4 formal scrum events","text":"<p>Although scrum encourages daily stand-up meetings, it discourages longer more formal events. It recommends each team to organically figure out at what frequency they meet and for how long they discuss. Having said, below are four formal events that scrum teams practice:  - sprint planning - a meeting where the entire team plans the work to be done during that sprint / iteration  - daily scrum - a short daily meeting where entire team takes stock of work done the previous day  - sprint review - a meeting toward the end of the sprint where the product delivered incrementally is visited and reviewed.  - sprint retrospective - another meeting toward the end of the sprint with emphasis on interpersonal relationships, productivity where the team discusses their development methodology, impedances and figure what can be improved.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#roles-in-a-scrum","title":"Roles in a scrum","text":"<p>Scrum believes in teams composed of multi-disciplinary and cross-functional team members such that all the talent and skills required to finish the product are self-contained within the team. As product complexities grow, inevitably each of your team members start becoming subject matter experts specializing in a narrow functional area. Yet this introduces a unique risk in the form of single point failure. Thus scrum encourages developing a healthy mix of multi-disciplinary team members who are sufficiently acquainted in areas other than their own. Some managers call these as T members (since their spread both horizontally and also vertically). Thus whenever a shortage of resource or skill set arises, the existing team can expand and fulfill the gap without immediately becoming brittle. In this light, each of the team member fit into one of the following roles:</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#scrum-team","title":"Scrum team","text":"<p>A team consists of   - product owner  - development team  - scrum master</p> <p>Scrum teams are self organizing - they decide how to work on things without external direction, cross functional - have all competencies to finish the project without relying on other teams</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#product-owner-po","title":"Product Owner (PO)","text":"<p>Manages product backlog (more on this later) - furnishing with details, prioritizing it, exposing it to all team members.</p> <p>Only the PO has the right to change the backlog and the entire organization must respect this decision. The development team works from this backlog and no one can inject a different set of requirements bypassing the PO.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#po-responsibilities","title":"PO responsibilities","text":"<ul> <li>Manage profitability and ROI (return on investment). A PO does this by prioritizing the backlog to maximize the business value delivered. To start with, the PO must arrive at a way to determine the value delivered, a metric of sorts</li> <li>Call for releases. PO decides what constitutes a minimum shippable product. PO also has liberty to move the release time line forward or backward to maximize the ROI.</li> <li>Guides product development. PO would establish, nurture and communicate the vision. Knows what to build and in what sequence (note, how to build is not a requirement for PO. The development team figures this out).</li> </ul>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#development-team","title":"Development team","text":"<p>This amazing team is self organizing, not even the scrum master tells how to turn the backlog into products. Scrum calls everyone a developer no matter the work performed by the person. No sub teams can exist within a scrum team. 3-9 is a valid team size not including PO and SM unless they also act as development team.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#scrum-master-sm","title":"Scrum Master (SM)","text":"<p>Ensure scrum is understood and practiced. SM plays a servant-leader role in helping teams be self organizing and helping PO with backlog grooming, arrangement, interfacing with external influencers etc. The scrum master usually drives the team meetings.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#management-tools-and-metrics-in-scrum","title":"Management tools and metrics in scrum","text":"<p>Scrum provides a set of tools, terms and metrics to organize and manage your project. Let us visit them briefly here:</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#product-backlog","title":"Product backlog","text":"<p>The backlog contains the list of all requirements. Each item (a PBI - product backlog item) in the backlog is written as a user story. The PO prioritizes the backlog and arranges the items in the order in which it has to be finished. The order of backlog should maximize the ROI.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#user-stories-pbis","title":"User stories (PBIs)","text":"<p>Each PBI is written as a user story. User stories are of format:</p> <pre><code>As a &lt;role&gt;,\nI can &lt;feature / function&gt;,\nso that &lt;goal / value&gt;.\n</code></pre> <p>For example, the user story for backup camera in a car can be written as</p> <p>As a driver, I can use the back up camera when I am reversing the car so I can see what's behind the car more clearly than what my rear view mirrors show.</p> <p>What is management without acronyms :) Lets throw one for user stories.. A user story is good if its INVESTable: Independent, Negotiable, Valuable, Estimatable, Small and Testable. Resist from writing a user story until you have a clear understanding of what you are building.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#tasks","title":"Tasks","text":"<p>Each PBI written as a user story will have a set of clearly defined tasks which need to be finished to call that PBI <code>done</code>. The scrum master encourages the team to come up with a definition of done. This definition can vary slightly between each PBI. The purpose of this definition to establish and maintain the quality of the product being delivered over several sprints.</p> <p>Tasks are usually technical in nature and they are collectively created as a team during the scrum planning meeting.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#epics","title":"Epics","text":"<p>When a bunch of user stories come together, they form an epic. Consider an epic as a large feature that is composed of many sub features that together make a functionality complete and usable. In the case of the backup camera we user story we visited earlier, an epic would be a collection of few more user stories such as:</p> <p>Backup camera Epic:</p> <ol> <li>Turn on back up camera when put in reverse</li> <li>Enhance back up camera feed with steering predictors / guides</li> <li>Alert with audio and steering wheel vibrators when a moving object is detected in back up camera feed</li> <li>Alert lane departures using back up camera feed.</li> </ol> <p>Next, let us discuss about some metrics to quantify and estimate progress of a scrum team.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#pbi-estimates","title":"PBI estimates","text":"<p>Scrum believes in breaking down your product releases (or epics) into smaller identical building blocks called PBIs. No matter how you try, not all of your PBIs would be of identical nature in terms of complexity and effort. Further, some of your PBIs may be diagonally opposite that they may nothing in common. Some of your PBIs may be so new to everyone in your team that nobody knows exactly how to finish it. Then how do you compare and estimate their effort, provide a delivery date and bill the customer?</p> <p>One way to estimate is treat the PBIs as abstract entities and judge them relative to one another in terms of time or effort required to finish them. As a team, during the sprint planning meeting you sort arrange the PBIs in ascending order of effort involved. It is much easier to compare a PBI against another PBI which your team has completed and evaluate comparatively if it requires the same time, lesser or greater time. Then you assign effort in abstract terms such as T shirt sizes (S, M, L, XL, XXL, ..) or Fibonacci number sequence (1, 2, 3, 5, 8, 13 ..).</p> <p>Thus, in the backup camera example from earlier, a team may evaluate</p> <ol> <li>Turn on back up camera when put in reverse --&gt; S or 2</li> <li>Enhance back up camera feed with steering predictors / guides --&gt; M or 3</li> <li>Alert with audio and steering wheel vibrators when a moving object is detected in back up camera feed --&gt; L or 5</li> <li>Alert lane departures using back up camera feed. --&gt; M or 3</li> </ol> <p>Time and effort estimation are much accurate as long as you work with smaller numbers (S, M or 1, 2, 3 ..) and they get fuzzy and uncertain once they go beyond 8 or XL size. If you find yourself with PBIs that are in larger end of effort spectrum, try to break it into smaller pieces.</p> <p>Since you write the PBIs as user stories, the PBI estimates are also called as <code>story points</code>.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#sprint-velocity","title":"Sprint velocity","text":"<p>Once you start to quantify your PBIs with story points, you can calculate how much you manage to finish during a sprint. Remember, a PBI is considered finished only after it meets the <code>definition of done</code> explained earlier. This number which is the sum of finished PBI story points is your sprint velocity.</p> <p>In the backup camera example, if your team finishes the first two PBIs in a sprint, then your <code>velocity</code> is <code>2 + 3 = 5</code>. If your sprint consisted of 3 weeks then you may understand that is the time required for finishing an effort 5.</p> <p>Just like the story points, the velocity is highly subjective to each team. Two scrum teams cannot compare their velocities since it is arbitrary in nature. However, the velocity is a useful metric as it allows you to estimate how many PBIs can be completed in a typical sprint.</p> <p>From the example above, if your sprint velocity is a <code>5</code>, then you may estimate that you need a minimum of <code>2</code> more sprints to finish the remaining 5 and 3 story point PBIs. Once that epic is finished, the PO can call for a release cycle and release the full product.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#the-burn-down-chart","title":"The burn down chart","text":"<p>Now that you have enough metrics to play with, you can create a chart to visualize them. Simply plot the time (perhaps in the form of sprints) in X axis and PBI story points for an epic in the Y axis. Next plot your the velocity for each sprint as a line chart, you get yourself a burn down chart.</p> <p></p> <p>The chart helps you to inspect the pace and estimate when the epic would get done. You can take precautionary measures if you anticipate requiring more time.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#using-scrum-to-plan-your-projects","title":"Using scrum to plan your projects","text":"<p>Now that you are familiar with the terminologies and tools involved, let us review how to apply them in project management. In the agile management approach, you plan for releases in three levels. </p> <ol> <li>First is the <code>release plan</code>. Here, you arrange the PBIs and or epics in the order of importance or value. The PO takes a call on what to include and what to leave out. All the PBIs go into the product backlog list.</li> <li>Each release is composed of one or more <code>sprints</code>. A sprint is a short two to three week period where the entire scrum team comes together, works on one or two PBIs and finishes it. During the <code>sprint planning</code> meeting, the scrum master coaches the development team to work with the PO and pick the PBIs they want to finish during that sprint. These PBIs enter the <code>sprint backlog</code>.</li> <li>During the sprint, the team meets for a short 15 min stand-up meeting where each member states what they worked on, plan to work and if they have anything impeding their progress. The scrum master takes responsibility to clear these impedences. The product owner clarifies any questions the team has.</li> </ol> <p>At the end of the sprint, in a <code>sprint review</code> meeting the team demonstrates the finished product increment to the stake holders and get their sign off. This is also a chance for the stake holders to request for new features or modify the goal of the project based on changing market needs. If such a case arises, the product owner adds them to the product backlog. At no cost can the stake holders interfere a scrum team during a development cycle.</p> <p>Also at the end of the sprint, the team convenes for a <code>sprint retrospective</code> to discuss how the iteration went, what can be improved or changed. The team either proceeds to or reconvenes to plan the next sprint in a <code>sprint planning</code> meeting. The cycle repeats with the team working with product owner to pick the PBIs they want to finish in that sprint cycle and so on.</p> <p>Over this course, each team member have clearly defined roles and know what they are responsible for. The product owner continues to interface with the stake holders, get new requirements, keep the product backlog organized and prioritized by business value. The scrum master continues to perform the servant-leader role and coaches the team toward peak performance. The SM ensures the team is safe from external disturbances or injection of work items. The development team continues make progress by working on as few PBIs possible at a time as a team. They finish the PBIs to meet the <code>done</code> specification before proceeding to work on a new PBI. If a task is left undone, the team members feel free to grab it and work on it. There is no ownership of duties, but there is ownership of the product as a whole.</p>"},{"location":"blog/2017/07/10/practical-scrum-for-agile-teams/#lessons-learned","title":"Lessons learned","text":"<ol> <li>Don't compare velocities between teams</li> <li>your velocity will change as members are added or removed from your team</li> <li>velocity changes seasonally as your members go on vacations, fall sick, or other tasks impede them</li> <li>when you break a larger PBI down to smaller ones, you almost always realize they break down into larger than expected pieces. For instance, I have noticed, breaking a 8 will not simply yield a 5 and 3 story point PBI, but often into two 5s or if we are lucky into three 3s. This is not bad, since it benefits you to over estimate time required than otherwise.</li> </ol>"},{"location":"blog/2017/07/09/arcgis-api-for-python-v1-2/","title":"ArcGIS API for Python v1.2 is Here!","text":"<p>Since its official debut in winter 2016, the ArcGIS API for Python has been a huge hit. The API caters to a wide spectrum of ArcGIS users ranging from GIS administrators, DevOps, content publishers, GIS analysts, data scientists, developers and power users. The API provided these users with a Pythonic representation of their GIS... Read more here</p>"},{"location":"blog/2017/12/22/arcgis-api-for-python-v1-3/","title":"Turbocharge your Python scripts with ArcGIS API for Python v1.3","text":"<p>We are pleased to announce the newest release of the ArcGIS API for Python (version 1.3) ahead of the holiday season. This version packs some serious enhancements for GIS administration, content management, and performing spatial analysis... Read more here</p>"},{"location":"blog/2019/11/26/python-distributed-ml/","title":"Python and distributed machine learning","text":"<p>In today's computing world, machine learning is hitting a performance block. More and more companies want to run them on-demand, instead of as batch processes and want their ML models to deliver results in real-time. Often, the datasets are big-data. Thus, the ML frameworks that data scientsits learnt (<code>pandas</code>, <code>scikit-learn</code>, <code>pyTorch</code>, <code>keras</code>) and know to use don't scale well in this fast production environment or is too cumbersome to implement. This blog explores the approaches the ML, DevOps, HPC industry has arrived at in 2019.</p> <p>We can broadly categorize the efforts into two buckets depending on processor architecture. Companies that are vested in CPU (such as Intel) have their own solution and companies vested in GPU (such as NVIDIA) have their own solutions. Let us explore them:</p>"},{"location":"blog/2019/11/26/python-distributed-ml/#category-1-distributed-cpu-processing","title":"Category 1: distributed CPU processing","text":"<p>Intel has the BigDL that allows running DL models on CPUs using a Spark cluster.</p>"},{"location":"blog/2019/11/26/python-distributed-ml/#category-2-distributed-gpu-processing","title":"Category 2: distributed GPU processing","text":"<p>NVIDIA has created RapidsAI which allows running end-to-end ML on GPUs. Running DL on NVIDIA GPU is well known. How RapidsAI differs is, it provides GPU optimized versions of most popular traditional ML models that can see termendous speed-ups on GPU infrastrucure. Below are some of the components of Rapids AI.</p>"},{"location":"blog/2019/11/26/python-distributed-ml/#cuspatial","title":"cuSpatial","text":"<p>Summary: cuSpatial is a Python library that allows you to perform highly performant spatial computations on GPUs. At the moment, it only has about 10 capabilities / tools. When compared with FOSS4G Python libraries, it provides 1000x to 100,000x fold speed improvements on millions of records.</p> <p>Details: <code>cuSpatial</code> is a <code>C++</code> library accelerated on GPUs using NVIDIA <code>CUDA</code>, <code>cuDF</code> the RAPIDS DataFrame library. It provides GPU acceleration to common spatial, spatio-temporal ops such as point-in-polygon tests, distances, trajectory clustering etc. Speed-ups are in the order of 10x to 10,000x.</p> <p> cuSpatial technology stack</p> <p>cuSpatial data is laid out as columns in GPU memory for fast data processing using GPU data parallelism. cuSpatial data can seamlessly interoperate with cuDF. Thus, for ETL, users will use cuDF to first clean non-spatial fields and then pass it to cuSpatial for spatial filtering.</p> <p>The example quoted in the medium blog article explains how cuDF and cuSpatial is used to identify objects in video camera feeds, geolocate the objects from image frame to spatial coordinates for downstream GIS analysis.</p> <p>cuSpatial allows reading of data from relational formats - CSV, Parquet etc. and GIS formats like shapefiles. It also allows CPU datastructures like NumPy Arrays to flow into GPU data structures in a transparent fashion.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/","title":"A short tour of Open Geospatial Tools of the scientific Python ecosystem","text":"<p>One of my favorite statistics is that about 80% of data in the world, contain some element that is spatial. For instance, take the list of gas stations in a city or restaurants in a city, revenue from medical industry, there is always some element of this data that can be categorized as being spatial - be it locations, routes the products take, cost variations in gas prices etc.</p> <p>This spatial relationship is of significant interest to me and I have been analyzing them for over a decade and have been building software to analyze them for more than half of the past decade. While majority of what I built were proprietary, this blog is a look at what is available in the open-source ecosystem and when to use which tool.</p> <p>Something that is common and beautiful about the opensource Python data analysis ecosystem is, a number of these libraries have self-organized themselves around a common platform or standard of interoperability. You will find many libraries interchangeably using each other for parts they are good at, emphasizing the 'dont reinvent the wheel' philosophy. For then end user, even though the number of libraries is large, it is possible to mark a clear boundary of what lib does what functionality and why it is needed.</p> <p>This blog is just a high level overview to navigate this python-open-geospatial community and packages.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#data-io","title":"Data IO","text":"<p>Fiona is a GDAL Python wrapper and is for reading vector data. Fiona is written to be a clean Pythonic wrapper at the cost of performance and memory usage.</p> <p>An alternate is PyShape which reads and writes Esri Shape files in pure Python vs Fiona which is a GDAL wrapper for libs in C.</p> <p>OSMnx is a library used to retrieve network and vector data from Open Street Maps database. This library pairs well with networkx library that can perform network analysis</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#raster-data-io","title":"Raster data IO","text":"<p>rasterio is a popular library from Mapbox that is a Pythonic wrapper over GDAL Python bindings. This library makes it easy to read satellite images, DEMs (and all formats supported by GDAL) quickly into a numpy array. In addition, it has some handy methods such as plotting, histogram to quickly visualize the images.</p> <p>Once in numpy, you can perform any analysis over raw arrays. However, rasterio provides modules to perform several different processing such as Georeferencing, masking, mosaicing, reprojecting, resampling and writing to disk.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#geometry-and-projection-engines","title":"Geometry and projection engines","text":"<p>Geopandas provides a spatial extension to the famous pandas library. It uses shapely for geometry, fiona for reading vector data, descartes and matplotlib for plotting. Geopandas allows spatial operations that would otherwise require PostGIS.</p> <p>Shapely is a Python package for manipulation and analysis of geometries. Shapely cannot IO datasets, but can help in projections. Shapely is a Pythonic wrapper to GEOS (Geometry Engine, Open Source), which in turn is a <code>C++</code> port of Java Topology Suite. You can already see a heavy re-use of libraries. Shapely forms the geometry backbone of geopandas</p> <p>Pyproj is used by Geopandas to perform conversions to and between different <code>crs</code> (coordinate reference system). In addition, PyCRS appears to be an alternate implementation in pure Python for the same functionality.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#spatial-indexing","title":"Spatial indexing","text":""},{"location":"blog/2018/06/06/python-geospatial-world/#data-wrangling","title":"Data wrangling","text":"<p>Geopandas supports pretty much all data wrangling provided by pandas. Thus you can treat it as a regular data frame and work with its non-spatial columns.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#visualization","title":"Visualization","text":"<p>Descartes uses Shapely to convert them to matplotlib paths and patches. Thus it uses matplotlib for plotting geometries. The <code>GeoDataFrame.plot()</code> method of GeoPandas directly integrates with <code>Descartes</code> making it easy to directly plot your data frames.</p> <p>Folium uses Leaflet.js to plot an interactive map on the Jupyter notebook. It is pretty slick and looks great. Folium, in its current release does not seem to support reading Geopandas dataframes. You need to serialize it to a GeoJSON and then add it to the map. Although this can happen in memory, it may become problematic for large datasets. Another option to look into is IpyLeaflet.</p> <p>GeoViews is a spatial extension for Holoviews viz project. Geoviews is based on Cartopy and renders the plot either using matplotlib or bokeh (interactive) general plotting libraries. Cartopy is a common library that is used by many other high level plotting libraries.</p> <p>Geoplot has an ambitious goal of being a seaborn like library for plotting spatial data. Similar at GeoViews, it is an extension to Cartopy and matplotlib.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#sharing-web-gis","title":"Sharing - web GIS","text":""},{"location":"blog/2018/06/06/python-geospatial-world/#spatial-analysis","title":"Spatial analysis","text":"<p>Geopandas allows for basic overlay and set operations. You can do  - intersection,   - union,   - symmetrical difference,   - difference,   - buffer  - dissolve and aggregation  - attribute and spatial joins  - geocoding using google, bing, googlev3, yahoo, mapquest, openmapquest.</p> <p>A number of data wrangling such as reclassification is possible through regular pandas expressions and functionality.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#pysal","title":"Pysal","text":"<p>Python spatial analysis library, is the go-to for spatial data analysis in open source world.</p>"},{"location":"blog/2018/06/06/python-geospatial-world/#geocoding","title":"Geocoding","text":"<p><code>geopy</code> is a Pythonic wrapper for a number of different providers (including Google, Esri, Yahoo, Mapzen etc.) <code>geopandas</code> integrates with <code>geopy</code> and returns the hits as a <code>geodataframe</code>, how neat! Another option is <code>geocoder</code></p>"},{"location":"blog/2018/06/06/python-geospatial-world/#conclusion","title":"Conclusion","text":"<p>By no means this is an exhaustive list, and that is the beauty of open source ecosystem. There are parallel efforts around the world to improve existing packages and building simpler and more powerful packages. Here is a blog that lists a few more packages for Python geospatial work.</p> <p>Learning resources  - Geohackweek has a nice tutorial for vector, raster data processing using open source Python packages.  - Python GIS course by Henrikki Tenkanen</p>"},{"location":"blog/2017/01/08/scrum-crash-course-1/","title":"A crash course on Scrum part 1 - an agile project management methodology","text":"<p>Companies and organizations have never felt the need to innovate and adapt quickly to changes in market conditions like they face today. This requirement boils down to each individual team in your organization. Thankfully methodologies like agile help us with a framework to accomplish that. Scrum is one such methodology that has gained traction in the last few years. Scrum is simple and sticks with you once you start following it for a few cycles. However there are a lot of terms involved which might feel foreign, especially if you are only familiar with the water flow model of project management. Hence, in this brief two part article series, I aim to get you up to speed with the tools, terms and metrics involved in scrum... Read more here</p>"},{"location":"blog/2017/01/08/scrum-crash-course-2/","title":"A crash course on Scrum part 2 - an agile project management methodology","text":"<p>This is the second part of a two part series on \"Scrum\" - an agile project management methodology. Read the part 1 of this article here. Part 1 gave you an introduction to what scrum is, why use it and the various roles members of a scrum team play. The part 2 below gets more technical and elaborates on metrics and tools used in scrum for tracking progress and estimation... Read more here</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/","title":"Building stand-alone command line tools using Python","text":"<p>You have built a neat little tool using Python. Following modern software building paradigms, you built your tool to be user friendly, used a number of FOSS libraries instead of reinventing the wheel, etc. Now you are ready to ship and you face the dilemma of whether to package it as a conda package or PyPI package or whether to send it as a GitHub repository. What if your user does not want to even install Python on their computer?</p> <p>Fear not, you can build truly self-contained stand-alone applications using PyInstaller. The rest of this blog walks you through an example.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#what-is-pyinstaller","title":"What is PyInstaller","text":"<p>From PyInstaller's page:      PyInstaller bundles a Python application and all its dependencies into a single package. The user can run the packaged app without installing a Python interpreter or any modules. PyInstaller supports Python 2.7 and Python 3.3+, and correctly bundles the major Python packages such as numpy, PyQt, Django, wxPython, and others. PyInstaller is tested against Windows, Mac OS X, and Linux.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#how-to-install-pyinstaller","title":"How to install PyInstaller","text":"<p>Before using PyInstaller, you need to install it in your active Python environment. If you are not already using, I would highly recommend using <code>conda</code> to isolate your various dev environments. You can install conda from here and once installed, you can create a new environment using the command below:</p> <pre><code>conda create --name stand_alone_app_project python=3.5\n</code></pre> <p>Here, I am creating a new environment called <code>stand_alone_app_project</code> and installing a Python 3.5 kernel in it. To use this environment, I need to activate it - as shown below</p> <pre><code>source activate stand_alone_app_project\n</code></pre> <p>Environments are folders at the end of the day and activation pretty much adds the path to this folder to your system path. Activation is transient and is only applicable for the life of that terminal's session. Once activated, install all the packages you need for your script to work. Next, install PyInstaller using PIP (an older package manager for Python)</p> <pre><code>pip install PyInstaller\n</code></pre>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#building-stand-alone-applications","title":"Building stand-alone applications.","text":"<p>PyInstaller allows you to create apps in two formats - as folder with dependencies and an application or option 2 - is to create a fully packaged single executable file. I prefer the latter as it resembles closely what my end users would want and also allows to run the tool from a flash drive, without any installation. In both cases, you will package a Python runtime and all dependencies.</p> <p>To build an app and package it into a folder, run</p> <pre><code>pyinstaller your_main_script.py\n</code></pre> <p>To build it as a single file executable, run</p> <pre><code>pyinstaller your_main_script.py --onefile\n</code></pre> <p>Upon running this, if there are no errors, PyInstaller creates a <code>dist</code> and a <code>build</code> folder. You ship the contents of the <code>dist</code> to your end users.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#building-for-different-os","title":"Building for different OS","text":"<p>To build the executable for different OS, you need to run the steps on each OS of choice. I was able to build the same script tool into a Windows <code>exe</code> and a Unix <code>app</code> for Mac. Since Python is platform independent, doing this was a breeze and the app behaved identical on both platforms.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#errors-and-what-to-do","title":"Errors and what to do","text":"<p>Your first few times with PyInstaller will be rough. You will run into errors unless you are packaging a <code>hello world.py</code> kind of script. In my case, I was packaging a tool that relied on <code>requests</code> package and PyInstaller could not understand this package. After a brief search on Stack Exchange, I was able to figure out that I needed to install <code>urllib3</code> on Mac to get it working. Similarly, on Windows, I needed to install <code>pypiwin32</code>. The latter is true no matter what package you use.</p> <p>Although PyInstaller aims to recursively parse the <code>import</code> statements in your script and all its dependencies to figure out the libraries to package, it might fall short if you are using irregular imports. In such cases, read their appropriately named When things go wrong article.</p>"},{"location":"blog/2017/06/20/building-stand-alone-cli-using-python/#conclusion","title":"Conclusion","text":"<p>Conda and Pip are excellent mechanisms to ship your Python code. Yet, they are suitable if you are building libraries and your audience is fellow developers. However, if you are creating an application for end users, then you need to ship it as an executable. Python still falls short in this area compared to other languages like C++, Java, .NET. We are yet to find an IDE that will build your script into an executable (similar to what visual studio does out of the box since ages). Tools like py2exe and PyInstaller are great as they attempt to solve this gap. Yet, this is only the beginning and I am sure the Python community would greatly benefit if we can expand this area.</p> <p>If you are looking for an example project, checkout my command line app at https://github.com/AtmaMani/vector_style_manager</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/","title":"Takeaways from Strata Data Conference - NewYork City Sep 2019","text":"<p>I had the opportunity to attend 2 days of Strata Data Conference last month (9/25-26) at the Javits Convention center in Manhattan, NYC. This is my report of the conference and my opinion on a few things I noticed. If you want the gist, please read just the summary.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#executive-summary","title":"Executive summary","text":"<p>I noticed 3 important things</p> <ol> <li> <p>The number of hosted notebook solutions is only increasing. LinkedIn is building Darwin and Uber is building a Data Science Workbench. Both these firms admitted the difficulty in defining the skill set of a typical \u2018data scientist\u2019 and which tools, features and frameworks to provide. Uber decided to build two products \u2013 Michelangelo \u2013 a highly scalable ML engine, but with very limited features meant for a narrow user base and another product called Data Science Workbench for the broader set of users. LinkedIn acknowledged the challenges in building a hosted notebook solution. What was clear from these talks is, although the number of notebook products is proliferating, it is not necessarily simplifying. Both these products have a complicated setup requiring experts and are for internal users at this point.</p> </li> <li> <p>The next big thing in deep learning might be distributed DL on Spark based CPU clusters. Intel is building a BigDL library for distributed DL on Spark. Most organizations of today are likely to have server farms with CPU than with GPU dedicated for DL and might be interested in utilizing this existing infrastructure for DL.</p> </li> <li> <p>Several companies are building data catalogs or data management software for big data warehouses. Features include \u2013 building data manifests, data usage graphs, flagging &amp; removal of PII. The overall theme is, following the big data hype, a number of companies have started collecting large amounts of business process data. They do not have a way to make sense of this massive collection or sometimes, even know what data they have. These vendors are building products to help manage these data warehouses.</p> </li> </ol>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#interesting-talks-i-attended","title":"Interesting talks I attended","text":""},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#linkedin-darwin-a-hosted-notebook-product","title":"LinkedIn \u2013 Darwin \u2013 a hosted notebook product","text":"<p>Darwin is a hosted notebook product for internal use. They plan to release this as open source project later. The challenges they are trying to solve is similar to our AGS notebook project.</p> <p> </p> <p>Darwin tries to provide a self-service environment. User\u2019s have access to private data, either local datasets or hosted on server. Analysts can write in Python or R and can share their notebooks across the team. They can version their notebooks, but they don\u2019t yet support reviewing of notebooks, just a git integration. Finally, users can schedule the notebooks or \u2018productionize by other means\u2019 (which wasn\u2019t clear from their description).</p> <p>Below is the architecture of Darwin:</p> <p></p> <p>At the core, Darwin is based off JupyterHub. The notebook container image has the notebook, Python / R libraries and sparkmagic. A persistent storage is made available (not sure if mounted) via a storage service. The notebook instance connects to a Spark farm via an Apache Livy service and Jupyter sparkmagic. Data Science users of Darwin are encouraged to execute jobs on this Spark infrastructure (via Livy) thereby overcoming memory limits of their containers.</p> <p>Users can also pick their Docker Image from an internal registry (artifactory was mentioned). The Docker containers of the Jupyter Hub are run on a Kubernetes cluster. The architecture of Jupyter Hub looked familiar to the default setup in Hub docs. The farm they execute processing against contains Spark (as seen earlier), Tensorflow, and some internal tools like Pinot analytic engine.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#democratizing-ml-at-uber","title":"Democratizing ML at Uber","text":"<p>Uber spoke about Michelangelo a machine learning task engine. Their motivation: Internally, most teams found it difficult to get started with using ML to solve problems. They found it hard to access data and to collaborate. While the teams can build a ML solution using tools of their preference (scikit-learn, R, other tools), they found it hard to take it to production and scale it up. Only a very few teams had the know-how of building ML algorithms and the skill of deploying them in HPCs in a distributed and horizontally scalable manner. Further, teams that were capable were building custom solution and the DevOps and SREs found it hard to provide a simpler scaling solution to all data science teams. There wasn\u2019t a standard to collaborate or replicate.</p> <p></p> <p>The speaker acknowledged the difference between data analysts and scientists if fuzzy and that building a comprehensive platform that can support varied use cases is not quite possible. Thus, Uber chose to build a ML platform that is specialized, narrow and only serves a narrow audience. Motto: build for a few, build for production.</p> <p></p> <p>Key features of Michelangelo</p> <p></p> <p>For all other users and use cases that Michelangelo did not serve, the built another product called \u2018Data Science Workbench.\u2019 DSW is a web-based GUI for self-service data science. It provides a notebook interface but has support for r-studio and shiny.</p> <p>Uber then spoke about PyML, a Python library that sits at the intersection of Michelangelo and DSW. It allows users to deploy their custom ML solutions to production on Michelangelo. For models that require big-data, users build and train directly on Michelangelo\u2019s Spark infrastructure. They are limited to a finite number of algorithms. For smaller datasets, users build using the library of their preference (scikit-learn for instance), package using PyML and deploy it as a Docker image on Michelangelo\u2019s farm.</p> <p>Today\u2019s Uber is a large company (about 19,000 as of 2019) with several data science teams. The speaker shared some learnings in adopting ML:</p> <p>How they \u2018democratized ML\u2019 at Uber:</p> <ul> <li>They started an internal conference for ML: Uber ML conf, and continued the conversation via brown bag events</li> <li>They held regular and repeat ML education events and bootcamps</li> <li>They marketed heavily championed the new way of doing ML \u2013 DSW for interactive dev and Michelangelo for deployment to production</li> <li>Coordinated planning with teams as new features were being planned</li> </ul> <p>Design philosophy behind these two tools:</p> <ul> <li>Build for experts, but design for less technical users (which I interpreted as: allow customization, but start with intuitive UI and have good defaults)</li> <li>Build for unknown</li> <li>Don\u2019t force workflows</li> <li>Develop iteratively, but constantly align with long term vision</li> <li>Build a community of scientists &amp; non-scientists / technical &amp; less technical users. Do not build separate products for these user groups.</li> </ul>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#my-takeaways-from-these-talks","title":"My Takeaways from these talks:","text":"<p>It was clear that big Silicon Valley firms like to build their own data science platforms. This desire for custom tools is so deep that, when I asked LinkedIn why they did not make use of Azure notebooks (since MS bought LinkedIn), they simply said, we started with this effort before the acquisition. What was also clear is, building a custom data science platform is a very challenging business.  There are several open source components and a variety of ways these can be architected.</p> <p>Smaller firms and companies whose core product are not software development, are better off buying a hosted notebook solution (such as AGS Notebooks), than attempting to build one.</p> <p>It is very hard to identify which features to include in a data science platform. Skillset and tools of preference amongst people with title \u2018data scientist\u2019 is so varied even within these two firms, let alone the broad data science industry. Uber and LinkedIn admitted the proliferation of Python libraries, ML and DL frameworks, their cascading dependencies is a challenge.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#companies-and-products-seen-at-the-expo","title":"Companies and products seen at the Expo","text":""},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#intel-and-bigdl-distributed-dl-on-spark","title":"Intel and BigDL \u2013 distributed DL on Spark:","text":"<p>Intel is a leading chip maker for general purpose computing but was late to the game when it came to making GPUs for deep learning. Most DL frameworks of today support Nvidia GPUs. Most organizations realize the power of DL, but to implement large scale DL solutions, they need to invest in new GPU powered hardware (either on prem or hired on the cloud). However, most of these orgs are likely to have generic server infrastructure (CPU powered) in place for existing business processes. Thus, Intel is building BigDL \u2013 a distributed deep learning library on Apache Spark. At the booth, Intel staff demonstrated performance benchmarks for running DL training on GPU vs distributed CPU.</p> <p></p> <p>Intel\u2019s booth had a unique layout. Under a large Intel banner, I found 6 separate stations where business partners of Intel set up their booths. I spoke to</p> <ul> <li>H20.ai \u2013 automatic feature engineering, training and model development. It is an intelligent assistant for data scientists. The sales person was a former GIS analyst and was happy to meet someone from Esri.</li> <li>OneConvergence \u2013 another cloud native DL platform</li> <li>Domino datalab -  another data science platform</li> </ul>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#semoss-open-source-browser-based-ui-driven-analysis","title":"SEMOSS: open source, browser based, UI driven analysis","text":"<p>It provides Python and R shells that allow users to write custom scripts, execute on a server back-end and get the results updated back on the browser. This is a challenging feature to achieve (maintaining data context) and their demo was of this feature was impressive. SEMOSS provides limited geospatial capabilities \u2013 viz and geocoding at best. For geocoding, they make use of Esri World Geocoding service.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#alteryx-gui-based-self-service-data-analysis-platform","title":"Alteryx: GUI based, self-service data analysis platform","text":"<p>Alteryx has many features including geospatial analysis, but the staff at the booth were not equipped to talk to those aspects. Their emphasis was on the self-service nature.</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#data-catalog-companies","title":"Data catalog companies","text":"<p>Several companies are building data catalogs or data management software for your big data warehouse. Features include \u2013 building data manifests, data usage graphs, flagging &amp; removal of PII. The overall theme is, following the big data hype, a number of companies have started collecting large amounts of business process data. They do not have a way to make sense of this massive collection and these vendors are building products to help manage these data warehouses.</p> <p>Okera (data cataloger), Cryptonumerics (PII identification), Alation (data catalog), Waterline Data (data catalog), Matillion (data management), Dremio (fast data queries), Immuta (data governance), Ataccama (data cleanser/curator), Collibra (data lineage), Alluxio (data orchestration).</p>"},{"location":"blog/2019/09/20/takeaways-strata-data-nyc2019/#summary","title":"Summary","text":"<p>I found my time at Strata productive and informative. I wish I could have spent a bit more time at the Esri Booth, but the people I spoke to were thrilled to hear the analytical applications possible with spatial data. It was interesting to meet and talk to people that crossed over from being a GIS analyst to the Financial industry. The challenges that Uber and LinkedIn\u2019s shared while building notebook and ML solutions were insightful.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/","title":"The Big Data Ecosystem","text":"<p>Big data is the talk of the town. What is it? What are the components? Let us demystify some of these jargons</p> <p></p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#spark","title":"Spark","text":"<p>Open source - Apache. New, but very widely used.  - flexible alternative to MapReduce  - Data stored in HDFS, Cassandra  - not replace Hadoop</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#hadoop-and-hadoop-ecosystem","title":"Hadoop and Hadoop ecosystem","text":"<p>Hadoop - way to distribute very large files over multiple machines. Uses HDFS (Hadoop File System). HDFS will duplicate data for fault tolerance. Hadoop uses MapReduce which allows computation on that data. Together <code>MapReduce + HDFS = Hadoop</code></p> <p>HDFS - 128 MB blocks of data that is duplicated and split across multiple nodes. Small blocks allow massive parallelization.</p> <p>Hadoop has a <code>schema on read</code> whereas RDBMS has a <code>schema on write</code> logic. HDFS does not use RDBMS tables, data files are generally .txt or binary files.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#hdfs","title":"HDFS","text":"<p>HDFS is written in Java and sits on top of a standard OS of that machine. HDFS is great only if your data files are quite large, like <code>1GB</code> and not if they are tiny - like <code>1kb</code>.</p> <p>Writing data into HDFS is interesting. All datasets are to be replicated <code>3</code> times. You talk to the co-ordinator node which tells which nodes to write / read data from. You write to the first node which tells you where to put the other 2 replicas. This format of processing is called <code>pipeline</code> processing in HDFS.</p> <p>When a node fails, the co-ordinator finds which copies of data were lost and it instructs the remaining nodes to replicate those blocks amongst the remaining nodes so all blocks of data are now replicated <code>3</code> times.</p> <p>Data Locality is important in HDFS for speed. Since data is replicated 3 times, the coordinator can instruct different processing operations to be performed on different nodes. Priority is given to that node which has the max data blocks required for that data operation, since working on data in that node is much faster than reading data from network.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#mapreduce","title":"MapReduce","text":"<p>Allows computation across Hadoop nodes. It uses a job tracker on main node and task trackers on all slave nodes. Task trackers allocate CPU and memory and monitor the tasks.</p> <p>Developed by Google in 2004 which led to development of Hadoop. MapReduce was hard to master and use, requires lot of Java code. Building complex jobs is very hard.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#yarn","title":"YARN","text":"<p>Yarn manages compute resources and sits on top of HDFS. Yarn stands for Yet Another Resource Manager.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#data-loaders","title":"Data loaders.","text":""},{"location":"blog/2017/12/10/the-big-data-ecosystem/#sqoop","title":"SQOOP","text":"<p><code>SQOOP = SQL + HadOOP</code> for ETL (Extract, Transform, Load), get data from SQL like tables into Hadoop</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#flume","title":"FLUME","text":"<p>To get streaming data into Hadoop</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#data-searching","title":"Data searching","text":""},{"location":"blog/2017/12/10/the-big-data-ecosystem/#hive","title":"HIVE","text":"<p>Accessing data from HDFS is via a <code>.java</code> program and not a <code>sql</code> like in a RDBMS. A number of flavors of Hadoop came out that answered this limit. For instance, Facebook created <code>Hive</code> which allowed you to write standard <code>sql</code> which will be converted to <code>java</code> for Hadoop. Hive reduces 1000s of lines of Java into SQL, rather the reverse.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#pig","title":"PIG","text":"<p>Same as HIVE, but higher level language and is anologous to <code>PL/SQL</code>.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#impala","title":"IMPALA","text":"<p>Developed as Hive and Pig were not just as fast. So for low latency sql, Impala was created and it bypasses MapReduce as it communicates directly with HDFS</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#solr","title":"SOLR","text":"<p>If you wan to Google (search) your Hadoop data. You can pipe SQOOP and FLUME into SOLR on the way into HDFS so it indexes on the fly for great search.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#spark_1","title":"SPARK","text":"<p>Apache open source framework that allows performing batch and real-time data processing on HDFS. It was built by learning the difficulties with MapReduce. It can be considered as a general purpose compute engine.</p> <p>Integrates very well if Python, Scala, Java. New products like <code>Spark Streaming</code> is designed for developer ease of use.</p> <p></p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#rdd","title":"RDD","text":"<p><code>Resilient Distributed Datasets</code> representation of data in object format and allow computation on them. They contain data lineage. <code>Transformation</code>s are what you do to <code>RDD</code> that result in creation of a new <code>RDD</code>. <code>Action</code>s is questions / searches on the <code>RDD</code> to get your answer.</p> <p>Spark does all work as lazy evaluations and work on the data only when necessary. Further, <code>data lineage</code> allows them to rebuild data or RDD that is lost in the event of a fault.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#gfs-google-file-system","title":"GFS - Google File System","text":"<p>A very early form of a distributed file system. It had a <code>3</code> time data replication. With more nodes, Google improved both their compute and storage power. It built MapReduce to work on this GFS.</p> <p>After Google published their white paper on this, Hadoop was born out of a custom implementation of this.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#commercial-packages","title":"Commercial packages","text":"<p>Since there are numerous alternates, some companies bundle them and sell as their products with some additional value added components.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#mapr","title":"MapR","text":"<p>MapR stack comprises <code>MapR = HDFS + YARN + SPARK + SPARK SQL</code>.</p>"},{"location":"blog/2017/12/10/the-big-data-ecosystem/#resources","title":"Resources","text":"<ol> <li>Apache Hadoop &amp; Big data 101</li> <li>Hadoop, SQL comparison</li> <li>What is Hadoop</li> <li>Understanding HDFS using Legos</li> <li>What is Apache Spark?</li> <li>Apache Spark vs MapReduce</li> </ol>"},{"location":"blog/2018/01/26/tutorials-for-arcgis/","title":"A few tutorials I authored for ArcGIS","text":"<p>Below is a non-exhaustive list of tutorials that I authored for ArcGIS. The objective of these tutorials is to demonstrate spatial data analysis using the scientific computing ecosystem of Python and the ArcGIS stack.</p> <ul> <li>Analysis: Fighting California forest fires using spatial analysis</li> <li>Analysis: Creating hurricane tracks using Geoanalytics</li> <li>Analysis: Analyzing New York City taxi data using big data tools</li> <li>Analysis: Finding suitable spots for placing heart defibrillator equipments in public</li> <li>Automation: Publishing SDs, Shapefiles and CSVs</li> <li>Automation: Publishing packages as web layers</li> <li>Automation: Publishing web maps and web scenes</li> <li>Automation: Using and updating GIS content</li> <li>Automation: Updating features in a feature layer</li> <li>Automation: Batch creation of Groups</li> <li>Automation: Your first notebook</li> <li>Automation: Clone Portal users, groups and content</li> <li>Overwriting feature layers</li> <li>Most guide pages here</li> </ul>"},{"location":"books/","title":"Books","text":"<ul> <li>2021: Climate modeling work featured in the technology showcase of Esri's GIS for Science flip book<ul> <li></li> </ul> </li> <li> <p>2020: Reviewed and contributed to the 9th chapter of Advanced Python Scripting for ArcGIS Pro book.</p> <ul> <li></li> </ul> </li> <li> <p>2019: Reviewed and contributed to the 2nd edition of Python Scripting for ArcGIS Pro book.</p> <ul> <li></li> </ul> </li> <li>2019: Contributed to chapter 13 of GIS for Science. See the article here and the web app here<ul> <li></li> </ul> </li> <li>2018: Reviewer for Essential Python.<ul> <li></li> </ul> </li> </ul>"},{"location":"books/2018-essential-python/","title":"Essential Python","text":"<p>Lucky to be a reviewer on a Python book by my friend Ravi Chityala. Get the book on Amazon: Essential Python.</p> <p></p>"},{"location":"books/2019-py-scripting/","title":"Python Scripting for ArcGIS Pro","text":"<p>Reviewed and contributed to the 2nd edition of Python Scripting for ArcGIS Pro book.</p> <p></p>"},{"location":"books/2020-adv-py-scripting/","title":"Advanced Python Scripting for ArcGIS Pro","text":"<p>Reviewed and contributed to the 9th chapter of Advanced Python Scripting for ArcGIS Pro book.</p> <p></p>"},{"location":"books/2021-gis-for-science/","title":"GIS for Science - Technology Showcases","text":"<p>Climate modeling work featured in the technology showcase of Esri's GIS for Science flip book </p> <p></p>"},{"location":"books/2021-gis-for-science/#jupyter-notebooks-of-the-analysis-work","title":"Jupyter Notebooks of the analysis work","text":"<ul> <li>Part 1: Preparing larger-than-memory hurricane data using Dask and GeoAnalytics</li> <li>Part 2: EDA on hurricane tracks</li> <li>Part 3: Does intensity of hurricanes increase over time?</li> </ul>"},{"location":"cheatsheets/gsutil-1/","title":"`gsutil` commands","text":""},{"location":"cheatsheets/gsutil-1/#introduction","title":"Introduction","text":"<p><code>gsutil</code> is a CLI tool to work with data on Google Storage Service. Using the tool, you can query public data anonymously. However, to query private data, you need to authenticate. Gsutil can inherit the auth from <code>gcloud</code> CLI.</p> <p>The general syntax to represent a bucket is:</p> <pre><code>gs://BUCKET_NAME/&lt;FOLDER_NAME&gt;/OBJECT_NAME\n</code></pre> <p>You can have any number of nested sub-folders within a bucket.</p> <p>Buckets in GCS are globally-unique, even for private buckets.</p>"},{"location":"cheatsheets/gsutil-1/#commands","title":"Commands","text":"<p>Adapted from https://cloud.google.com/storage/docs/discover-object-storage-gsutil</p>"},{"location":"cheatsheets/gsutil-1/#creating-a-bucket","title":"Creating a bucket","text":"<pre><code>gsutil mb -b on -l us-east1 gs://my-awesome-bucket/\n\n&gt;&gt;&gt; Creating gs://my-awesome-bucket/...\n</code></pre>"},{"location":"cheatsheets/gsutil-1/#uploading-objects-to-a-bucket","title":"Uploading objects to a bucket","text":"<pre><code>gsutil cp &lt;path&gt;/&lt;file&gt; gs://&lt;bucket&gt;/folder\n</code></pre>"},{"location":"cheatsheets/gsutil-1/#downloading-objects-from-a-bucket","title":"Downloading objects from a bucket","text":"<pre><code>gsutil cp gs://&lt;bucket&gt;/folder/file &lt;local dest&gt;/&lt;path&gt;\n</code></pre> <p>You can also copy content from one bucket / folder to another using the same command.</p> <pre><code>gsutil cp gs://my-awesome-bucket/kitten.png gs://my-awesome-bucket/just-a-folder/kitten3.png\n</code></pre>"},{"location":"cheatsheets/gsutil-1/#list-contents-of-a-bucket","title":"List contents of a bucket","text":"<pre><code>gsutil ls gs://&lt;bucket&gt;\n\n# returns\ngs://my-awesome-bucket/kitten.png\ngs://my-awesome-bucket/just-a-folder/\n</code></pre> <p>You can use <code>-l</code> flag for details.</p>"},{"location":"cheatsheets/gsutil-1/#deleting-objects","title":"Deleting objects","text":"<pre><code>gsutil rm gs://&lt;bucket&gt;/file\n</code></pre> <p>You can use the <code>-r</code> recursive flag to remove folders, and even buckets.</p>"},{"location":"cheatsheets/helm-1/","title":"Helm charts","text":"<p> Nautical charts - called Helm charts where used to navigate vessels in open waters</p>"},{"location":"cheatsheets/helm-1/#what-are-helm-charts","title":"What are Helm charts?","text":"<p>Helm is a package manager for Kubernetes. When deploying an application on K8s, you need to have not only the pod-spec and the deployment-spec, but also YAML files that describe the stateful-set, ConfigMap, user permissions, secrets, services etc. Helm charts package up all these commonly used YAML files as charts and shares it as standard sets. This bundle of YAML files are called Helm charts.</p> <p>Commonly used apps such as DB apps (Mongo, PostGRES), monitoring apps such as Prometheus all have commonly available Helm charts. Today there are repositories of such charts that are both public and private.</p>"},{"location":"cheatsheets/helm-1/#helm-as-a-templating-engine","title":"Helm as a templating engine","text":"<p>Helm is also a templating engine. Consider an application with multiple micro-services. The spec for each of these are usually identical except for metadata. With Helm, you can define a common blueprint called a <code>template</code> or <code>tpl</code> file and associate it with one or more <code>values.yaml</code> file that contains the specific info for each service.</p> <p>For example, you may have to deploy the same service on dev, staging and prod. The spec for these clusters might be identical except for some key, but uniform differences. Thus you can have Helm charts such as the one shown below:</p> <p>deployment_tpl.yaml</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name: {{.Values.name}}\nspec:\n    containers:\n    - name: {{.Values.container.name}}\n      image: {{.Values.container.image}}\n      port: {{.Values.container.port}}\n</code></pre> <p>and values for each env as shown below: dev.values.yaml</p> <pre><code>name: my-app-dev\ncontainer:\n    name: cont1\n    image: gcr.io/cont1img1\n    port: 5000\n</code></pre> <p>This ensures the deployment on each cluster is identical.</p>"},{"location":"cheatsheets/helm-1/#helm-chart-structure","title":"Helm chart structure","text":"<p>A Helm chart is usually a folder with the following files:</p> <pre><code>myChart/\n    Chart.yaml      # Metadata about the chart\n    templates/      # Folder with Helm template files\n    values.yaml     # Values for the template files\n    charts/         # Folder for any chart dependencies. If the main chart depends on other charts.\n    readme.md       # optional\n    license         # optional\n</code></pre> <p>These files are deployed using the command: <code>helm install &lt;chartname&gt;</code></p>"},{"location":"cheatsheets/k8s-1/","title":"Kubectl commands","text":"<p><code>kubectl</code> is the CLI tool that interfaces with Kubernetes. It can work with either a local setup (such as with Docker desktop's K8 or Minikube) or with one or more remote clusters. Most operations on K8s will be via <code>kubectl</code>.</p>"},{"location":"cheatsheets/k8s-1/#1minikube-commands","title":"1.<code>minikube</code> Commands","text":"<p>First, we need to start the local cluster. For that, we need to install minikube.</p>"},{"location":"cheatsheets/k8s-1/#install-minikube","title":"Install <code>minikube</code>","text":"<p><code>brew</code> makes install easier, whether or not you are on a M1 mac. Run:</p> <pre><code>brew install minikube\n</code></pre> <p>followed by:</p> <pre><code>which minikube\nminikube version\n</code></pre>"},{"location":"cheatsheets/k8s-1/#starting-and-stopping-the-local-cluster","title":"Starting and stopping the local cluster","text":"<p>Minikube uses virtualbox to isolate the local set up of kubernetes. Once installed, you can run </p> <pre><code>minikube start\n</code></pre> <p>which returns the following in my case:</p> <pre><code>$ minikube start\n\ud83d\ude04  minikube v1.27.1 on Darwin 12.6 (arm64)\n\ud83c\udd95  Kubernetes 1.25.2 is now available. If you would like to upgrade, specify: --kubernetes-version=v1.25.2\n\u2728  Using the docker driver based on existing profile\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\ude9c  Pulling base image ...\n\ud83d\udd04  Restarting existing docker container for \"minikube\" ...\n\ud83d\udc33  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n    \u25aa Using image docker.io/kubernetesui/metrics-scraper:v1.0.8\n    \u25aa Using image docker.io/kubernetesui/dashboard:v2.7.0\n\ud83c\udf1f  Enabled addons: default-storageclass, storage-provisioner, dashboard\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n</code></pre> <p>Note: When minikube starts, <code>kubectl</code> will automatically bind to minikube's kubectl by modifying the <code>kubectl</code> context. Thus, running <code>k get pods</code> will only return the ones running in minikube, not your remote cluster. To talk to a different cluster, use the <code>k config use-context &lt;context_name&gt;</code> command.</p> <p>Once done, you can stop the cluster by running</p> <pre><code>minikube stop\n</code></pre>"},{"location":"cheatsheets/k8s-1/#setting-up-shpod-namespace","title":"Setting up <code>shpod</code> namespace","text":"<p>The k8s course recommends using <code>shpod</code> for a consistent shell experience. The yaml file in the link has the definition of a namespace.</p> <pre><code>k apply -f https://k8smastery.com/shpod.yaml\nk attach -n shpod -it shpod\n</code></pre> <p>The attach command follows the syntax <code>kubectl attach -n &lt;namespace&gt; --interactive --tty &lt;contianer_name&gt;</code></p> <p>Once done, the pod can be terminated using</p> <pre><code>k delete -f https://k8smastery.com/shpod.yaml\n</code></pre>"},{"location":"cheatsheets/k8s-1/#2kubectl-commands","title":"2.<code>kubectl</code> Commands","text":"<p>The <code>kubectl</code> command needs to know which cluster to talk to and how to authenticate. This information is stored in the <code>~/.kube/config</code> file. The provisioner (which can be GKE or minikube) will also provide / edit this file. The file as the IP address of the k8s server and the TLS certs for auth.</p> <p>Get contexts First, you need to know which k8s cluster you are talking to. For this run: The <code>*</code> points to the active cluster. All <code>kubectl</code> commands apply to that cluster now.</p> <pre><code>k config get-contexts\nCURRENT   NAME        CLUSTER    AUTHINFO   NAMESPACE\n          gke_dev     gke_dev    gke_dev    default\n          gke_prod    gke_prod   gke_prod   default\n          gke_stage   gke_stage  gke_stage  default\n*         minikube    minikube   minikube   default\n</code></pre> <p>Change contexts When you want to talk to a different cluster, change the cluster using the syntax <code>k config use-context &lt;contxtname&gt;</code></p> <pre><code>k config use-context gke_dev\n</code></pre> <p>The run <code>k config get-contexts</code> to confirm the switch.</p> <p>Get nodes <code>k get nodes</code> Nodes are the physical machines that run the Kubernetes cluster. </p> <pre><code>(local2) \u279c  Documents k get nodes\nNAME       STATUS   ROLES           AGE     VERSION\nminikube   Ready    control-plane   7d22h   v1.24.1\n</code></pre> <p>The <code>get</code> command is the most frequently used command. <code>get</code> can return output in a variety of formats:</p> <pre><code>k get nodes -o wide\nNAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nminikube   Ready    control-plane   97d   v1.24.1   192.168.49.2   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.10.104-linuxkit   docker://20.10.17\n</code></pre> <p>You can get return in <code>json</code> format and pipe that to a CLI tool called <code>jq</code> which can parse and extract certain info like this:</p> <pre><code> k get node -o json | jq \".items[] | {name:.metadata.name} + .status.capacity\"\n{\n  \"name\": \"minikube\",\n  \"cpu\": \"5\",\n  \"ephemeral-storage\": \"61255492Ki\",\n  \"hugepages-1Gi\": \"0\",\n  \"hugepages-2Mi\": \"0\",\n  \"hugepages-32Mi\": \"0\",\n  \"hugepages-64Ki\": \"0\",\n  \"memory\": \"8039792Ki\",\n  \"pods\": \"110\"\n}\n</code></pre> <p>Describe node <code>k describe node &lt;node_name&gt;</code> If you want to delve into the details of how the node is configured and its health, you can run <code>k describe node minikube</code>.</p> <p>Get namespaces <code>k get ns</code></p> <pre><code>(flood_ml_local2) \u279c  Documents k get ns\nNAME                   STATUS   AGE\ndefault                Active   7d22h\nkube-node-lease        Active   7d22h\nkube-public            Active   7d22h\nkube-system            Active   7d22h\nkubernetes-dashboard   Active   7d22h\n</code></pre> <p>Create a new namespace <code>k create ns &lt;ns_name&gt;</code></p> <pre><code>(base) \u279c  Documents k create ns argo-local\nnamespace/argo-local created\n\n# verify\n\n(base) \u279c  Documents k get ns             \nNAME                   STATUS   AGE\nargo-local             Active   4h54m\ndefault                Active   8d\nkube-node-lease        Active   8d\nkube-public            Active   8d\nkube-system            Active   8d\nkubernetes-dashboard   Active   8d\n(base) \u279c  Documents \n</code></pre> <p>Create argo server on k8s</p> <pre><code>(base) \u279c  Documents k apply -n argo-local -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml\ncustomresourcedefinition.apiextensions.k8s.io/clusterworkflowtemplates.argoproj.io created\ncustomresourcedefinition.apiextensions.k8s.io/cronworkflows.argoproj.io created\n....\n</code></pre> <p>Then forward the port to local machine:</p> <pre><code>(base) \u279c  Documents k -n argo-local port-forward deployment/argo-server 2746:2746\nForwarding from 127.0.0.1:2746 -&gt; 2746\nForwarding from [::1]:2746 -&gt; 2746\n\n</code></pre> <p>Argo server UI is now accessible at https://localhost:2746. You may have to agree to security warnings before accessing this page.</p> <p>Get pods <code>k get pods -n &lt;namespace&gt;</code></p> <pre><code>(base) \u279c  Documents k get pods -n argo-local\nNAME                                   READY   STATUS    RESTARTS        AGE\nargo-server-7fbf57bc87-f82wl           1/1     Running   3 (2m11s ago)   2m31s\nminio-74474c548b-6hf48                 1/1     Running   0               2m31s\npostgres-6b5944c545-tpnfb              1/1     Running   0               2m31s\nworkflow-controller-7d4bf4fd7d-x4qkk   1/1     Running   2 (2m9s ago)    2m31s\n(base) \u279c  Documents \n\n</code></pre> <p>Creating Argo workflow using <code>kubectl</code></p> <pre><code>(base) \u279c  ~ k create -n argo-local -f Documents/code/temp/wf-hello-world.yaml \nworkflow.argoproj.io/hello-world-p4wlj created\n</code></pre>"},{"location":"projects/","title":"Projects","text":"<p>Use the menubar to find relevant projects</p>"},{"location":"projects/cloud/","title":"Cloud computing","text":"<p>Some useful resources for building on the cloud. This page will primarily contain resources on AWS and GCP.</p>"},{"location":"projects/cloud/#pages","title":"Pages","text":"<ul> <li>Argo for Kubernetes</li> <li>Google cloud platform</li> <li>Intro to Kubernetes</li> <li>Kubernetes objects and specification</li> <li>Helm charts</li> <li><code>kubectl</code> commands</li> <li><code>gsutil</code> commands</li> </ul>"},{"location":"projects/cloud/#a-little-bit-of-history","title":"A little bit of history","text":"<p>Cloud is a general purpose definition used to refer to everything from compute, storage, networking and security on the cloud. Cloud is the current paradigm shift in computing. The shift of the early 21st century. Arguably, AI is the second paradigm shift. The shift to cloud computing involves a leap-of-faith move for most companies and those which embrace this are set to survive this tech era. The tech space is always evolving. The evolution here is especially noticeable due to its speed. The practice of moving to newer (and better) technologies involves abandoning the old ways of building software systems and rebuilding them on newer paradigms. This practice is called the burning-platform-effect. While the name sounds sounds ominous, the move is essential for survival in today's tech scene.</p> <p>As companies migrated from on-premises to cloud, they saw a switch from capital expenditures (CapEx) to operational expenditures (OpEx). This shift meant lower up-front cost and leveled the field for big and small companies to start up and expand globally. While earlier, you needed to have geo-political presence if you wanted to spread multi-nationally, today you can rent hardware / platform from cloud providers and accomplish the same. This benefit also meant, smaller, leaner companies can have an out-sized impact if they leveraged the power of cloud (and had everything else right - such as product market fit, demand, incentives, marketing, funding, staffing etc.)</p> <p>The most profound effect of cloud tech is the vast amounts of data that is collected and consumed. Increasingly, humankind is switching from \"connecting to online\" to \"living online\". The result is the highly accurate and highly personalized predictive models that can be built. This leads us to the second paradigm shift - Artificial Intelligence.</p> <p>AI requires not-only vast amounts of data, but also commiserate compute power. AI's compute demand has led to newer generations of processors be developed. The newer GPU based processors (such as TPU) break Moore's law. Instead of being 2x powerful, each subsequent generation is ~50x powerful.</p>"},{"location":"projects/cloud/#cloud-is-in-layers","title":"Cloud is in layers","text":"<ul> <li>IaaS: Infrastructure as a Service - is the rawest form of cloud offering. Users get access to compute, storage and networking. They pay for what they allocate.</li> <li>PaaS: Platform as a Service - is an offering that wraps a layer over IaaS. PaaS is a bunch of managed services and users pay for what they consume rather than allocate.</li> <li>SaaS: Software as a Service - can be thought of a layer on top of PaaS. This is typically managed, server-less and oriented toward consumers. Most web applications can be considered as SaaS. Users pay for a service model / subscription tier rather than allocation or consumption.</li> </ul>"},{"location":"projects/cloud/argo/","title":"Argo workflows for Kubernetes","text":""},{"location":"projects/cloud/argo/#what-is-argo","title":"What is Argo?","text":"<p>Argo helps make Kubernetes more accessible to everyone. It provides services for creating workflows and jobs that build on Kubernetes. Argo is composed of the following services:</p> <ul> <li>Argo Workflows - orchestrate parallel jobs on K8. Represent workflows as DAGs and easily run compute intensive jobs.</li> <li>Argo CD - uses git repo as the source of truth and builds the deployment env to conform to the repo. Config is via a <code>YAML</code> file or <code>Helm</code> package.</li> <li>Argo Events - dependency manager that is events based. It can hook up and listen to sources like AWS SNS, SQS, GCP PubSub and execute workflows.</li> </ul>"},{"location":"projects/cloud/argo/#why-argo","title":"Why Argo?","text":"<p>Argo is a compelling solution for those that already build on K8. Argo does not reinvent K8 features, instead builds on them. It enables implementing each step in the workflow as a container. It provides artifact management that allows to specify the output from any step as input to another. Since everything is as containers, the entire workflow, including each step and their interactions can be managed as source code (in YAML). This is called container native workflow management. Thus a workflow that runs on one Argo env will run exactly the same on another, allowing for better portability.</p>"},{"location":"projects/cloud/argo/#argo-cli-commands","title":"Argo CLI commands","text":""},{"location":"projects/cloud/argo/#list-workflows-argo-list","title":"List workflows <code>argo list</code>","text":"<pre><code>$ argo list -n &lt;namespace&gt; &lt;flags&gt;\n$ argo list -n flood --running  # will list all running workflows in the flood namespace\n$ argo list -n flood --completed  # for completed wf\n</code></pre> <p>example output:</p> <pre><code>NAME                                               STATUS                AGE   DURATION   PRIORITY\ningest-weather-data-compass-lisflood-japan-xb7pm   Running               4m    4m         0\nflood-pipeline-ps8rf                               Running               42m   41m        0\njp-3hr-live-cgtrb                                  Running               44m   44m        0\njp-3hr-hist-lc89r                                  Running               2d    2d         0\n</code></pre>"},{"location":"projects/cloud/argo/#create-submit-workflow-using-kubectl","title":"Create / submit workflow using <code>kubectl</code>","text":"<p>You can use argo CLI or <code>kubectl</code> to submit or create workflows.</p> <pre><code>(base) \u279c  ~ k create -n argo-local -f wf-resource-template-localfile.yaml \nworkflow.argoproj.io/wf-resource-tmpl-55s7b created\n</code></pre>"},{"location":"projects/cloud/argo/#argo-workflow-definition-files","title":"Argo workflow definition files","text":"<p>Below is a sample argo workflow from argo doc website. A template starts by declaring the version it is based off, followed by <code>kind</code> and <code>metadata</code> (name, etc). The <code>spec</code> is the most important part. Spec in-turn has two main parts, <code>entrypoint</code> and <code>templates</code>.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-  # Name of this Workflow\nspec:\n  entrypoint: whalesay        # Defines \"whalesay\" as the \"main\" template\n  templates:\n  - name: whalesay            # Defining the \"whalesay\" template\n    container:\n      image: docker/whalesay\n      command: [cowsay]\n      args: [\"hello world\"]   # This template runs \"cowsay\" in the \"whalesay\" image with arguments \"hello world\"\n</code></pre> <p>The <code>templates</code> section accepts an array of objects. In Yaml, you prefix each element in an array of objects with a <code>-</code>. For, an array of elements, you enclose elements within <code>[]</code> in a single line or in a broken line.</p> <p>Argo has the following template types:</p> <ul> <li>Container: specs to schedule a container. This follows the same spec used by K8s. So you can cross use the specs</li> <li>Script: convenience wrapper around a container and follows the same spec. It has an additional <code>script</code> field which you can specify which file to be executed.</li> <li>Resource: template to modify and operate on K8s resources</li> <li>Suspend: to suspend operations</li> </ul>"},{"location":"projects/cloud/argo/#container-template","title":"Container template","text":"<p>An example container template is shown below. Notice the container object within the templates section of the spec which defines this is a container template.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: wf-container-templ-\nspec:\n  entrypoint: container-template\n  templates:\n    - name: container-template\n      container:\n        image: python:3.8-slim\n        command: [echo]\n        args: [\"Hello, from within container running argo workflow\"]\n</code></pre>"},{"location":"projects/cloud/argo/#script-template","title":"Script template","text":"<p>A script template inherits from container template. It is a convenience wrapper to allow execution of scripts like Python. Note the script object which takes up the place of container:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: wf-script-tmpl-\nspec:\n  entrypoint: script-template\n  templates:\n    - name: script-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"This script is embedded into the template and is executed\")\n</code></pre> <p>In the case above, the Python script is embedded right into the workflow yaml file.</p>"},{"location":"projects/cloud/argo/#resource-template","title":"Resource template","text":"<p>A resource template is used to act on K8s or Argo resources, such as create child workflows. As an example below, we write a resource template to spawn another argo workflow that executes a Python script (using a script template). Notice the resource object within the template section of the workflow yaml. The manifest field takes an entire script template. There is one small caveat - the metadata uses <code>name</code> instead of <code>generateName</code>.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: wf-resource-tmpl-\nspec:\n  entrypoint: resource-template\n  templates:\n    - name: resource-template\n      resource:\n        action: create\n        manifest: |\n          apiVersion: argoproj.io/v1alpha1\n          kind: Workflow\n          metadata:\n            name: wf-res-spawn\n          spec:\n            entrypoint: script-template\n            templates:\n              - name: script-template\n                script:\n                  image: python:3.8-slim\n                  command: [python]\n                  source: |\n                    print(\"WF spawned by another res wf.\")\n</code></pre>"},{"location":"projects/cloud/argo/#template-invocation","title":"Template invocation","text":"<p>Argo provides invoker templates that can invoke other workflows. There are two types of invoker templates:</p> <ul> <li>Steps: Defines a list of steps. Inner steps will run in parallel and outer lists will run sequentially</li> <li>DAG: Defines the tasks in a directed acyclic graph. A DAG specifies the interdependencies between tasks. This allows Argo to know which tasks can be run sequentially and which in parallel.</li> </ul>"},{"location":"projects/cloud/argo/#steps-template","title":"Steps template","text":"<p>The steps template resembles other templates seen so far, with the <code>steps</code> object in place of <code>resource</code> or <code>script</code>. The <code>steps</code> object accepts an array of objects, each with a <code>name</code> and <code>template</code> property. In the example below, the step template defines 3 steps, followed by a script template that has the actual logic for each of the steps.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: wf-steps-template-serial\nspec:\n  entrypoint: steps-template\n  templates:\n    - name: steps-template\n      steps:\n      - - name: step1\n          template: task-template\n      - - name: step2\n          template: task-template\n      - - name: step3\n          template: task-template\n\n    - name: task-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"Task - hello\")\n</code></pre> <p>Outer steps: Note the double dash <code>- -</code> prefix for each of the step element in the template. The double dash signify these are outer tasks and by design, the execute in serial.</p> <p>Inner steps: Parallel exec: In the case above, all the steps can execute in parallel as there is no inter-dependency between them. To make them execute in parallel, you remove a dash and indent the step as shown below:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: wf-steps-template-parallel\nspec:\n  entrypoint: steps-template\n  templates:\n    - name: steps-template\n      steps:\n      - - name: step1  # outer step\n          template: task-template\n        - name: step2  # inner step (single dash)\n          template: task-template \n        - name: step3  # inner step\n          template: task-template\n      - - name: step4  # outer step\n          template: task-template\n\n    - name: task-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"Task - hello\")\n</code></pre> <p>When executed, the workflow looks like below. Note that steps1, 2, and 3 can run in parallel.</p> <p></p> <p>and the timeline tab looks like below:</p> <p></p>"},{"location":"projects/cloud/argo/#suspend-template","title":"Suspend template","text":"<p>This template can be used to add a pause / sleep timer between steps in a workflow. See example below:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: wf-suspend-template\nspec:\n  entrypoint: steps-template\n  templates:\n    - name: steps-template\n      steps:\n      - - name: step1  # outer step\n          template: task-template\n        - name: step2  # inner step (single dash)\n          template: task-template \n      - - name: delay  # adds the delay\n          template: suspend-template\n      - - name: step4  # outer step\n          template: task-template\n\n    - name: task-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"Task - hello\")\n\n    - name: suspend-template\n      suspend:  # The suspend template\n        duration: \"10s\"\n</code></pre>"},{"location":"projects/cloud/argo/#dag-template","title":"DAG template","text":"<p>The DAG template solves the same workflow representation as the steps template. Instead of the you specifying which tasks to run in sequence or parallel, in a DAG, you flip the problem and specify which tasks have a dependency on which other task. The argo executor then attempts to run all tasks in parallel, except when blocked by a dependency. In a DAG, the steps are now called tasks.</p> <p>Below is an example of a DAG that produces a diamond pattern workflow:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: wf-dag-template\nspec:\n  entrypoint: dag-template\n  templates:\n    - name: dag-template\n      dag:\n        tasks:\n          - name: task1\n            template: task-template\n          - name: task2\n            template: task-template \n            dependencies: [task1]\n          - name: task3\n            template: task-template\n            dependencies: [task1]  # makes a binary branch pattern\n          - name: task4\n            template: task-template\n            dependencies: [task2, task3]  # closes dag with a diamond pattern\n\n    - name: task-template\n      script:\n        image: python:3.8-slim\n        command: [python]\n        source: |\n          print(\"Task - hello\")\n</code></pre> <p></p>"},{"location":"projects/cloud/argo/#resources","title":"Resources","text":"<ul> <li>Medium.com: What is Argo and how it works on GKE</li> <li>Argo blog: Introductory article.</li> <li>Argo version on Dev cloud: 3.3.6</li> </ul>"},{"location":"projects/cloud/gcp-1/","title":"An introduction to Google Cloud Platform - part 1","text":""},{"location":"projects/cloud/gcp-1/#basics-logging-in-and-getting-set-up","title":"Basics - logging in and getting set up.","text":""},{"location":"projects/cloud/gcp-1/#about-gcp","title":"About GCP","text":""},{"location":"projects/cloud/gcp-1/#resource-hierarchy-in-google-cloud","title":"Resource hierarchy in Google Cloud","text":"<p>At the bottom of the stack are Resources which are storage buckets, databases, scripts. These are organized into Projects. One or more projects can be in a folder or sub-folders, which themselves are organized under an Org node. Access, budgeting policies are defined at Org or folder level and sometimes at granular levels such as projects and resources. Policies are inherited downward.</p>"},{"location":"projects/cloud/gcp-1/#gcp-project","title":"GCP \"Project\"","text":"<p>A Google Cloud project is an organizing entity for your Google Cloud resources. Each resources can belong to just 1 project. Projects also contain settings and permissions, which specify security rules and who has access to what resources. Orgs can allocate budgets to projects and set spending alerts so they don't accidentally run up a large GCP bill.</p> <p>Each project has <code>3</code> identifying entities: </p> <ul> <li>Project ID: a unique identifier that is used to link Google Cloud resources and APIs to your specific project. Project IDs are unique across Google Cloud. Project IDs are created by GCP and once created, they are immutable.</li> <li>Project Name: a label created by user. Not unique, can be edited later</li> <li>Project Number: globally unique, created by GCP and cannot be altered by user.</li> </ul>"},{"location":"projects/cloud/gcp-1/#iam","title":"IAM","text":"<p>For smaller orgs, it is sufficient to just manage access via settings on Folders and Projects. However, when companies need to manage access in a sophisticated and granular manner, they can use IAM - Identity and Access Management. An IAM consists of a bunch of Who has access to What. The Who can be a google account, google group, service account or a cloud identity domain and the What part is defined by a Role which is a collection of permissions.</p> <p>There are 3 types of Roles:</p> <ul> <li>basic: Owner, Editor, Viewer, Billing Admin. These are broad by design.</li> <li>predefined: More nuanced such as Instance Admin, VM Admin etc.</li> <li>custom IAM: go crazy, go as narrow as you want.</li> </ul> <p>A special type of account is the Service Account which is tuned for all kinds of automation scenarios. Unlike regular accounts, this uses a cryptographic key instead of a password. Service accounts are also considered resources (not user accounts) and so can be tightly managed using policies.</p>"},{"location":"projects/cloud/gcp-1/#ways-of-accessing-gcp","title":"Ways of accessing GCP","text":"<p>You can access GCP via 4 ways:</p> <ul> <li>Web interface: GUI. Allows you to check health, manage, set budgets. Can also connect to instances via SSH</li> <li>Cloud SDK &amp; Cloud shell: SDK has <code>gcloud</code> main CLI tool, <code>gsutil</code> CLI for storage, <code>bq</code> CLI for big query. SDK is installed on your workstation. Cloud Shell is a CLI running on the cloud and accessed via browser. Is Debian based with <code>5</code> GB persistent storage and loaded with SDK.</li> <li>GCP API: Web and client libraries in different languages - Java, Python, C#, node, Ruby, C++ etc.</li> <li>Cloud console mobile app: certain management, start-stop operations, view budgets, view server status etc, incident management. </li> </ul>"},{"location":"projects/cloud/gcp-1/#networking-on-gcp","title":"Networking on GCP","text":"<p>Each project in GCP has a default VPC (virtual private cloud) configured. The resources within a project can talk to each other via this internal IP Address. By default, the firewall is configured to block all incoming traffic, but allow all out-going traffic from within a project.</p>"},{"location":"projects/cloud/gcp-1/#storage-on-gcp","title":"Storage on GCP","text":"<p>There are multiple storage services on GCP - depending on the type of data being stored and the application that is intended. Below is a list of 5 types of services:</p> <p></p> <p>For Cloud Storage, there are multiple classes of storage, depending on how often you access the data as shown below:</p> <p></p> <p>No matter which class you use, all types allow for global access, no minimum amount, pay as you go rates. Data is always encrypted at rest.</p> <p>Cloud SQL provides fully managed RDBMS including <code>mysql</code>, <code>postgresql</code>, <code>sql server</code>. You can scale up to <code>64</code> cores, <code>400 GB</code> of RAM and <code>30 TB</code> of storage.</p> <p>Cloud spanner is also a fully managed RDBMS, but for high throughput SQL operations including joins, reads, writes. It sounds like you would start with Cloud SQL and upgrade to spanner if your needs warrant that.</p> <p>Firestore - scalable, NoSQL DB where data is stored in documents and stored in collections. Firestore is suitable for web or mobile apps (in addition to other users), allows for offline replication &amp; sync. Cost is fine grained, per read, write, query ops and the amount of data stored.</p> <p>Cloud Bigtable - NoSQL, big data DB. Bigtable is suitable when data is high throughput, exceeds 1TB, either structured or unstructured, supports time-series. Frequently customers that run ML jobs on data use bigtable.</p>"},{"location":"projects/cloud/gcp-1/#7-main-categories-of-services","title":"<code>7</code> main categories of services","text":"<p>There are seven categories of Google Cloud services:</p> <ul> <li>Compute: A variety of machine types that support any type of workload. The different computing options let you decide how much control you want over operational details and infrastructure.</li> <li>Storage: Data storage and database options for structured or unstructured, relational or non relational data.</li> <li>Networking: Services that balance application traffic and provision security rules.</li> <li>Cloud Operations: A suite of cross-cloud logging, monitoring, trace, and other service reliability tools.</li> <li>Tools: Services that help developers manage deployments and application build pipelines.</li> <li>Big Data: Services that allow you to process and analyze large datasets.</li> <li>Artificial Intelligence: A suite of APIs that run specific artificial intelligence and machine learning tasks on Google Cloud.</li> </ul> <p>Here is a cheetsheat containing all GCP services.</p>"},{"location":"projects/cloud/gke-1/","title":"Google Kubernetes Engine - An introduction","text":"<p>Kubernetes is an open-source platform for managing containerized workloads and services. <code>K8s</code> or <code>K8</code> (as it is colloquially known) makes it easy to orchestrate different containers on different hosts, scale them as microservices, deploy rollouts or rollbacks etc. K8s was open-sourced by Google in 2014. Below are some of the advantages of using K8s</p> <ul> <li>Load balancing &amp; service discovery: K8s can expose your a container to outside traffic. If incoming traffic is high, it can load balance and distribute so the deployment is stable.</li> <li>Storage orchestration - allows you to declaratively mount a storage system (local or cloud store)</li> <li>Structured rollout and rollbacks - you can instruct K8s your desired system state and it can control the update of the container images in a controlled manner.</li> <li>Automatic resource allocation - you provide K8s a set of nodes (a cluster) and you specify how much CPU and memory each container needs. K8s takes care of efficiently running all of the containers in the provided node pool.</li> <li>Self-healing - K8s can automatically divert traffic from unresponsive containers, kill them, restart them and advertise only after they recover</li> <li>Secret management - you can store &amp; manage sensitive info like passwords, SSH keys, oAuth tokens separately from container images</li> </ul>"},{"location":"projects/cloud/gke-1/#components-of-a-kubernetes-cluster","title":"Components of a Kubernetes cluster","text":"<ul> <li>Node - a physical machine that runs containers (with on-prem set up). In a cloud set up, this could be a VM.</li> <li>Cluster - a set of such nodes together running an application. A cluster can have a group of nodes that run as  control plane and the remaining running as nodes for containers.</li> <li>Node pool - designate subset of nodes within a cluster.</li> <li>Pod - a process running a container (with a wrapper around it). Pods are the smallest unit of reference in K8. Usually only one container runs in a pod. But you can package more than 1 container, especially if they have to communicate or share resources. A pod can provide a unique IP per pod and different ports for different containers running within it. To interact with a pod, you can use the <code>kubectl</code> command.<ul> <li></li> <li>To see the list of pods in your cluster, run <code>kubectl get pods</code></li> </ul> </li> </ul> <p>The IP address of a pod can change over time, especially as pods are created and terminated over time. Thus if the front-end pods refer to back-end pods by IP, the application may break. This is why you wrap the relevant pods behind a service and give it a fixed IP. As you scale your app, any number of pods can share that IP of that service.</p> <p>You can use <code>kubectl</code> commands to create pods, create load balancer, network etc. However, k8 allows you to declaratively define your architecture in a Yaml file, called a deployment config file.</p>"},{"location":"projects/cloud/gke-1/#the-control-plane","title":"The control plane","text":"<p>The control plane makes decisions about the cluster by detecting and responding to cluster events. The control plane can ideally be run on any node in the cluster. However, by convention, cluster start-up scripts start control plane first and do not run any user containers on that node.</p> <ul> <li>kube-apiserver is the API server component of the control plane which exposes K8 via the K8 API.</li> <li>etcd - is a consistent, highly available key-value store that stores all data about the cluster</li> <li>kube-scheduler - watches newly created pods with no assigned node and selects a node for them to run on. The scheduler will take into account the resource requirements, affinity specifications, data locality etc.</li> <li>kube-controller-manager - fine grained controller component that listens to events happening in the cluster.</li> <li>cloud-controller-manager - allows you to link your k8 cluster into the cloud provider's API.</li> </ul>"},{"location":"projects/cloud/gke-1/#components-of-a-node","title":"Components of a Node","text":"<p>These components run on every node (physical or virtual worker machine) providing a kubernetes runtime environment.</p> <ul> <li>kubelet - agent that runs on every node. It makes sure that containers are running in a pod.</li> <li>kube-proxy - maintains network rules on each node. This network allows communication within the cluster or from outside to your cluster.</li> <li>Container runtime - responsible for running the container.</li> </ul>"},{"location":"projects/cloud/gke-1/#google-kubernetes-engine","title":"Google Kubernetes Engine","text":"<p>GKE is a managed K8 in the cloud. It is a bunch of compute engine instances that are bound together to form a cluster. This cluster can be created using web UI or <code>gcloud</code> CLI. When using GKE, Google provides a built-in load-balancer service. </p>"},{"location":"projects/cloud/gke-2/","title":"Kubernetes objects and specification","text":"<p>A K8s object is a \"record of intent\" which tells K8 how you want the cluster to look like - aka, desired state. You can create and manage these objects by using the Kubernetes API. You typically do so by using the <code>kubectl</code> CLI tool Every K8 object has two fields <code>spec</code> (specification of desired state) and <code>status</code> (current status that k8 system will update by the control plane). The k8 system will work actively to bring the <code>status</code> to match the <code>spec</code>, the desired state.</p>"},{"location":"projects/cloud/gke-2/#k8s-object-spec","title":"K8s Object spec","text":"<p>Below is an example of a Kubernetes deployment in Yaml specification:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n\n  replicas: 2 # tells deployment to run 2 pods matching the template\n  template:\n    metadata:\n      labels:\n        app: nginx\n\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre> <p>You will typically pass this to <code>kubectl</code> as shown below to create a deployment:</p> <pre><code>kubectl apply -f &lt;path to yaml file&gt;\n</code></pre> <p>The following fields are compulsory</p> <ul> <li><code>apiVersion</code>: corresponds to K8s version</li> <li><code>kind</code>: what type of object is this spec for</li> <li><code>metadata</code>: unique identifiers for the object - <code>name</code>, <code>UID</code>, <code>namespace</code></li> <li><code>spec</code>: the state specification</li> </ul> <p>The spec is different for each type of object and can have nested objects. The K8s API ref contains templates that can be used for different object types</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/","title":"Analyzing historic hurricane tracks - Part 1/3","text":"In\u00a0[\u00a0]: Copied! <pre># imports for downloading data from FTP site\nimport os\nfrom ftplib import FTP\n\n# imports to process data using DASK\nfrom dask import delayed\nimport dask.dataframe as ddf\n\n# imports for data analysis and visualization\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# imports to perform spatial aggregation using ArcGIS GeoAnalytics server\nfrom arcgis.gis import GIS\nfrom arcgis.geoanalytics import get_datastores\nfrom arcgis.geoanalytics.summarize_data import reconstruct_tracks\nimport arcgis\n</pre> # imports for downloading data from FTP site import os from ftplib import FTP  # imports to process data using DASK from dask import delayed import dask.dataframe as ddf  # imports for data analysis and visualization import pandas as pd import matplotlib.pyplot as plt %matplotlib inline  # imports to perform spatial aggregation using ArcGIS GeoAnalytics server from arcgis.gis import GIS from arcgis.geoanalytics import get_datastores from arcgis.geoanalytics.summarize_data import reconstruct_tracks import arcgis <p>Establish an anonymous connection to FTP site</p> In\u00a0[4]: Copied! <pre>conn = FTP(host='eclipse.ncdc.noaa.gov')\nconn.login()\n</pre> conn = FTP(host='eclipse.ncdc.noaa.gov') conn.login() Out[4]: <pre>'230 Anonymous access granted, restrictions apply'</pre> <p>Change directory to folder contianing the hurricane files. List the files</p> In\u00a0[5]: Copied! <pre>conn.cwd('/pub/ibtracs/v03r10/all/csv/year/')\nfile_list = conn.nlst()\nlen(file_list)\n</pre> conn.cwd('/pub/ibtracs/v03r10/all/csv/year/') file_list = conn.nlst() len(file_list) Out[5]: <pre>176</pre> <p>Print the top 10 items</p> In\u00a0[6]: Copied! <pre>file_list[:10]\n</pre> file_list[:10] Out[6]: <pre>['Year.1842.ibtracs_all.v03r10.csv',\n 'Year.1843.ibtracs_all.v03r10.csv',\n 'Year.1844.ibtracs_all.v03r10.csv',\n 'Year.1845.ibtracs_all.v03r10.csv',\n 'Year.1846.ibtracs_all.v03r10.csv',\n 'Year.1847.ibtracs_all.v03r10.csv',\n 'Year.1848.ibtracs_all.v03r10.csv',\n 'Year.1849.ibtracs_all.v03r10.csv',\n 'Year.1850.ibtracs_all.v03r10.csv',\n 'Year.1851.ibtracs_all.v03r10.csv']</pre> In\u00a0[1]: Copied! <pre>data_dir = './me'\n</pre> data_dir = './me' In\u00a0[31]: Copied! <pre>if 'hurricanes_raw' not in os.listdir(data_dir):\n    os.mkdir(os.path.join(data_dir,'hurricanes_raw'))\n\nhurricane_raw_dir = os.path.join(data_dir,'hurricanes_raw')\nos.listdir(data_dir)\n</pre> if 'hurricanes_raw' not in os.listdir(data_dir):     os.mkdir(os.path.join(data_dir,'hurricanes_raw'))  hurricane_raw_dir = os.path.join(data_dir,'hurricanes_raw') os.listdir(data_dir) Out[31]: <pre>['Allstorms.ibtracs_all.v03r09.csv',\n 'hurricanes_raw',\n '.nb_auth_file',\n 'Allstorms.ibtracs_all.v03r09.csv.gz']</pre> In\u00a0[11]: Copied! <pre>%%time\nfile_path = hurricane_raw_dir\nfor file in file_list:\n    with open(os.path.join(file_path, file), 'wb') as file_handle:\n        try:\n            conn.retrbinary('RETR ' + file, file_handle.write, 1024)\n            print(f'Downloaded {file}')\n        \n        except Exception as download_ex:\n            print(f'Error downloading {file} + {str(download_ex)}')\n</pre> %%time file_path = hurricane_raw_dir for file in file_list:     with open(os.path.join(file_path, file), 'wb') as file_handle:         try:             conn.retrbinary('RETR ' + file, file_handle.write, 1024)             print(f'Downloaded {file}')                  except Exception as download_ex:             print(f'Error downloading {file} + {str(download_ex)}') <pre>Downloaded Year.1842.ibtracs_all.v03r10.csv\nDownloaded Year.1843.ibtracs_all.v03r10.csv\nDownloaded Year.1844.ibtracs_all.v03r10.csv\nDownloaded Year.1845.ibtracs_all.v03r10.csv\n....Downloaded Year.2015.ibtracs_all.v03r10.csv\nDownloaded Year.2016.ibtracs_all.v03r10.csv\nDownloaded Year.2017.ibtracs_all.v03r10.csv\nCPU times: user 8.63 s, sys: 12.1 s, total: 20.8 s\nWall time: 12min 5s\n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[3]: Copied! <pre>csv_path = os.path.join(hurricane_raw_dir,'Year.2017.ibtracs_all.v03r10.csv')\n</pre> csv_path = os.path.join(hurricane_raw_dir,'Year.2017.ibtracs_all.v03r10.csv') In\u00a0[18]: Copied! <pre>df = pd.read_csv(csv_path)\ndf.head()\n</pre> df = pd.read_csv(csv_path) df.head() Out[18]: IBTrACS -- Version: v03r10 Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude Longitude Wind(WMO) Pres(WMO) Center Wind(WMO) Percentile Pres(WMO) Percentile Track_type Latitude_for_mapping Longitude_for_mapping Current Basin hurdat_atl_lat hurdat_atl_lon hurdat_atl_grade hurdat_atl_wind hurdat_atl_pres td9636_lat td9636_lon td9636_grade td9636_wind td9636_pres reunion_lat reunion_lon reunion_grade reunion_wind reunion_pres atcf_lat atcf_lon atcf_grade atcf_wind atcf_pres mlc_natl_lat mlc_natl_lon mlc_natl_grade mlc_natl_wind mlc_natl_pres ds824_sh_lat ds824_sh_lon ds824_sh_grade ds824_sh_wind ds824_sh_pres ds824_ni_lat ds824_ni_lon ds824_ni_grade ds824_ni_wind ds824_ni_pres bom_lat bom_lon bom_grade bom_wind bom_pres ds824_au_lat ds824_au_lon ds824_au_grade ds824_au_wind ds824_au_pres jtwc_sh_lat jtwc_sh_lon jtwc_sh_grade jtwc_sh_wind jtwc_sh_pres jtwc_wp_lat jtwc_wp_lon jtwc_wp_grade jtwc_wp_wind jtwc_wp_pres td9635_lat td9635_lon td9635_grade td9635_wind td9635_pres ds824_wp_lat ds824_wp_lon ds824_wp_grade ds824_wp_wind ds824_wp_pres jtwc_io_lat jtwc_io_lon jtwc_io_grade jtwc_io_wind jtwc_io_pres cma_lat cma_lon cma_grade cma_wind cma_pres hurdat_epa_lat hurdat_epa_lon hurdat_epa_grade hurdat_epa_wind hurdat_epa_pres jtwc_ep_lat jtwc_ep_lon jtwc_ep_grade jtwc_ep_wind jtwc_ep_pres ds824_ep_lat ds824_ep_lon ds824_ep_grade ds824_ep_wind ds824_ep_pres jtwc_cp_lat jtwc_cp_lon jtwc_cp_grade jtwc_cp_wind jtwc_cp_pres tokyo_lat tokyo_lon tokyo_grade tokyo_wind tokyo_pres neumann_lat neumann_lon neumann_grade neumann_wind neumann_pres hko_lat hko_lon hko_grade hko_wind hko_pres cphc_lat cphc_lon cphc_grade cphc_wind cphc_pres wellington_lat wellington_lon wellington_grade wellington_wind wellington_pres newdelhi_lat newdelhi_lon newdelhi_grade newdelhi_wind newdelhi_pres nadi_lat nadi_lon nadi_grade nadi_wind nadi_pres reunion_rmw reunion_wind_radii_1_ne reunion_wind_radii_1_se reunion_wind_radii_1_sw reunion_wind_radii_1_nw reunion_wind_radii_2_ne reunion_wind_radii_2_se reunion_wind_radii_2_sw reunion_wind_radii_2_nw bom_mn_hurr_xtnt bom_mn_gale_xtnt bom_mn_eye_diam bom_roci atcf_rmw atcf_poci atcf_roci atcf_eye atcf_wrad34_rad1 atcf_wrad34_rad2 atcf_wrad34_rad3 atcf_wrad34_rad4 atcf_wrad50_rad1 atcf_wrad50_rad2 atcf_wrad50_rad3 atcf_wrad50_rad4 atcf_wrad64_rad1 atcf_wrad64_rad2 atcf_wrad64_rad3 atcf_wrad64_rad4 tokyo_dir50 tokyo_long50 tokyo_short50 tokyo_dir30 tokyo_long30 tokyo_short30 jtwc_??_rmw jtwc_??_poci jtwc_??_roci jtwc_??_eye jtwc_??_wrad34_rad1 jtwc_??_wrad34_rad2 jtwc_??_wrad34_rad3 jtwc_??_wrad34_rad4 jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 NaN Year # BB BB NaN YYYY-MM-DD HH:MM:SS NaN deg_north deg_east kt mb NaN % % NaN degrees_north degrees_east NaN deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb deg_north deg_east kt mb nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile mb nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile Quad nmile nmile Quad nmile nmile nmile mb nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 06:00:00 NR -13.70 63.90 0.0 0.0 reunion -100.000 -100.000 main -13.70 63.90 SI -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -13.7 63.9 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 1874-01-11 12:00:00 NR -999. -999. -999. -999. NaN -999. -999. main -13.75 63.86 SI -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -13.7 63.9 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 1874-01-11 18:00:00 NR -999. -999. -999. -999. NaN -999. -999. main -13.88 63.77 SI -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -13.9 63.8 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.0 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 <p>The input looks mangled. This is because the file's row <code>1</code> has a header that pandas fails to read. So let us skip that row</p> In\u00a0[19]: Copied! <pre>df = pd.read_csv(csv_path, skiprows=1)\ndf.head()\n</pre> df = pd.read_csv(csv_path, skiprows=1) df.head() Out[19]: Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude Longitude ... jtwc_??_wrad34_rad3 jtwc_??_wrad34_rad4 jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 0 NaN Year # BB BB NaN YYYY-MM-DD HH:MM:SS NaN deg_north deg_east ... nmile nmile nmile nmile nmile nmile nmile nmile nmile nmile 1 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 06:00:00 NR -13.70 63.90 ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 2 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 12:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 3 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 18:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 4 1874011S14064 1874 01 SI MM XXXX874148 1874-01-12 00:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 <p>5 rows \u00d7 200 columns</p> <p>A little better. But the file's 3rd row is also a header. Let us drop that row</p> In\u00a0[20]: Copied! <pre>df.drop(labels=0, axis=0, inplace=True)\ndf.head()\n</pre> df.drop(labels=0, axis=0, inplace=True) df.head() Out[20]: Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude Longitude ... jtwc_??_wrad34_rad3 jtwc_??_wrad34_rad4 jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 1 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 06:00:00 NR -13.70 63.90 ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 2 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 12:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 3 1874011S14064 1874 01 SI MM XXXX874148 1874-01-11 18:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 4 1874011S14064 1874 01 SI MM XXXX874148 1874-01-12 00:00:00 NR -999. -999. ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 5 1874011S14064 1874 01 SI MM XXXX874148 1874-01-12 06:00:00 NR -14.80 63.30 ... -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 -999.000 <p>5 rows \u00d7 200 columns</p> In\u00a0[9]: Copied! <pre>%%time\nfile_path = hurricane_raw_dir\nnum_records = {}\nfor file in file_list:\n    df = pd.read_csv(os.path.join(file_path, file), skiprows=1)\n    num_records[str(file.split('.')[1])] = df.shape[0]\n    \n    df.drop(labels=0, axis=0, inplace=True)\n    df.to_csv(os.path.join(file_path, file))\n    print(f'Processed {file}')\n</pre> %%time file_path = hurricane_raw_dir num_records = {} for file in file_list:     df = pd.read_csv(os.path.join(file_path, file), skiprows=1)     num_records[str(file.split('.')[1])] = df.shape[0]          df.drop(labels=0, axis=0, inplace=True)     df.to_csv(os.path.join(file_path, file))     print(f'Processed {file}') <pre>Processed Year.1842.ibtracs_all.v03r10.csv\nProcessed Year.1843.ibtracs_all.v03r10.csv\nProcessed Year.1844.ibtracs_all.v03r10.csv\nProcessed Year.1845.ibtracs_all.v03r10.csv\n...Processed Year.2013.ibtracs_all.v03r10.csv\nProcessed Year.2014.ibtracs_all.v03r10.csv\nProcessed Year.2015.ibtracs_all.v03r10.csv\nProcessed Year.2016.ibtracs_all.v03r10.csv\nProcessed Year.2017.ibtracs_all.v03r10.csv\nCPU times: user 36.4 s, sys: 3.39 s, total: 39.8 s\nWall time: 46.8 s\n</pre> In\u00a0[3]: Copied! <pre>fld_path = hurricane_raw_dir\ncsv_path = os.path.join(fld_path,'*.csv')\n</pre> fld_path = hurricane_raw_dir csv_path = os.path.join(fld_path,'*.csv') <p>Preemptively, specify the assortment of values that should be treated as null values.</p> In\u00a0[36]: Copied! <pre>%%time\ntable_na_values=['-999.','-999','-999.000', '-1', '-1.0','0','0.0']\nfull_df = ddf.read_csv(csv_path, na_values=table_na_values, dtype={'Center': 'object'})\n</pre> %%time table_na_values=['-999.','-999','-999.000', '-1', '-1.0','0','0.0'] full_df = ddf.read_csv(csv_path, na_values=table_na_values, dtype={'Center': 'object'}) <pre>CPU times: user 1.26 s, sys: 17.6 ms, total: 1.28 s\nWall time: 1.29 s\n</pre> <p>You can query the top few (or bottom few) records as you do on a regular Pandas <code>DataFrame</code> object.</p> In\u00a0[37]: Copied! <pre>full_df.head()\n</pre> full_df.head() Out[37]: Unnamed: 0 Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude ... jtwc_??_wrad34_rad3 jtwc_??_wrad34_rad4 jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 0 1 1842298N11080 1842 1 NI BB NOT NAMED 1842-10-25 06:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 2 1842298N11080 1842 1 NI BB NOT NAMED 1842-10-25 12:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 3 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-25 18:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 4 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-26 00:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 5 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-26 06:00:00 NR NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN <p>5 rows \u00d7 201 columns</p> <p>Drop the first, duplicate index column</p> In\u00a0[38]: Copied! <pre>full_df = full_df.drop(labels=['Unnamed: 0'], axis=1)\n</pre> full_df = full_df.drop(labels=['Unnamed: 0'], axis=1) In\u00a0[39]: Copied! <pre>all_columns=list(full_df.columns)\nlen(all_columns)\n</pre> all_columns=list(full_df.columns) len(all_columns) Out[39]: <pre>200</pre> <p>This dataset has <code>200</code> columns. Not all are unique as you can see from the print below:</p> In\u00a0[40]: Copied! <pre>from pprint import pprint\npprint(all_columns, compact=True, width=100)\n</pre> from pprint import pprint pprint(all_columns, compact=True, width=100) <pre>['Serial_Num', 'Season', 'Num', 'Basin', 'Sub_basin', 'Name', 'ISO_time', 'Nature', 'Latitude',\n 'Longitude', 'Wind(WMO)', 'Pres(WMO)', 'Center', 'Wind(WMO) Percentile', 'Pres(WMO) Percentile',\n 'Track_type', 'Latitude_for_mapping', 'Longitude_for_mapping', 'Current Basin', 'hurdat_atl_lat',\n 'hurdat_atl_lon', 'hurdat_atl_grade', 'hurdat_atl_wind', 'hurdat_atl_pres', 'td9636_lat',\n 'td9636_lon', 'td9636_grade', 'td9636_wind', 'td9636_pres', 'reunion_lat', 'reunion_lon',\n 'reunion_grade', 'reunion_wind', 'reunion_pres', 'atcf_lat', 'atcf_lon', 'atcf_grade', 'atcf_wind',\n 'atcf_pres', 'mlc_natl_lat', 'mlc_natl_lon', 'mlc_natl_grade', 'mlc_natl_wind', 'mlc_natl_pres',\n 'ds824_sh_lat', 'ds824_sh_lon', 'ds824_sh_grade', 'ds824_sh_wind', 'ds824_sh_pres', 'ds824_ni_lat',\n 'ds824_ni_lon', 'ds824_ni_grade', 'ds824_ni_wind', 'ds824_ni_pres', 'bom_lat', 'bom_lon',\n 'bom_grade', 'bom_wind', 'bom_pres', 'ds824_au_lat', 'ds824_au_lon', 'ds824_au_grade',\n 'ds824_au_wind', 'ds824_au_pres', 'jtwc_sh_lat', 'jtwc_sh_lon', 'jtwc_sh_grade', 'jtwc_sh_wind',\n 'jtwc_sh_pres', 'jtwc_wp_lat', 'jtwc_wp_lon', 'jtwc_wp_grade', 'jtwc_wp_wind', 'jtwc_wp_pres',\n 'td9635_lat', 'td9635_lon', 'td9635_grade', 'td9635_wind', 'td9635_pres', 'ds824_wp_lat',\n 'ds824_wp_lon', 'ds824_wp_grade', 'ds824_wp_wind', 'ds824_wp_pres', 'jtwc_io_lat', 'jtwc_io_lon',\n 'jtwc_io_grade', 'jtwc_io_wind', 'jtwc_io_pres', 'cma_lat', 'cma_lon', 'cma_grade', 'cma_wind',\n 'cma_pres', 'hurdat_epa_lat', 'hurdat_epa_lon', 'hurdat_epa_grade', 'hurdat_epa_wind',\n 'hurdat_epa_pres', 'jtwc_ep_lat', 'jtwc_ep_lon', 'jtwc_ep_grade', 'jtwc_ep_wind', 'jtwc_ep_pres',\n 'ds824_ep_lat', 'ds824_ep_lon', 'ds824_ep_grade', 'ds824_ep_wind', 'ds824_ep_pres', 'jtwc_cp_lat',\n 'jtwc_cp_lon', 'jtwc_cp_grade', 'jtwc_cp_wind', 'jtwc_cp_pres', 'tokyo_lat', 'tokyo_lon',\n 'tokyo_grade', 'tokyo_wind', 'tokyo_pres', 'neumann_lat', 'neumann_lon', 'neumann_grade',\n 'neumann_wind', 'neumann_pres', 'hko_lat', 'hko_lon', 'hko_grade', 'hko_wind', 'hko_pres',\n 'cphc_lat', 'cphc_lon', 'cphc_grade', 'cphc_wind', 'cphc_pres', 'wellington_lat', 'wellington_lon',\n 'wellington_grade', 'wellington_wind', 'wellington_pres', 'newdelhi_lat', 'newdelhi_lon',\n 'newdelhi_grade', 'newdelhi_wind', 'newdelhi_pres', 'nadi_lat', 'nadi_lon', 'nadi_grade',\n 'nadi_wind', 'nadi_pres', 'reunion_rmw', 'reunion_wind_radii_1_ne', 'reunion_wind_radii_1_se',\n 'reunion_wind_radii_1_sw', 'reunion_wind_radii_1_nw', 'reunion_wind_radii_2_ne',\n 'reunion_wind_radii_2_se', 'reunion_wind_radii_2_sw', 'reunion_wind_radii_2_nw',\n 'bom_mn_hurr_xtnt', 'bom_mn_gale_xtnt', 'bom_mn_eye_diam', 'bom_roci', 'atcf_rmw', 'atcf_poci',\n 'atcf_roci', 'atcf_eye', 'atcf_wrad34_rad1', 'atcf_wrad34_rad2', 'atcf_wrad34_rad3',\n 'atcf_wrad34_rad4', 'atcf_wrad50_rad1', 'atcf_wrad50_rad2', 'atcf_wrad50_rad3', 'atcf_wrad50_rad4',\n 'atcf_wrad64_rad1', 'atcf_wrad64_rad2', 'atcf_wrad64_rad3', 'atcf_wrad64_rad4', 'tokyo_dir50',\n 'tokyo_long50', 'tokyo_short50', 'tokyo_dir30', 'tokyo_long30', 'tokyo_short30', 'jtwc_??_rmw',\n 'jtwc_??_poci', 'jtwc_??_roci', 'jtwc_??_eye', 'jtwc_??_wrad34_rad1', 'jtwc_??_wrad34_rad2',\n 'jtwc_??_wrad34_rad3', 'jtwc_??_wrad34_rad4', 'jtwc_??_wrad50_rad1', 'jtwc_??_wrad50_rad2',\n 'jtwc_??_wrad50_rad3', 'jtwc_??_wrad50_rad4', 'jtwc_??_wrad64_rad1', 'jtwc_??_wrad64_rad2',\n 'jtwc_??_wrad64_rad3', 'jtwc_??_wrad64_rad4']\n</pre> <p>Reading the metadata from NOAA NCDC site, we find sensor measurements get unique columns if they are collected by a different agency. Thus we find multiple pressure, wind speed, latitude, longitude, etc. columns with different suffixes and prefixes. Data is sparse as it gets distributed between these columns. For our geospatial analysis, it suffices if we can merge these columns together and get location information from the co-ordinates.</p> In\u00a0[41]: Copied! <pre>lat_columns = [x for x in all_columns if 'lat' in x.lower()]\nlon_columns = [x for x in all_columns if 'lon' in x.lower()]\nfor x in zip(lat_columns, lon_columns):\n    print(x)\n</pre> lat_columns = [x for x in all_columns if 'lat' in x.lower()] lon_columns = [x for x in all_columns if 'lon' in x.lower()] for x in zip(lat_columns, lon_columns):     print(x) <pre>('Latitude', 'Longitude')\n('Latitude_for_mapping', 'Longitude_for_mapping')\n('hurdat_atl_lat', 'hurdat_atl_lon')\n('td9636_lat', 'td9636_lon')\n('reunion_lat', 'reunion_lon')\n('atcf_lat', 'atcf_lon')\n('mlc_natl_lat', 'mlc_natl_lon')\n('ds824_sh_lat', 'ds824_sh_lon')\n('ds824_ni_lat', 'ds824_ni_lon')\n('bom_lat', 'bom_lon')\n('ds824_au_lat', 'ds824_au_lon')\n('jtwc_sh_lat', 'jtwc_sh_lon')\n('jtwc_wp_lat', 'jtwc_wp_lon')\n('td9635_lat', 'td9635_lon')\n('ds824_wp_lat', 'ds824_wp_lon')\n('jtwc_io_lat', 'jtwc_io_lon')\n('cma_lat', 'cma_lon')\n('hurdat_epa_lat', 'hurdat_epa_lon')\n('jtwc_ep_lat', 'jtwc_ep_lon')\n('ds824_ep_lat', 'ds824_ep_lon')\n('jtwc_cp_lat', 'jtwc_cp_lon')\n('tokyo_lat', 'tokyo_lon')\n('neumann_lat', 'neumann_lon')\n('hko_lat', 'hko_lon')\n('cphc_lat', 'cphc_lon')\n('wellington_lat', 'wellington_lon')\n('newdelhi_lat', 'newdelhi_lon')\n('nadi_lat', 'nadi_lon')\n</pre> <p>In this dataset, if data is collected by 1 agency, the corresponding duplicate columns from other agencies are empty. However there may be exceptions. Hence we define a custom function that will pick median value for a row, from a given list of columns. This way, we can consolidate latitude / longitude information from all the agencies.</p> In\u00a0[42]: Copied! <pre>def pick_median_value(row, col_list):\n    return row[col_list].median()\n</pre> def pick_median_value(row, col_list):     return row[col_list].median() In\u00a0[43]: Copied! <pre>%%time\nfull_df['latitude_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = lat_columns)\n</pre> %%time full_df['latitude_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = lat_columns) <pre>CPU times: user 56.9 ms, sys: 5.31 ms, total: 62.2 ms\nWall time: 58.3 ms\n</pre> In\u00a0[44]: Copied! <pre>%%time\nfull_df['longitude_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = lon_columns)\n</pre> %%time full_df['longitude_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = lon_columns) <pre>CPU times: user 58.3 ms, sys: 5.43 ms, total: 63.7 ms\nWall time: 59.1 ms\n</pre> <p>With <code>dask</code>, the above operation was delayed and stored in a queue. It has not been evaluated yet. Next, let us evaluate for <code>5</code> records and print output. If results look good, we will merge all remaining related columns together.</p> In\u00a0[45]: Copied! <pre>%%time\nfull_df.head(5)\n</pre> %%time full_df.head(5) <pre>CPU times: user 137 ms, sys: 6.17 ms, total: 143 ms\nWall time: 141 ms\n</pre> Out[45]: Serial_Num Season Num Basin Sub_basin Name ISO_time Nature Latitude Longitude ... jtwc_??_wrad50_rad1 jtwc_??_wrad50_rad2 jtwc_??_wrad50_rad3 jtwc_??_wrad50_rad4 jtwc_??_wrad64_rad1 jtwc_??_wrad64_rad2 jtwc_??_wrad64_rad3 jtwc_??_wrad64_rad4 latitude_merged longitude_merged 0 1842298N11080 1842 1 NI BB NOT NAMED 1842-10-25 06:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.885 79.815 1 1842298N11080 1842 1 NI BB NOT NAMED 1842-10-25 12:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.810 78.890 2 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-25 18:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.795 77.910 3 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-26 00:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.795 76.915 4 1842298N11080 1842 1 NI AS NOT NAMED 1842-10-26 06:00:00 NR NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 10.805 75.820 <p>5 rows \u00d7 202 columns</p> <p>The results look good. Two additional columns(<code>latitude_merged</code>, <code>longitude_merged</code>) have been added. Thus by merging related columns, the redundant sparse columns can be removed and thereby simplifying the dimension of the input dataset.</p> <p>Now that this prototype looks good, we will proceed by identifying the lists of remaining columns that are redundant and can be merged.</p> In\u00a0[46]: Copied! <pre>from copy import deepcopy\ncolumns_tracker = deepcopy(all_columns)\nlen(columns_tracker)\n</pre> from copy import deepcopy columns_tracker = deepcopy(all_columns) len(columns_tracker) Out[46]: <pre>200</pre> <p>From the <code>columns_tracker</code> list, let us remove the redundant columns we already identified for location columns:</p> In\u00a0[47]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in lat_columns]\ncolumns_tracker = [x for x in columns_tracker if x not in lon_columns]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in lat_columns] columns_tracker = [x for x in columns_tracker if x not in lon_columns] len(columns_tracker) Out[47]: <pre>142</pre> <p>Thus, we have reduced the number of columns from <code>200</code> to <code>142</code>. We will progressively reduce this while retaining key information.</p> In\u00a0[48]: Copied! <pre># pick all columns that have 'wind' in name\nwind_columns = [x for x in columns_tracker if 'wind' in x.lower()]\n\n# based on metadata doc, we decide to eliminate percentile and wind distance columns\ncolumns_to_eliminate = [x for x in wind_columns if 'radii' in x or 'percentile' in x.lower()]\n\n# trim wind_columns by removing the ones we need to eliminate\nwind_columns = [x for x in wind_columns if x not in columns_to_eliminate]\nwind_columns\n</pre> # pick all columns that have 'wind' in name wind_columns = [x for x in columns_tracker if 'wind' in x.lower()]  # based on metadata doc, we decide to eliminate percentile and wind distance columns columns_to_eliminate = [x for x in wind_columns if 'radii' in x or 'percentile' in x.lower()]  # trim wind_columns by removing the ones we need to eliminate wind_columns = [x for x in wind_columns if x not in columns_to_eliminate] wind_columns Out[48]: <pre>['Wind(WMO)',\n 'hurdat_atl_wind',\n 'td9636_wind',\n 'reunion_wind',\n 'atcf_wind',\n 'mlc_natl_wind',\n 'ds824_sh_wind',\n 'ds824_ni_wind',\n 'bom_wind',\n 'ds824_au_wind',\n 'jtwc_sh_wind',\n 'jtwc_wp_wind',\n 'td9635_wind',\n 'ds824_wp_wind',\n 'jtwc_io_wind',\n 'cma_wind',\n 'hurdat_epa_wind',\n 'jtwc_ep_wind',\n 'ds824_ep_wind',\n 'jtwc_cp_wind',\n 'tokyo_wind',\n 'neumann_wind',\n 'hko_wind',\n 'cphc_wind',\n 'wellington_wind',\n 'newdelhi_wind',\n 'nadi_wind']</pre> In\u00a0[49]: Copied! <pre>%%time\nfull_df['wind_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = wind_columns)\n</pre> %%time full_df['wind_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = wind_columns) <pre>CPU times: user 56.7 ms, sys: 4.92 ms, total: 61.6 ms\nWall time: 57.6 ms\n</pre> In\u00a0[50]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in wind_columns]\ncolumns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in wind_columns] columns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate] len(columns_tracker) Out[50]: <pre>106</pre> In\u00a0[51]: Copied! <pre># pick all columns that have 'pres' in name\npressure_columns = [x for x in columns_tracker if 'pres' in x.lower()]\n\n# from metadata, we eliminate percentile and pres distance columns\nif columns_to_eliminate:\n    columns_to_eliminate.extend([x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()])\nelse:\n    columns_to_eliminate = [x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()]\n\n# trim wind_columns by removing the ones we need to eliminate\npressure_columns = [x for x in pressure_columns if x not in columns_to_eliminate]\npressure_columns\n</pre> # pick all columns that have 'pres' in name pressure_columns = [x for x in columns_tracker if 'pres' in x.lower()]  # from metadata, we eliminate percentile and pres distance columns if columns_to_eliminate:     columns_to_eliminate.extend([x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()]) else:     columns_to_eliminate = [x for x in pressure_columns if 'radii' in x or 'percentile' in x.lower()]  # trim wind_columns by removing the ones we need to eliminate pressure_columns = [x for x in pressure_columns if x not in columns_to_eliminate] pressure_columns Out[51]: <pre>['Pres(WMO)',\n 'hurdat_atl_pres',\n 'td9636_pres',\n 'reunion_pres',\n 'atcf_pres',\n 'mlc_natl_pres',\n 'ds824_sh_pres',\n 'ds824_ni_pres',\n 'bom_pres',\n 'ds824_au_pres',\n 'jtwc_sh_pres',\n 'jtwc_wp_pres',\n 'td9635_pres',\n 'ds824_wp_pres',\n 'jtwc_io_pres',\n 'cma_pres',\n 'hurdat_epa_pres',\n 'jtwc_ep_pres',\n 'ds824_ep_pres',\n 'jtwc_cp_pres',\n 'tokyo_pres',\n 'neumann_pres',\n 'hko_pres',\n 'cphc_pres',\n 'wellington_pres',\n 'newdelhi_pres',\n 'nadi_pres']</pre> In\u00a0[52]: Copied! <pre>%%time\nfull_df['pressure_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = pressure_columns)\n</pre> %%time full_df['pressure_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = pressure_columns) <pre>CPU times: user 122 ms, sys: 5.33 ms, total: 127 ms\nWall time: 123 ms\n</pre> In\u00a0[53]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in pressure_columns]\ncolumns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in pressure_columns] columns_tracker = [x for x in columns_tracker if x not in columns_to_eliminate] len(columns_tracker) Out[53]: <pre>78</pre> <p>Notice the length of <code>columns_tracker</code> is reducing progressively as we identify redundant columns.</p> In\u00a0[54]: Copied! <pre># pick all columns that have 'grade' in name\ngrade_columns = [x for x in columns_tracker if 'grade' in x.lower()]\ngrade_columns\n</pre> # pick all columns that have 'grade' in name grade_columns = [x for x in columns_tracker if 'grade' in x.lower()] grade_columns Out[54]: <pre>['hurdat_atl_grade',\n 'td9636_grade',\n 'reunion_grade',\n 'atcf_grade',\n 'mlc_natl_grade',\n 'ds824_sh_grade',\n 'ds824_ni_grade',\n 'bom_grade',\n 'ds824_au_grade',\n 'jtwc_sh_grade',\n 'jtwc_wp_grade',\n 'td9635_grade',\n 'ds824_wp_grade',\n 'jtwc_io_grade',\n 'cma_grade',\n 'hurdat_epa_grade',\n 'jtwc_ep_grade',\n 'ds824_ep_grade',\n 'jtwc_cp_grade',\n 'tokyo_grade',\n 'neumann_grade',\n 'hko_grade',\n 'cphc_grade',\n 'wellington_grade',\n 'newdelhi_grade',\n 'nadi_grade']</pre> In\u00a0[55]: Copied! <pre>%%time\nfull_df['grade_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = grade_columns)\n</pre> %%time full_df['grade_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = grade_columns) <pre>CPU times: user 54.9 ms, sys: 5.59 ms, total: 60.5 ms\nWall time: 56.4 ms\n</pre> In\u00a0[56]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in grade_columns]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in grade_columns] len(columns_tracker) Out[56]: <pre>52</pre> In\u00a0[57]: Copied! <pre># pick all columns that have 'eye' in name\neye_dia_columns = [x for x in columns_tracker if 'eye' in x.lower()]\neye_dia_columns\n</pre> # pick all columns that have 'eye' in name eye_dia_columns = [x for x in columns_tracker if 'eye' in x.lower()] eye_dia_columns Out[57]: <pre>['bom_mn_eye_diam', 'atcf_eye', 'jtwc_??_eye']</pre> In\u00a0[58]: Copied! <pre>%%time\nfull_df['eye_dia_merged'] = full_df.apply(pick_median_value, axis=1,\n                                          col_list = eye_dia_columns)\n</pre> %%time full_df['eye_dia_merged'] = full_df.apply(pick_median_value, axis=1,                                           col_list = eye_dia_columns) <pre>CPU times: user 53.6 ms, sys: 4.74 ms, total: 58.4 ms\nWall time: 54.8 ms\n</pre> In\u00a0[59]: Copied! <pre>columns_tracker = [x for x in columns_tracker if x not in eye_dia_columns]\nlen(columns_tracker)\n</pre> columns_tracker = [x for x in columns_tracker if x not in eye_dia_columns] len(columns_tracker) Out[59]: <pre>49</pre> <p>We are down to <code>49</code> columns, let us visualize what those look like.</p> In\u00a0[60]: Copied! <pre>pprint(columns_tracker, width=119, compact=True)\n</pre> pprint(columns_tracker, width=119, compact=True) <pre>['Serial_Num', 'Season', 'Num', 'Basin', 'Sub_basin', 'Name', 'ISO_time', 'Nature', 'Center', 'Track_type',\n 'Current Basin', 'reunion_rmw', 'bom_mn_hurr_xtnt', 'bom_mn_gale_xtnt', 'bom_roci', 'atcf_rmw', 'atcf_poci',\n 'atcf_roci', 'atcf_wrad34_rad1', 'atcf_wrad34_rad2', 'atcf_wrad34_rad3', 'atcf_wrad34_rad4', 'atcf_wrad50_rad1',\n 'atcf_wrad50_rad2', 'atcf_wrad50_rad3', 'atcf_wrad50_rad4', 'atcf_wrad64_rad1', 'atcf_wrad64_rad2',\n 'atcf_wrad64_rad3', 'atcf_wrad64_rad4', 'tokyo_dir50', 'tokyo_short50', 'tokyo_dir30', 'tokyo_short30', 'jtwc_??_rmw',\n 'jtwc_??_poci', 'jtwc_??_roci', 'jtwc_??_wrad34_rad1', 'jtwc_??_wrad34_rad2', 'jtwc_??_wrad34_rad3',\n 'jtwc_??_wrad34_rad4', 'jtwc_??_wrad50_rad1', 'jtwc_??_wrad50_rad2', 'jtwc_??_wrad50_rad3', 'jtwc_??_wrad50_rad4',\n 'jtwc_??_wrad64_rad1', 'jtwc_??_wrad64_rad2', 'jtwc_??_wrad64_rad3', 'jtwc_??_wrad64_rad4']\n</pre> <p>Based on metadata shared by data provider, we choose to retain only the first <code>11</code> columns. We add the rest to the list <code>columns_to_eliminate</code>.</p> In\u00a0[61]: Copied! <pre>columns_to_eliminate.extend(columns_tracker[11:])\npprint(columns_to_eliminate, width=119, compact=True)\n</pre> columns_to_eliminate.extend(columns_tracker[11:]) pprint(columns_to_eliminate, width=119, compact=True) <pre>['Wind(WMO) Percentile', 'reunion_wind_radii_1_ne', 'reunion_wind_radii_1_se', 'reunion_wind_radii_1_sw',\n 'reunion_wind_radii_1_nw', 'reunion_wind_radii_2_ne', 'reunion_wind_radii_2_se', 'reunion_wind_radii_2_sw',\n 'reunion_wind_radii_2_nw', 'Pres(WMO) Percentile', 'reunion_rmw', 'bom_mn_hurr_xtnt', 'bom_mn_gale_xtnt', 'bom_roci',\n 'atcf_rmw', 'atcf_poci', 'atcf_roci', 'atcf_wrad34_rad1', 'atcf_wrad34_rad2', 'atcf_wrad34_rad3', 'atcf_wrad34_rad4',\n 'atcf_wrad50_rad1', 'atcf_wrad50_rad2', 'atcf_wrad50_rad3', 'atcf_wrad50_rad4', 'atcf_wrad64_rad1',\n 'atcf_wrad64_rad2', 'atcf_wrad64_rad3', 'atcf_wrad64_rad4', 'tokyo_dir50', 'tokyo_short50', 'tokyo_dir30',\n 'tokyo_short30', 'jtwc_??_rmw', 'jtwc_??_poci', 'jtwc_??_roci', 'jtwc_??_wrad34_rad1', 'jtwc_??_wrad34_rad2',\n 'jtwc_??_wrad34_rad3', 'jtwc_??_wrad34_rad4', 'jtwc_??_wrad50_rad1', 'jtwc_??_wrad50_rad2', 'jtwc_??_wrad50_rad3',\n 'jtwc_??_wrad50_rad4', 'jtwc_??_wrad64_rad1', 'jtwc_??_wrad64_rad2', 'jtwc_??_wrad64_rad3', 'jtwc_??_wrad64_rad4']\n</pre> In\u00a0[62]: Copied! <pre>len(full_df.columns)\n</pre> len(full_df.columns) Out[62]: <pre>206</pre> In\u00a0[63]: Copied! <pre>columns_to_drop = lat_columns + lon_columns + wind_columns + pressure_columns + \\\n                    grade_columns + eye_dia_columns+columns_to_eliminate\nlen(columns_to_drop)\n</pre> columns_to_drop = lat_columns + lon_columns + wind_columns + pressure_columns + \\                     grade_columns + eye_dia_columns+columns_to_eliminate len(columns_to_drop) Out[63]: <pre>189</pre> In\u00a0[34]: Copied! <pre>full_df.visualize()\n</pre> full_df.visualize() Out[34]: <p>Below we perform execute all column merge and column drop operations that we have queued so far. We store the resulting <code>DataFrame</code> in a new variable.</p> In\u00a0[\u00a0]: Copied! <pre>if 'hurricanes_merged' not in os.listdir(data_dir):\n    os.mkdir(os.path.join(data_dir,'hurricanes_merged'))\n\nmerged_csv_path = os.path.join(data_dir, 'hurricanes_merged')\n</pre> if 'hurricanes_merged' not in os.listdir(data_dir):     os.mkdir(os.path.join(data_dir,'hurricanes_merged'))  merged_csv_path = os.path.join(data_dir, 'hurricanes_merged') In\u00a0[64]: Copied! <pre>%%time\nmerged_df = full_df.drop(columns_to_drop, axis=1)\nmerged_df.to_csv(os.path.join(merged_csv_path, 'hurr_dask_*.csv'))\n</pre> %%time merged_df = full_df.drop(columns_to_drop, axis=1) merged_df.to_csv(os.path.join(merged_csv_path, 'hurr_dask_*.csv')) <pre>CPU times: user 43min 46s, sys: 6min 3s, total: 49min 49s\nWall time: 45min 45s\n</pre> <p>The <code>save()</code> operation spawns several workers that compute in parallel. Notice the ouput file name contains a wildcard (<code>*</code>). This allows Dask to both read data in chunks and write outputs in chunks. The save operation will result in creating a number of processed CSV files whose names are prefixed with <code>hurr_dask</code> and suffixed with a number.</p> In\u00a0[65]: Copied! <pre>merged_df.shape\n</pre> merged_df.shape Out[65]: <pre>(348703, 17)</pre> In\u00a0[7]: Copied! <pre>gis = GIS(\"home\")\n</pre> gis = GIS(\"home\") <p>Get the geoanalytics datastores and search it for the registered datasets:</p> In\u00a0[20]: Copied! <pre># Query the data stores available\ndatastores = get_datastores()\nbigdata_fileshares = datastores.search()\nbigdata_fileshares\n</pre> # Query the data stores available datastores = get_datastores() bigdata_fileshares = datastores.search() bigdata_fileshares Out[20]: <pre>[&lt;Datastore title:\"/enterpriseDatabases/AGSDataStore_ds_41yu9aqv\" type:\"egdb\"&gt;,\n &lt;Datastore title:\"/bigDataFileShares/hurricanes_dask_csv\" type:\"bigDataFileShare\"&gt;,\n &lt;Datastore title:\"/nosqlDatabases/AGSDataStore_nosqldb_tcs_yjayxzc4\" type:\"nosql\"&gt;,\n &lt;Datastore title:\"/nosqlDatabases/AGSDataStore_bigdata_bds_wuqp7mbg\" type:\"nosql\"&gt;,\n &lt;Datastore title:\"/rasterStores/LocalRasterDS\" type:\"rasterStore\"&gt;]</pre> <p>The dataset <code>hurricanes_dask_csv</code> data is registered as a big data file share with the Geoanalytics datastore, so we can reference it:</p> In\u00a0[21]: Copied! <pre>data_item = bigdata_fileshares[1]\n</pre> data_item = bigdata_fileshares[1] <p>If there is no big data file share for hurricane track data registered on the server, we can register one that points to the shared folder containing the CSV files.</p> <pre><code>data_item = datastores.add_bigdata(\"Hurricane_tracks\", r\"\\\\path_to_hurricane_data\")</code></pre> <p>Once a big data file share is registered, the GeoAnalytics server processes all the valid file types to discern the schema of the data, including information about the geometry in a dataset. If the dataset is time-enabled, as is required to use some GeoAnalytics Tools, the manifest reports the necessary metadata about how time information is stored as well.</p> <p>This process can take a few minutes depending on the size of your data. Once processed, querying the manifest property will return a schema. As you can see from below, the schema is contains the columns we merged using DASK previously.</p> In\u00a0[22]: Copied! <pre>datasets = data_item.manifest['datasets']\nlen(datasets)\n</pre> datasets = data_item.manifest['datasets'] len(datasets) Out[22]: <pre>1</pre> In\u00a0[23]: Copied! <pre>[dataset['name'] for dataset in datasets]\n</pre> [dataset['name'] for dataset in datasets] Out[23]: <pre>['hurricanes_merged']</pre> In\u00a0[24]: Copied! <pre>datasets[0]\n</pre> datasets[0] Out[24]: <pre>{'name': 'hurricanes_merged',\n 'format': {'quoteChar': '\"',\n  'fieldDelimiter': ',',\n  'hasHeaderRow': True,\n  'encoding': 'UTF-8',\n  'escapeChar': '\"',\n  'recordTerminator': '\\n',\n  'type': 'delimited',\n  'extension': 'csv'},\n 'schema': {'fields': [{'name': 'col_1', 'type': 'esriFieldTypeBigInteger'},\n   {'name': 'Serial_Num', 'type': 'esriFieldTypeString'},\n   {'name': 'Season', 'type': 'esriFieldTypeBigInteger'},\n   {'name': 'Num', 'type': 'esriFieldTypeBigInteger'},\n   {'name': 'Basin', 'type': 'esriFieldTypeString'},\n   {'name': 'Sub_basin', 'type': 'esriFieldTypeString'},\n   {'name': 'Name', 'type': 'esriFieldTypeString'},\n   {'name': 'ISO_time', 'type': 'esriFieldTypeString'},\n   {'name': 'Nature', 'type': 'esriFieldTypeString'},\n   {'name': 'Center', 'type': 'esriFieldTypeString'},\n   {'name': 'Track_type', 'type': 'esriFieldTypeString'},\n   {'name': 'Current Basin', 'type': 'esriFieldTypeString'},\n   {'name': 'latitude_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'longitude_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'wind_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'pressure_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'grade_merged', 'type': 'esriFieldTypeDouble'},\n   {'name': 'eye_dia_merged', 'type': 'esriFieldTypeDouble'}]},\n 'geometry': {'geometryType': 'esriGeometryPoint',\n  'spatialReference': {'wkid': 4326},\n  'fields': [{'name': 'longitude_merged', 'formats': ['x']},\n   {'name': 'latitude_merged', 'formats': ['y']}]},\n 'time': {'timeType': 'instant',\n  'fields': [{'name': 'ISO_time', 'formats': ['yyyy-MM-dd HH:mm:ss']}],\n  'timeReference': {'timeZone': 'UTC'}}}</pre> In\u00a0[25]: Copied! <pre>search_result = gis.content.search(\"\", item_type = \"big data file share\")\nsearch_result\n</pre> search_result = gis.content.search(\"\", item_type = \"big data file share\") search_result Out[25]: <pre>[&lt;Item title:\"bigDataFileShares_hurricanes_dask_csv\" type:Big Data File Share owner:amani001&gt;]</pre> In\u00a0[26]: Copied! <pre>data_item = search_result[0]\ncleaned_csv = data_item.layers[0]\ncleaned_csv\n</pre> data_item = search_result[0] cleaned_csv = data_item.layers[0] cleaned_csv Out[26]: <pre>&lt;Layer url:\"https://datascience-arcpy.esri.com/server/rest/services/DataStoreCatalogs/bigDataFileShares_hurricanes_dask_csv/BigDataCatalogServer/hurricanes_merged\"&gt;</pre> In\u00a0[27]: Copied! <pre>agg_result = reconstruct_tracks(cleaned_csv, \n                                track_fields='Serial_Num',   # the Hurricane id number\n                                method='GEODESIC', output_name='hurricane_tracks_aggregated_ga')\n</pre> agg_result = reconstruct_tracks(cleaned_csv,                                  track_fields='Serial_Num',   # the Hurricane id number                                 method='GEODESIC', output_name='hurricane_tracks_aggregated_ga') <pre>Submitted.\nExecuting...\nExecuting (ReconstructTracks): ReconstructTracks \"Feature Set\" Serial_Num Geodesic # # # # # # \"{\"serviceProperties\": {\"name\": \"hurricane_tracks_aggregated_ga\", \"serviceUrl\": \"https://datascience-arcpy.esri.com/server/rest/services/Hosted/hurricane_tracks_aggregated_ga/FeatureServer\"}, \"itemProperties\": {\"itemId\": \"545adb07c4ba4da7b72ddbe47bd275d2\"}}\" \"{\"defaultAggregationStyles\": false}\"\nStart Time: Thu Nov 15 21:57:30 2018\nUsing URL based GPRecordSet param: https://datascience-arcpy.esri.com/server/rest/services/DataStoreCatalogs/bigDataFileShares_hurricanes_dask_csv/BigDataCatalogServer/hurricanes_merged\n{\"messageCode\":\"BD_101028\",\"message\":\"Starting new distributed job with 352 tasks.\",\"params\":{\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"0/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"0\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"177/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"177\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"241/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"241\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"308/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"308\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101029\",\"message\":\"352/352 distributed tasks completed.\",\"params\":{\"completedTasks\":\"352\",\"totalTasks\":\"352\"}}\n{\"messageCode\":\"BD_101081\",\"message\":\"Finished writing results:\"}\n{\"messageCode\":\"BD_101082\",\"message\":\"* Count of features = 12757\",\"params\":{\"resultCount\":\"12757\"}}\n{\"messageCode\":\"BD_101083\",\"message\":\"* Spatial extent = {\\\"xmin\\\":-180,\\\"ymin\\\":-68.5,\\\"xmax\\\":180,\\\"ymax\\\":81}\",\"params\":{\"extent\":\"{\\\"xmin\\\":-180,\\\"ymin\\\":-68.5,\\\"xmax\\\":180,\\\"ymax\\\":81}\"}}\n{\"messageCode\":\"BD_101084\",\"message\":\"* Temporal extent = Interval(MutableInstant(1842-10-25 06:00:00.000),MutableInstant(2017-06-13 06:00:00.000))\",\"params\":{\"extent\":\"Interval(MutableInstant(1842-10-25 06:00:00.000),MutableInstant(2017-06-13 06:00:00.000))\"}}\n</pre> <pre>{\"messageCode\":\"BD_101054\",\"message\":\"Some records have either missing or invalid geometries.\"}\n</pre> <pre>{\"messageCode\":\"BD_101054\",\"message\":\"Some records have either missing or invalid geometries.\"}\nSucceeded at Thu Nov 15 21:57:54 2018 (Elapsed Time: 24.21 seconds)\n</pre> In\u00a0[28]: Copied! <pre>agg_tracks_layer = agg_result.layers[0]\nagg_fields = [f.name for f in agg_tracks_layer.properties.fields]\npprint(agg_fields, compact=True)\n</pre> agg_tracks_layer = agg_result.layers[0] agg_fields = [f.name for f in agg_tracks_layer.properties.fields] pprint(agg_fields, compact=True) <pre>['Serial_Num', 'COUNT', 'COUNT_col_1', 'SUM_col_1', 'MIN_col_1', 'MAX_col_1',\n 'MEAN_col_1', 'RANGE_col_1', 'SD_col_1', 'VAR_col_1', 'COUNT_Season',\n 'SUM_Season', 'MIN_Season', 'MAX_Season', 'MEAN_Season', 'RANGE_Season',\n 'SD_Season', 'VAR_Season', 'COUNT_Num', 'SUM_Num', 'MIN_Num', 'MAX_Num',\n 'MEAN_Num', 'RANGE_Num', 'SD_Num', 'VAR_Num', 'COUNT_Basin', 'ANY_Basin',\n 'COUNT_Sub_basin', 'ANY_Sub_basin', 'COUNT_Name', 'ANY_Name', 'COUNT_ISO_time',\n 'ANY_ISO_time', 'COUNT_Nature', 'ANY_Nature', 'COUNT_Center', 'ANY_Center',\n 'COUNT_Track_type', 'ANY_Track_type', 'COUNT_Current_Basin',\n 'ANY_Current_Basin', 'COUNT_latitude_merged', 'SUM_latitude_merged',\n 'MIN_latitude_merged', 'MAX_latitude_merged', 'MEAN_latitude_merged',\n 'RANGE_latitude_merged', 'SD_latitude_merged', 'VAR_latitude_merged',\n 'COUNT_longitude_merged', 'SUM_longitude_merged', 'MIN_longitude_merged',\n 'MAX_longitude_merged', 'MEAN_longitude_merged', 'RANGE_longitude_merged',\n 'SD_longitude_merged', 'VAR_longitude_merged', 'COUNT_wind_merged',\n 'SUM_wind_merged', 'MIN_wind_merged', 'MAX_wind_merged', 'MEAN_wind_merged',\n 'RANGE_wind_merged', 'SD_wind_merged', 'VAR_wind_merged',\n 'COUNT_pressure_merged', 'SUM_pressure_merged', 'MIN_pressure_merged',\n 'MAX_pressure_merged', 'MEAN_pressure_merged', 'RANGE_pressure_merged',\n 'SD_pressure_merged', 'VAR_pressure_merged', 'COUNT_grade_merged',\n 'SUM_grade_merged', 'MIN_grade_merged', 'MAX_grade_merged',\n 'MEAN_grade_merged', 'RANGE_grade_merged', 'SD_grade_merged',\n 'VAR_grade_merged', 'COUNT_eye_dia_merged', 'SUM_eye_dia_merged',\n 'MIN_eye_dia_merged', 'MAX_eye_dia_merged', 'MEAN_eye_dia_merged',\n 'RANGE_eye_dia_merged', 'SD_eye_dia_merged', 'VAR_eye_dia_merged',\n 'TRACK_DURATION', 'globalid', 'OBJECTID', 'END_DATETIME', 'START_DATETIME']\n</pre> <p>To get the number of hurricanes the reconstruct tracks tool identified, we run a query on the aggregation layer and get just the number of records.</p> In\u00a0[7]: Copied! <pre>agg_tracks_layer.query(return_count_only=True)\n</pre> agg_tracks_layer.query(return_count_only=True) Out[7]: <pre>12362</pre>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#analyzing-historic-hurricane-tracks-part-13","title":"Analyzing historic hurricane tracks - Part 1/3\u00b6","text":"<p>Hurricanes are large swirling storms that produce winds of speeds <code>74</code> miles per hour (<code>119</code> kmph) or higher. When hurricanes make a landfall, they produce heavy rainfall, cause storm surges and intense flooding. Often hurricanes strike places that are dense in population, causing devastating amounts of death and destruction throughout the world.</p> <p>Since the recent past, agencies such as the National Hurricane Center have been collecting quantitative data about hurricanes. In this study we use meteorological data of hurricanes recorded in the past <code>169</code> years to analyze their location, intensity and investigate if there are any statistically significant trends. We also analyze the places most affected by hurricanes and what their demographic make up is. We conclude by citing releavant articles that draw similar conclusions.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#part-1-prepare-hurricane-data","title":"Part 1 - prepare hurricane data\u00b6","text":"<p>This notebook covers part 1 of this study. In this notebook we download data from NCEI portal, do extenstive pre-processing in the form of clearing headers, merging redundant columns and aggregate the observations into hurricane tracks.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#download-hurricane-data-from-ncei-ftp-portal","title":"Download hurricane data from NCEI FTP portal\u00b6","text":"<p>The National Centers for Environmental Information, formerly National Climatic Data Center shares the historic hurricane track datasets at ftp://eclipse.ncdc.noaa.gov/pub/ibtracs/v03r09/all/csv/. We use the <code>ftplib</code> Python library to login in and download these datasets.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#download-each-file-into-hurricanes_raw-directory","title":"Download each file into <code>hurricanes_raw</code> directory\u00b6","text":""},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#process-csv-files-by-removing-header-rows","title":"Process CSV files by removing header rows\u00b6","text":"<p>The CSV files have multiple header rows. Let us start by processing one of the files as an example</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#automate-across-all-files","title":"Automate across all files\u00b6","text":"<p>Now we need to repeat the above cleaning steps across all CSV files. In the steps below, we will read all CSV files, drop the headers and write to disk. This step is necessary as it will ease subsequent processing using the DASK library.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#cleaning-hurricane-observations-with-dask","title":"Cleaning hurricane observations with Dask\u00b6","text":"<p>The data collected from NOAA NCDC source is just too large to clean with Pandas or Excel. With <code>350,000 x 200</code> in dense matrix, this data is larger than memory for a normal computer. Hence traditional packages such as Pandas cannot be used as they expect data to fit fully in memory.</p> <p>Thus, in this part of the study, we use Dask, a distributed data analysis library. Functionally, Dask provides a <code>DataFrame</code> object that behaves similar to a traditional pandas <code>DataFrame</code> object. You can perform slicing, dicing, exploration on them. However transformative operations on the <code>DataFrame</code> get queued and are operated only when necessary. When executed, Dask will read data in chunks, distribute it to workers (be it cores on a single machine or multiple machines in a cluster set up) and collect the data back for you. Thus, DASK allows you to work with any larger than memory dataset as it performs operations on chunks of it, in a distributed manner.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#read-input-csv-data","title":"Read input CSV data\u00b6","text":"<p>As mentioned earlier DASK allows you to work with larger than memory datasets. These datasets can reside as one large file or as multiple files in a folder. For latter, DASK allows you to just specify the folder containing the datasets as input. In turn, it provides you a single <code>DataFrame</code> object that represents all your datasets combined together. The operations you perform on this <code>DataFrame</code> get queued and executed only when necessary.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-all-location-columns","title":"Merge all location columns\u00b6","text":"<p>Below we prototype merging location columns. If this succeeds, we will proceed to merge all remaining columns</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-similar-columns","title":"Merge similar columns\u00b6","text":"<p>To keep track of which columns have been accounted for, we will duplicate the <code>all_columns</code> list and remove ones that we have identified.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-wind-columns","title":"Merge wind columns\u00b6","text":"<p>Wind, pressure, grade are some of the meteorological observations this dataset contains. To start off, let us identify the wind columns:</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-pressure-columns","title":"Merge pressure columns\u00b6","text":"<p>We proceed to identify all <code>pressure</code> columns. But before that, we update the <code>columns_tracker</code> list by removing those we identified for wind:</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-grade-columns","title":"Merge grade columns\u00b6","text":""},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#merge-eye-diameter-columns","title":"Merge eye diameter columns\u00b6","text":""},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#identify-remaining-redundant-columns","title":"Identify remaining redundant columns\u00b6","text":""},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#drop-all-redundant-columns","title":"Drop all redundant columns\u00b6","text":"<p>So far, we have merged similar columns together and collected the lists of redundant columns to drop. Below we compile them into a single list.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#perform-delayed-computation","title":"Perform delayed computation\u00b6","text":"<p>In Dask, all computations are delayed and queued. The <code>apply()</code> functions called earlier are not executed yet, however respective columns have been created as you can see from the DataFrame display above. In the cells below, we will call <code>save()</code> to make Dask compute on this larger than memory dataset.</p> <p>Calling <code>visualize()</code> on the delayed compute operation or the <code>DataFrame</code> object will plot the dask task queue as shown below. The graphic below provides a glimpse on how Dask distributes its tasks and how it reads this 'larger than memory dataset' in chunks and operates on them.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#preview-results","title":"Preview results\u00b6","text":"<p>Once Dask realizes a delayed computation, it returns the result as an in-memory Pandas DataFrame object. Thus, the <code>merged_df</code> variable represents a Pandas <code>DataFrame</code> object with <code>348,703</code> records and <code>17</code> columns.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#creating-hurricane-tracks-using-geoanalytics","title":"Creating hurricane tracks using Geoanalytics\u00b6","text":"<p>The data collected so far are a set of Point observations representing all hurricanes recorded in the last <code>169</code> years. To make sense of these points, we need to connect them together to create a track for each hurricane. This part of the notebook uses ArcGIS GeoAnalytics server to reconstruct such hurricane tracks. The GeoAnalytics server is capable of processing on massive datasets in a scalable and distributed fashion.</p> <p>The DASK process merged redundant columns together and ouput a folder full of CSV files. The GeoAnalytics server is also capabale of accepting a folder of datasets as 1 dataset and working on them. Thus in this part, we register that folder as a datastore and on the GeoAnalytics server for processing.</p> <p>Reconstruct tracks: Reconstruct tracks is a type of data aggregation tool available in the <code>arcgis.geoanalytics</code> module. This tool works with a layer of point features or polygon features that are time enabled. It first determines which points belong to a track using an identification number or identification string. Using the time at each location, the tracks are ordered sequentially and transformed into a line representing the path of movement. The map below shows a subset of the point datasets.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#create-a-data-store","title":"Create a data store\u00b6","text":"<p>For the GeoAnalytics server to process your big data, it needs the data to be registered as a data store. In our case, the data is in multiple CSV files and we will register the folder containing the files as a data store of type <code>bigDataFileShare</code>.</p> <p>Let us connect to an ArcGIS Enterprise</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#perform-data-aggregation-using-reconstruct-tracks-tool","title":"Perform data aggregation using reconstruct tracks tool\u00b6","text":"<p>When you add a big data file share, a corresponding item gets created in your GIS. You can search for it like a regular item and query its layers.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#execute-reconstruct-tracks-tool","title":"Execute reconstruct tracks tool\u00b6","text":"<p>The <code>reconstruct_tracks()</code> function is available in the <code>arcgis.geoanalytics.summarize_data</code> module. In this example, we are using this tool to aggregate the numerous points into line segments showing the tracks followed by the hurricanes. The tool creates a feature layer item as an output which can be accessed once the processing is complete.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#analyze-the-result-of-aggregation","title":"Analyze the result of aggregation\u00b6","text":"<p>The reconstruct tracks produces summary statistics such as <code>MIN</code>, <code>MAX</code>, <code>MEAN</code>, <code>MEDIAN</code>, <code>RANGE</code>, <code>SD</code>, <code>VAR</code>, <code>SUM</code>, for numeric columns and <code>COUNT</code> for ordinal columns during the aggregation process. Let us list the fields in this dataset to view them.</p>"},{"location":"projects/dist-computing/dask/part1_prepare_hurricane_data/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook we observed how to download meteorological data over FTP from NEIC website. The data came in <code>169</code> CSV files for the past <code>169</code> years. We sanitized it initially using Pandas to remove bad header rows. We then used DASK library to read all <code>169</code> files as a single file and merged data from redundant columns. This pre-processing resulted in multiple output CSV files covering a total of 348k records and 17 columns.</p> <p>This data was fed to the ArcGIS GeoAnalytics server for aggregation. The reconstruct tracks tool on GeoAnalytics server reduced this point dataset into hurricane tracks (lines) and during this aggregation, it calculated summary statistics for the numerical columns. The tool identified <code>12,362</code> individual hurricanes from the past <code>169</code> years.</p> <p>In Part 2 of this study, we will visualize and explore this dataset to understand the prevelance, duration of hurricanes and the communities affected by hurricanes worldwide.</p> <p>In Part 3, we will analyze this aggregated result comprehensively, answer important questions such as, does the intensity of hurricanes increase over time and draw conclusions.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/","title":"Analyzing hurricane tracks - Part 2/3","text":"<p>Import the libraries necessary for this notebook.</p> In\u00a0[1]: Copied! <pre># import ArcGIS Libraries\nfrom arcgis.gis import GIS\nfrom arcgis.geometry import filters\nfrom arcgis.geocoding import geocode\nfrom arcgis.features.manage_data import overlay_layers\nfrom arcgis.geoenrichment import enrich\n\n# import Pandas for data exploration\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# import plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# import display tools\nfrom pprint import pprint\nfrom IPython.display import display\n\n# import system libs\nfrom sys import getsizeof\n</pre> # import ArcGIS Libraries from arcgis.gis import GIS from arcgis.geometry import filters from arcgis.geocoding import geocode from arcgis.features.manage_data import overlay_layers from arcgis.geoenrichment import enrich  # import Pandas for data exploration import pandas as pd import numpy as np from scipy import stats  # import plotting libraries import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline  # import display tools from pprint import pprint from IPython.display import display  # import system libs from sys import getsizeof In\u00a0[3]: Copied! <pre>gis = GIS('home')\n</pre> gis = GIS('home') In\u00a0[2]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n</pre> import warnings warnings.filterwarnings('ignore') In\u00a0[4]: Copied! <pre>hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga')[0]\nhurricane_fl = hurricane_tracks_item.layers[0]\n</pre> hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga')[0] hurricane_fl = hurricane_tracks_item.layers[0] <p>The GeoAnalytics step calculated summary statistics of all numeric fields. However only a few of the columns are of interest to us.</p> In\u00a0[5]: Copied! <pre>pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80)\n</pre> pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80) <pre>['objectid', 'serial_num', 'count', 'count_col_1', 'sum_col_1', 'min_col_1',\n 'max_col_1', 'mean_col_1', 'range_col_1', 'sd_col_1', 'var_col_1',\n 'count_season', 'sum_season', 'min_season', 'max_season', 'mean_season',\n 'range_season', 'sd_season', 'var_season', 'count_num', 'sum_num', 'min_num',\n 'max_num', 'mean_num', 'range_num', 'sd_num', 'var_num', 'count_basin',\n 'any_basin', 'count_sub_basin', 'any_sub_basin', 'count_name', 'any_name',\n 'count_iso_time', 'any_iso_time', 'count_nature', 'any_nature', 'count_center',\n 'any_center', 'count_track_type', 'any_track_type', 'count_current_basin',\n 'any_current_basin', 'count_latitude_merged', 'sum_latitude_merged',\n 'min_latitude_merged', 'max_latitude_merged', 'mean_latitude_merged',\n 'range_latitude_merged', 'sd_latitude_merged', 'var_latitude_merged',\n 'count_longitude_merged', 'sum_longitude_merged', 'min_longitude_merged',\n 'max_longitude_merged', 'mean_longitude_merged', 'range_longitude_merged',\n 'sd_longitude_merged', 'var_longitude_merged', 'count_wind_merged',\n 'sum_wind_merged', 'min_wind_merged', 'max_wind_merged', 'mean_wind_merged',\n 'range_wind_merged', 'sd_wind_merged', 'var_wind_merged',\n 'count_pressure_merged', 'sum_pressure_merged', 'min_pressure_merged',\n 'max_pressure_merged', 'mean_pressure_merged', 'range_pressure_merged',\n 'sd_pressure_merged', 'var_pressure_merged', 'count_grade_merged',\n 'sum_grade_merged', 'min_grade_merged', 'max_grade_merged',\n 'mean_grade_merged', 'range_grade_merged', 'sd_grade_merged',\n 'var_grade_merged', 'count_eye_dia_merged', 'sum_eye_dia_merged',\n 'min_eye_dia_merged', 'max_eye_dia_merged', 'mean_eye_dia_merged',\n 'range_eye_dia_merged', 'sd_eye_dia_merged', 'var_eye_dia_merged',\n 'track_duration', 'end_datetime', 'start_datetime']\n</pre> <p>Below we select the following fields for the rest of this analysis.</p> In\u00a0[9]: Copied! <pre>fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',\n                   'any_name', 'mean_latitude_merged', 'mean_longitude_merged',\n                   'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',\n                   'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',\n                   'end_datetime', 'start_datetime']\n</pre> fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',                    'any_name', 'mean_latitude_merged', 'mean_longitude_merged',                    'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',                    'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',                    'end_datetime', 'start_datetime'] In\u00a0[10]: Copied! <pre>%%time\nall_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True)\n</pre> %%time all_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True) <pre>CPU times: user 1.12 s, sys: 318 ms, total: 1.43 s\nWall time: 4.5 s\n</pre> In\u00a0[11]: Copied! <pre>all_hurricanes_df.shape\n</pre> all_hurricanes_df.shape Out[11]: <pre>(12362, 17)</pre> <p>There are <code>12,362</code> hurricanes identified by GeoAnalytics aggregate tracks tool. To get an idea about this aggregated dataset, call the <code>head()</code> method.</p> In\u00a0[12]: Copied! <pre>all_hurricanes_df.head()\n</pre> all_hurricanes_df.head() Out[12]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged min_pressure_merged min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration 0 {\"paths\": [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 NaN 1854.0 1 NaN NaN 1854-02-08 06:00:00 1.296000e+08 1 {\"paths\": [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 NaN 1859.0 2 NaN 10.0 1859-08-24 12:00:00 1.728000e+08 2 {\"paths\": [[[-23.19999999999999, 12.1000000000... NA UNNAMED NA 50.0 1853-09-12 18:00:00 NaN 130.0 26.982000 -51.776000 924.0 1853.0 3 53.0 90.0 1853-08-30 00:00:00 1.058400e+09 3 {\"paths\": [[[59.80000000000001, -15.5], [59.49... SI XXXX856017 MM 13.0 1856-04-05 18:00:00 NaN NaN -20.185385 59.573077 NaN 1856.0 4 NaN NaN 1856-04-02 18:00:00 2.592000e+08 4 {\"paths\": [[[99.60000000000002, -11.5], [98.30... SI NOT NAMED WA 13.0 1861-03-15 18:00:00 NaN NaN -12.940769 94.183846 NaN 1861.0 5 NaN NaN 1861-03-12 18:00:00 2.592000e+08 <p>To better analyze this data set, the date columns need to be changed to a format that Pandas understands better. This is accomplished by calling <code>to_datetime()</code> method and passing the appropriate time columns.</p> In\u00a0[14]: Copied! <pre>all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime'])\nall_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime'])\nall_hurricanes_df.index = all_hurricanes_df['start_datetime']\nall_hurricanes_df.head()\n</pre> all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime']) all_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime']) all_hurricanes_df.index = all_hurricanes_df['start_datetime'] all_hurricanes_df.head() Out[14]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged min_pressure_merged min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration start_datetime 1854-02-08 06:00:00 {\"paths\": [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 NaN 1854.0 1 NaN NaN 1854-02-08 06:00:00 1.296000e+08 1859-08-24 12:00:00 {\"paths\": [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 NaN 1859.0 2 NaN 10.0 1859-08-24 12:00:00 1.728000e+08 1853-08-30 00:00:00 {\"paths\": [[[-23.19999999999999, 12.1000000000... NA UNNAMED NA 50.0 1853-09-12 18:00:00 NaN 130.0 26.982000 -51.776000 924.0 1853.0 3 53.0 90.0 1853-08-30 00:00:00 1.058400e+09 1856-04-02 18:00:00 {\"paths\": [[[59.80000000000001, -15.5], [59.49... SI XXXX856017 MM 13.0 1856-04-05 18:00:00 NaN NaN -20.185385 59.573077 NaN 1856.0 4 NaN NaN 1856-04-02 18:00:00 2.592000e+08 1861-03-12 18:00:00 {\"paths\": [[[99.60000000000002, -11.5], [98.30... SI NOT NAMED WA 13.0 1861-03-15 18:00:00 NaN NaN -12.940769 94.183846 NaN 1861.0 5 NaN NaN 1861-03-12 18:00:00 2.592000e+08 <p>The track duration and length columns need to be projected to units (days, hours, miles) that are meaningful for analysis.</p> In\u00a0[15]: Copied! <pre>all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000\nall_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24)\n</pre> all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000 all_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24) In\u00a0[21]: Copied! <pre>map1 = gis.map('USA')\nmap1\n</pre> map1 = gis.map('USA') map1 Out[21]: In\u00a0[17]: Copied! <pre>all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map1, \n                                                             renderer_type='u',\n                                                             col='any_basin',\n                                                            cmap='prism')\n</pre> all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map1,                                                               renderer_type='u',                                                              col='any_basin',                                                             cmap='prism') Out[17]: <pre>True</pre> <p>The map above draws a set of <code>500</code> hurricanes chosen at random. You can visualize the Spatially Enabled DataFrame object with different types of renderers. In the example above a unique value renderer is applied on the basin column. You can switch the map to 3D mode and view the same on a globe.</p> In\u00a0[22]: Copied! <pre>map2 = gis.map()\nmap2.mode= '3D'\nmap2\n</pre> map2 = gis.map() map2.mode= '3D' map2 Out[22]: In\u00a0[27]: Copied! <pre>all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map2, \n                                                             renderer_type='u',\n                                                             col='any_basin',\n                                                            cmap='prism')\n</pre> all_hurricanes_df.sample(n=500, random_state=2).spatial.plot(map2,                                                               renderer_type='u',                                                              col='any_basin',                                                             cmap='prism') In\u00a0[18]: Copied! <pre>ax = sns.distplot(all_hurricanes_df['min_season'], kde=False, bins=50)\nax.set_title('Number of hurricanes recorded over time')\n</pre> ax = sns.distplot(all_hurricanes_df['min_season'], kde=False, bins=50) ax.set_title('Number of hurricanes recorded over time') Out[18]: <pre>Text(0.5,1,'Number of hurricanes recorded over time')</pre> <p>The number of hurricanes recorded increases steadily until <code>1970</code>. This could be due to advances in geospatial technologies allowing scientists to better monitor hurricanes. However, after <code>1970</code> we notice a reduction in the number of hurricanes. This is in line with what scientists observe and predict.</p> In\u00a0[19]: Copied! <pre>fig1, ax1 = plt.subplots(1,2, figsize=(12,5))\n\nbasin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1[0])\nbasin_ax.set_title('Number of hurricanes per basin')\nbasin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',\n                          'Eastern Pacicifc', 'North Indian','Southern Pacific',\n                          'South Atlantic'])\n\nsub_basin_ax = all_hurricanes_df['any_sub_basin'].value_counts().plot(kind='bar', ax=ax1[1])\nsub_basin_ax.set_title('Number of hurricanes per sub basin')\nsub_basin_ax.set_xticklabels(['MM','North Atlantic','Bay of Bengal','Western Australia',\n                             'Eastern Australia', 'Carribean Sea', 'Gulf of Mexico',\n                             'Arabian Sea', 'Central Pacific'])\nsub_basin_ax.tick_params()\n</pre> fig1, ax1 = plt.subplots(1,2, figsize=(12,5))  basin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1[0]) basin_ax.set_title('Number of hurricanes per basin') basin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',                           'Eastern Pacicifc', 'North Indian','Southern Pacific',                           'South Atlantic'])  sub_basin_ax = all_hurricanes_df['any_sub_basin'].value_counts().plot(kind='bar', ax=ax1[1]) sub_basin_ax.set_title('Number of hurricanes per sub basin') sub_basin_ax.set_xticklabels(['MM','North Atlantic','Bay of Bengal','Western Australia',                              'Eastern Australia', 'Carribean Sea', 'Gulf of Mexico',                              'Arabian Sea', 'Central Pacific']) sub_basin_ax.tick_params() <p>Thus, most hurricanes occur in Wester Pacific basin. This is the region that is east of China, Phillipines and rest of South East Asia. This is followed by South Indian which spans from west of Australia to east of Southern Africa. North Atlantic basin which is the source of hurricanes in the continental United States ranks as the third busiest hurricane basin.</p> <p>Pandas provides a handy API called <code>value_counts()</code> to count unique occurrences. We use that below to count the number of times each hurricane name has been used. We then print the top <code>25</code> most frequently used names.</p> In\u00a0[20]: Copied! <pre># Get the number of occurrences of top 25 hurricane names\nall_hurricanes_df['any_name'].value_counts()[:25]\n</pre> # Get the number of occurrences of top 25 hurricane names all_hurricanes_df['any_name'].value_counts()[:25] Out[20]: <pre>NOT NAMED          4099\nUNNAMED            1408\n06B                  31\n05B                  30\n04B                  30\n09B                  30\n07B                  29\n08B                  29\n10B                  29\n03B                  28\n01B                  27\n12B                  26\n11B                  23\n13B                  23\n02B                  22\n14B                  17\nSUBTROP:UNNAMED      16\nIRMA                 15\nFLORENCE             15\n02A                  14\nJUNE                 13\nALICE                13\nOLGA                 13\nSUSAN                13\nFREDA                13\nName: any_name, dtype: int64</pre> <p>Names like <code>FLORENCE</code>, <code>IRMA</code>, <code>OLGA</code>.. appear to be more popular. Interestingly all are of female gender. We can take this further to explore at what time periods have the name <code>FLORENCE</code> been used?</p> In\u00a0[21]: Copied! <pre>all_hurricanes_df[all_hurricanes_df['any_name']=='FLORENCE'].index\n</pre> all_hurricanes_df[all_hurricanes_df['any_name']=='FLORENCE'].index Out[21]: <pre>DatetimeIndex(['1953-09-23 12:00:00', '1954-09-10 12:00:00',\n               '1963-07-14 12:00:00', '1967-01-03 06:00:00',\n               '1964-09-05 18:00:00', '1965-09-08 00:00:00',\n               '1973-07-25 00:00:00', '1960-09-17 06:00:00',\n               '1994-11-02 00:00:00', '1969-09-02 00:00:00',\n               '2012-08-03 06:00:00', '1977-09-20 12:00:00',\n               '2000-09-10 18:00:00', '1988-09-07 06:00:00',\n               '2006-09-03 18:00:00'],\n              dtype='datetime64[ns]', name='start_datetime', freq=None)</pre> <p>The name <code>FLORENCE</code> had been used consistently in since the 1950s, reaching a peak in popularity during the 60s.</p> <p>Hurricanes happen when water temperatures (<code>Sea Surface Temperature</code> SST) are warm. Solar incidence is one of the key factors affecting SST and this typically happens during summer months. However, summer happens during different months in northern and southern hemispheres. To visualize this seasonality, we need to group our data by month as well as basin. Thus the snippet below creates a multilevel index grouper in Pandas</p> In\u00a0[22]: Copied! <pre># Create a grouper object\ngrouper = all_hurricanes_df.start_datetime.dt.month_name()\n\n# use grouper along with basin name to create a multilevel groupby object\nhurr_by_basin = all_hurricanes_df.groupby([grouper,'any_basin'], as_index=True)\nhurr_by_basin_month = hurr_by_basin.count()[['count', 'min_pressure_merged']]\nhurr_by_basin_month.head()\n</pre> # Create a grouper object grouper = all_hurricanes_df.start_datetime.dt.month_name()  # use grouper along with basin name to create a multilevel groupby object hurr_by_basin = all_hurricanes_df.groupby([grouper,'any_basin'], as_index=True) hurr_by_basin_month = hurr_by_basin.count()[['count', 'min_pressure_merged']] hurr_by_basin_month.head() Out[22]: count min_pressure_merged start_datetime any_basin April NA 5 2 NI 41 5 SI 242 85 SP 97 74 WP 83 56 <p>Now we turn the index into columns for further processing.</p> In\u00a0[23]: Copied! <pre># turn index into columns\nhurr_by_basin_month.reset_index(inplace=True)\nhurr_by_basin_month.drop('min_pressure_merged', axis=1, inplace=True)\nhurr_by_basin_month.columns = ['month', 'basin', 'count']\nhurr_by_basin_month.head()\n</pre> # turn index into columns hurr_by_basin_month.reset_index(inplace=True) hurr_by_basin_month.drop('min_pressure_merged', axis=1, inplace=True) hurr_by_basin_month.columns = ['month', 'basin', 'count'] hurr_by_basin_month.head() Out[23]: month basin count 0 April NA 5 1 April NI 41 2 April SI 242 3 April SP 97 4 April WP 83 <p>We add the month column back, but this time we will help Pandas understand how to sort months other than by alphabetical order.</p> In\u00a0[64]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(15,7))\nmonth_order = ['January','February', 'March','April','May','June',\n              'July','August','September','October','November','December']\n\nsns.barplot(x='month', y='count', hue='basin', data=hurr_by_basin_month, ax=ax,\n            order=month_order)\n</pre> fig, ax = plt.subplots(1,1, figsize=(15,7)) month_order = ['January','February', 'March','April','May','June',               'July','August','September','October','November','December']  sns.barplot(x='month', y='count', hue='basin', data=hurr_by_basin_month, ax=ax,             order=month_order) Out[64]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2705d908&gt;</pre> <p>The bars in red represent the number of hurricanes in Sounth Indian ocean which spans from west of Australia to east of southern Africa while the brown bars are for Western Pacific which spans east of China and orange bars for North Atlantic. The sinusoidal nature of these bars show the charateristic offset in summer between northern and southern hemispheres. The green bars represent North Indian hurricanes which is dominated by monsoon effect and is seen to be prevalant for most part of the year.</p> In\u00a0[70]: Copied! <pre>agol = GIS(set_active=False)\n</pre> agol = GIS(set_active=False) In\u00a0[107]: Copied! <pre>world_boundaries_item = agol.content.get('57c1ade4fa7c4e2384e6a23f2b3bd254')\nworld_boundaries_item\n</pre> world_boundaries_item = agol.content.get('57c1ade4fa7c4e2384e6a23f2b3bd254') world_boundaries_item Out[107]: World Continents This layer represents the boundaries for the continents of the world. The layer is suitable for display to a largest scale of 1:15,000,000.Feature Layer Collection by esri                         Last Modified: November 11, 2018                         0 comments, 39,101 views                      <p>We will import the <code>overlay_layers</code> tool from <code>manage_data</code> toolset to perform the overlay analysis.</p> In\u00a0[108]: Copied! <pre>boundary_fl = world_boundaries_item.layers[0]\nfrom arcgis.features.manage_data import overlay_layers\n</pre> boundary_fl = world_boundaries_item.layers[0] from arcgis.features.manage_data import overlay_layers In\u00a0[96]: Copied! <pre>%%time\ninland_tracks = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl, \n                               overlay_type='INTERSECT', output_type='INPUT', \n                               output_name='hurricane_landfall_tracks', gis=gis)\n</pre> %%time inland_tracks = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl,                                 overlay_type='INTERSECT', output_type='INPUT',                                 output_name='hurricane_landfall_tracks', gis=gis) <pre>CPU times: user 1.37 s, sys: 231 ms, total: 1.6 s\nWall time: 22min 13s\n</pre> <p>As part of the intersect operation, the output type is specified as <code>Input</code>. Since the input layer is hurricane tracks (a line layer), the result will continue to be a line layer. We can draw this layer on a map to view those hurricanes that have made a landfall and traveled inland.</p> In\u00a0[24]: Copied! <pre>landfall_tracks_map = gis.map(\"USA\")\nlandfall_tracks_map\n</pre> landfall_tracks_map = gis.map(\"USA\") landfall_tracks_map Out[24]: In\u00a0[98]: Copied! <pre>landfall_tracks_map.add_layer(inland_tracks)\n</pre> landfall_tracks_map.add_layer(inland_tracks) <p>We query the landfall tracks layer into a DataFrame. We will then plot a bar chart showing what percent of hurricanes in each basin make a landfall.</p> In\u00a0[27]: Copied! <pre>fields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged', \n                   'min_pressure_merged', 'track_duration','end_datetime', \n                   'start_datetime', 'analysislength']\n\nlandfall_tracks_fl = inland_tracks.layers[0]\n</pre> fields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged',                     'min_pressure_merged', 'track_duration','end_datetime',                     'start_datetime', 'analysislength']  landfall_tracks_fl = inland_tracks.layers[0] In\u00a0[28]: Copied! <pre>landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df\nlandfall_tracks_df.head(3)\n</pre> landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df landfall_tracks_df.head(3) Out[28]: analysislength any_basin any_name end_datetime max_wind_merged min_pressure_merged min_season objectid start_datetime track_duration SHAPE 0 4.376642 NA NOT NAMED -3663424800000 95.0 965.0 1853.0 1 -3664699200000 1.317600e+09 {'paths': [[[-74.47272727299998, 24], [-74.463... 1 117.097286 NA UNNAMED -3645172800000 70.0 NaN 1854.0 2 -3645475200000 2.160000e+08 {'paths': [[[-99.13749999999999, 26.5699999999... 2 256.909588 NA UNNAMED -3645172800000 70.0 NaN 1854.0 3 -3645475200000 2.160000e+08 {'paths': [[[-102.21739130399999, 27.686956522... In\u00a0[30]: Copied! <pre>fig1, ax1 = plt.subplots(1,1, figsize=(12,5))\n\nbasin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1)\nbasin_ax = landfall_tracks_df['any_basin'].value_counts().plot(kind='bar', \n                                                               ax=ax1, \n                                                               cmap='viridis', \n                                                               alpha=0.5)\nbasin_ax.set_title('Number of hurricanes per basin that make landfall')\nbasin_ax.tick_params(axis='x', labelrotation=65)\nbasin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',\n                          'Eastern Pacicifc', 'North Indian','Southern Pacific',\n                          'South Atlantic'])\nbasin_ax.tick_params()\n</pre> fig1, ax1 = plt.subplots(1,1, figsize=(12,5))  basin_ax = all_hurricanes_df['any_basin'].value_counts().plot(kind='bar', ax=ax1) basin_ax = landfall_tracks_df['any_basin'].value_counts().plot(kind='bar',                                                                 ax=ax1,                                                                 cmap='viridis',                                                                 alpha=0.5) basin_ax.set_title('Number of hurricanes per basin that make landfall') basin_ax.tick_params(axis='x', labelrotation=65) basin_ax.set_xticklabels(['Western Pacific', 'South Indian', 'North Atlantic',                           'Eastern Pacicifc', 'North Indian','Southern Pacific',                           'South Atlantic']) basin_ax.tick_params() <p>The bar chart above plots the number of hurricanes (per basin) that made landfall over another bar chart of the total number of hurricanes per basin. From the chart, most hurricanes in the Western Pacific, South Indian, North Atlantic make landfall. Hurricanes in Southern Pacific on the otherhand rarely make landfall. This helps us guage the severity of hurricanes in different geographic basins.</p> In\u00a0[43]: Copied! <pre>landfall_tracks_df['analysislength'].plot(kind='hist', bins=100,\n                                          title='Histogram of distance traveled inland',\n                                         figsize=(12,7), xlim=[-100,2500])\nplt.xlabel('Distance in miles')\n</pre> landfall_tracks_df['analysislength'].plot(kind='hist', bins=100,                                           title='Histogram of distance traveled inland',                                          figsize=(12,7), xlim=[-100,2500]) plt.xlabel('Distance in miles') Out[43]: <pre>Text(0.5,0,'Distance in miles')</pre> <p>Thus, majority of hurricanes travel less than <code>500</code> miles after making a landfall. We can query which are the top <code>50</code> hurricanes that have traveled longest inland.</p> In\u00a0[54]: Copied! <pre># filter the top 50 longest hurricanes (distance traveled inland)\ntop_50_longest = landfall_tracks_df.sort_values(by=['analysislength'], axis=0, ascending=False).head(50)\ntop_50_longest['any_basin'].value_counts().plot(kind='bar')\n</pre> # filter the top 50 longest hurricanes (distance traveled inland) top_50_longest = landfall_tracks_df.sort_values(by=['analysislength'], axis=0, ascending=False).head(50) top_50_longest['any_basin'].value_counts().plot(kind='bar') Out[54]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2c808ef0&gt;</pre> <p>Southern Pacific basin, followed by South Indian basin contains the hurricanes that have traveled longest inland.</p> In\u00a0[59]: Copied! <pre>inland_map = gis.map()\ninland_map\n</pre> inland_map = gis.map() inland_map Out[59]: In\u00a0[58]: Copied! <pre>top_50_longest.spatial.plot(inland_map, renderer_type='u', col='any_basin',cmap='prism')\n</pre> top_50_longest.spatial.plot(inland_map, renderer_type='u', col='any_basin',cmap='prism') Out[58]: <pre>True</pre> <p>Plotting this on the map, we notice hurricanes have traveled longest inland over the east coast of North America, China and Australia. Interestingly, Australia bears landfall of hurricanes from both South Indian and Western Pacific basins.</p> In\u00a0[109]: Copied! <pre>%%time\nlandfall_item = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl, \n                               overlay_type='INTERSECT', output_type='POINT', \n                               output_name='hurricane_landfall_locations', gis=gis)\n</pre> %%time landfall_item = overlay_layers(input_layer=hurricane_fl, overlay_layer = boundary_fl,                                 overlay_type='INTERSECT', output_type='POINT',                                 output_name='hurricane_landfall_locations', gis=gis) <pre>CPU times: user 1.43 s, sys: 258 ms, total: 1.69 s\nWall time: 24min 41s\n</pre> In\u00a0[110]: Copied! <pre>landfall_points_fl = landfall_item.layers[0]\n</pre> landfall_points_fl = landfall_item.layers[0] In\u00a0[25]: Copied! <pre>landfall_map = gis.map()\nlandfall_map\n</pre> landfall_map = gis.map() landfall_map Out[25]: In\u00a0[50]: Copied! <pre>landfall_map.add_layer(landfall_item)\n</pre> landfall_map.add_layer(landfall_item) In\u00a0[112]: Copied! <pre>%%time\nfrom arcgis.features.analyze_patterns import calculate_density\nlandfall_density_item = calculate_density(input_layer=landfall_points_fl, radius=30,\n                                          radius_units='Miles', area_units='SquareMiles',\n                                          classification_type='NaturalBreaks', num_classes=7,\n                                          output_name='landfall_density', gis=gis)\n</pre> %%time from arcgis.features.analyze_patterns import calculate_density landfall_density_item = calculate_density(input_layer=landfall_points_fl, radius=30,                                           radius_units='Miles', area_units='SquareMiles',                                           classification_type='NaturalBreaks', num_classes=7,                                           output_name='landfall_density', gis=gis) <pre>CPU times: user 80.9 ms, sys: 15.3 ms, total: 96.2 ms\nWall time: 42.7 s\n</pre> In\u00a0[129]: Copied! <pre>landfall_map = gis.map('Florida, USA')\nlandfall_map\n</pre> landfall_map = gis.map('Florida, USA') landfall_map Out[129]: In\u00a0[128]: Copied! <pre>landfall_map.add_layer(landfall_density_item)\n</pre> landfall_map.add_layer(landfall_density_item) <p>The map here computes the kernel density of landfall locations. It does this by summing the number of landfalls within a radius of <code>30</code> miles and dividing it by the area of this radius. Thus it spreads the number of landfalls over a smooth surface, then classifies this surface into <code>7</code> classes. By performing density anlaysis, we are able to wade through large clouds of landfall points and identify locations that have more landfalls compared to the rest of the world.</p> <p>Let us visualize the density analysis results on a table and as a chart.</p> In\u00a0[130]: Copied! <pre>landfall_density_sdf = landfall_density_item.layers[0].query(as_df=True)\nlandfall_density_sdf.head()\n</pre> landfall_density_sdf = landfall_density_item.layers[0].query(as_df=True) landfall_density_sdf.head() Out[130]: SHAPE analysisarea class objectid value_max_per_squaremile value_min_per_squaremile 0 {\"rings\": [[[53.845677534778474, 81.0534260146... 920.649527 2 1 0.010679 0.00267 1 {\"rings\": [[[24.78192557105382, 79.98490572185... 1896.053559 2 2 0.010679 0.00267 2 {\"rings\": [[[15.16524293599764, 79.45064557546... 162.088581 2 3 0.010679 0.00267 3 {\"rings\": [[[12.600794233316094, 65.2393256814... 183.094538 2 4 0.010679 0.00267 4 {\"rings\": [[[-13.79165699844873, 64.5982135057... 1672.631881 2 5 0.010679 0.00267 In\u00a0[133]: Copied! <pre>ax = landfall_density_sdf['class'].hist()\nax.set_title('Histogram of hurricane landfall densities')\n</pre> ax = landfall_density_sdf['class'].hist() ax.set_title('Histogram of hurricane landfall densities') Out[133]: <pre>Text(0.5,1,'Histogram of hurricane landfall densities')</pre> <p>From the histogram above, we notice there are only a very few places that can be classified as having a high density of hurricane landfalls. Let us analyze these places a bit further.</p> In\u00a0[134]: Copied! <pre>high_density_landfalls = landfall_density_sdf[(landfall_density_sdf['class']==6) | \n                                              (landfall_density_sdf['class']==7)]\n\nhigh_density_landfalls.shape\n</pre> high_density_landfalls = landfall_density_sdf[(landfall_density_sdf['class']==6) |                                                (landfall_density_sdf['class']==7)]  high_density_landfalls.shape Out[134]: <pre>(40, 6)</pre> <p>We have identified <code>40</code> sites worldwide that have a high density of hurricane landfalls based on the anlaysis of data spanning the last <code>169</code> years. Below, we plot them on a map.</p> In\u00a0[146]: Copied! <pre>high_density_landfall_map1 = gis.map('North Carolina')\nhigh_density_landfall_map2 = gis.map('China')\ndisplay(high_density_landfall_map1)\ndisplay(high_density_landfall_map2)\n</pre> high_density_landfall_map1 = gis.map('North Carolina') high_density_landfall_map2 = gis.map('China') display(high_density_landfall_map1) display(high_density_landfall_map2) Out[146]: In\u00a0[145]: Copied! <pre>high_density_landfalls.spatial.plot(map_widget=high_density_landfall_map1, line_width=0, outline_color=0)\nhigh_density_landfalls.spatial.plot(map_widget=high_density_landfall_map2, line_width=0, outline_color=0)\n</pre> high_density_landfalls.spatial.plot(map_widget=high_density_landfall_map1, line_width=0, outline_color=0) high_density_landfalls.spatial.plot(map_widget=high_density_landfall_map2, line_width=0, outline_color=0) Out[145]: <pre>True</pre> <p>The places that turn up are not much of a surprise. We notice the coast of Carolinas in the United States, the states of West Bengal, Orissa in India, several places along the East coast of China, southern tip of Japan and most of the island of Philippines are the places that are most affected on a repeat basis.</p> In\u00a0[210]: Copied! <pre>landfalls_enriched = enrich(high_density_landfalls, data_collections='keyGlobalFacts')\nlandfalls_enriched.head()\n</pre> landfalls_enriched = enrich(high_density_landfalls, data_collections='keyGlobalFacts') landfalls_enriched.head() Out[210]: AVGHHSZ HasData ID OBJECTID_0 SHAPE TOTFEMALES TOTHH TOTMALES TOTPOP aggregationMethod analysisarea apportionmentConfidence class objectid populationToPolygonSizeRating sourceCountry value_max_per_squaremile value_min_per_squaremile 0 2.49 1 0 1 {\"rings\": [[[-75.76583397992073, 36.0687216884... 3514 2815 3500 7014 BlockApportionment:US.BlockGroups 118.233701 2.576 7 170 2.191 US 0.226918 0.122803 1 2.44 1 1 2 {\"rings\": [[[-76.19324209703427, 36.3892777762... 23548 19172 23631 47179 BlockApportionment:US.BlockGroups 1165.144645 2.576 6 174 2.191 US 0.122803 0.067631 2 2.21 1 2 3 {\"rings\": [[[-76.40694615559107, 34.8933493663... 6210 5485 6521 12731 BlockApportionment:US.BlockGroups 731.245665 2.576 6 194 2.191 US 0.122803 0.067631 3 2.70 1 3 4 {\"rings\": [[[130.2448784688346, 32.54260472224... 11118 7816 9969 21087 BlockApportionment:JP.Blocks 106.236904 2.090 6 275 1.695 JP 0.122803 0.067631 4 2.39 1 4 5 {\"rings\": [[[-80.57417529744856, 32.2220486344... 27843 21425 28081 55924 BlockApportionment:US.BlockGroups 255.801403 2.576 6 280 2.191 US 0.122803 0.067631 <p>The <code>enrich()</code> operation accepts the Spatially Enabled <code>DataFrame</code>, performs spatial aggregation and returns another <code>DataFrame</code> with socio-economic and demographic columns added to it. The <code>data_collections</code> parameter decides which additional columns get added.</p> <p>Let us visualize the population that is affected by country. For this, we group by the <code>sourceCountry</code> column and sum up the results. The cell below plots the total number of men, women and households that live within the high density polygons.</p> In\u00a0[216]: Copied! <pre>fig, ax = plt.subplots(1,2, figsize=(15,5))\n# plot bar chart 1\ngrouper1 = landfalls_enriched[['TOTFEMALES','TOTMALES','sourceCountry']].groupby(by='sourceCountry')\ngrouper1.sum().plot(kind='bar', stacked=True, ax=ax[0], \n                    title='Population living within high density areas')\n\n# plot bar chart 2\ngrouper2 = landfalls_enriched[['TOTHH','sourceCountry']].groupby(by='sourceCountry')\ngrouper2.sum().plot(kind='bar', ax=ax[1], \n                    title='Total number of households within high density areas')\n\nplt.tight_layout()\n</pre> fig, ax = plt.subplots(1,2, figsize=(15,5)) # plot bar chart 1 grouper1 = landfalls_enriched[['TOTFEMALES','TOTMALES','sourceCountry']].groupby(by='sourceCountry') grouper1.sum().plot(kind='bar', stacked=True, ax=ax[0],                      title='Population living within high density areas')  # plot bar chart 2 grouper2 = landfalls_enriched[['TOTHH','sourceCountry']].groupby(by='sourceCountry') grouper2.sum().plot(kind='bar', ax=ax[1],                      title='Total number of households within high density areas')  plt.tight_layout() <p>Hurricanes make landfalls on coasts, which invariably are places of dense population. By geo enriching the density maps, we are able to determine that the coasts of China (7 million), Hongkong (7M), India (6M) and Philippines (3M) are at high risk as they suffer repeat landfalls and also support large populations.</p> <p>This information can be used by city planners to better zone the coasts and avoid development along hurricane prone areas.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#analyzing-hurricane-tracks-part-23","title":"Analyzing hurricane tracks - Part 2/3\u00b6","text":"<p>This is the second part to a three part set of notebooks that process and analyze historic hurricane tracks. In the previous notebook we saw</p> <ol> <li>downloading historic hurricane datasets using Python</li> <li>cleaning and merging hurricane observations using DASK</li> <li>aggregating point observations into hurricane tracks using ArcGIS GeoAnalytics server</li> </ol> <p>In this notebook you will analyze the aggregated tracks to answer important questions about prevalance of hurricanes, their seasonality, their density, places where they make landfall and investigate the communities that are most affected.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#access-aggregated-hurricane-data","title":"Access aggregated hurricane data\u00b6","text":"<p>Below, we access the tracks aggregated using GeoAnalytics in the previous notebook.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#query-hurricane-tracks-into-a-spatially-enabled-dataframe","title":"Query hurricane tracks into a Spatially enabled <code>DataFrame</code>\u00b6","text":""},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#exploratory-data-analysis","title":"Exploratory data analysis\u00b6","text":"<p>In this section we perform exploratory analysis of the dataset and answer some interesting questions.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#does-the-number-of-hurricanes-increase-with-time","title":"Does the number of hurricanes increase with time?\u00b6","text":"<p>To understand if number of hurricanes have increased over time, we will plot a histogram of the <code>MIN_Season</code> column.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#how-many-hurricanes-occuer-per-basin-and-sub-basin","title":"How many hurricanes occuer per basin and sub basin?\u00b6","text":"<p>Climate scientists have organized global hurricanes into <code>7</code> basins and a number of sub basins. The snippet below plots groups the data by basin and sub basin, counts the occurrences and plots the frequency in bar charts.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#are-certain-hurricane-names-more-popular","title":"Are certain hurricane names more popular?\u00b6","text":""},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#is-there-a-seasonality-in-the-occurrence-of-hurricanes","title":"Is there a seasonality in the occurrence of hurricanes?\u00b6","text":""},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#what-percent-of-hurricanes-make-landfall","title":"What percent of hurricanes make landfall?\u00b6","text":"<p>While exploring the hurricane data on maps, we noticed their geographic distribution and that they travel long distances over the oceans. Thus, do all hurricanes eventually make landfall? If not, what percent of them do? This is an important question to answer as the threat to human life decreases dramatically when a hurricane does not make a landfall.</p> <p>We will answer this question by performing overlay analysis. For this, we need to intersect the hurricane tracks with world boundary dataset. We will make an anonymous connection to ArcGIS Online to look for a layer published by Esri in living atlas.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#how-far-do-hurricanes-travel-inland-after-landfall","title":"How far do hurricanes travel inland after landfall?\u00b6","text":"<p>Hurricanes in general lose velocity and intensity after they make a landfall. Thus they can only travel a short distance inland. As a result of the overlay analysis, an <code>analysislength</code> column is created. We can plot the histogram of that column to understand how far hurricanes have traveled inland after landfall.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#where-do-hurricanes-make-landfall","title":"Where do hurricanes make landfall?\u00b6","text":"<p>Of equal interest is finding where hurricanes make landfall. From experience we know certain regions are prone to hurricane damage more than the rest. Using spatial data science, we can empirically derive those regions that have statistically more hurricane landfalls compared to the rest.</p> <p>For this, we will repeat the overlay analysis, however this time, we will change the <code>output_type</code> to <code>POINT</code>. The tool will return the points along the coast lines where hurricanes make landfall.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#perform-density-anlaysis-on-hurricane-landfall-locations","title":"Perform density anlaysis on hurricane landfall locations\u00b6","text":"<p>The map above shows hundreds of thousands of points spread around the world. Do all these places have equal probability of being hit by a hurricane? To answer this, we will perform density analysis. The <code>calculate_density</code> tool available under the <code>analyze_patterns</code> toolset is used for this.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#what-is-the-demographics-of-places-with-highest-density-of-landfalls","title":"What is the demographics of places with highest density of landfalls?\u00b6","text":"<p>Now that we have found the places that have the highest desnity of landfalls, we are faced with the question, who lives there? What is the impact of repeat natural calamities on these places? We can answer those questions by geoenriching those polygons with demographic and socio-economic attributes.</p>      Note: Geoenrichment costs ArcGIS Online credits and requires this functionality to be configured in your Portal for ArcGIS.  <p>Below we use the <code>enrich()</code> function to add socio-economic columns to the landfall density <code>DataFrame</code>. For a data collection, we pick <code>keyGlobalFacts</code> which is available for most part of the world.</p>"},{"location":"projects/dist-computing/dask/part2_explore_hurricane_tracks/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook we accessed the aggregated hurricane track data from Part 1 as Spatially Enabled <code>DataFrame</code> objects. We visualized these DataFrames both spatially and as charts to reveal interesting information about the global hurricane dataset.</p> <p>We learnt that global hurricanes are classified into <code>7</code> basins, with the most hurricanes occurring over Western Pacific. The North Atlantic basin which affects the continental United States ranks as third busiest basin. The number of hurricanes recorded worldwide has been steadily climbing as technology improves. However, after <code>1970</code>s, we notice a year-over-year reduction in the number of hurricanes. We analyze more of this phenomena in the next part of this study.</p> <p>As for hurricane names, the top spot is a tie between Irma and Florence, with <code>15</code> occurrences for each so far. By turning this DataFrame into a timeseries, we were able to observe a sinusoidal seasonality. The peaks between hurricanes in northern and cyclones in southern hemisphere were offest by about <code>6</code> months, matching the time when summer occurs in these hemispheres. We also noticed that hurricanes over the North Indian basin occur throughout the year as they are influenced by a monsoon phenomena.</p> <p>We then performed overlay analysis to understand where hurricanes make landfall, the path they take once they make landfall. We noticed that the majority of hurricanes make landfall. Once they make a landfall, the majority travel under <code>100</code> miles inland.</p> <p>We extended the overlay analysis to calculate the exact points where landfall occurs. After performing a density analysis on the landfall locations we found places along the coasts of Carolinas in the United States, Orissa, West Bengal in India, several places along the east coast of China, southern tip of Japan and most of Phillippines are affected from a repeat landfall / historical perspective.</p> <p>By geo-enriching the high density places, we were able to understand the number of people that live in these places. China tops the list with most people affected.</p> <p>In the next notebook, we extend this analysis to answer the important question: Does the intensity of hurricanes increase over time?</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/","title":"Analyzing hurricane tracks - Part 3/3","text":"<p>Import the libraries necessary for this notebook.</p> In\u00a0[1]: Copied! <pre># import ArcGIS Libraries\nfrom arcgis.gis import GIS\nfrom arcgis.geometry import filters\nfrom arcgis.geocoding import geocode\nfrom arcgis.features.manage_data import overlay_layers\nfrom arcgis.geoenrichment import enrich\n\n# import Pandas for data exploration\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# import plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# import display tools\nfrom pprint import pprint\nfrom IPython.display import display\n\n# import system libs\nfrom sys import getsizeof\n</pre> # import ArcGIS Libraries from arcgis.gis import GIS from arcgis.geometry import filters from arcgis.geocoding import geocode from arcgis.features.manage_data import overlay_layers from arcgis.geoenrichment import enrich  # import Pandas for data exploration import pandas as pd import numpy as np from scipy import stats  # import plotting libraries import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline  # import display tools from pprint import pprint from IPython.display import display  # import system libs from sys import getsizeof In\u00a0[3]: Copied! <pre>gis = GIS('home')\n</pre> gis = GIS('home') In\u00a0[2]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n</pre> import warnings warnings.filterwarnings('ignore') In\u00a0[4]: Copied! <pre>hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga')[0]\nhurricane_fl = hurricane_tracks_item.layers[0]\n</pre> hurricane_tracks_item = gis.content.search('title:hurricane_tracks_aggregated_ga')[0] hurricane_fl = hurricane_tracks_item.layers[0] <p>The GeoAnalytics step calculated summary statistics of all numeric fields. However only a few of the columns are of interest to us.</p> In\u00a0[5]: Copied! <pre>pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80)\n</pre> pprint([f['name'] for f in hurricane_fl.properties.fields], compact=True, width=80) <pre>['objectid', 'serial_num', 'count', 'count_col_1', 'sum_col_1', 'min_col_1',\n 'max_col_1', 'mean_col_1', 'range_col_1', 'sd_col_1', 'var_col_1',\n 'count_season', 'sum_season', 'min_season', 'max_season', 'mean_season',\n 'range_season', 'sd_season', 'var_season', 'count_num', 'sum_num', 'min_num',\n 'max_num', 'mean_num', 'range_num', 'sd_num', 'var_num', 'count_basin',\n 'any_basin', 'count_sub_basin', 'any_sub_basin', 'count_name', 'any_name',\n 'count_iso_time', 'any_iso_time', 'count_nature', 'any_nature', 'count_center',\n 'any_center', 'count_track_type', 'any_track_type', 'count_current_basin',\n 'any_current_basin', 'count_latitude_merged', 'sum_latitude_merged',\n 'min_latitude_merged', 'max_latitude_merged', 'mean_latitude_merged',\n 'range_latitude_merged', 'sd_latitude_merged', 'var_latitude_merged',\n 'count_longitude_merged', 'sum_longitude_merged', 'min_longitude_merged',\n 'max_longitude_merged', 'mean_longitude_merged', 'range_longitude_merged',\n 'sd_longitude_merged', 'var_longitude_merged', 'count_wind_merged',\n 'sum_wind_merged', 'min_wind_merged', 'max_wind_merged', 'mean_wind_merged',\n 'range_wind_merged', 'sd_wind_merged', 'var_wind_merged',\n 'count_pressure_merged', 'sum_pressure_merged', 'min_pressure_merged',\n 'max_pressure_merged', 'mean_pressure_merged', 'range_pressure_merged',\n 'sd_pressure_merged', 'var_pressure_merged', 'count_grade_merged',\n 'sum_grade_merged', 'min_grade_merged', 'max_grade_merged',\n 'mean_grade_merged', 'range_grade_merged', 'sd_grade_merged',\n 'var_grade_merged', 'count_eye_dia_merged', 'sum_eye_dia_merged',\n 'min_eye_dia_merged', 'max_eye_dia_merged', 'mean_eye_dia_merged',\n 'range_eye_dia_merged', 'sd_eye_dia_merged', 'var_eye_dia_merged',\n 'track_duration', 'end_datetime', 'start_datetime']\n</pre> <p>Below we select the following fields for the rest of this analysis.</p> In\u00a0[9]: Copied! <pre>fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',\n                   'any_name', 'mean_latitude_merged', 'mean_longitude_merged',\n                   'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',\n                   'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',\n                   'end_datetime', 'start_datetime']\n</pre> fields_to_query = ['objectid', 'count', 'min_season', 'any_basin', 'any_sub_basin',                    'any_name', 'mean_latitude_merged', 'mean_longitude_merged',                    'max_wind_merged', 'range_wind_merged', 'min_pressure_merged',                    'range_pressure_merged', 'max_eye_dia_merged', 'track_duration',                    'end_datetime', 'start_datetime'] In\u00a0[10]: Copied! <pre>%%time\nall_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True)\n</pre> %%time all_hurricanes_df = hurricane_fl.query(out_fields=','.join(fields_to_query), as_df=True) <pre>CPU times: user 1.12 s, sys: 318 ms, total: 1.43 s\nWall time: 4.5 s\n</pre> In\u00a0[11]: Copied! <pre>all_hurricanes_df.shape\n</pre> all_hurricanes_df.shape Out[11]: <pre>(12362, 17)</pre> <p>There are <code>12,362</code> hurricanes identified by GeoAnalytics aggregate tracks tool. To get an idea about this aggregated dataset, call the <code>head()</code> method.</p> In\u00a0[12]: Copied! <pre>all_hurricanes_df.head()\n</pre> all_hurricanes_df.head() Out[12]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged min_pressure_merged min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration 0 {\"paths\": [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 NaN 1854.0 1 NaN NaN 1854-02-08 06:00:00 1.296000e+08 1 {\"paths\": [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 NaN 1859.0 2 NaN 10.0 1859-08-24 12:00:00 1.728000e+08 2 {\"paths\": [[[-23.19999999999999, 12.1000000000... NA UNNAMED NA 50.0 1853-09-12 18:00:00 NaN 130.0 26.982000 -51.776000 924.0 1853.0 3 53.0 90.0 1853-08-30 00:00:00 1.058400e+09 3 {\"paths\": [[[59.80000000000001, -15.5], [59.49... SI XXXX856017 MM 13.0 1856-04-05 18:00:00 NaN NaN -20.185385 59.573077 NaN 1856.0 4 NaN NaN 1856-04-02 18:00:00 2.592000e+08 4 {\"paths\": [[[99.60000000000002, -11.5], [98.30... SI NOT NAMED WA 13.0 1861-03-15 18:00:00 NaN NaN -12.940769 94.183846 NaN 1861.0 5 NaN NaN 1861-03-12 18:00:00 2.592000e+08 <p>To better analyze this data set, the date columns need to be changed to a format that Pandas understands better. This is accomplished by calling <code>to_datetime()</code> method and passing the appropriate time columns.</p> In\u00a0[14]: Copied! <pre>all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime'])\nall_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime'])\nall_hurricanes_df.index = all_hurricanes_df['start_datetime']\nall_hurricanes_df.head()\n</pre> all_hurricanes_df['start_datetime'] = pd.to_datetime(all_hurricanes_df['start_datetime']) all_hurricanes_df['end_datetime'] = pd.to_datetime(all_hurricanes_df['end_datetime']) all_hurricanes_df.index = all_hurricanes_df['start_datetime'] all_hurricanes_df.head() Out[14]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged min_pressure_merged min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration start_datetime 1854-02-08 06:00:00 {\"paths\": [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 NaN 1854.0 1 NaN NaN 1854-02-08 06:00:00 1.296000e+08 1859-08-24 12:00:00 {\"paths\": [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 NaN 1859.0 2 NaN 10.0 1859-08-24 12:00:00 1.728000e+08 1853-08-30 00:00:00 {\"paths\": [[[-23.19999999999999, 12.1000000000... NA UNNAMED NA 50.0 1853-09-12 18:00:00 NaN 130.0 26.982000 -51.776000 924.0 1853.0 3 53.0 90.0 1853-08-30 00:00:00 1.058400e+09 1856-04-02 18:00:00 {\"paths\": [[[59.80000000000001, -15.5], [59.49... SI XXXX856017 MM 13.0 1856-04-05 18:00:00 NaN NaN -20.185385 59.573077 NaN 1856.0 4 NaN NaN 1856-04-02 18:00:00 2.592000e+08 1861-03-12 18:00:00 {\"paths\": [[[99.60000000000002, -11.5], [98.30... SI NOT NAMED WA 13.0 1861-03-15 18:00:00 NaN NaN -12.940769 94.183846 NaN 1861.0 5 NaN NaN 1861-03-12 18:00:00 2.592000e+08 <p>The track duration and length columns need to be projected to units (days, hours, miles) that are meaningful for analysis.</p> In\u00a0[15]: Copied! <pre>all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000\nall_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24)\n</pre> all_hurricanes_df['track_duration_hrs'] = all_hurricanes_df['track_duration'] / 3600000 all_hurricanes_df['track_duration_days'] = all_hurricanes_df['track_duration'] / (3600000*24) In\u00a0[27]: Copied! <pre>inland_tracks = gis.content.search('hurricane_landfall_tracks')[0]\n\nfields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged', \n                   'min_pressure_merged', 'track_duration','end_datetime', \n                   'start_datetime', 'analysislength']\n\nlandfall_tracks_fl = inland_tracks.layers[0]\n</pre> inland_tracks = gis.content.search('hurricane_landfall_tracks')[0]  fields_to_query = ['min_season', 'any_basin','any_name', 'max_wind_merged',                     'min_pressure_merged', 'track_duration','end_datetime',                     'start_datetime', 'analysislength']  landfall_tracks_fl = inland_tracks.layers[0] In\u00a0[28]: Copied! <pre>landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df\nlandfall_tracks_df.head(3)\n</pre> landfall_tracks_df = landfall_tracks_fl.query(out_fields=fields_to_query).df landfall_tracks_df.head(3) Out[28]: analysislength any_basin any_name end_datetime max_wind_merged min_pressure_merged min_season objectid start_datetime track_duration SHAPE 0 4.376642 NA NOT NAMED -3663424800000 95.0 965.0 1853.0 1 -3664699200000 1.317600e+09 {'paths': [[[-74.47272727299998, 24], [-74.463... 1 117.097286 NA UNNAMED -3645172800000 70.0 NaN 1854.0 2 -3645475200000 2.160000e+08 {'paths': [[[-99.13749999999999, 26.5699999999... 2 256.909588 NA UNNAMED -3645172800000 70.0 NaN 1854.0 3 -3645475200000 2.160000e+08 {'paths': [[[-102.21739130399999, 27.686956522... In\u00a0[122]: Copied! <pre>all_hurricanes_df.shape\n</pre> all_hurricanes_df.shape Out[122]: <pre>(12362, 19)</pre> In\u00a0[140]: Copied! <pre>missing_data_viz = all_hurricanes_df.replace(0,np.NaN)\nmissing_data_viz = missing_data_viz.replace(-9999.0,np.NaN)\nmissing_data_viz['min_pressure_merged'] = missing_data_viz['min_pressure_merged'].replace(100.0,np.NaN)\n\nplt.figure(figsize=(10,10))\nmissing_data_ax = sns.heatmap(missing_data_viz[['max_wind_merged', 'min_pressure_merged',\n                                                'max_eye_dia_merged', 'track_duration']].isnull(),\n                              cbar=False, cmap='viridis')\nmissing_data_ax.set_ylabel('Years')\nmissing_data_ax.set_title('Missing values (yellow) visualized as a heatmap')\n</pre> missing_data_viz = all_hurricanes_df.replace(0,np.NaN) missing_data_viz = missing_data_viz.replace(-9999.0,np.NaN) missing_data_viz['min_pressure_merged'] = missing_data_viz['min_pressure_merged'].replace(100.0,np.NaN)  plt.figure(figsize=(10,10)) missing_data_ax = sns.heatmap(missing_data_viz[['max_wind_merged', 'min_pressure_merged',                                                 'max_eye_dia_merged', 'track_duration']].isnull(),                               cbar=False, cmap='viridis') missing_data_ax.set_ylabel('Years') missing_data_ax.set_title('Missing values (yellow) visualized as a heatmap') Out[140]: <pre>Text(0.5,1,'Missing values (yellow) visualized as a heatmap')</pre> <p>All three observation columns - wind speed, atmospheric pressure and eye diameter, suffer from missing values. In general as technology improved over time, we were able to collect better data with fewer missing observations. In the sections below we attempt to fill these values using different techniques. We will compare how they fare and pick one of them for rest of the analysis.</p> <p>Technique 1: Drop missing values: An easy way to deal with missing values is to drop those record from analysis. If we were to do that, we lose over a third of the hurricanes.</p> In\u00a0[141]: Copied! <pre>hurricanes_nona = missing_data_viz.dropna(subset=['max_wind_merged','min_pressure_merged'])\nhurricanes_nona.shape\n</pre> hurricanes_nona = missing_data_viz.dropna(subset=['max_wind_merged','min_pressure_merged']) hurricanes_nona.shape Out[141]: <pre>(5857, 19)</pre> <p>Technique 2: Fill using median value: A common technique is to fill using median value (or a different measure of centrality). This technique computes the median of the entire column and applies that to all the missing values.</p> In\u00a0[142]: Copied! <pre>fill_values = {'max_wind_merged': missing_data_viz['max_wind_merged'].median(),\n                'min_pressure_merged': missing_data_viz['min_pressure_merged'].median(),\n              'track_duration_hrs': missing_data_viz['track_duration_hrs'].median()}\nhurricanes_fillna = missing_data_viz.fillna(value=fill_values)\n</pre> fill_values = {'max_wind_merged': missing_data_viz['max_wind_merged'].median(),                 'min_pressure_merged': missing_data_viz['min_pressure_merged'].median(),               'track_duration_hrs': missing_data_viz['track_duration_hrs'].median()} hurricanes_fillna = missing_data_viz.fillna(value=fill_values) <p>Technique 3: Fill by interpolating between existing values: A sophisticated approach is to interploate a missing value based on two of its closest observations.</p> In\u00a0[143]: Copied! <pre>hurricanes_ipl = missing_data_viz\nhurricanes_ipl['max_wind_merged'] = hurricanes_ipl['max_wind_merged'].interpolate()\nhurricanes_ipl['min_pressure_merged'] = hurricanes_ipl['min_pressure_merged'].interpolate()\nhurricanes_ipl['track_duration_hrs'] = hurricanes_ipl['track_duration_hrs'].interpolate()\n</pre> hurricanes_ipl = missing_data_viz hurricanes_ipl['max_wind_merged'] = hurricanes_ipl['max_wind_merged'].interpolate() hurricanes_ipl['min_pressure_merged'] = hurricanes_ipl['min_pressure_merged'].interpolate() hurricanes_ipl['track_duration_hrs'] = hurricanes_ipl['track_duration_hrs'].interpolate() <p>Visualize all 3 techniques</p> <p>To compare how each of these techniques fared, we will plot the histogram of wind speed column after managing for missing values.</p> In\u00a0[144]: Copied! <pre>fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5))\nfig.suptitle('Comparing effects of missing value imputations on Wind speed column', \n             fontsize=15)\n\nhurricanes_nona['max_wind_merged'].plot(kind='hist', ax=ax[0], bins=35, title='Drop null values')\nhurricanes_fillna['max_wind_merged'].plot(kind='hist', ax=ax[1], bins=35, title='Impute with median')\nhurricanes_ipl['max_wind_merged'].plot(kind='hist', ax=ax[2], bins=35, title='Impute via interpolation')\nfor a in ax:\n    a.set_xlabel('Wind Speed')\n</pre> fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5)) fig.suptitle('Comparing effects of missing value imputations on Wind speed column',               fontsize=15)  hurricanes_nona['max_wind_merged'].plot(kind='hist', ax=ax[0], bins=35, title='Drop null values') hurricanes_fillna['max_wind_merged'].plot(kind='hist', ax=ax[1], bins=35, title='Impute with median') hurricanes_ipl['max_wind_merged'].plot(kind='hist', ax=ax[2], bins=35, title='Impute via interpolation') for a in ax:     a.set_xlabel('Wind Speed') <p>Next, we will plot the histogram of atmospheric pressure column after managing for missing values.</p> In\u00a0[145]: Copied! <pre>fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5))\nfig.suptitle('Comparing effects of missing value imputations on Pressure column', \n             fontsize=15)\n\nhurricanes_nona['min_pressure_merged'].plot(kind='hist', ax=ax[0], title='Drop null values')\nhurricanes_fillna['min_pressure_merged'].plot(kind='hist', ax=ax[1], title='Impute with median')\nhurricanes_ipl['min_pressure_merged'].plot(kind='hist', ax=ax[2], title='Impute via interpolation')\nfor a in ax:\n    a.set_xlabel('Atmospheric Pressure')\n</pre> fig, ax = plt.subplots(1,3, sharex=True, figsize=(15,5)) fig.suptitle('Comparing effects of missing value imputations on Pressure column',               fontsize=15)  hurricanes_nona['min_pressure_merged'].plot(kind='hist', ax=ax[0], title='Drop null values') hurricanes_fillna['min_pressure_merged'].plot(kind='hist', ax=ax[1], title='Impute with median') hurricanes_ipl['min_pressure_merged'].plot(kind='hist', ax=ax[2], title='Impute via interpolation') for a in ax:     a.set_xlabel('Atmospheric Pressure') <p>Fill using interpolation preserves shape of the original distribution. So it will be used for further anlaysis.</p> In\u00a0[363]: Copied! <pre>ax = all_hurricanes_df['min_season'].hist(bins=50)\nax.set_title('Number of hurricanes per season')\n</pre> ax = all_hurricanes_df['min_season'].hist(bins=50) ax.set_title('Number of hurricanes per season') Out[363]: <pre>Text(0.5,1,'Number of hurricanes per season')</pre> <p>From the previous notebook, we noticed the number of hurricanes recorded has been steadily increasing, partly due to advancements in technology. We notice a reduction in number of hurricanes after 1970s. Let us split this up by basin and observe the the trend is similar.</p> In\u00a0[378]: Copied! <pre>fgrid = sns.FacetGrid(data=all_hurricanes_df, col='any_basin', col_wrap=3,\n                     sharex=False, sharey=False)\nfgrid.map(plt.hist, 'min_season', bins=50)\nfgrid.set_axis_labels(x_var='Seasons', y_var='Frequency of hurricanes')\n</pre> fgrid = sns.FacetGrid(data=all_hurricanes_df, col='any_basin', col_wrap=3,                      sharex=False, sharey=False) fgrid.map(plt.hist, 'min_season', bins=50) fgrid.set_axis_labels(x_var='Seasons', y_var='Frequency of hurricanes') Out[378]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x1a35c9f908&gt;</pre> <p>Plotting the frequency of hurricanes by basin shows a similar trend with the number of hurricanes reducing globally after 1970s. This is consistent with certain studies (1). However this is only one part of the story. Below, we continue to analyze if the nature of hurricanes itself is changing, while the total number may reduce.</p> In\u00a0[266]: Copied! <pre>jgrid = sns.jointplot(x='min_season', y='max_wind_merged', data=hurricanes_ipl,\n             kind='reg', joint_kws={'line_kws':{'color':'green'}}, height=7, space=0.5)\nj = jgrid.annotate(stats.pearsonr)\nj = jgrid.ax_joint.set_title('Does hurricane wind speed increase over time?')\n</pre> jgrid = sns.jointplot(x='min_season', y='max_wind_merged', data=hurricanes_ipl,              kind='reg', joint_kws={'line_kws':{'color':'green'}}, height=7, space=0.5) j = jgrid.annotate(stats.pearsonr) j = jgrid.ax_joint.set_title('Does hurricane wind speed increase over time?') <p>From the plot above, we notice a small positive correlation. Wind speeds are observed to increase with time. The small <code>p-value</code> suggests this correlation (albeit small) is statistically significant. The plot above considers hurricanes across all the basins and regresses that against time. To get a finer picture, we need to split the data by basins and observe the correlation.</p> In\u00a0[329]: Copied! <pre># since there are not many hurricanes observed over South Atlantic basin (SA), \n# we drop it from analysis\nhurricanes_major_basins_df = hurricanes_ipl[hurricanes_ipl['any_basin'].isin(\n                                            ['WP','SI','NA','EP','NI','SP'])]\n</pre> # since there are not many hurricanes observed over South Atlantic basin (SA),  # we drop it from analysis hurricanes_major_basins_df = hurricanes_ipl[hurricanes_ipl['any_basin'].isin(                                             ['WP','SI','NA','EP','NI','SP'])] <p>Define a function that can compute <code>pearson-r</code> correlation coefficient for any two columns across all basins.</p> In\u00a0[322]: Copied! <pre>def correlate_by_basin(column_a, column_b, df=hurricanes_major_basins_df, \n                       category_column = 'any_basin'):\n    # clean data by dropping any NaN values\n    df_nona = df.dropna(subset=[column_a, column_b])\n    \n    # loop through the basins\n    basins = list(df[category_column].unique())\n    return_dict = {}\n    for basin in basins:\n        subset_df = df_nona[df_nona[category_column] == basin]\n        r, p = stats.pearsonr(list(subset_df[column_a]), list(subset_df[column_b]))\n        \n        return_dict[basin] = [round(r,4), round(p,4)]\n    \n    # return correlation coefficient and p-value for each basin as a DataFrame\n    return_df = pd.DataFrame(return_dict).T\n    return_df.columns=['pearson-r','p-value']\n    return return_df\n</pre> def correlate_by_basin(column_a, column_b, df=hurricanes_major_basins_df,                         category_column = 'any_basin'):     # clean data by dropping any NaN values     df_nona = df.dropna(subset=[column_a, column_b])          # loop through the basins     basins = list(df[category_column].unique())     return_dict = {}     for basin in basins:         subset_df = df_nona[df_nona[category_column] == basin]         r, p = stats.pearsonr(list(subset_df[column_a]), list(subset_df[column_b]))                  return_dict[basin] = [round(r,4), round(p,4)]          # return correlation coefficient and p-value for each basin as a DataFrame     return_df = pd.DataFrame(return_dict).T     return_df.columns=['pearson-r','p-value']     return return_df In\u00a0[342]: Copied! <pre>fgrid = sns.lmplot('min_season', 'max_wind_merged', col='any_basin', \n                   data=hurricanes_major_basins_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> fgrid = sns.lmplot('min_season', 'max_wind_merged', col='any_basin',                     data=hurricanes_major_basins_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) <p>From the scatter plots above, we notice the wind speeds in most basins show a slight positive trend, with North Atlantic being an exception. To explore this further, we compute the correlation coefficient and its p-value below.</p> In\u00a0[328]: Copied! <pre>wind_vs_season = correlate_by_basin('min_season','max_wind_merged')\nprint('Correlation coefficients for min_season vs max_wind_merged')\nwind_vs_season\n</pre> wind_vs_season = correlate_by_basin('min_season','max_wind_merged') print('Correlation coefficients for min_season vs max_wind_merged') wind_vs_season <pre>Correlation coefficients for min_season vs max_wind_merged\n</pre> Out[328]: pearson-r p-value SI 0.0686 0.0003 NA -0.1073 0.0000 NI 0.0528 0.0357 WP 0.0891 0.0000 SP 0.3415 0.0000 EP 0.1177 0.0001 <p>The table above displays the correlation coefficient of hurricane wind speed over time. Hurricanes over Southern Pacific basin exhibit a positive trend of increasing wind speeds. The <code>r</code> value over North Atlantic shows a weak negative trend. Since all the <code>p-value</code>s are less than <code>0.05</code>, these correlations are statistically significant.</p> In\u00a0[414]: Copied! <pre>def categorize_hurricanes(row, wind_speed_column='max_wind_merged'):\n    wind_speed = row[wind_speed_column] * 1.152  # knots to mph\n    if 74 &lt;= wind_speed &lt;= 95:\n        return '1'\n    elif 96 &lt;= wind_speed &lt;= 110:\n        return '2'\n    elif 111 &lt;= wind_speed &lt;= 129:\n        return '3'\n    elif 130 &lt;= wind_speed &lt;= 156:\n        return '4'\n    elif 157 &lt;= wind_speed &lt;= 500:\n        return '5'\n</pre> def categorize_hurricanes(row, wind_speed_column='max_wind_merged'):     wind_speed = row[wind_speed_column] * 1.152  # knots to mph     if 74 &lt;= wind_speed &lt;= 95:         return '1'     elif 96 &lt;= wind_speed &lt;= 110:         return '2'     elif 111 &lt;= wind_speed &lt;= 129:         return '3'     elif 130 &lt;= wind_speed &lt;= 156:         return '4'     elif 157 &lt;= wind_speed &lt;= 500:         return '5' In\u00a0[435]: Copied! <pre>hurricanes_major_basins_df['category_str'] = hurricanes_major_basins_df.apply(categorize_hurricanes, \n                                                                              axis=1)\nhurricanes_major_basins_df['category'] = pd.to_numeric(arg=hurricanes_major_basins_df['category_str'],\n                                                      errors='coerce', downcast='integer')\n\nhurricanes_major_basins_df.head(2)\n</pre> hurricanes_major_basins_df['category_str'] = hurricanes_major_basins_df.apply(categorize_hurricanes,                                                                                axis=1) hurricanes_major_basins_df['category'] = pd.to_numeric(arg=hurricanes_major_basins_df['category_str'],                                                       errors='coerce', downcast='integer')  hurricanes_major_basins_df.head(2) Out[435]: SHAPE any_basin any_name any_sub_basin count end_datetime max_eye_dia_merged max_wind_merged mean_latitude_merged mean_longitude_merged ... min_season objectid range_pressure_merged range_wind_merged start_datetime track_duration track_duration_hrs track_duration_days category category_str start_datetime 1854-02-08 06:00:00 {'paths': [[[59.60000000000002, -17.6000000000... SI NOT NAMED MM 7.0 1854-02-10 18:00:00 NaN NaN -19.318571 60.639286 ... 1854.0 1 NaN NaN 1854-02-08 06:00:00 129600000.0 36.0 1.5 NaN None 1859-08-24 12:00:00 {'paths': [[[-23.5, 12.5], [-24.19999999999999... NA NOT NAMED NA 9.0 1859-08-26 12:00:00 NaN 45.0 14.000000 -26.222222 ... 1859.0 2 NaN 10.0 1859-08-24 12:00:00 172800000.0 48.0 2.0 NaN None <p>2 rows \u00d7 21 columns</p> <p>We will create violin and bar plots to visualize the number of hurricane categories over different basins.</p> In\u00a0[429]: Copied! <pre>fig, ax = plt.subplots(1,2, figsize=(15,6))\nvplot = sns.violinplot(x='any_basin', y='category', data=hurricanes_major_basins_df, ax=ax[0])\nvplot.set_title('Number of hurricanes per category in each basin')\n\ncplot = sns.countplot(x='any_basin', hue='category_str', data=hurricanes_major_basins_df,\n             hue_order=['1','2','3','4','5'], ax=ax[1])\ncplot.set_title('Number of hurricanes per category in each basin')\n</pre> fig, ax = plt.subplots(1,2, figsize=(15,6)) vplot = sns.violinplot(x='any_basin', y='category', data=hurricanes_major_basins_df, ax=ax[0]) vplot.set_title('Number of hurricanes per category in each basin')  cplot = sns.countplot(x='any_basin', hue='category_str', data=hurricanes_major_basins_df,              hue_order=['1','2','3','4','5'], ax=ax[1]) cplot.set_title('Number of hurricanes per category in each basin') Out[429]: <pre>Text(0.5,1,'Number of hurricanes per category in each basin')</pre> <p>We notice all basins are capable of generating major hurricanes (over 3). The Eastern Pacific basin appears to have a larger than the proportional number of major hurricanes. Below, we will regress the hurricane category against time to observe if there is a positive trend.</p> In\u00a0[440]: Copied! <pre>kde_regplot = sns.jointplot(x='min_season', y='category', \n                            data=hurricanes_major_basins_df, kind='kde', \n                            stat_func=stats.pearsonr).plot_joint(sns.regplot, \n                                                                 scatter=False)\nkde_regplot.ax_joint.set_title('Scatter plot of hurricane categories over seasons')\n</pre> kde_regplot = sns.jointplot(x='min_season', y='category',                              data=hurricanes_major_basins_df, kind='kde',                              stat_func=stats.pearsonr).plot_joint(sns.regplot,                                                                   scatter=False) kde_regplot.ax_joint.set_title('Scatter plot of hurricane categories over seasons') Out[440]: <pre>Text(0.5,1,'Scatter plot of hurricane categories over seasons')</pre> <p>Even at a global level, we notice a strong positive correlation between hurricane category and seasons. Below, we will split this across basins to observe if the trend holds good.</p> In\u00a0[419]: Copied! <pre>wgrid = sns.lmplot('min_season', 'category', col='any_basin', \n                   data=hurricanes_major_basins_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> wgrid = sns.lmplot('min_season', 'category', col='any_basin',                     data=hurricanes_major_basins_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) In\u00a0[442]: Copied! <pre>category_corr_df = correlate_by_basin('min_season','category', df=hurricanes_major_basins_df)\nprint('Correlation coefficients for min_season vs hurricane category')\ncategory_corr_df\n</pre> category_corr_df = correlate_by_basin('min_season','category', df=hurricanes_major_basins_df) print('Correlation coefficients for min_season vs hurricane category') category_corr_df <pre>Correlation coefficients for min_season vs hurricane category\n</pre> Out[442]: pearson-r p-value SI 0.3314 0.0 NA 0.2161 0.0 NI 0.4612 0.0 WP 0.1849 0.0 SP 0.2908 0.0 EP 0.2919 0.0 <p>Thus, at both global and basin scales, we notice a positive trend in the number of hurricanes of category <code>4</code> and higher, while there is a general reduction in the quantity of hurricanes per season. This is along the lines of several studies [1] [2] [3] [4]. Thus while the total number of hurricanes per season may reduce, we notice an increase in the intensity of them.</p> In\u00a0[330]: Copied! <pre>pgrid = sns.lmplot('min_season', 'min_pressure_merged', col='any_basin', \n                   data=hurricanes_major_basins_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> pgrid = sns.lmplot('min_season', 'min_pressure_merged', col='any_basin',                     data=hurricanes_major_basins_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) In\u00a0[333]: Copied! <pre>pressure_corr_df = correlate_by_basin('min_season','min_pressure_merged')\nprint('Correlation coefficients for min_season vs min_pressure_merged')\npressure_corr_df\n</pre> pressure_corr_df = correlate_by_basin('min_season','min_pressure_merged') print('Correlation coefficients for min_season vs min_pressure_merged') pressure_corr_df <pre>Correlation coefficients for min_season vs min_pressure_merged\n</pre> Out[333]: pearson-r p-value SI -0.0160 0.4027 NA 0.1911 0.0000 NI 0.2551 0.0000 WP -0.0230 0.1606 SP -0.2456 0.0000 EP -0.0270 0.3612 <p>Lower the atmospheric pressure, more intense is the hurricane. Hence we are looking for strong negative correlation between the pressure and season columns. From the charts and table above, we notice South Pacific basin once again tops the list with a mild negative correlation over time. The <code>p-value</code>s of Western Pacific and Eastern Pacific is larger than <code>0.05</code>, so we disregard their correlation coefficients. Over North American and Indian basins, we notice a weak positive correlation.</p> In\u00a0[159]: Copied! <pre>jgrid = sns.jointplot(x='min_season', y='track_duration_days', data=hurricanes_ipl,\n             kind='hex', height=7, space=0.5)\nj = jgrid.annotate(stats.pearsonr)\n\nsns.regplot(x='min_season', y='track_duration_days', data=hurricanes_ipl, \n            ax=jgrid.ax_joint, color='green',scatter=False)\n\nj = jgrid.ax_joint.set_title('Does hurricane duration increase over time?')\n</pre> jgrid = sns.jointplot(x='min_season', y='track_duration_days', data=hurricanes_ipl,              kind='hex', height=7, space=0.5) j = jgrid.annotate(stats.pearsonr)  sns.regplot(x='min_season', y='track_duration_days', data=hurricanes_ipl,              ax=jgrid.ax_joint, color='green',scatter=False)  j = jgrid.ax_joint.set_title('Does hurricane duration increase over time?') <p>At a global scale, we notice an increase in the duration of hurricanes. Below we split this up by basins to get a finer look.</p> In\u00a0[332]: Copied! <pre>lgrid = sns.lmplot('min_season', 'track_duration_days', col='any_basin', \n                   data=hurricanes_major_basins_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> lgrid = sns.lmplot('min_season', 'track_duration_days', col='any_basin',                     data=hurricanes_major_basins_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) In\u00a0[334]: Copied! <pre>linger_time_corr_df = correlate_by_basin('min_season','track_duration_days')\nprint('Correlation coefficients for min_season vs track_duration_days')\nlinger_time_corr_df\n</pre> linger_time_corr_df = correlate_by_basin('min_season','track_duration_days') print('Correlation coefficients for min_season vs track_duration_days') linger_time_corr_df <pre>Correlation coefficients for min_season vs track_duration_days\n</pre> Out[334]: pearson-r p-value SI 0.3577 0.0000 NA -0.0295 0.1738 NI 0.0736 0.0034 WP 0.3627 0.0000 SP 0.2716 0.0000 EP 0.2560 0.0000 <p>At most basins, we notice a positive trend in hurricane track duration. An exception to this is the North Atlantic basin where the <code>p-value</code> is not significant enough to let us draw any conclusion.</p> <p>The trend we notice here could be partly due to technological advancements that allow us to identify and track hurricanes at a very early stage. Hence, to complement this, we will analyze if hurricanes travel longer than usual once the make a landfall.</p> In\u00a0[341]: Copied! <pre>jgrid = sns.jointplot(x='min_season', y='analysislength', data=landfall_tracks_df,\n                      kind='reg', joint_kws={'line_kws':{'color':'green'}}, \n                      height=7, space=0.5, ylim=[0,2000])\nj = jgrid.annotate(stats.pearsonr)\nj = jgrid.ax_joint.set_title('Do hurricanes travel longer inland over time?')\n</pre> jgrid = sns.jointplot(x='min_season', y='analysislength', data=landfall_tracks_df,                       kind='reg', joint_kws={'line_kws':{'color':'green'}},                        height=7, space=0.5, ylim=[0,2000]) j = jgrid.annotate(stats.pearsonr) j = jgrid.ax_joint.set_title('Do hurricanes travel longer inland over time?') In\u00a0[343]: Copied! <pre>lgrid = sns.lmplot('min_season', 'analysislength', col='any_basin', \n                   data=landfall_tracks_df, col_wrap=3,\n                   sharex=False, sharey=False, line_kws={'color':'green'})\n</pre> lgrid = sns.lmplot('min_season', 'analysislength', col='any_basin',                     data=landfall_tracks_df, col_wrap=3,                    sharex=False, sharey=False, line_kws={'color':'green'}) In\u00a0[344]: Copied! <pre>linger_distance_corr_df = correlate_by_basin('min_season','analysislength', df=landfall_tracks_df)\nprint('Correlation coefficients for min_season vs inland track length')\nlinger_distance_corr_df\n</pre> linger_distance_corr_df = correlate_by_basin('min_season','analysislength', df=landfall_tracks_df) print('Correlation coefficients for min_season vs inland track length') linger_distance_corr_df <pre>Correlation coefficients for min_season vs inland track length\n</pre> Out[344]: pearson-r p-value NA 0.0035 0.8611 NI -0.2320 0.0000 SI -0.0013 0.9684 WP 0.0555 0.0004 SP -0.1722 0.0000 EP -0.0296 0.6344 SA 1.0000 0.0000 <p>When we correlated inland track length over time, we were able to unravel and interesting observation. At basins where there correlation is statistically significant, it is negative (North Indian, Western Pacific, South Pacific). Thus while the duration of hurricanes continues to increase (due to reasons discussed previously), we notice hurricanes travel shorter distances inland. This could be problematic of communities affected as the hurricane could remain stagnant and produce stronger than usual storm surges and precipitation.</p> In\u00a0[443]: Copied! <pre># clean data by dropping any NaN values\nsubset_cols = ['min_season', 'max_wind_merged',\n                'min_pressure_merged', 'track_duration_days', 'category']\n\ndf_nona = hurricanes_major_basins_df.dropna(subset=subset_cols)\n\n# loop through the basins\nbasins = list(df_nona['any_basin'].unique())\nreturn_dict = {}\nfor basin in basins:\n    subset_df =df_nona[df_nona['any_basin'] == basin]\n    row_vector = []\n    for col in subset_cols[1:]:\n        r, p = stats.pearsonr(list(subset_df['min_season']), list(subset_df[col]))\n        if p &lt; 0.05:\n            row_vector.append(round(r,4))\n        else:\n            row_vector.append(pd.np.NaN)\n\n    return_dict[basin] = row_vector\n\n# return as a DataFrame\nreturn_df = pd.DataFrame.from_dict(return_dict, orient='index', columns=subset_cols[1:])\nreturn_df\n</pre> # clean data by dropping any NaN values subset_cols = ['min_season', 'max_wind_merged',                 'min_pressure_merged', 'track_duration_days', 'category']  df_nona = hurricanes_major_basins_df.dropna(subset=subset_cols)  # loop through the basins basins = list(df_nona['any_basin'].unique()) return_dict = {} for basin in basins:     subset_df =df_nona[df_nona['any_basin'] == basin]     row_vector = []     for col in subset_cols[1:]:         r, p = stats.pearsonr(list(subset_df['min_season']), list(subset_df[col]))         if p &lt; 0.05:             row_vector.append(round(r,4))         else:             row_vector.append(pd.np.NaN)      return_dict[basin] = row_vector  # return as a DataFrame return_df = pd.DataFrame.from_dict(return_dict, orient='index', columns=subset_cols[1:]) return_df Out[443]: max_wind_merged min_pressure_merged track_duration_days category NA 0.2171 -0.0904 0.1633 0.2161 SI 0.3156 -0.3608 0.5433 0.3314 NI 0.4456 -0.2234 0.3414 0.4612 WP 0.1755 -0.3666 0.5860 0.1849 SP 0.3028 -0.3508 0.3819 0.2908 EP 0.2820 -0.3510 0.3077 0.2919 <p>We can visualize this correlation table as a heat map to appreciate how the hurricane severity indicators correlate over seasons for each basin.</p> In\u00a0[450]: Copied! <pre>hm_plot = sns.heatmap(return_df, annot=True, linecolor='black', cmap='RdBu_r')\nhm_plot.set_title('Heatmap of correlation coefficients of hurricane severity indicators')\n</pre> hm_plot = sns.heatmap(return_df, annot=True, linecolor='black', cmap='RdBu_r') hm_plot.set_title('Heatmap of correlation coefficients of hurricane severity indicators') Out[450]: <pre>Text(0.5,1,'Heatmap of correlation coefficients of hurricane severity indicators')</pre>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#analyzing-hurricane-tracks-part-33","title":"Analyzing hurricane tracks - Part 3/3\u00b6","text":"<p>This is the third part to a three part set of notebooks that process and analyze historic hurricane tracks. In the previous notebooks we saw</p> <p>Part 1</p> <ol> <li>downloading historic hurricane datasets using Python</li> <li>cleaning and merging hurricane observations using DASK</li> <li>aggregating point observations into hurricane tracks using ArcGIS GeoAnalytics server</li> </ol> <p>Part 2</p> <ol> <li>Visualize the locations of hurricane tracks</li> <li>Different basins and the number of hurricanes per basin</li> <li>Number of hurricanes over time</li> <li>Seasonality in occurrence of hurricanes</li> <li>Places where hurricanes make landfall and the people affected</li> </ol> <p>In this notebook you will analyze the aggregated tracks to answer important questions about hurricane severity and how they correlate over time.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#access-aggregated-hurricane-data","title":"Access aggregated hurricane data\u00b6","text":"<p>Below, we access the tracks aggregated using GeoAnalytics in the previous notebook.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#query-hurricane-tracks-into-a-spatially-enabled-dataframe","title":"Query hurricane tracks into a Spatially enabled <code>DataFrame</code>\u00b6","text":""},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#query-landfall-tracks-layer-into-a-spatially-enabled-dataframe","title":"Query landfall tracks layer into a Spatially Enabled <code>DataFrame</code>\u00b6","text":"<p>We query the landfall tracks layer created in the pervious notebook into a DataFrame.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#manage-missing-sesnsor-data","title":"Manage missing sesnsor data\u00b6","text":"<p>Before we can analyze if hurricanes intensify over time, we need to identify and account for missing values in our data. Sensor measurements such as wind speed, atmospheric pressure, eye diameter, generally suffer from missing values and outliers. The reconstruct tracks tool has identified <code>12,362</code> individual hurricanes that occurred during the past <code>169</code> years.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#visualize-missing-records","title":"Visualize missing records\u00b6","text":"<p>An easy way to visualize missing records is to hack the <code>heatmap</code> of <code>seaborn</code> library to display missing records. The snippet below shows missing records in yellow color.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#missing-value-imputation","title":"Missing value imputation\u00b6","text":""},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#does-intensity-of-hurricanes-increase-over-time","title":"Does intensity of hurricanes increase over time?\u00b6","text":"<p>This last part of this study analyzes if there a temporal trend in the intensity of hurricanes. A number of studies have concluded that anthropogenic influences in the form of global climate change make hurricanes worse and dangerous. We analyze if such patterns can be noticed from an empirical standpoint.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#does-the-number-of-hurricanes-increase-over-time","title":"Does the number of hurricanes increase over time?\u00b6","text":""},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#does-hurricane-wind-speed-increase-over-time","title":"Does hurricane wind speed increase over time?\u00b6","text":"<p>To understand if wind speed increases over time, we create a scatter plot of <code>min_season</code> against the <code>max_wind_merged</code> column. The <code>seaborn</code> plotting library can enhance this plot with correlation coefficient and its level of signifance (p value).</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#analyzing-hurricane-wind-speed-over-time-by-basin","title":"Analyzing hurricane wind speed over time by basin\u00b6","text":"<p>Below we plot a grid of scatter plots with linear regression plots overlaid over them. The <code>seaborn</code> library's <code>lmplot()</code> function makes it trivial accomplish this in a single command.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#analyzing-hurricane-category-over-time-by-basin","title":"Analyzing hurricane category over time by basin\u00b6","text":"<p>Hurricanes are classified on a Saffir-Simpson scale of <code>1-5</code> based on their wind speed. Let us compute this column on the dataset and observe if there are teomporal aspects to it.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#does-eye-pressure-decrease-over-time","title":"Does eye pressure decrease over time?\u00b6","text":"<p>Just like a high wind speed, lower atmospheric pressure increases the intensity of hurricanes. To analyze this, we produce a scatter grid of <code>min_pressure_merged</code> column and regress it against <code>min_season</code> column. We split this by basins.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#do-hurricanes-linger-longer-over-time","title":"Do hurricanes linger longer over time?\u00b6","text":"<p>While wind speed and atmospheric pressure measure two types of intensities, a neverending hurricane can also hurt the communities affected as it inundates the coast with rainfall and storm surge for longer periods of time. In this section we correlate the track duration in days against seasons.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#do-hurricanes-travel-longer-inland-over-time","title":"Do hurricanes travel longer inland over time?\u00b6","text":"<p>Along the lines of track duration, it is relevant for us to investigate whether hurricanes travel longer inland over time. Thus, we correlate track length column of hurricanes that made landfall against seasons.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#correlate-observations-over-time","title":"Correlate observations over time\u00b6","text":"<p>Let us collect all trend analysis we performed so far into a single <code>DataFrame</code>.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook used the aggregated hurricane tracks produced in part 1 and the landfall tracks layer produced in part 2. We checked for missing sensor observations and imputed those records via interpolation. We then comprehensively analyzed how hurricane severity indicators such as wind speed, atmospheric pressure, track duration, track length inland and category correlate over time (seasons). We noticed the total number of hurricanes have decreased since <code>1970</code>s globally and across all basins. While the number of hurricanes is less, we noticed their wind speed, track duration and category correlate positively and atmospheric pressure correlates negatively against seasons. This aligns with findings from major studies. Thus, we notice a reduction in the number of category 1 and 2 and an increase in the number of more severe, category 4, 5 hurricanes. We noticed the North Indian basin to be highly correlated in this regard.</p> <p>In this study, through these 3 part notebooks, we saw how to download hurricane data for the past <code>169</code> years from NOAA NEIC site over FTP and how to it initially using Pandas. Since this data is larger than memory for average computers, we learnt how to aggregate the columns using the DASK distributed, delayed processing library. We input the result of DASK to the ArcGIS GeoAnalytics server to aggregate these point observations into hurricane tracks. The \"reconstruct tracks\" tool which performed this aggregation identified <code>12,362</code> individual hurricanes across the globe.</p> <p>In the second notebook, we performed comprehensive visualization and exploratory analysis to understand the geography of hurricanes and the various basins they are categorized into. We observed a strong sinusoidal nature where hurricanes in the northern and southern hemisphere are offset by <code>6</code> months. We noticed certain names (<code>Irma</code>, <code>Florence</code>) are more popular than the rest. We overlaid the tracks over land boundary dataset to compute tracks traveled inland and identify places where landfall occur. Through density analysis of the landfall locations, we were able to identify the places there were most affected from a repeat landfall basis. By geo-enriching these places, we learnt that China, Hongkong and India are home to population that is most affected.</p> <p>Many studies and articles [5] [6] [7] [8] shine light on anthropogenic influences on global Sea Surface Temperature (SST). Higher sea surface temperatures are observed to produce more intense storms; they are attributed to polar ice cap melting and rising sea levels. These combined with increased hurricane intensity can lead to severe strom surges causing increased flooding and damage of the coastal communities. Based on our density and geoenrichment analysis, we noticed these are places along the coast that are densely populated.</p> <p>This study showcases how a comprehensive spatial data science project can be performed using ArcGIS and open source Python libraries on the ArcGIS Notebook server. The notebook medium makes it convenient to document code, narrative and graphics in one place. This helps in making research reproducible and approachable. Thus in this study we were able to empirically validate the conclusions derived by other studies. In future, when more hurricane datasets become available, this study can be repeated just by rerunning these notebooks.</p>"},{"location":"projects/dist-computing/dask/part3_analyze_hurricane_tracks/#references","title":"References\u00b6","text":"<ul> <li>1. Recent intense hurricane response to global climate change. Holland, G. &amp; Bruy\u00e8re, C.L. Clim Dyn (2014) 42: 617.</li> <li>2. US Hurricane Clustering: A New Reality?</li> <li>3. Increasing destructiveness of tropical cyclones over the past 30\u2009years</li> <li>4. The Hurricanes, and Climate-Change Questions, Keep Coming. Yes, They\u2019re Linked. </li> <li>5. We have 12 years to limit climate change catastrophe, warns UN </li> <li>6. Four big takeaways from the UN\u2019s alarming climate change report</li> <li>7. Hurricane Florence caused up to 22 billion in damages. Climate change made the storm worse.</li> <li>8. What Climate Change Taught Us About Hurricanes </li> </ul>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/","title":"Getting started with PySpark","text":"<p>PySpark library gives you a Python API to read and work with your RDDs in HDFS through Apache spark. This tutorial explains the caveats in installing and getting started with PySpark.</p>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#installing-pyspark-scala-java-spark","title":"Installing PySpark, Scala, Java, Spark","text":"<p>Follow this tutorial. The overall steps are</p> <ul> <li>get a linux VM ready. It could be an EC2 instance on AWS</li> <li>get SSH ability into this VM</li> <li>install anaconda. Note: Spark 2.0.0 cannot work with Python 3.6 and needs 3.5. So you can get a version of anaconda that installs 3.5 by default or you can get a higher version of Spark.</li> <li>change the default system Python to use Anaconda python</li> <li>install pip and <code>py4j</code> lib that allows you to run java via Python.</li> <li>download and extract Spark. Here if you get <code>spark-2.0.0-bin-hadoop2.7.tgz</code>, then you need Python 3.5 and not higher. I am getting <code>spark-2.2.1-bin-hadoop2.7.tgz</code> and it works well. Spark JIRA issue for reference</li> </ul> <p>To premanently store the SPARK path store this in the <code>.bashrc</code> file on the home dir of the user account</p> <pre><code>export PATH=\"your default path output\"\n\n# anaconda bin dir - this replaces default Python to anaconda python\nexport PATH=\"/home/USERNAME/anaconda3/bin:$PATH\"\n\n# linux uses : for path separators\n# now add Spark home to Path and PythonPath\nexport SPARK_HOME=\"/home/USERNAME/spark-2.2.1-bin-hadoop2.7\"\nexport PATH=$SPARK_HOME:$PATH\nexport PYTHONPATH=$SPARK_HOME/python #this adds pyspark to python path.\n</code></pre>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#scala-vs-python","title":"Scala vs Python","text":"<p>Advantages of using Scala  - concurrency as Scala supports async.  - Type safety during compile time  - User Defined Functions (UDF) are more efficient in Scala  - type safety - Scala is suitable for bigger projects as its hassle free when you are refactoring a large codebase.  - due to absence of type safety, it does not make sense to with spark Datasets in Python. You can only work with RDD and DataFrames.</p> <p>Advantages of using Python  - easy to learn and use  - suitable for ad-hoc and small projects  - SparkMLib for ML.</p> <p>tie points  - Spark streaming is equally good in both  - DataFrames are similar in both</p>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#rdds-dataframes-datasets","title":"RDDs, DataFrames, Datasets","text":"<p>For an overview on RDDs, refer here. RDDs are compile-time type-safe and evaluate lazily. RDDs can slow in non-JVM langs like Python, cannot be optimized by spark. DataFrames are built on top of RDDs and you let Spark figure out how to work with RDDs. Hence DF is optimized. The only downside is compile-time type-safety. To rectify this, Spark built Datasets.</p> <p>In Spark 2.0, DataFrames and Datasets are merged. Conceptually, if you work with untyped data (as in Python, R), you can use a DataFrame where as if you work in typed languages (Scala), you can work with DataSets.</p>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#actions-and-transformations","title":"Actions and transformations","text":"<p>Transformations create a new dataset. Actions return a value (like a summary statistic). All transformations in Spark are lazy. You can also persist RDDs on disk if you expect to read it later.</p> <p>When working with Spark, you use lambda functions a lot. Lambda plays well with Spark's motto of lazy evaluations.</p>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#some-spark-commands","title":"Some Spark commands","text":"<ul> <li><code>rdd.textFile()</code> to read text files, csv files etc.</li> <li><code>rdd.collect()</code> brings entire RDD to a single machine for processing and displays the result. This is mem intensive and can overwhelm the master if you use it on a large dataset.</li> <li><code>rdd.take(n)</code> on the other hand will only collect and return <code>n</code> lines.</li> <li><code>rdd.toDF()</code> to convert RDD to Spark DF</li> <li><code>df.first()</code> and <code>df.top(n)</code> also work like take.</li> <li><code>df.printSchema()</code> to list the columns and their types. </li> <li>You can also use <code>df.describe().show()</code> to get summary stats.</li> <li><code>df.select('column1','column2').show(m)</code> to select a couple of columns and show their first m rows.</li> <li><code>df.withColumn('colname', transformation_expression)</code> is the primary way you to update values in a DataFrame column.</li> </ul>"},{"location":"projects/dist-computing/pyspark/getting-started-pyspark/#resources","title":"Resources","text":"<ul> <li>A tale of 3 Apache Spark APIs - RDDs, DataFrames, Datasets</li> <li>Datacamp - Apache Spark and Python</li> <li>Datacamp - Predict CA housing prices using SparkMLib and PySpark</li> </ul>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/","title":"Predicting CA housing prices using SparkMLib","text":"In\u00a0[19]: Copied! <pre># Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Build the SparkSession\nspark = SparkSession.builder \\\n   .master(\"local\") \\\n   .appName(\"Linear Regression Model\") \\\n   .config(\"spark.executor.memory\", \"1gb\") \\\n   .getOrCreate()\n   \nsc = spark.sparkContext\n</pre> # Import SparkSession from pyspark.sql import SparkSession  # Build the SparkSession spark = SparkSession.builder \\    .master(\"local\") \\    .appName(\"Linear Regression Model\") \\    .config(\"spark.executor.memory\", \"1gb\") \\    .getOrCreate()     sc = spark.sparkContext <p>Number of records: 20640</p> <p>variables: Lat, Long, Median Age, #rooms, #bedrooms, population in block, households, med income, med house value</p> In\u00a0[4]: Copied! <pre>!ls ../datasets/CaliforniaHousing/\n</pre> !ls ../datasets/CaliforniaHousing/ <pre>cal_housing.data  cal_housing.domain\r\n</pre> In\u00a0[2]: Copied! <pre># load data file\nrdd = sc.textFile('../datasets/CaliforniaHousing/cal_housing.data')\n\n# load header\nheader = sc.textFile('../datasets/CaliforniaHousing/cal_housing.domain')\n</pre> # load data file rdd = sc.textFile('../datasets/CaliforniaHousing/cal_housing.data')  # load header header = sc.textFile('../datasets/CaliforniaHousing/cal_housing.domain') In\u00a0[8]: Copied! <pre>len(rdd.collect())\n</pre> len(rdd.collect()) Out[8]: <pre>20640</pre> In\u00a0[10]: Copied! <pre>len(rdd.take(5))\n</pre> len(rdd.take(5)) Out[10]: <pre>5</pre> In\u00a0[11]: Copied! <pre>rdd.take(5)\n</pre> rdd.take(5) Out[11]: <pre>['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000',\n '-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000',\n '-122.240000,37.850000,52.000000,1467.000000,190.000000,496.000000,177.000000,7.257400,352100.000000',\n '-122.250000,37.850000,52.000000,1274.000000,235.000000,558.000000,219.000000,5.643100,341300.000000',\n '-122.250000,37.850000,52.000000,1627.000000,280.000000,565.000000,259.000000,3.846200,342200.000000']</pre> In\u00a0[3]: Copied! <pre># split by comma\nrdd = rdd.map(lambda line : line.split(','))\n\n# get the first two lines\nrdd.first()\n</pre> # split by comma rdd = rdd.map(lambda line : line.split(','))  # get the first two lines rdd.first() Out[3]: <pre>['-122.230000',\n '37.880000',\n '41.000000',\n '880.000000',\n '129.000000',\n '322.000000',\n '126.000000',\n '8.325200',\n '452600.000000']</pre> In\u00a0[4]: Copied! <pre># convert RDD to a dataframe\nfrom pyspark.sql import Row\n\n# Map the RDD to a DF\ndf = rdd.map(lambda line: Row(longitude=line[0], \n                              latitude=line[1], \n                              housingMedianAge=line[2],\n                              totalRooms=line[3],\n                              totalBedRooms=line[4],\n                              population=line[5], \n                              households=line[6],\n                              medianIncome=line[7],\n                              medianHouseValue=line[8])).toDF()\n\n# show the top few DF rows\ndf.show(5)\n</pre> # convert RDD to a dataframe from pyspark.sql import Row  # Map the RDD to a DF df = rdd.map(lambda line: Row(longitude=line[0],                                latitude=line[1],                                housingMedianAge=line[2],                               totalRooms=line[3],                               totalBedRooms=line[4],                               population=line[5],                                households=line[6],                               medianIncome=line[7],                               medianHouseValue=line[8])).toDF()  # show the top few DF rows df.show(5) <pre>+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n| households|housingMedianAge| latitude|  longitude|medianHouseValue|medianIncome| population|totalBedRooms| totalRooms|\n+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n| 126.000000|       41.000000|37.880000|-122.230000|   452600.000000|    8.325200| 322.000000|   129.000000| 880.000000|\n|1138.000000|       21.000000|37.860000|-122.220000|   358500.000000|    8.301400|2401.000000|  1106.000000|7099.000000|\n| 177.000000|       52.000000|37.850000|-122.240000|   352100.000000|    7.257400| 496.000000|   190.000000|1467.000000|\n| 219.000000|       52.000000|37.850000|-122.250000|   341300.000000|    5.643100| 558.000000|   235.000000|1274.000000|\n| 259.000000|       52.000000|37.850000|-122.250000|   342200.000000|    3.846200| 565.000000|   280.000000|1627.000000|\n+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\nonly showing top 5 rows\n\n</pre> In\u00a0[5]: Copied! <pre>df.printSchema()\n</pre> df.printSchema() <pre>root\n |-- households: string (nullable = true)\n |-- housingMedianAge: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n |-- medianHouseValue: string (nullable = true)\n |-- medianIncome: string (nullable = true)\n |-- population: string (nullable = true)\n |-- totalBedRooms: string (nullable = true)\n |-- totalRooms: string (nullable = true)\n\n</pre> In\u00a0[6]: Copied! <pre># convert all strings to float using a User Defined Function\n\nfrom pyspark.sql.types import *\n\ndef cast_columns(df):\n    for column in df.columns:\n        df = df.withColumn(column, df[column].cast(FloatType()))\n    return df\n\nnew_df = cast_columns(df)\n</pre> # convert all strings to float using a User Defined Function  from pyspark.sql.types import *  def cast_columns(df):     for column in df.columns:         df = df.withColumn(column, df[column].cast(FloatType()))     return df  new_df = cast_columns(df) In\u00a0[7]: Copied! <pre>new_df.show(2)\n</pre> new_df.show(2) <pre>+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n|households|housingMedianAge|latitude|longitude|medianHouseValue|medianIncome|population|totalBedRooms|totalRooms|\n+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\n|     126.0|            41.0|   37.88|  -122.23|        452600.0|      8.3252|     322.0|        129.0|     880.0|\n|    1138.0|            21.0|   37.86|  -122.22|        358500.0|      8.3014|    2401.0|       1106.0|    7099.0|\n+----------+----------------+--------+---------+----------------+------------+----------+-------------+----------+\nonly showing top 2 rows\n\n</pre> In\u00a0[8]: Copied! <pre>new_df.printSchema()\n</pre> new_df.printSchema() <pre>root\n |-- households: float (nullable = true)\n |-- housingMedianAge: float (nullable = true)\n |-- latitude: float (nullable = true)\n |-- longitude: float (nullable = true)\n |-- medianHouseValue: float (nullable = true)\n |-- medianIncome: float (nullable = true)\n |-- population: float (nullable = true)\n |-- totalBedRooms: float (nullable = true)\n |-- totalRooms: float (nullable = true)\n\n</pre> In\u00a0[28]: Copied! <pre>new_df.describe().show()\n</pre> new_df.describe().show() <pre>+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n|summary|       households|  housingMedianAge|         latitude|          longitude|  medianHouseValue|      medianIncome|        population|    totalBedRooms|        totalRooms|\n+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n|  count|            20640|             20640|            20640|              20640|             20640|             20640|             20640|            20640|             20640|\n|   mean|499.5396802325581|28.639486434108527|35.63186143109965|-119.56970444871473|206855.81690891474|3.8706710030346416|1425.4767441860465|537.8980135658915|2635.7630813953488|\n| stddev|382.3297528316098| 12.58555761211163|2.135952380602968|  2.003531742932898|115395.61587441359|1.8998217183639696|  1132.46212176534| 421.247905943133|2181.6152515827944|\n|    min|              1.0|               1.0|            32.54|            -124.35|           14999.0|            0.4999|               3.0|              1.0|               2.0|\n|    max|           6082.0|              52.0|            41.95|            -114.31|          500001.0|           15.0001|           35682.0|           6445.0|           39320.0|\n+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>from pyspark.sql.functions import col\n\ndf = df.withColumn('medianHouseValue', col('medianHouseValue')/100000)\n</pre> from pyspark.sql.functions import col  df = df.withColumn('medianHouseValue', col('medianHouseValue')/100000) In\u00a0[10]: Copied! <pre>df.first()\n</pre> df.first() Out[10]: <pre>Row(households='126.000000', housingMedianAge='41.000000', latitude='37.880000', longitude='-122.230000', medianHouseValue=4.526, medianIncome='8.325200', population='322.000000', totalBedRooms='129.000000', totalRooms='880.000000')</pre> In\u00a0[11]: Copied! <pre># add rooms per household\ndf = df.withColumn('roomsPerHousehold', col('totalRooms')/col('households'))\n\n# add population per household (num people in the home)\ndf = df.withColumn('popPerHousehold', col('population')/col('households'))\n\n# add bedrooms per room\ndf = df.withColumn('bedroomsPerRoom', col('totalBedRooms')/col('totalRooms'))\n</pre> # add rooms per household df = df.withColumn('roomsPerHousehold', col('totalRooms')/col('households'))  # add population per household (num people in the home) df = df.withColumn('popPerHousehold', col('population')/col('households'))  # add bedrooms per room df = df.withColumn('bedroomsPerRoom', col('totalBedRooms')/col('totalRooms')) In\u00a0[12]: Copied! <pre>df.first()\n</pre> df.first() Out[12]: <pre>Row(households='126.000000', housingMedianAge='41.000000', latitude='37.880000', longitude='-122.230000', medianHouseValue=4.526, medianIncome='8.325200', population='322.000000', totalBedRooms='129.000000', totalRooms='880.000000', roomsPerHousehold=6.984126984126984, popPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)</pre> In\u00a0[13]: Copied! <pre>df.columns\n</pre> df.columns Out[13]: <pre>['households',\n 'housingMedianAge',\n 'latitude',\n 'longitude',\n 'medianHouseValue',\n 'medianIncome',\n 'population',\n 'totalBedRooms',\n 'totalRooms',\n 'roomsPerHousehold',\n 'popPerHousehold',\n 'bedroomsPerRoom']</pre> In\u00a0[14]: Copied! <pre>df = df.select('medianHouseValue','households',\n 'housingMedianAge',\n 'latitude',\n 'longitude',\n 'medianIncome',\n 'population',\n 'totalBedRooms',\n 'totalRooms',\n 'roomsPerHousehold',\n 'popPerHousehold',\n 'bedroomsPerRoom')\n</pre> df = df.select('medianHouseValue','households',  'housingMedianAge',  'latitude',  'longitude',  'medianIncome',  'population',  'totalBedRooms',  'totalRooms',  'roomsPerHousehold',  'popPerHousehold',  'bedroomsPerRoom') <p>Create a new DataFrame that explicitly labels the columns as labels and features. <code>DenseVector</code> is used to temporarily convert the data into numpy array and regroup into a named column DataFrame</p> In\u00a0[15]: Copied! <pre>from pyspark.ml.linalg import DenseVector\n\n# return a tuple of first column and all other columns\ntemp_data = df.rdd.map(lambda x:(x[0], DenseVector(x[1:])))\n\n#construct back a new DataFrame\ndf2 = spark.createDataFrame(temp_data, ['label','features'])\n</pre> from pyspark.ml.linalg import DenseVector  # return a tuple of first column and all other columns temp_data = df.rdd.map(lambda x:(x[0], DenseVector(x[1:])))  #construct back a new DataFrame df2 = spark.createDataFrame(temp_data, ['label','features']) In\u00a0[16]: Copied! <pre>df2.take(2)\n</pre> df2.take(2) Out[16]: <pre>[Row(label=4.526, features=DenseVector([126.0, 41.0, 37.88, -122.23, 8.3252, 322.0, 129.0, 880.0, 6.9841, 2.5556, 0.1466])),\n Row(label=3.585, features=DenseVector([1138.0, 21.0, 37.86, -122.22, 8.3014, 2401.0, 1106.0, 7099.0, 6.2381, 2.1098, 0.1558]))]</pre> In\u00a0[17]: Copied! <pre># use StandardScaler to scale the features to std normal distribution\nfrom pyspark.ml.feature import StandardScaler\n\ns_scaler_model = StandardScaler(inputCol='features', outputCol='features_scaled')\nscaler_fn = s_scaler_model.fit(df2)\nscaled_df = scaler_fn.transform(df2)\n\nscaled_df.take(2)\n</pre> # use StandardScaler to scale the features to std normal distribution from pyspark.ml.feature import StandardScaler  s_scaler_model = StandardScaler(inputCol='features', outputCol='features_scaled') scaler_fn = s_scaler_model.fit(df2) scaled_df = scaler_fn.transform(df2)  scaled_df.take(2) Out[17]: <pre>[Row(label=4.526, features=DenseVector([126.0, 41.0, 37.88, -122.23, 8.3252, 322.0, 129.0, 880.0, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3296, 3.2577, 17.7345, -61.0073, 4.3821, 0.2843, 0.3062, 0.4034, 2.8228, 0.2461, 2.5264])),\n Row(label=3.585, features=DenseVector([1138.0, 21.0, 37.86, -122.22, 8.3014, 2401.0, 1106.0, 7099.0, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.9765, 1.6686, 17.7251, -61.0023, 4.3696, 2.1202, 2.6255, 3.254, 2.5213, 0.2031, 2.6851]))]</pre> In\u00a0[18]: Copied! <pre>train_data, test_data = scaled_df.randomSplit([.8,.2], seed=101)\n</pre> train_data, test_data = scaled_df.randomSplit([.8,.2], seed=101) In\u00a0[19]: Copied! <pre>type(train_data)\n</pre> type(train_data) Out[19]: <pre>pyspark.sql.dataframe.DataFrame</pre> In\u00a0[21]: Copied! <pre>from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(labelCol='label', maxIter=20)\n\nlinear_model = lr.fit(train_data)\n</pre> from pyspark.ml.regression import LinearRegression  lr = LinearRegression(labelCol='label', maxIter=20)  linear_model = lr.fit(train_data) In\u00a0[22]: Copied! <pre>type(linear_model)\n</pre> type(linear_model) Out[22]: <pre>pyspark.ml.regression.LinearRegressionModel</pre> In\u00a0[23]: Copied! <pre>linear_model.coefficients\n</pre> linear_model.coefficients Out[23]: <pre>DenseVector([0.0011, 0.0109, -0.4173, -0.4236, 0.4188, -0.0005, 0.0001, 0.0, 0.0275, 0.0012, 3.2844])</pre> <p>Print columns and their coefficients</p> In\u00a0[39]: Copied! <pre>list(zip(df.columns[1:], linear_model.coefficients))\n</pre> list(zip(df.columns[1:], linear_model.coefficients)) Out[39]: <pre>[('households', 0.0011435392550412861),\n ('housingMedianAge', 0.010914556934928758),\n ('latitude', -0.41728655702892636),\n ('longitude', -0.42357898833074664),\n ('medianIncome', 0.41879550542755656),\n ('population', -0.00047200983464106163),\n ('totalBedRooms', 0.00011060741530102377),\n ('totalRooms', 4.099208155268924e-05),\n ('roomsPerHousehold', 0.027483252262545631),\n ('popPerHousehold', 0.0011993665224223444),\n ('bedroomsPerRoom', 3.2844476401153044)]</pre> In\u00a0[28]: Copied! <pre>linear_model.intercept\n</pre> linear_model.intercept Out[28]: <pre>-36.56273436779799</pre> In\u00a0[31]: Copied! <pre>linear_model.summary.numInstances\n</pre> linear_model.summary.numInstances Out[31]: <pre>16535</pre> <p>MAE from training data</p> In\u00a0[35]: Copied! <pre>linear_model.summary.meanAbsoluteError * 100000\n</pre> linear_model.summary.meanAbsoluteError * 100000 Out[35]: <pre>49805.60256405839</pre> <p>Thus, MAE on training data is off by $50,000</p> In\u00a0[34]: Copied! <pre>linear_model.summary.meanSquaredError\n</pre> linear_model.summary.meanSquaredError Out[34]: <pre>0.46775402314782377</pre> In\u00a0[45]: Copied! <pre>linear_model.summary.rootMeanSquaredError * 100000\n</pre> linear_model.summary.rootMeanSquaredError * 100000 Out[45]: <pre>68392.54514549255</pre> <p>Thus, RMSE shows fitting on training data is off by $68,392</p> In\u00a0[40]: Copied! <pre>list(zip(df.columns[1:], linear_model.summary.pValues))\n</pre> list(zip(df.columns[1:], linear_model.summary.pValues)) Out[40]: <pre>[('households', 0.0),\n ('housingMedianAge', 0.0),\n ('latitude', 0.0),\n ('longitude', 0.0),\n ('medianIncome', 0.0),\n ('population', 0.0),\n ('totalBedRooms', 0.2242631044109853),\n ('totalRooms', 0.00010585023878628697),\n ('roomsPerHousehold', 0.0),\n ('popPerHousehold', 0.011952235555041435),\n ('bedroomsPerRoom', 0.0)]</pre> In\u00a0[41]: Copied! <pre>predicted = linear_model.transform(test_data)\npredicted.columns\n</pre> predicted = linear_model.transform(test_data) predicted.columns Out[41]: <pre>['label', 'features', 'features_scaled', 'prediction']</pre> In\u00a0[43]: Copied! <pre>type(predicted)\n</pre> type(predicted) Out[43]: <pre>pyspark.sql.dataframe.DataFrame</pre> In\u00a0[47]: Copied! <pre>test_predictions = predicted.select('prediction').rdd.map(lambda x:x[0])\ntest_labels = predicted.select('label').rdd.map(lambda x:x[0])\n\ntest_predictions_labels = test_predictions.zip(test_labels)\ntest_predictions_labels_df = spark.createDataFrame(test_predictions_labels, \n                                                   ['predictions','labels'])\n\ntest_predictions_labels_df.take(2)\n</pre> test_predictions = predicted.select('prediction').rdd.map(lambda x:x[0]) test_labels = predicted.select('label').rdd.map(lambda x:x[0])  test_predictions_labels = test_predictions.zip(test_labels) test_predictions_labels_df = spark.createDataFrame(test_predictions_labels,                                                     ['predictions','labels'])  test_predictions_labels_df.take(2) Out[47]: <pre>[Row(predictions=1.8357791571765532, labels=0.225),\n Row(predictions=-0.9555783395577535, labels=0.225)]</pre> In\u00a0[49]: Copied! <pre>from pyspark.ml.evaluation import RegressionEvaluator\n\nlinear_reg_eval = RegressionEvaluator(predictionCol='predictions', labelCol='labels')\n</pre> from pyspark.ml.evaluation import RegressionEvaluator  linear_reg_eval = RegressionEvaluator(predictionCol='predictions', labelCol='labels') In\u00a0[50]: Copied! <pre>linear_reg_eval.evaluate(test_predictions_labels_df)\n</pre> linear_reg_eval.evaluate(test_predictions_labels_df) Out[50]: <pre>0.6962295496358668</pre> In\u00a0[58]: Copied! <pre># mean absolute error\nprediction_mae = linear_reg_eval.evaluate(test_predictions_labels_df, \n                                          {linear_reg_eval.metricName:'mae'}) * 100000\nprediction_mae\n</pre> # mean absolute error prediction_mae = linear_reg_eval.evaluate(test_predictions_labels_df,                                            {linear_reg_eval.metricName:'mae'}) * 100000 prediction_mae Out[58]: <pre>49690.440586665725</pre> In\u00a0[59]: Copied! <pre># RMSE\nprediction_rmse = linear_reg_eval.evaluate(test_predictions_labels_df, \n                                           {linear_reg_eval.metricName:'rmse'}) * 100000\n\nprediction_rmse\n</pre> # RMSE prediction_rmse = linear_reg_eval.evaluate(test_predictions_labels_df,                                             {linear_reg_eval.metricName:'rmse'}) * 100000  prediction_rmse Out[59]: <pre>69622.95496358669</pre> In\u00a0[60]: Copied! <pre>print('(training error, prediction error)')\nprint((linear_model.summary.rootMeanSquaredError * 100000, prediction_rmse))\nprint((linear_model.summary.meanAbsoluteError * 100000, prediction_mae))\n</pre> print('(training error, prediction error)') print((linear_model.summary.rootMeanSquaredError * 100000, prediction_rmse)) print((linear_model.summary.meanAbsoluteError * 100000, prediction_mae)) <pre>(training error, prediction error)\n(68392.54514549255, 69622.95496358669)\n(49805.60256405839, 49690.440586665725)\n</pre> In\u00a0[61]: Copied! <pre>predicted_pandas_df = predicted.select('features','prediction').toPandas()\npredicted_pandas_df.head()\n</pre> predicted_pandas_df = predicted.select('features','prediction').toPandas() predicted_pandas_df.head() Out[61]: features prediction 0 [63.0, 33.0, 37.93, -122.32, 2.675, 216.0, 73.... 1.835779 1 [1439.0, 8.0, 35.43, -116.57, 2.7138, 6835.0, ... -0.955578 2 [15.0, 17.0, 33.92, -114.67, 1.2656, 29.0, 24.... -0.426930 3 [288.0, 20.0, 38.56, -121.36, 1.8288, 667.0, 3... 0.843599 4 [382.0, 52.0, 37.78, -122.41, 1.8519, 1055.0, ... 2.335877 In\u00a0[62]: Copied! <pre>predicted_pandas_df.columns\n</pre> predicted_pandas_df.columns Out[62]: <pre>Index(['features', 'prediction'], dtype='object')</pre> In\u00a0[65]: Copied! <pre>import pandas as pd\npredicted_pandas_df2 = pd.DataFrame(predicted_pandas_df['features'].values.tolist(), \n                                   columns=df.columns[1:])\n\npredicted_pandas_df2.head()\n</pre> import pandas as pd predicted_pandas_df2 = pd.DataFrame(predicted_pandas_df['features'].values.tolist(),                                     columns=df.columns[1:])  predicted_pandas_df2.head() Out[65]: households housingMedianAge latitude longitude medianIncome population totalBedRooms totalRooms roomsPerHousehold popPerHousehold bedroomsPerRoom 0 63.0 33.0 37.93 -122.32 2.6750 216.0 73.0 296.0 4.698413 3.428571 0.246622 1 1439.0 8.0 35.43 -116.57 2.7138 6835.0 1743.0 9975.0 6.931897 4.749826 0.174737 2 15.0 17.0 33.92 -114.67 1.2656 29.0 24.0 97.0 6.466667 1.933333 0.247423 3 288.0 20.0 38.56 -121.36 1.8288 667.0 332.0 1232.0 4.277778 2.315972 0.269481 4 382.0 52.0 37.78 -122.41 1.8519 1055.0 422.0 1014.0 2.654450 2.761780 0.416174 In\u00a0[66]: Copied! <pre>predicted_pandas_df2['predictedHouseValue'] = predicted_pandas_df['prediction']\n</pre> predicted_pandas_df2['predictedHouseValue'] = predicted_pandas_df['prediction'] In\u00a0[67]: Copied! <pre>predicted_pandas_df2.to_csv('CA_house_prices_predicted.csv')\n</pre> predicted_pandas_df2.to_csv('CA_house_prices_predicted.csv') In\u00a0[68]: Copied! <pre>!ls\n</pre> !ls <pre>CA_house_prices_predicted.csv  spark_sanity.ipynb\r\nensure_machine.ipynb\t       spark-warehouse\r\nhello-world.ipynb\t       Untitled.ipynb\r\nspark-ml-CA-housing.ipynb      using-spark-dataframes.ipynb\r\n</pre> In\u00a0[69]: Copied! <pre>predicted_pandas_df2.shape\n</pre> predicted_pandas_df2.shape Out[69]: <pre>(4105, 12)</pre> In\u00a0[9]: Copied! <pre>import pandas as pd\nfrom arcgis.gis import GIS\ngis = GIS(\"https://www.arcgis.com\",\"arcgis_python\")\n</pre> import pandas as pd from arcgis.gis import GIS gis = GIS(\"https://www.arcgis.com\",\"arcgis_python\") <pre>Enter password: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[3]: Copied! <pre>from arcgis.features import SpatialDataFrame\n</pre> from arcgis.features import SpatialDataFrame In\u00a0[4]: Copied! <pre>sdf = SpatialDataFrame.from_csv('CA_house_prices_predicted.csv')\nsdf.head(5)\n</pre> sdf = SpatialDataFrame.from_csv('CA_house_prices_predicted.csv') sdf.head(5) Out[4]: households housingMedianAge latitude longitude medianIncome population totalBedRooms totalRooms roomsPerHousehold popPerHousehold bedroomsPerRoom predictedHouseValue 0 63.0 33.0 37.93 -122.32 2.6750 216.0 73.0 296.0 4.698413 3.428571 0.246622 1.835779 1 1439.0 8.0 35.43 -116.57 2.7138 6835.0 1743.0 9975.0 6.931897 4.749826 0.174737 -0.955578 2 15.0 17.0 33.92 -114.67 1.2656 29.0 24.0 97.0 6.466667 1.933333 0.247423 -0.426930 3 288.0 20.0 38.56 -121.36 1.8288 667.0 332.0 1232.0 4.277778 2.315972 0.269481 0.843599 4 382.0 52.0 37.78 -122.41 1.8519 1055.0 422.0 1014.0 2.654450 2.761780 0.416174 2.335877 In\u00a0[11]: Copied! <pre>houses_predicted_fc = gis.content.import_data(sdf[:999])\nhouses_predicted_fc\n</pre> houses_predicted_fc = gis.content.import_data(sdf[:999]) houses_predicted_fc Out[11]: <pre>&lt;FeatureCollection&gt;</pre> In\u00a0[14]: Copied! <pre>ca_map = gis.map('California')\nca_map\n</pre> ca_map = gis.map('California') ca_map <p></p> In\u00a0[18]: Copied! <pre>ca_map.add_layer(houses_predicted_fc, {'renderer':'ClassedColorRenderer',\n                                      'field_name':'predictedHouseValue'})\n</pre> ca_map.add_layer(houses_predicted_fc, {'renderer':'ClassedColorRenderer',                                       'field_name':'predictedHouseValue'}) <pre> \n</pre> <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#predicting-ca-housing-prices-using-sparkmlib","title":"Predicting CA housing prices using SparkMLib\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#boiler-plate-initialize-sparksession-context","title":"Boiler plate - initialize SparkSession &amp; Context\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#about-ca-housing-dataset","title":"About CA housing dataset\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#preprocess-data","title":"Preprocess data\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#convert-rdd-to-spark-dataframe","title":"Convert RDD to Spark DataFrame\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#exploratory-data-analysis","title":"Exploratory data analysis\u00b6","text":"<p>Print the summary stats of the table</p>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#feature-engineering","title":"Feature engineering\u00b6","text":"<p>Add more columns such as 'number of bedrooms per room', 'rooms per household'. Also scale the 'medianHouseValue' by 1000 so it falls within range of other numbers.</p>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#re-order-columns-and-split-table-into-label-and-features","title":"Re-order columns and split table into label and features\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#scale-data-by-shifting-mean-to-0-and-making-sd-1","title":"Scale data by shifting mean to 0 and making SD = 1\u00b6","text":"<p>This ensures all columns have similar levels of variability</p>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#split-data-into-training-and-test-sets","title":"Split data into training and test sets\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#perform-multiple-regression","title":"Perform Multiple Regression\u00b6","text":"<p>Train the model</p>"},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#inspect-model-properties","title":"Inspect model properties\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#perform-predictions","title":"Perform predictions\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#regression-evaluator","title":"Regression evaluator\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#errors-mae-rmse","title":"Errors - MAE, RMSE\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#compare-training-vs-prediction-errors","title":"Compare training vs prediction errors\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#export-data-as-a-pandas-dataframe","title":"Export data as a Pandas DataFrame\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#write-to-disk-as-csv","title":"Write to disk as CSV\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#publish-to-gis","title":"Publish to GIS\u00b6","text":""},{"location":"projects/dist-computing/pyspark/spark-ml-ca-housing/#spark-jobs","title":"Spark jobs\u00b6","text":""},{"location":"projects/dl/","title":"Deep Learning","text":""},{"location":"projects/dl/#prologue","title":"Prologue","text":"<p>GPUs are required to learn DL, this far seems clear. But why you ask.. read this for more. At a high level, GPUs by Nvidia is optimal for DL as most libraries support this hardware. <code>CUDA</code> prog language developed by Nvidia is also the most developed in terms of features.</p>"},{"location":"projects/dl/#dl-libraries-in-the-market","title":"DL Libraries in the market","text":"<p>The libraries for DL come and go; <code>theano</code>, <code>caffe</code> to name a few. <code>fastai</code> library is built on top of <code>pyTorch</code> from Facebook. Why pyTorch you ask, Rachel reasons that \"the speed at which programmers can experiment and iterate is more important than theoretical speedps\". Hence pytorch which supports dynamic computation. Dynamic computation means the program is executed in the order you wrote it. Contrast this with static computation where you define the architecture of neural net and run code on it. Tensorflow famously uses static computation, but announced support for dynamic computation recently.</p>"},{"location":"projects/dl/#uncommon-wisdom","title":"Uncommon wisdom","text":"<ul> <li>You don't need GPU in production. Rachel writes about a number of reasons why this is the case. Even if you need to train your model every day.</li> <li>You don't need to be an expert in Math</li> <li>You don't need to be an expert programmer. Knowing the above two will certainly help.</li> <li>You don't need massive datasets... hold on. With techniques like transfer learning, data augmentation make it easy to take a pre-trained model and apply it to your problem.</li> </ul>"},{"location":"projects/dl/#setup-google-colab-the-free-gpu-notebook-service","title":"Setup - Google Colab, the free GPU notebook service","text":"<p>Google colab at this point lacks an interface to see what notebooks you have and the interface to import notebooks from GitHub and other sources is hidden somewhere deep. </p> <p>However, once you open, you need to run <code>!curl -s https://course.fast.ai/setup/colab | bash</code> to prepare the colab runtime to use Fast.ai and GPU. After this, you can run fastai course materials for free on GPU. THere are some notes about GPU availability, but this is yet to be seen by me.</p>"},{"location":"projects/dl/#setup-local-gpu-machine","title":"Setup - local GPU machine","text":"<p>If you have GPU enabled hardware and want to set it up to learn Fastai, follow these instructions:</p> <ul> <li>Configure GPU on Windows</li> <li>Set up Windows OS with GPU for Fastai v1</li> <li>Set up Windows OS with GPU for Fastai v2</li> </ul>"},{"location":"projects/dl/#detailed-notes","title":"Detailed notes","text":"<ul> <li>Neural network - concepts</li> <li>Neural network - understanding backpropagation</li> <li>Deep learning concepts</li> <li>Getting started with fast.ai</li> <li>Fast.ai course lesson 1 - classifying pets<ul> <li>lesson 1 mind map courtesy of Fast.ai community members.</li> </ul> </li> <li>Fast.ai Vision</li> </ul>"},{"location":"projects/dl/configure-gpu-windows/","title":"Configure GPU on Windows OS","text":"<p>Note: This doc assumes a GPU has been properly attached on your machine. The purpose of this doc is to help you configure it correctly for deep learning.</p> <p>Jeremy Howard says don't spend time setting up your own GPU workstation for deep learning and accordingly, their documentation for the same is slim. Hence this is a doc for how to use your GPU enabled Windows workstation for learning and building on top of Fastai for deep learning.</p>"},{"location":"projects/dl/configure-gpu-windows/#step-1-check-if-you-have-a-gpu-and-the-appropriate-drivers","title":"Step 1 - Check if you have a GPU and the appropriate drivers","text":"<ul> <li> <p>Ensure your machine has GPU and appropriate drivers. As of this wiki, only Nvidia CUDA compatible drivers are usable for deep learning. You can verify this from Windows Device Manager. See pic below:</p> <p></p> </li> <li> <p>If you don't have a GPU, you need to recheck how the hardware is installed.</p> </li> <li> <p>Use the task manager to see if you have a 'GPU' section (similar to the 'CPU' section) as shown below. If you don't see the GPU section, you might be running an older build of Windows 10. We will solve this issue as well. If GPU shows up, ignore step 3.</p> <p></p> </li> <li> <p>Check if drivers for the GPU are installed. Run <code>nvidia-smi</code> from terminal. The output should look like below. If it does, ignore step 4. If you see an error, you either don't have the drivers, or you have drivers, but not added the nvidia tool to <code>%path%</code>. We will do these in the following steps.</p> <p></p> </li> </ul>"},{"location":"projects/dl/configure-gpu-windows/#step-2-optional-install-new-windows-terminal","title":"Step 2 - Optional - Install new Windows terminal","text":"<ul> <li>Unlike OSX and Linux, Windows does not have a proper terminal. The default command prompt has limited features and is cumbersome for most advanced users.</li> <li>Install the new terminal from https://github.com/Microsoft/Terminal. This is still a beta product, however adds some useful features like multiple tabs, configurable prompts, better font, better copy paste experience, resize &amp; reflow to name a few.</li> <li> <p>I was able to add Git bash, Pro Python Command Prompt, Anaconda Navigator as additional prompts for convenience (as shown below:)</p> <p></p> </li> </ul>"},{"location":"projects/dl/configure-gpu-windows/#step-3-update-windows-10-build","title":"Step 3 - Update Windows 10 build.","text":"<ul> <li>You need the the 2017 Fall update or later builds to see the GPU tab in the task manager. Your GPU should also be compatible, but the chances that it is not is slim.</li> <li> <p>To update your OS, open the command center / control panel and search for 'Update'. Then enable Windows to update. Your machine might restart multiple times depending on where you are on the update cycle. My machine was reimaged to have the latest OS and below is the OS version for reference:</p> <p></p> </li> </ul>"},{"location":"projects/dl/configure-gpu-windows/#step-4-install-gpu-drivers","title":"Step 4 - Install GPU drivers","text":"<ul> <li>If you suspect NVIDIA driver is not installed, you need to download it from https://www.nvidia.com/Download/index.aspx?lang=en-us. Enter your GPU type and download the drivers. Install the drivers.</li> <li>Try to rerun the <code>nvidia-smi</code> tool. If you see an error that such a tool is not found, search for it in the Program Files. In my case, it was located at <code>C:\\Program Files\\NVIDIA Corporation\\NVSMI</code>.</li> <li>Add <code>C:\\Program Files\\NVIDIA Corporation\\NVSMI</code> to Windows Path. Here is a help article for this. Then retry the command.</li> </ul>"},{"location":"projects/dl/coursera-neural-nets-concepts/","title":"Neural networks - concepts","text":""},{"location":"projects/dl/coursera-neural-nets-concepts/#why-use-neural-nets","title":"Why use neural nets?","text":"<p>Consider a classification problem where the decision boundary is non-linear as shown below:</p> <p></p> <p>We can represent non-linearity in a linear model by adding higher order features. However, when the original dataset already comes with a large number of features (say <code>100</code>), then feature engineered features increases by \\(\\frac{O(n^{2})}{2}\\) if we want to include quadratic features. Thus, for input data set with <code>100</code> features, the feature engineered features is in the order of <code>5000s</code>. Fitting a model on such a data set is expensive, further, the model will overfit. Furthermore, if we want to represent cubic features, then order increases to \\(O(n^{3})\\).</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#why-not-traditional-ml","title":"Why not traditional ML?","text":"<p>Image classification is also a non-linear problem. This is because the algorithm sees images as matrices. In the graphic below, we build a training set that classifies cars from non-cars.</p> <p></p> <p>Each pixel in the image is now a feature. Thus a <code>50x50</code> grayscale image has <code>2500</code> features! Since the decision boundary is usually non-linear, the number of feature required for a quadratic fit is <code>3 million</code> features. Trying to fit a logistic regression to this dataset is not feasible.</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#why-are-neural-nets-powerful","title":"Why are neural nets powerful?","text":"<p>Neural nets mimic the biological neural nets found in animal brains. In brains, specific regions are responsible for specific functions. However, when scientists have conducted experiments where they would cut the signals from the ear to the sound processing region and rewrite the signals from eyes to it, the sound processing region now learns to process vision and functions just as good as the original vision processing engine. Similarly, they were able to repeat this for touch as well. Animal brain is effective as each region is not a bunch of complex algorithms, instead, most regions are general purpose systems built to infer data / signals.</p> <p>An example of this approach are usecases for differently abled people shown below:</p> <p></p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#neural-net-representation","title":"Neural net representation","text":"<p>The physical neuron in a brain looks like below. It has a set of dendrites which act as inputs, a processing engine and the axon which acts as output.</p> <p></p> <p>ANNs model these 3 parts of the neuron as shown below. A set of inputs, multiplied by their weights are fed to an activation function, which is a logit or sigmoid function.</p> <p></p> <p>A group of neurons working together forms a neural net. The first layer is called the input layer and the last called the output layer. Sometimes, the bias is represented as an explicit node.</p> <p></p> <p>Weights in a neural net: The graphic below shows how weights are applied in a neural net. The hypothesis function for each neuron takes the familiar \\(g(\\theta^{T}X)\\) form. <code>g</code> is the sigmoid function and \\(\\theta_{i,k}^{j}\\) represents the weight for <code>jth</code> layer, hidden node <code>i</code>, input node <code>k</code>. There is always a bias node which is represented with index <code>0</code>.</p> <p></p> <p>Thus, when you have <code>2</code> nodes in layer 1 (input) and <code>3</code> nodes in  layer 2, the dimension of the weight matrix for layer 2 is <code>3 x (2+1)</code>, we add <code>+1</code> to include the bias node in the first layer. Since weights is a matrix, we represent it with capital theta \\(\\Theta\\).</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#vectorized-implementation-of-forward-propagation","title":"Vectorized implementation of forward propagation","text":"<p>The input parameters in the previous slide can be represented as a vector \\(x\\) $$ x = \\begin{bmatrix}     x_{0}\\\\     x_{1}\\\\     x_{2}\\\\     x_{3} \\end{bmatrix} $$ The activation function can be represented as \\(a^{(j)} = g(z^{(j)})\\) where </p> \\[ z^{(2)} = \\begin{bmatrix}     z_{1}^{2}\\\\\\     z_{2}^{2}\\\\\\     z_{3}^{2} \\end{bmatrix} \\] <p>Thus, \\(z^{(2)} = \\Theta^{(1)}x\\) and \\(a^{(2)} = g(z^{(2)})\\). By extension, for the next layer, \\(z^{(3)} = \\Theta^{(2)}a^{(2)}\\) and \\(h_{\\Theta}(x) = a^{(3)} = g(z^{(3)})\\)</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#neural-nets-learn-their-own-features","title":"Neural nets learn their own features","text":"<p>If you look at the second half of the simple neural net presented earlier, it is simply a logistic regression. The inputs are however, not inputs from real world, but activations of the previous layer. Thus, neural net can create its own input features. Because of this, it is capable of representing non-linear and higher order functions, even when the real world input does not have them.</p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#logical-operations-with-neurons","title":"Logical operations with neurons","text":"<p>Neurons in neural nets build complex representations using simple condition checks. Below is an example of how logical <code>AND</code>, <code>OR</code> operators are represented:</p> <p></p> <p>Then, by simply changing the weights, the same neuron can be switched to an <code>OR</code> operator:</p> <p></p> <p>Why are these useful? Many layers of such neurons can build to represent more complex decision boundaries such as <code>XOR</code> or <code>XNOR</code> or even non-linear boundaries. Below is an example of how <code>2</code> layers of NN are used to build <code>XNOR</code> gate using <code>OR</code>, <code>AND</code>, <code>NOR</code> gates. <code>XNOR</code> gives <code>1</code> if both <code>x1</code>, <code>x2</code> are <code>0</code> or <code>1</code>.</p> <p></p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#multiclass-classification-with-nn","title":"Multiclass classification with NN","text":"<p>Multiclass classification in NN is essentially a on-vs-all classification. The output layer has as many nodes as the number of classes. Further, the value of the output layer looks like one-hot encoding</p> <p></p>"},{"location":"projects/dl/coursera-neural-nets-concepts/#ocr-on-mnist-digits-database-using-nn","title":"OCR on MNIST digits database using NN","text":"<p>The MNIST database has <code>14</code> million images of handdrawn digits. We work with a subset of <code>5000</code> images. Each image is <code>20x20</code> pixels. When laid out as a column vector (which is how Neural Nets and log reg algorithms will read it), we get a <code>1x400</code> row vector. A sample of 100 images is below:</p> <p></p> <p>When classifying these digits, we work with <code>1</code> image at a time. This is unlike linear or logistic regression where we would represent the whole training set as matrix <code>X</code>. Here, we treat each pixel as a feature. Thus our input layer has <code>400+1</code> nodes (1 added to represent bias). The hidden layer from pre-trained network has <code>25</code> nodes. The output layer should have <code>10</code> nodes to represent the <code>10</code> classes we predict.</p> <p>Thus, input layer is x = \\(a^{(1)}_{401x1}\\). The weight matrix</p> \\[ a^{(1)} = x_{401x1} \\] \\[ z^{(2)} = \\Theta^{(1)}_{25x401} . a^{(1)} \\] \\[ a^{(2)}_{25x1} = sigmoid(z^{(2)}) \\] <p>We will add a bias to \\(a^{(2)}\\) when computing the next layer, making it \\(a^{(2)}_{26x1}\\)</p> \\[ z^{(3)} = \\Theta^{(2)}_{10x26} . a^{(2)} \\] \\[ a^{(3)}_{10x1} = sigmoid(z^{(3)}) \\] \\[ h_{\\Theta}(x) = max(sigmoid(a^{(3)})) \\] <p>The implementation code can be see here.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/","title":"Understanding backpropagation in neural networks","text":"<p>This page talks about the formula, intuition and the mechanics of the backpropagation optimization function. </p> <p>A neural network is composed of several layers of neurons connected to one another. Each neuron is activated when the sum of weights multiplied by the values of the neurons in previous layer is over a threshold when you pass it through a sigmoid / logit function. The threshold is usually <code>0.5</code>. In order for a neural network to get trained, the weights of all the different neurons need to be adjusted to yield the minimal loss. Backpropagation is the process by which the weights are adjusted during the training process. To understand how this works, we need to understand how to calculate the loss/cost function of a neural network.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#cost-function-of-a-neural-network","title":"Cost function of a neural network","text":"<p>The cost function of a NN builds on the cost function of a logistic regression. Recall from Logistic regression page, the cost function of a logistic regression is</p> \\[ J(\\theta) = -\\frac{1}{m}\\left[\\sum_{i=1}^{m}y^{(i)}log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))\\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2} \\] <p>where \\(m\\) is number of training samples and \\(n\\) is number of parameters.</p> <p>A neural network is like a multi-class logistic regression. Thus, we need to sum over \\(K\\) classes. In reality, \\(K\\) refers to the number of nodes in the output layer. Thus, in binary classification, although the number of classes is <code>2</code>, the number of nodes in output layer is just <code>1</code>. Thus <code>K=1</code>. The cost function is given as</p> \\[ J(\\Theta) = \\frac{-1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} \\left[ y_{k}^{(i)}log(h_{\\theta}(x_{k}^{(i)})) + (1-y_{k}^{(i)})log(1-h_{\\theta}(x_{k}^{(i)}))\\right] \\] <p>where  - \\(m\\) is number of training samples. \\(i=1\\;to\\;m\\)  - \\(K\\) is number of nodes in output layer and \\(k=1 \\; to \\; K\\)</p> <p>The above formula is cost without penalty or regularization. It is similar to the cost of a multiclass logistic regression. In practice, for logistic regression, we compute cost for training data as a whole. In NN, we compute cost for each training sample per by summing the loss over each class, each sample and finally dividing by number of samples.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#cost-function-of-neural-network-with-regularization","title":"Cost function of neural network with regularization","text":"<p>We penalize a network for the number of hidden layers and the number of nodes in each layer, simple. The penalty is simply the square of weights for each node, summed up and divided by twice the number of training samples. We finally multiply this result by a factor denoted by \\(\\lambda\\). Thus</p> \\[ J(\\Theta) = \\frac{-1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K}              \\left[                  y_{k}^{(i)}log(h_{\\theta}(x_{k}^{(i)})) +              (1-y_{k}^{(i)})log(1-h_{\\theta}(x_{k}^{(i)}))             \\right] +              \\frac{\\lambda}{2m} \\left[                                       \\sum_{l=1}^{L-1}\\sum_{r=1}^{s_{l}}\\sum_{c=1}^{s_{l+1}} (\\Theta_{r,c}^{l})^{2}                                 \\right] \\] <p>where </p> <ul> <li>\\(m\\) is number of training samples. \\(i=1\\;to\\;m\\)</li> <li>\\(K\\) is number of nodes in output layer and \\(k=1 \\; to \\; K\\)</li> <li>\\(L\\) is number of layers in the network and \\(l=1 \\; to \\; L-1\\) as we exclude the input layer</li> <li>\\(s_{l}\\) is number of nodes in a given layer \\(l\\). </li> <li>In the penalty term we sum from for each row and column of the weight matrix. That is, from \\(r=1 \\; to \\; s_{l}\\) and \\(c=1 \\; to \\; s_{l+1}\\) <code>r</code> and <code>c</code> represent the rows and columns of the weight matrix for each layer. </li> </ul> <p>Intuitively, the number rows in a weight matrix depends on number of nodes in that layer. The number of columns however, depends on number of nodes in the next layer (hence summing up to \\(s_{l+1}\\)).</p> <p>From the course, the cost function is given as below. Notice a slight change in the suffix used for penalty. I changed it for clarity in my formula above.</p> <p></p> <p>The \\(\\Theta\\) (weight) matrix is 2D for each layer. When you stack all layers together, it becomes a 3D matrix.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#backpropagation","title":"Backpropagation","text":"<p>The cost function gives us a single unit of measure on how well the network performs. The value of the cost function itself is not to be interpreted as such (unlike RMSE or \\(R^{2}\\) which give you an interpretable result). However, you can compare the performance of different hyperparameters or weights by comparing the resulting loss reported by the cost function. </p> <p>Thus, the objective of backpropagation is to minimize the cost function described above using partial derivatives. For each training sample, we compute an error matrix, which reflects the difference between predicted and output values. It is straightforward to compute the error for the last layer, which is the difference between expected and predicted outputs. Progressively, we compute the error for each of the previous layers.</p> <p>Let us start by reviewing the steps in forward propagation. The vectorized implementation of it is given in the slide below:</p> <p></p> <p>Next, we start by computing the error in the last layer. </p> \\[ \\delta_{j}^{(4)} = a_{j}^{(4)} - y_{j} $$$$ Where \\; j \\; stands \\; for \\; each \\; node \\; in \\; the \\; layer.\\; In \\; vectorized \\; terms $$$$ \\delta^{(4)} = a^{(4)} - y \\] <p>The error vector is now simply the difference between the expected and predicted vectors. We can compute the delta terms for the earlier layers of the network as follows:</p> \\[ \\delta^{(3)} = (\\Theta^{(3)})^{T}\\delta^{(4)}.*g'(z^{(3)}) \\] <p>Where \\(g'\\) for <code>l=3</code> is</p> \\[ g' = a^{(2)}.*(1-a^{(2)}) \\] <p>and so on..</p> <p>We don't calculate error for layer 1. Thus no \\(\\delta^{(1)}\\). The equations above give the formulae to calculate error for one training sample. When calculating the backprop for many training samples, we first perform forward propagation, compute the weights, then immediately, perform backprop to calculate the error terms and update the weights immediately. Next, we proceed to the next training sample.</p> <p></p> <p>The figure above shows the vectorized implementations of calculating the error term for all samples as a 2D matrix \\(\\Delta_{ij}^{(l)}\\), where \\(i=1 to m\\) represents individual training samples and \\(j\\) and \\(l\\) stand for number of nodes in each layer and layer number respectively. The vectorized implementation is given by</p> \\[ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^{T} \\] <p>The matrixes \\(D^(l)_{ij}\\) represent the gradient matrices.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#backpropagation-intuition","title":"Backpropagation Intuition","text":"<p>Consider the following example of a simple neural net.</p> <p></p> <p>The error term for last layer is calculated as \\(\\delta_{1}^{(4)} = y^{(i)} - a_{1}^{(4)}\\). Then, the error term for previous layers is given by</p> \\[ \\delta_{1}^{(3)} = \\Theta_{1,1}^{(3)}\\delta_{1}^{(4)} $$ $$ \\delta_{2}^{(3)} = \\Theta_{1,2}^{(3)}\\delta_{1}^{(4)} \\] <p>Similarly, for the previous layer, $$ \\delta_{1}^{(2)} = \\Theta_{1,1}^{(2)}\\delta_{1}^{(3)} + \\Theta_{2,1}^{(2)}\\delta_{2}^{(3)} $$</p> <p>Thus, to calculate the errors in each of the nodes, we need to errors in the terms to the right. In other words, we calculate the error terms from the right to the left, in reverse of the network direction, thus the name backpropagation (of errors).</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#backpropagation-implementation","title":"Backpropagation implementation","text":"<p>Unrolling: The input layer of a neural network is usually a 1-D array. Thus, when training on image data (which is at a minimum 2D), the images are flattened to a long 1D array (vector), where each row is spliced one after another. This process is called unrolling. The inverse transformation is called rolling which returns the matrix back to its n-D form.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#gradient-checking","title":"Gradient checking","text":"<p>Gradient checking is an alternate method of getting gradients through numerical approximation techniques. The idea is, consider the cost function is a curve and is a function of your weights \\(\\theta\\). The objective is to compute the gradient of the curve at different locations. This numerical approximation method computes the gradient by treating infinitesimally small segments as a straight line and compute the slope of such segments as an approximation for the gradient.</p> <p>Thus:</p> \\[ gradApprox \\; = \\; \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon} \\]"},{"location":"projects/dl/coursera-understanding-backpropagation/#random-initialization","title":"Random initialization","text":"<p>In logistic regression and linear regression, we initialized the weights to 0 and were able to compute the cost and gradients and finally arrive at the best result. However, this is not possible with neural networks. When weights are all <code>0</code> (or a constant), then each neuron is essentially identical to the rest. Thus, we need to break this symmetry by initializing the weights randomly.</p> <p>In practice, a lower and upper bound (denoted as \\([-\\epsilon_{init}, \\epsilon_{init}]\\)) is chosen and weights are assigned randomly between these values. A rule of thumb for calculating the limits is to base it on the number of units in the network as:</p> \\[ \\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}} \\] <p>where  - \\(L_{in} = s_{l}\\) and \\(L_{out} = s_{l+1}\\) which are the number of units in the current and next layer.</p>"},{"location":"projects/dl/coursera-understanding-backpropagation/#network-architecture-choices","title":"Network architecture choices","text":"<p>Architecture choices refer to number of hidden layers and number of nodes / neurons in each layer etc. In general, the number of nodes in input layer depend on number of parameters (or pixels in an image) and the number nodes in output layer depend on number of classes. </p> <p>As to the hidden layers, having <code>1</code> hidden layer is pretty common, the next common is if you are having multiple hidden layers, then the number of nodes in each hidden layer is maintained the same. By and large, the more hidden units / nodes in each layer, the better it is.</p> <p>The number of nodes in hidden layers is comparable or (slightly more than) to number of units in input layer.</p>"},{"location":"projects/dl/setup-win-fastai-v1/","title":"Set up Windows OS for Fastai v1","text":""},{"location":"projects/dl/setup-win-fastai-v1/#preparation-set-up-gpu-drivers","title":"Preparation - Set up GPU drivers","text":"<p>Follow the instructions in Configure GPU on windows page first.</p>"},{"location":"projects/dl/setup-win-fastai-v1/#install-fastai-v1","title":"Install Fastai v1","text":"<p>The steps to install v1 of Fastai is made simpler if you use the ArcGIS Deep Learning Essentials conda meta package. The steps are, from your terminal / Anaconda bash run</p> <pre><code>conda create env -n agslearn python=3.8\nconda activate agslearn\n\nconda install -c esri arcgis_learn\n</code></pre> <p>The <code>ags_learn</code> conda meta-package contains a list of all necessary Fastai and related libs for making Deep Learning and Geospatial deep learning possible on Windows and Linux.</p>"},{"location":"projects/dl/setup-win-fastai-v1/#verify-gpu-is-picked-up","title":"Verify GPU is picked up","text":"<p>From terminal or Notebook, run the following</p> <pre><code>import torch\ntorch.cuda.is_available()\n</code></pre> <p>If you get a <code>True</code>, you are good to go. Else revisit the config doc page listed above.</p> <p>Additionally, you can time how long it takes between CPU and GPU to run the same compute operation:</p> <pre><code>import torch\nt_cpu = torch.rand(800,800,800)  # uses CPU\n%timeit t_cpu @ t_cpu\n\n&gt;&gt;&gt; 4.29 s \u00b1 778 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nimport torch\nt_gpu = torch.rand(800,800,800).cuda()  # uses GPU\n%timeit t_gpu @ t_gpu\n\n&gt;&gt;&gt; 56.7 \u00b5s \u00b1 13.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>"},{"location":"projects/dl/setup-win-fastai-v2/","title":"Set up Windows OS for Fastai v2","text":""},{"location":"projects/dl/setup-win-fastai-v2/#preparation-set-up-gpu-drivers","title":"Preparation - Set up GPU drivers","text":"<p>Follow the instructions in Configure GPU on windows page first.</p>"},{"location":"projects/dl/setup-win-fastai-v2/#install-fastai-v2","title":"Install Fastai v2.","text":"<p>What did not work:  - The Fastbook install instructions don't work as the automatic Pip install steps fail. Manually running Pip install also results in conflicts.  - Running <code>conda install -c fastchan fastai</code> will not resolve as conda satsolver takes forever without results.</p> <p>What worked:  - Install Anaconda individual edition  - Create a new env: <code>conda create -n fastaiv2</code> without any packages. This provides conda a fresh start and makes it easy for the solver  - Then run <code>conda install -c fastai -c pytorch fastai</code> to install all fastaiv2 and all of its dependencies.  - If you want to run the notebooks, then run <code>conda install jupyter</code> to install Jupyter  - Optional: Clone the v2 book repo: https://github.com/fastai/fastbook  - Run <code>pip install -U fastbook</code> to install the book's deps and files on disk.</p>"},{"location":"projects/dl/setup-win-fastai-v2/#verify-gpu-is-picked-up","title":"Verify GPU is picked up","text":"<p>From terminal or Notebook, run the following</p> <pre><code>import torch\ntorch.cuda.is_available()\n</code></pre> <p>If you get a <code>True</code>, you are good to go. Else revisit the config doc page listed above.</p>"},{"location":"projects/dl/setup-win-fastai-v2/#verify-fastai-v2-can-be-imported","title":"Verify FastAI v2 can be imported","text":"<p>Run the quickstart example from the book (or copy below):</p> <pre><code>from fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n</code></pre> <p>As you run the above, open the Task Manager and monitor the GPU usage. You should see a spike while the CPU rate seems constant. The training should go much faster. Instead if you notice CPU peaking to 95-100%, then verify if <code>torch.cuda.is_available()</code> returns <code>True</code>. If it does not, then recheck the driver and OS version steps from above.</p> <p>Congrats! You are all set to use fastaiv2 on Windows OS without needing a dual boot for Linux or installing the Windows subsystem for linux options.</p>"},{"location":"projects/dl/fastai/fastai-1/","title":"Getting started with Fast.ai","text":"In\u00a0[\u00a0]: Copied! <pre>import fastai\n</pre> import fastai In\u00a0[6]: Copied! <pre>from fastai.vision import *\nfrom fastai.metrics import error_rate\n</pre> from fastai.vision import * from fastai.metrics import error_rate <p>Inside colab, importing fastai, automatically imports the <code>datasets</code> module</p> In\u00a0[4]: Copied! <pre>fastai.datasets.URLs.PETS\n</pre> fastai.datasets.URLs.PETS Out[4]: <pre>'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet'</pre> In\u00a0[10]: Copied! <pre>help(fastai.untar_data)\n</pre> help(fastai.untar_data) <pre>Help on function untar_data in module fastai.datasets:\n\nuntar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True) -&gt; pathlib.Path\n    Download `url` to `fname` if it doesn't exist, and un-tgz to folder `dest`.\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs\n                                  ).normalize(imagenet_stats)\n</pre> data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs                                   ).normalize(imagenet_stats) <p>Which gives you an Image data bunch object.</p> <p>Just calling out the object will reveal the number of training, test datasets</p> <pre>&gt;&gt;&gt;data\n\nImageDataBunch;\n\nTrain: LabelList (5912 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\nsamoyed,newfoundland,american_bulldog,american_pit_bull_terrier,saint_bernard\nPath: /content/data/oxford-iiit-pet/images;\n\nValid: LabelList (1478 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\nbasset_hound,Persian,wheaten_terrier,shiba_inu,keeshond\nPath: /content/data/oxford-iiit-pet/images;\n\nTest: None\n</pre> <p>You can query just the validation data set as below:</p> <pre>&gt;&gt;&gt; data.valid_ds.x\n\nImageList (1478 items)\nImage (3, 333, 500),Image (3, 333, 500),Image (3, 500, 333),Image (3, 500, 375),Image (3, 375, 500)\nPath: /content/data/oxford-iiit-pet/images\n</pre> <p>To visually see a sample of the training data, use</p> <pre>data.show_batch(rows=3, figsize=(7,6))\n</pre> <p></p> <p>To get the list of data classes present in the training data, use</p> <pre>&gt;&gt;&gt; print(data.classes)\nlen(data.classes),data.c\n\n\n['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']\n\n(37, 37)\n</pre> <p>fast.ai comes with several models. If you do a</p> <pre><code>dir(fastai.vision.models)\n</code></pre> <p>you get</p> <pre><code>['BasicBlock', 'Darknet', 'DynamicUnet', 'ResLayer', 'ResNet', 'SqueezeNet',\n 'UnetBlock', 'WideResNet', 'XResNet',\n 'alexnet', 'darknet', 'densenet121', 'densenet161', 'densenet169',\n 'densenet201', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50',\n 'squeezenet1_0', 'squeezenet1_1', 'unet', 'vgg16_bn', 'vgg19_bn', 'wrn',\n 'wrn_22', 'xception', 'xresnet', 'xresnet101', 'xresnet152', 'xresnet18',\n 'xresnet34', 'xresnet50']\n</code></pre> <p>The <code>learner</code> object created already is validated against a validation set. The <code>ImageDataBunch</code> object already knows which is training and which is validation. Thus the <code>error_rate</code> parameter seeks to minimize test error and thereby avoid overfitting.</p> <p>To start with, use <code>resnet34</code> which is pretty capable for most problems.</p> <p>When it comes to training, it is always recommended to use <code>fit_one_cycle()</code> rather than <code>fit()</code> method. This is to avoid overfitting. Fit one cycle is based on a 2018 paper which changed the approach to image DL. The images are shown only once and the learner is expected to figure out the pattern. Thus:</p> <pre>&gt;&gt;&gt; learn.fit_one_cycle(4)\n</pre> <p>which will run 4 times on the images. Each time it runs, it gets a bit better</p> <pre><code>Total time: 07:24\n\n epoch \ttrain_loss \tvalid_loss \terror_rate \ttime\n    0 \t1.387328 \t0.305607 \t0.084574 \t01:50\n    1 \t0.550968 \t0.220240 \t0.080514 \t01:50\n    2 \t0.353485 \t0.186418 \t0.066306 \t01:52\n    3 \t0.258271 \t0.169682 \t0.060217 \t01:51\n\nCPU times: user 1min 25s, sys: 41.1 s, total: 2min 6s\nWall time: 7min 24s\n</code></pre> <p>Thus at <code>4</code>th time, we get an error rate of <code>6%</code> or <code>94%</code> accuracy. This is phenomenal accuracy in DL speak compared to the most sophisticated approch of 2012 which got around <code>80%</code> accuracy.</p> <p>Then we save the model using</p> <pre><code>learn.save('stage01', return_path=True)\n</code></pre> <p>This stores the model along with the training data used to create it. Note: This fast.ai model is based on the <code>restnet34</code> model which is about <code>84</code>mb in size. The fast.ai model is <code>87</code>mb in size, the thin layer of specialization is about <code>3</code>mb in size now.</p> <p></p> <p>Another option is to plot all misclassifications using a confusion matrix:</p> <pre><code>interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n</code></pre> <p></p> <p>If you notice above, a lot of classes have values <code>1</code>. To view the list of classes most misclassified as a list, use:</p> <pre>&gt;&gt;&gt; learn.most_confused(min_val=2) # display descending order all values other than diagonal. Ignore 1s though.\n\n[('american_pit_bull_terrier', 'staffordshire_bull_terrier', 9),\n ('British_Shorthair', 'Russian_Blue', 6),\n ('Ragdoll', 'Birman', 6),\n ('Egyptian_Mau', 'Bengal', 3),\n ('Siamese', 'Birman', 3),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 3),\n ('yorkshire_terrier', 'havanese', 3),\n ('Bengal', 'Abyssinian', 2),\n ('Bengal', 'Egyptian_Mau', 2),\n ('Egyptian_Mau', 'Abyssinian', 2),\n ('Maine_Coon', 'Ragdoll', 2),\n ('Persian', 'Maine_Coon', 2),\n ('Ragdoll', 'Persian', 2),\n ('american_bulldog', 'american_pit_bull_terrier', 2),\n ('american_pit_bull_terrier', 'american_bulldog', 2),\n ('chihuahua', 'miniature_pinscher', 2),\n ('leonberger', 'newfoundland', 2),\n ('miniature_pinscher', 'chihuahua', 2),\n ('newfoundland', 'english_cocker_spaniel', 2),\n ('staffordshire_bull_terrier', 'american_bulldog', 2)]\n</pre> <pre>&gt;&gt;&gt; learn.lr_find()\n&gt;&gt;&gt; learn.recorder.plot()\n</pre> <p></p> <p>As you see the plot I have is quite different from what Jeremy has in his lecture. Not sure why. But the concept that loss decreases, plateaus then increases can be seen.</p> <p>I will retrian the model by unfreezing it and training with specific learning rate.</p> <pre>&gt;&gt;&gt; learn.unfreeze()\n&gt;&gt;&gt; learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-5))\n</pre> <p>which leads to</p> <pre><code>Total time: 04:17\n\nepoch \ttrain_loss \tvalid_loss \terror_rate \ttime\n0 \t0.233426 \t0.213525 \t0.067659 \t02:08\n1 \t0.214522 \t0.208705 \t0.064953 \t02:09\n</code></pre> <p>This gives me a model with error rate of <code>6.4%</code>, while I already had a <code>6%</code> error rate.</p>"},{"location":"projects/dl/fastai/fastai-1/#getting-started-with-fastai","title":"Getting started with Fast.ai\u00b6","text":"<p>As of 2019, fast.ai supports 4 types of DL applications</p> <ul> <li>computer vision</li> <li>natural language text</li> <li>tabular data</li> <li>collaborative filtering</li> </ul> <p>Fast.ai uses type hinting introduced in Python 3.5 quite heavily. Thus if you type <code>help(fastai.untar_data)</code>, you notice type hints.</p>"},{"location":"projects/dl/fastai/fastai-1/#genearl-notes","title":"Genearl notes\u00b6","text":"<ol> <li>The image dimensions used here is 224. This is a convention.</li> <li>normalizing images means turning them to (mean 0, 1 SD). This is done prior to training</li> <li><code>data.c</code> -&gt; gives number of classes. <code>data.classes</code> -&gt; gives the names of the classes.</li> <li>we use transfer learning. We pick a model that already knows something about images and tune it to our case study.</li> </ol>"},{"location":"projects/dl/fastai/fastai-1/#workflow","title":"Workflow\u00b6","text":"<ol> <li>download data into local directory</li> <li>import data files into a <code>data_bunch</code>. This process automatically creates a validation set.</li> <li>normalize</li> <li>run <code>show_batch</code> to see the classes and labels</li> <li>print the number of classes</li> <li>create a <code>ConvLearner</code> object by passing the data bunch, specifying the model architecture and metrics to use to evaluate training stats</li> <li>Fit the model. You can use <code>fit</code> or <code>fit_one_cycle</code> methods, but recommended is to use latter. Pass the epoch number (also called <code>cycles</code>)</li> <li>look at the results and if good, save by calling <code>learn.save('filename')</code></li> <li>Validation - create an <code>interpreter</code> object using <code>ClassificationInterpretation.from_learner(learn)</code>. The learn object so far knows the data and the model used to train. Now its time to validate</li> <li>Find the biggest losses using <code>interp.plot_top_losses(9, figsize=(15,11))</code>. You can also plot <code>interp.plot_confusion_matrix()</code> to view the CF matrix. Fastai also has <code>interp.most_confused(min_val=2)</code> which will return the top losses.</li> </ol>"},{"location":"projects/dl/fastai/fastai-1/#making-model-better","title":"Making model better\u00b6","text":"<ol> <li>Generally, when you call <code>fit_one_cycle</code> it only trains the last or last few layers. To improve this better, you need to call <code>learn.unfreeze()</code> to unfreeze the model.</li> </ol> <p>Next, you repeat the <code>learn.fit_one_cycle(numepochs)</code>. Sometimes, the error goes up when doing this. This happens because you have a reckless learning rate which makes the model lose it original learning. We need to be more nuanced here.</p> <ol> <li><p>Find the optimal learning rate: Now load the original model using <code>learn.load('stage-1')</code>, then run <code>learn.lr_find()</code> and find the highest learning rate that has the lowest loss.</p> </li> <li><p>With this new information retrain the model. <code>learn.unfreeze(); learn.fit_one_cycle(epochs=2, max_lr=slice(1e-6, 1e-4))</code>. What the slice suggests is, train the initial layers at start value specified and last layer at the end value specified and interpolate for the rest of the layers. It is tradecraft to make the end learning rate about 10 times smaller than rate at which errors start to increase.</p> </li> </ol>"},{"location":"projects/dl/fastai/fastai-1/#import-fastai","title":"Import fast.ai\u00b6","text":""},{"location":"projects/dl/fastai/fastai-1/#image-data-bunches","title":"Image data bunches\u00b6","text":"<p>Fast.ai has a useful called <code>ImageDataBunch</code> under the <code>fastai.vision.data</code> module. THis class helps in creating a structure of training, test data, data images, annotations etc, all into 1 class.</p> <p>To load data into an image data bunch, do</p>"},{"location":"projects/dl/fastai/fastai-1/#training-a-neural-net-in-fastai","title":"Training a neural net in fast.ai\u00b6","text":"<p>There are 2 concepts at a high level:</p> <ul> <li>DataBunch: A general fastai concept for your data, and from there, there are subclasses for particular applications like <code>ImageDataBunch</code></li> <li>Learner: A general concept for things that can learn to fit a model. From that, there are various subclasses to make things easier in particular, there is a convnet learner (something that will create a convolutional neural network for you).</li> </ul> <p>The general syntax to instantiate a learner in fast ai is as below:</p> <pre>learn = cnn_learner(&lt;DataBunch obj&gt;, &lt;models.model&gt;, metrics=error_rate)\n</pre> <p>such as</p> <pre>&gt;&gt;&gt; learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n&gt;&gt;&gt; type(learn)\nfastai.basic_train.Learner\n</pre>"},{"location":"projects/dl/fastai/fastai-1/#transfer-learning","title":"Transfer learning\u00b6","text":"<p>Resnet34 is a CNN that is trained on over a million images of various categories. This already knows to differentiatie between a large number of classes seen in everyday life. Thus, resnet34 is a generalist.</p> <p>Transfer learning is the process of taking a generalist neural net and training it to become a specialist. We train the restnet34 in lesson 1 to classify between <code>37</code> classes of cats and dogs.</p> <p>Transfer learning allows you to train nets with <code>1/100</code>th less time using <code>1/100</code> less data.</p>"},{"location":"projects/dl/fastai/fastai-1/#validation","title":"Validation\u00b6","text":"<p>Since this is a classification problem, we use confusion matrix for accuracy assessment. We create a <code>ClassificationInterpretation</code> object using the <code>Learner</code> object created earlier</p> <pre>&gt;&gt;&gt; interp = ClassificationInterpretation.from_learner(learn)\n&gt;&gt;&gt; type(interp)\nfastai.train.ClassificationInterpretation\n</pre> <p>We can plot the top losses using the <code>plot_top_losses()</code> method off the <code>Learner</code> object. This plots the top 'n' classes where the classifier has least <code>precision</code>.</p> <pre><code>interp.plot_top_losses(9, figsize=(15,11), heatmap=False)\n</code></pre>"},{"location":"projects/dl/fastai/fastai-1/#model-fine-tuning","title":"Model fine-tuning\u00b6","text":"<p>So far, we took a <code>resnet34</code> model, added a few layers to the end and trained. This was very fast. However, to improve this furture, we need to retrain the whole model, meaning, all its layers.</p> <p><code>resnet34</code> has <code>34</code> layers, <code>resnet50</code> has <code>50</code> layers. For instance, these are how the layers in resnet look like</p> <ol> <li>layer 1 - looks for edges</li> <li>layer 2 - activates for two edges, curves. Thus can detect window, table corners, circles such as clocks</li> <li>layer 3 - for patterns of layer 2 - thus can sense geometric shapes, lines of text or barcode</li> <li>layer 4 - dog faces</li> <li>layer 5 - people faces, eyeball of animaps, tires, faces of breeds of dogs</li> </ol> <p>Thus earlier, when we trained on resnet 34, we kept these layers as is and only trained on a few on top of them. To tune the model, we don't really have to change levels 1,2 which are fundamental. There are not many ways to improve levels 1,2. As levels increase, different levels of semantic complexity are handled.</p> <p>However, when we make the resnet learn on all layers, it performs worse! To balance, we unfreeze, then load the saved model we had earlier. Then we ask it to run a learning rate finder. The learning rate is pretty important, it says, \"how quickly am I updating the parameters in my model\". The general pattern of lr rate is, it improves and then degenerates after some point.</p> <p>Thus the general pattern advocated is to use the lr finder to find the shape of learning rate. Then unfreeze and call fit method with appropriate learning rate window. This trains the lower layers at a rate and higher abstraction layers are a different rate.</p>"},{"location":"projects/dl/fastai/fastai-1/#deeper-layers","title":"Deeper layers\u00b6","text":"<p>Another option is to use <code>resnet50</code>. However a typical GPU with <code>16</code>GB RAM is insufficient to handle this deep of layers.</p>"},{"location":"projects/dl/fastai/lesson1-pets/","title":"Image classification with FastAI","text":"In\u00a0[1]: Copied! <pre># install fast.ai course content.\n!curl -s https://course.fast.ai/setup/colab | bash\n</pre> # install fast.ai course content. !curl -s https://course.fast.ai/setup/colab | bash <pre>Updating fastai...\nDone.\n</pre> <p>Welcome to lesson 1! For those of you who are using a Jupyter Notebook for the first time, you can learn about this useful tool in a tutorial we prepared specially for you; click <code>File</code>-&gt;<code>Open</code> now and click <code>00_notebook_tutorial.ipynb</code>.</p> <p>In this lesson we will build our first image classifier from scratch, and see if we can achieve world-class results. Let's dive in!</p> <p>Every notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook.</p> In\u00a0[\u00a0]: Copied! <pre>#@title Default title text\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n</pre> #@title Default title text %reload_ext autoreload %autoreload 2 %matplotlib inline <p>We import all the necessary packages. We are going to work with the fastai V1 library which sits on top of Pytorch 1.0. The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models.</p> In\u00a0[\u00a0]: Copied! <pre>import fastai\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\n</pre> import fastai from fastai.vision import * from fastai.metrics import error_rate <p>If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel-&gt;Restart, uncomment the 2nd line below to use a smaller batch size (you'll learn all about what this means during the course), and try again.</p> In\u00a0[\u00a0]: Copied! <pre>bs = 64\n# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel-&gt;Restart\n</pre> bs = 64 # bs = 16   # uncomment this line if you run out of memory even after clicking Kernel-&gt;Restart <p>We are going to use the Oxford-IIIT Pet Dataset by O. M. Parkhi et al., 2012 which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate \"Image\", \"Head\", and \"Body\" models for the pet photos. Let's see how accurate we can be using deep learning!</p> <p>We are going to use the <code>untar_data</code> function to which we must pass a URL as an argument and which will download and extract the data.</p> In\u00a0[5]: Copied! <pre>help(untar_data)\n</pre> help(untar_data) <pre>Help on function untar_data in module fastai.datasets:\n\nuntar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True, force_download=False) -&gt; pathlib.Path\n    Download `url` to `fname` if it doesn't exist, and un-tgz to folder `dest`.\n\n</pre> In\u00a0[6]: Copied! <pre>URLs.PETS\n</pre> URLs.PETS Out[6]: <pre>'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet'</pre> In\u00a0[4]: Copied! <pre>path = untar_data(URLs.PETS); path\n</pre> path = untar_data(URLs.PETS); path Out[4]: <pre>PosixPath('/root/.fastai/data/oxford-iiit-pet')</pre> In\u00a0[5]: Copied! <pre>path.ls()\n</pre> path.ls() Out[5]: <pre>[PosixPath('/root/.fastai/data/oxford-iiit-pet/annotations'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images')]</pre> In\u00a0[\u00a0]: Copied! <pre>path_anno = path/'annotations'\npath_img = path/'images'\n</pre> path_anno = path/'annotations' path_img = path/'images' In\u00a0[7]: Copied! <pre>(path_anno,path_img)\n</pre> (path_anno,path_img) Out[7]: <pre>(PosixPath('/root/.fastai/data/oxford-iiit-pet/annotations'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images'))</pre> <p>The first thing we do when we approach a problem is to take a look at the data. We always need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.</p> <p>The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, <code>ImageDataBunch.from_name_re</code> gets the labels from the filenames using a regular expression.</p> In\u00a0[8]: Copied! <pre>fnames = get_image_files(path_img)\nfnames[:5]\n</pre> fnames = get_image_files(path_img) fnames[:5] Out[8]: <pre>[PosixPath('/root/.fastai/data/oxford-iiit-pet/images/japanese_chin_78.jpg'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images/leonberger_92.jpg'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images/basset_hound_194.jpg'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_33.jpg'),\n PosixPath('/root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_136.jpg')]</pre> In\u00a0[12]: Copied! <pre>help(get_image_files)\n</pre> help(get_image_files) <pre>Help on function get_image_files in module fastai.vision.data:\n\nget_image_files(c:Union[pathlib.Path, str], check_ext:bool=True, recurse=False) -&gt; Collection[pathlib.Path]\n    Return list of files in `c` that are images. `check_ext` will filter to `image_extensions`.\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(2)\npat = r'/([^/]+)_\\d+.jpg$'\n</pre> np.random.seed(2) pat = r'/([^/]+)_\\d+.jpg$' In\u00a0[14]: Copied! <pre>help(ImageDataBunch.from_name_re)\n</pre> help(ImageDataBunch.from_name_re) <pre>Help on method from_name_re in module fastai.vision.data:\n\nfrom_name_re(path:Union[pathlib.Path, str], fnames:Collection[pathlib.Path], pat:str, valid_pct:float=0.2, **kwargs) method of builtins.type instance\n    Create from list of `fnames` in `path` with re expression `pat`.\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs\n                                  ).normalize(imagenet_stats)\n</pre> data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs                                   ).normalize(imagenet_stats) In\u00a0[16]: Copied! <pre>type(data)\n</pre> type(data) Out[16]: <pre>fastai.vision.data.ImageDataBunch</pre> In\u00a0[11]: Copied! <pre>data\n</pre> data Out[11]: <pre>ImageDataBunch;\n\nTrain: LabelList (5912 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\njapanese_chin,leonberger,basset_hound,samoyed,samoyed\nPath: /root/.fastai/data/oxford-iiit-pet/images;\n\nValid: LabelList (1478 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\nnewfoundland,keeshond,chihuahua,keeshond,pug\nPath: /root/.fastai/data/oxford-iiit-pet/images;\n\nTest: None</pre> In\u00a0[12]: Copied! <pre>data.valid_ds.x\n</pre> data.valid_ds.x Out[12]: <pre>ImageList (1478 items)\nImage (3, 500, 468),Image (3, 375, 500),Image (3, 500, 332),Image (3, 375, 500),Image (3, 333, 500)\nPath: /root/.fastai/data/oxford-iiit-pet/images</pre> In\u00a0[\u00a0]: Copied! <pre>help(data.show_batch)\n</pre> help(data.show_batch) <pre>Help on method show_batch in module fastai.basic_data:\n\nshow_batch(rows:int=5, ds_type:fastai.basic_data.DatasetType=&lt;DatasetType.Train: 1&gt;, reverse:bool=False, **kwargs) -&gt; None method of fastai.vision.data.ImageDataBunch instance\n    Show a batch of data in `ds_type` on a few `rows`.\n\n</pre> In\u00a0[33]: Copied! <pre>data.show_batch(rows=3, figsize=(7,6))\n</pre> data.show_batch(rows=3, figsize=(7,6)) In\u00a0[13]: Copied! <pre>print(data.classes)\nlen(data.classes),data.c\n</pre> print(data.classes) len(data.classes),data.c <pre>['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']\n</pre> Out[13]: <pre>(37, 37)</pre> <p>Now we will start training our model. We will use a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).</p> <p>We will train for 4 epochs (4 cycles through all our data).</p> In\u00a0[14]: Copied! <pre>from pprint import pprint\npprint(dir(fastai.vision.models), compact=True)\n</pre> from pprint import pprint pprint(dir(fastai.vision.models), compact=True) <pre>['BasicBlock', 'Darknet', 'DynamicUnet', 'ResLayer', 'ResNet', 'SqueezeNet',\n 'UnetBlock', 'WideResNet', 'XResNet', '__builtins__', '__cached__', '__doc__',\n '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__',\n 'alexnet', 'darknet', 'densenet121', 'densenet161', 'densenet169',\n 'densenet201', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50',\n 'squeezenet1_0', 'squeezenet1_1', 'unet', 'vgg16_bn', 'vgg19_bn', 'wrn',\n 'wrn_22', 'xception', 'xresnet', 'xresnet101', 'xresnet152', 'xresnet18',\n 'xresnet34', 'xresnet50']\n</pre> In\u00a0[15]: Copied! <pre>%%time\nlearn = cnn_learner(data, models.resnet34, metrics=error_rate)\n</pre> %%time learn = cnn_learner(data, models.resnet34, metrics=error_rate) <pre>Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.torch/models/resnet34-333f7ec4.pth\n87306240it [00:00, 96012757.25it/s]\n</pre> <pre>CPU times: user 3.29 s, sys: 1.22 s, total: 4.51 s\nWall time: 8.05 s\n</pre> In\u00a0[16]: Copied! <pre>type(learn)\n</pre> type(learn) Out[16]: <pre>fastai.basic_train.Learner</pre> In\u00a0[17]: Copied! <pre>pprint(dir(learn), compact=True)\n</pre> pprint(dir(learn), compact=True) <pre>['TTA', '__annotations__', '__class__', '__dataclass_fields__',\n '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__',\n '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__',\n '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__',\n '__new__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__',\n '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__',\n '_test_writeable_path', 'add_time', 'backward', 'bn_wd', 'callback_fns',\n 'callbacks', 'clip_grad', 'create_opt', 'data', 'destroy', 'dl', 'export',\n 'fit', 'fit_one_cycle', 'freeze', 'freeze_to', 'get_preds', 'init',\n 'interpret', 'layer_groups', 'load', 'loss_func', 'lr_find', 'lr_range',\n 'metrics', 'mixup', 'model', 'model_dir', 'opt', 'opt_func', 'path',\n 'pred_batch', 'predict', 'purge', 'save', 'show_results', 'split', 'summary',\n 'to_fp16', 'to_fp32', 'train_bn', 'true_wd', 'tta_only', 'unfreeze',\n 'validate', 'wd']\n</pre> In\u00a0[18]: Copied! <pre>learn.model\n</pre> learn.model Out[18]: <pre>Sequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): Flatten()\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25)\n    (4): Linear(in_features=1024, out_features=512, bias=True)\n    (5): ReLU(inplace)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5)\n    (8): Linear(in_features=512, out_features=37, bias=True)\n  )\n)</pre> In\u00a0[19]: Copied! <pre>%%time\nlearn.fit_one_cycle(4)\n</pre> %%time learn.fit_one_cycle(4)  Total time: 07:56 <p> epoch train_loss valid_loss error_rate time 0 1.403851 0.349996 0.105548 01:58 1 0.554456 0.267080 0.084574 01:58 2 0.333741 0.225842 0.071719 01:59 3 0.250328 0.214102 0.071042 01:59 </p> <pre>CPU times: user 1min 26s, sys: 41.8 s, total: 2min 7s\nWall time: 7min 56s\n</pre> In\u00a0[62]: Copied! <pre>help(learn.save)\n</pre> help(learn.save) <pre>Help on method save in module fastai.basic_train:\n\nsave(name:Union[pathlib.Path, str], return_path:bool=False, with_opt:bool=True) method of fastai.basic_train.Learner instance\n    Save model and optimizer state (if `with_opt`) with `name` to `self.model_dir`.\n\n</pre> In\u00a0[20]: Copied! <pre>learn.save(name='stage-1', return_path=True)\n</pre> learn.save(name='stage-1', return_path=True) Out[20]: <pre>PosixPath('/root/.fastai/data/oxford-iiit-pet/images/models/stage-1.pth')</pre> <p>Let's see what results we have got.</p> <p>We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly.</p> <p>Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.</p> In\u00a0[\u00a0]: Copied! <pre>interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\n# len(data.valid_ds)==len(losses)==len(idxs)\n</pre> interp = ClassificationInterpretation.from_learner(learn)  losses,idxs = interp.top_losses()  # len(data.valid_ds)==len(losses)==len(idxs) In\u00a0[33]: Copied! <pre>type(interp)\n</pre> type(interp) Out[33]: <pre>fastai.train.ClassificationInterpretation</pre> In\u00a0[22]: Copied! <pre>losses\n</pre> losses Out[22]: <pre>tensor([8.1622e+00, 6.3674e+00, 6.2552e+00,  ..., 3.8147e-06, 1.9073e-06,\n        1.9073e-06])</pre> In\u00a0[23]: Copied! <pre>idxs\n</pre> idxs Out[23]: <pre>tensor([ 987,  591, 1415,  ..., 1276,   73, 1166])</pre> In\u00a0[24]: Copied! <pre>len(data.valid_ds) == len(losses) == len(idxs)\n</pre> len(data.valid_ds) == len(losses) == len(idxs) Out[24]: <pre>True</pre> In\u00a0[28]: Copied! <pre>interp.plot_top_losses(9, figsize=(15,11), heatmap=False)\n</pre> interp.plot_top_losses(9, figsize=(15,11), heatmap=False) In\u00a0[27]: Copied! <pre>help(interp.plot_top_losses)\n</pre> help(interp.plot_top_losses) <pre>Help on method _cl_int_plot_top_losses in module fastai.vision.learner:\n\n_cl_int_plot_top_losses(k, largest=True, figsize=(12, 12), heatmap:bool=True, heatmap_thresh:int=16, return_fig:bool=None) -&gt; Union[matplotlib.figure.Figure, NoneType] method of fastai.train.ClassificationInterpretation instance\n    Show images in `top_losses` along with their prediction, actual, loss, and probability of predicted class.\n\n</pre> In\u00a0[29]: Copied! <pre>interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n</pre> interp.plot_confusion_matrix(figsize=(12,12), dpi=60) In\u00a0[30]: Copied! <pre>interp.most_confused(min_val=2)  # display descending order all values other than diagonal. Ignore 1s though.\n</pre> interp.most_confused(min_val=2)  # display descending order all values other than diagonal. Ignore 1s though. Out[30]: <pre>[('american_pit_bull_terrier', 'staffordshire_bull_terrier', 9),\n ('British_Shorthair', 'Russian_Blue', 6),\n ('Ragdoll', 'Birman', 6),\n ('Egyptian_Mau', 'Bengal', 3),\n ('Siamese', 'Birman', 3),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 3),\n ('yorkshire_terrier', 'havanese', 3),\n ('Bengal', 'Abyssinian', 2),\n ('Bengal', 'Egyptian_Mau', 2),\n ('Egyptian_Mau', 'Abyssinian', 2),\n ('Maine_Coon', 'Ragdoll', 2),\n ('Persian', 'Maine_Coon', 2),\n ('Ragdoll', 'Persian', 2),\n ('american_bulldog', 'american_pit_bull_terrier', 2),\n ('american_pit_bull_terrier', 'american_bulldog', 2),\n ('chihuahua', 'miniature_pinscher', 2),\n ('leonberger', 'newfoundland', 2),\n ('miniature_pinscher', 'chihuahua', 2),\n ('newfoundland', 'english_cocker_spaniel', 2),\n ('staffordshire_bull_terrier', 'american_bulldog', 2)]</pre> In\u00a0[32]: Copied! <pre>help(interp.most_confused)\n</pre> help(interp.most_confused) <pre>Help on method most_confused in module fastai.train:\n\nmost_confused(min_val:int=1, slice_size:int=1) -&gt; Collection[Tuple[str, str, int]] method of fastai.train.ClassificationInterpretation instance\n    Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences.\n\n</pre> <p>Since our model is working as we expect it to, we will unfreeze our model and train some more.</p> In\u00a0[\u00a0]: Copied! <pre>learn.unfreeze()\n</pre> learn.unfreeze() In\u00a0[36]: Copied! <pre>%%time\nlearn.fit_one_cycle(1)\n</pre> %%time learn.fit_one_cycle(1)  Total time: 02:09 <p> epoch train_loss valid_loss error_rate time 0 0.490545 0.324573 0.106225 02:09 </p> <pre>CPU times: user 26.2 s, sys: 13.4 s, total: 39.6 s\nWall time: 2min 9s\n</pre> In\u00a0[\u00a0]: Copied! <pre>learn.summary\n# for i in learn.model.children():\n#   print(i)\n</pre> learn.summary # for i in learn.model.children(): #   print(i) In\u00a0[48]: Copied! <pre>learn.load('stage-1')\n</pre> learn.load('stage-1') Out[48]: <pre>Learner(data=ImageDataBunch;\n\nTrain: LabelList (5912 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\njapanese_chin,leonberger,basset_hound,samoyed,samoyed\nPath: /root/.fastai/data/oxford-iiit-pet/images;\n\nValid: LabelList (1478 items)\nx: ImageList\nImage (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\ny: CategoryList\nnewfoundland,keeshond,chihuahua,keeshond,pug\nPath: /root/.fastai/data/oxford-iiit-pet/images;\n\nTest: None, model=Sequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): Flatten()\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25)\n    (4): Linear(in_features=1024, out_features=512, bias=True)\n    (5): ReLU(inplace)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5)\n    (8): Linear(in_features=512, out_features=37, bias=True)\n  )\n), opt_func=functools.partial(&lt;class 'torch.optim.adam.Adam'&gt;, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[&lt;function error_rate at 0x7f3bc87b4488&gt;], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/oxford-iiit-pet/images'), model_dir='models', callback_fns=[functools.partial(&lt;class 'fastai.basic_train.Recorder'&gt;, add_time=True)], callbacks=[], layer_groups=[Sequential(\n  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace)\n  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (6): ReLU(inplace)\n  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (11): ReLU(inplace)\n  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (16): ReLU(inplace)\n  (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (21): ReLU(inplace)\n  (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (23): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (24): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n  (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (26): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (27): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (28): ReLU(inplace)\n  (29): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (30): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (31): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (33): ReLU(inplace)\n  (34): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (35): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (36): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (38): ReLU(inplace)\n  (39): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n), Sequential(\n  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace)\n  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (9): ReLU(inplace)\n  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (14): ReLU(inplace)\n  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (19): ReLU(inplace)\n  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (24): ReLU(inplace)\n  (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (29): ReLU(inplace)\n  (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (32): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (34): ReLU(inplace)\n  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (36): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (37): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n  (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (39): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (41): ReLU(inplace)\n  (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (43): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (46): ReLU(inplace)\n  (47): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n), Sequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): AdaptiveMaxPool2d(output_size=1)\n  (2): Flatten()\n  (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (4): Dropout(p=0.25)\n  (5): Linear(in_features=1024, out_features=512, bias=True)\n  (6): ReLU(inplace)\n  (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (8): Dropout(p=0.5)\n  (9): Linear(in_features=512, out_features=37, bias=True)\n)], add_time=True)</pre> In\u00a0[49]: Copied! <pre>%%time\nlearn.lr_find()\n</pre> %%time learn.lr_find() <pre>LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\nCPU times: user 15.7 s, sys: 7.82 s, total: 23.5 s\nWall time: 1min 15s\n</pre> In\u00a0[50]: Copied! <pre>learn.recorder.plot()\n</pre> learn.recorder.plot() In\u00a0[51]: Copied! <pre>learn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(1e-6,1e-5))\n</pre> learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-5))  Total time: 04:17 <p> epoch train_loss valid_loss error_rate time 0 0.233426 0.213525 0.067659 02:08 1 0.214522 0.208705 0.064953 02:09 </p> <p>That's a pretty accurate model!</p> <p>Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. It will be explained later in the course and you can learn the details in the resnet paper).</p> <p>Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let's see if we can achieve a higher performance here. To help it along, let's us use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory.</p> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),\n                                   size=299, bs=bs//2).normalize(imagenet_stats)\n</pre> data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),                                    size=299, bs=bs//2).normalize(imagenet_stats) In\u00a0[53]: Copied! <pre>learn = cnn_learner(data, models.resnet50, metrics=error_rate)\n</pre> learn = cnn_learner(data, models.resnet50, metrics=error_rate) <pre>Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.torch/models/resnet50-19c8e357.pth\n102502400it [00:03, 27500065.01it/s]\n</pre> In\u00a0[54]: Copied! <pre>%%time\nlearn.lr_find()\nlearn.recorder.plot()\n</pre> %%time learn.lr_find() learn.recorder.plot() <pre>LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\nCPU times: user 32 s, sys: 28.1 s, total: 1min\nWall time: 1min 41s\n</pre> In\u00a0[\u00a0]: Copied! <pre>learn.fit_one_cycle(8)\n</pre> learn.fit_one_cycle(8)        12.50% [1/8 03:35&lt;25:09]      epoch train_loss valid_loss error_rate time 0 0.753832 0.250755 0.072395 03:35 <p>        93.48% [172/184 02:50&lt;00:11 0.4062]      </p> In\u00a0[\u00a0]: Copied! <pre>learn.save('stage-1-50')\n</pre> learn.save('stage-1-50') <p>It's astonishing that it's possible to recognize pet breeds so accurately! Let's see if full fine-tuning helps:</p> In\u00a0[\u00a0]: Copied! <pre>learn.unfreeze()\nlearn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))\n</pre> learn.unfreeze() learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4)) <pre>Total time: 03:27\nepoch  train_loss  valid_loss  error_rate\n1      0.097319    0.155017    0.048038    (01:10)\n2      0.074885    0.144853    0.044655    (01:08)\n3      0.063509    0.144917    0.043978    (01:08)\n\n</pre> <p>If it doesn't, you can always go back to your previous model.</p> In\u00a0[\u00a0]: Copied! <pre>learn.load('stage-1-50');\n</pre> learn.load('stage-1-50'); In\u00a0[\u00a0]: Copied! <pre>interp = ClassificationInterpretation.from_learner(learn)\n</pre> interp = ClassificationInterpretation.from_learner(learn) In\u00a0[\u00a0]: Copied! <pre>interp.most_confused(min_val=2)\n</pre> interp.most_confused(min_val=2) Out[\u00a0]: <pre>[('american_pit_bull_terrier', 'staffordshire_bull_terrier', 6),\n ('Bengal', 'Egyptian_Mau', 5),\n ('Bengal', 'Abyssinian', 4),\n ('boxer', 'american_bulldog', 4),\n ('Ragdoll', 'Birman', 4),\n ('Egyptian_Mau', 'Bengal', 3)]</pre> In\u00a0[\u00a0]: Copied! <pre>path = untar_data(URLs.MNIST_SAMPLE); path\n</pre> path = untar_data(URLs.MNIST_SAMPLE); path Out[\u00a0]: <pre>PosixPath('/home/ubuntu/course-v3/nbs/dl1/data/mnist_sample')</pre> In\u00a0[\u00a0]: Copied! <pre>tfms = get_transforms(do_flip=False)\ndata = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26)\n</pre> tfms = get_transforms(do_flip=False) data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26) In\u00a0[\u00a0]: Copied! <pre>data.show_batch(rows=3, figsize=(5,5))\n</pre> data.show_batch(rows=3, figsize=(5,5)) In\u00a0[\u00a0]: Copied! <pre>learn = cnn_learner(data, models.resnet18, metrics=accuracy)\nlearn.fit(2)\n</pre> learn = cnn_learner(data, models.resnet18, metrics=accuracy) learn.fit(2) <pre>Total time: 00:23\nepoch  train_loss  valid_loss  accuracy\n1      0.116117    0.029745    0.991168  (00:12)\n2      0.056860    0.015974    0.994603  (00:10)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(path/'labels.csv')\ndf.head()\n</pre> df = pd.read_csv(path/'labels.csv') df.head() Out[\u00a0]: name label 0 train/3/7463.png 0 1 train/3/21102.png 0 2 train/3/31559.png 0 3 train/3/46882.png 0 4 train/3/26209.png 0 In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28)\n</pre> data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28) In\u00a0[\u00a0]: Copied! <pre>data.show_batch(rows=3, figsize=(5,5))\ndata.classes\n</pre> data.show_batch(rows=3, figsize=(5,5)) data.classes Out[\u00a0]: <pre>[0, 1]</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24)\ndata.classes\n</pre> data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24) data.classes Out[\u00a0]: <pre>[0, 1]</pre> In\u00a0[\u00a0]: Copied! <pre>fn_paths = [path/name for name in df['name']]; fn_paths[:2]\n</pre> fn_paths = [path/name for name in df['name']]; fn_paths[:2] Out[\u00a0]: <pre>[PosixPath('/home/ubuntu/course-v3/nbs/dl1/data/mnist_sample/train/3/7463.png'),\n PosixPath('/home/ubuntu/course-v3/nbs/dl1/data/mnist_sample/train/3/21102.png')]</pre> In\u00a0[\u00a0]: Copied! <pre>pat = r\"/(\\d)/\\d+\\.png$\"\ndata = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24)\ndata.classes\n</pre> pat = r\"/(\\d)/\\d+\\.png$\" data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24) data.classes Out[\u00a0]: <pre>['3', '7']</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,\n        label_func = lambda x: '3' if '/3/' in str(x) else '7')\ndata.classes\n</pre> data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,         label_func = lambda x: '3' if '/3/' in str(x) else '7') data.classes Out[\u00a0]: <pre>['3', '7']</pre> In\u00a0[\u00a0]: Copied! <pre>labels = [('3' if '/3/' in str(x) else '7') for x in fn_paths]\nlabels[:5]\n</pre> labels = [('3' if '/3/' in str(x) else '7') for x in fn_paths] labels[:5] Out[\u00a0]: <pre>['3', '3', '3', '3', '3']</pre> In\u00a0[\u00a0]: Copied! <pre>data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24)\ndata.classes\n</pre> data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24) data.classes Out[\u00a0]: <pre>['3', '7']</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/dl/fastai/lesson1-pets/#image-classification-with-fastai","title":"Image classification with FastAI\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#looking-at-the-data","title":"Looking at the data\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#training-resnet34","title":"Training: resnet34\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#results","title":"Results\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#unfreezing-fine-tuning-and-learning-rates","title":"Unfreezing, fine-tuning, and learning rates\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#training-resnet50","title":"Training: resnet50\u00b6","text":""},{"location":"projects/dl/fastai/lesson1-pets/#other-data-formats","title":"Other data formats\u00b6","text":""},{"location":"projects/dl/fastai/vision/","title":"Deep Learning on imagery using `fastai.vision` module","text":""},{"location":"projects/dl/fastai/vision/#list-of-relevant-functions-and-classes","title":"List of relevant functions and classes","text":""},{"location":"projects/dl/fastai/vision/#getting-sample-data","title":"Getting sample data","text":"<ul> <li><code>fastai.datasets</code> contains a set of curated datasets that sits on S3. You can get the list from <code>fastai.datasets.URLs</code></li> <li><code>URLs.PETS</code> for example will return the download URL.</li> <li><code>fastai.datasets.untar_data()</code> will download to a <code>.fastai/data</code> folder under local user data and will return that path as a <code>Pathlib.Path</code> object</li> </ul>"},{"location":"projects/dl/fastai/vision/#loading-image-data","title":"Loading image data","text":"<ul> <li><code>fastai.vision.data.get_image_files()</code> will scan a directory of image files and return a list of <code>Path</code> objects.</li> <li>The next step is to create an <code>ImageDataBunch</code> instance. In FastAI, <code>DataBunch</code> objects form the main way to represent and hold training and test datasets.</li> <li><code>fastai.vision.data.ImageDataBunch.from_name_re()</code> is a static, factory method allows you to construct an <code>ImageDataBunch</code> and while doing that, it can extract labels from file names. It accepts a Python Regular Expression syntax for this. You also feed it with transformations, size to resize and batch size the GPU can handle. See example below:</li> </ul> <pre><code>pat = r'/([^/]+)_\\d+.jpg$'\ndata = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs)\ndata = data.normalize(imagenet_stats)\n</code></pre> <ul> <li><code>fastai.vision.transform.get_transforms()</code> is a utility func that is used to specify and get back a list of transformation that need to applied on the DataBunch object.</li> <li>The <code>from_name_re()</code> will split the data into training and validation sets. These can be accessed via <code>data.valid_ds</code> and <code>data.train_ds</code> where, <code>data</code> is instance of <code>ImageDataBunch</code>.</li> <li><code>data.show_batch()</code> can be used to display training data in a notebook.</li> <li><code>data.classes</code> will return the label classes it has parsed using the regular expression earlier.</li> <li><code>data.batch_size</code> shows the batch size configured</li> </ul>"},{"location":"projects/dl/fastai/vision/#different-ways-of-loading-data-into-databunch-objects","title":"Different ways of loading data into DataBunch objects","text":"<ul> <li><code>data = ImageDataBunch.from_folder(path, ds_tfms, size)</code> can create it from folder, sub-folder structure</li> <li><code>data = ImageDataBunch.from_csv(path, ds_tfms, size)</code> can load it from a CSV containing file names and class values</li> <li><code>data = ImageDataBunch.from_df(path, df, ds_tfms, size)</code> can load data from a df</li> <li><code>data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms, size, label_func= lambda x:'3' if '/3/' in str(x) else '7')</code> to create from an anonymous function</li> <li><code>data = ImageDataBunch.from_lists(path, fn_paths, labels, ds_tfms, size)</code> to create from a list of class values.</li> </ul>"},{"location":"projects/dl/fastai/vision/#training","title":"Training","text":"<ul> <li><code>fastai.vision.models</code> module can list all models that are supported. For instance, <code>[mdl for mdl in dir(fastai.vision.models) if '__' not in mdl]</code> list comp will return <code>40</code> such models as of 2021.</li> </ul> <pre><code>from pprint import pprint\npprint([mdl for mdl in dir(fastai.vision.models) if '__' not in mdl], compact=True)\n\n&gt;&gt;&gt; ['BasicBlock', 'Darknet', 'DynamicUnet', 'ResLayer', 'ResNet', 'SqueezeNet',\n 'UnetBlock', 'WideResNet', 'XResNet', 'alexnet', 'darknet', 'densenet121',\n 'densenet161', 'densenet169', 'densenet201', 'mobilenet_v2', 'resnet101',\n 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'squeezenet1_0',\n 'squeezenet1_1', 'unet', 'vgg11_bn', 'vgg13_bn', 'vgg16_bn', 'vgg19_bn', 'wrn',\n 'wrn_22', 'xception', 'xresnet', 'xresnet101', 'xresnet152', 'xresnet18',\n 'xresnet18_deep', 'xresnet34', 'xresnet34_deep', 'xresnet50',\n 'xresnet50_deep']\n40\n</code></pre> <ul> <li><code>fastai.metrics.error_rate()</code> is a type of loss function, we use in training on images</li> <li><code>fastai.vision.learner.cnn_learner()</code> is a static, factory method that creates a convolutional neural network based on the backbone and loss function specified. For instance, <code>learn = cnn_learner(data, models.resnet34, metrics=error_rate)</code>.<ul> <li>Note, when creating the learner, you pass the whole data bunch - including both training and test data.</li> <li>The <code>error_rate</code> function will help in evaluating the performance on both the training data as well as the validation data.</li> </ul> </li> <li><code>learn.fit_one_cycle(cyc_len=4)</code> is used to train the <code>restnet34</code> model. The cycle length parameter determines how many times to repeat the one cycle learning. The output of this cell shows the following:</li> </ul> <pre><code>epoch   train_loss  valid_loss  error_rate  time\n0       1.361700    0.337071    0.104195    02:24\n1       0.601790    0.297722    0.089310    02:07\n2       0.380089    0.282888    0.079838    02:26\n3       0.271350    0.246164    0.071719    02:07\n\nWall time: 9min 6s\n</code></pre> <ul> <li>The output above shows at end of epoch 4, we have an error rate of <code>0.071</code> which means about <code>92.9%</code> accuracy.</li> <li>Calling <code>learn.summary()</code> returns you a high level info on each of the layers in the DL model along with summary info at the end.</li> <li>Finally, save the model by calling <code>learn.save(file='pets-lesson01-stage1', return_path=True)</code> which will return the path to the model file on disk, such as: <code>PosixPath('/Users/atma6951/.fastai/data/oxford-iiit-pet/images/models/pets-lesson01-stage-1.pth)</code> and weighs about <code>90 MB</code> in size. By default, Fastai tries to keep the models in the same location as the data bunch.</li> </ul>"},{"location":"projects/dl/fastai/vision/#model-accuracy","title":"Model accuracy","text":"<p>Once training is complete, you can use the following tools to evaluate the accuracy.</p> <ul> <li>Use <code>interp = fastai.train.ClassificationInterpretation.from_learner(learn)</code> to create an instance of <code>ClassificationInterpretation</code> class. Running this takes a while as fastai will compute the accuracy of each of the result in the validation dataset.</li> <li><code>interp.top_losses()</code> will return a tuple of losses and indices which correspond to loss value and index of that dataset in the <code>data.valid_ds</code> bunch. Since the function sorts the data by descending loss value, it supplies the index to match with original dataset.</li> <li><code>interp.plot_top_losses(k=9, figsize=(7,7))</code> will plot the top losses in a matrix along with the <code>predicted / actual / loss / probability</code> values as annotations.</li> <li><code>interp.plot_confusion_matrix(figsize=(16,16))</code> will plot the seaborn style confusion matrix with heatmap. For a <code>37</code> class problem like the pets, this matrix gets hard to read. When num classes is high and accuracy is also generally high, use,</li> <li><code>interp.most_confused(min_val=2)</code> will return a list of tuples - containing <code>prediction, actual, num_confusions</code>. The <code>min_val=2</code> tells the API to ignore cases where just <code>1</code> file is misclassified. It is essentially, the descending order of non-diagonal cells in the confusion matrix.</li> </ul>"},{"location":"projects/dl/fastai/vision/#model-fine-tuning","title":"Model fine-tuning","text":"<p>So far, the <code>fit_one_cycle()</code> method was used on <code>4</code> epochs and the training went fairly quickly (<code>10</code> mins). This is because, the <code>cnn_learner()</code> produced a model that is based on <code>resnet32</code> and added a few layers to the end. The <code>fit_one_cycle()</code> trained only those last few layers and left most of the earlier ones intact.</p> <p>This (transfer learning) principle works fairly well and gets us about <code>92%</code> accuracy. To improve this further, we need to <code>unfreeze</code> all the layers in the models and adjust their weights during the training.</p> <ul> <li><code>learn.unfreeze()</code> will unfreeze the whole model - backbone and the additional layers</li> <li><code>learn.fit_one_cycle(1)</code> thereafter will try to teach / change weights on the whole model. Often this results in lower accuracy because the initial layers of the model need not be changed as much since they often do preliminary work compared to the later layers which do the actual classification.</li> </ul> <p>To resolve the lower accuracy issue, we need to introduce learning rate and modify the weights of the initial layers much less frequently than those of the later layers. Learning rate is usually specified as a list of floats to match each layer in the model. In Fastai, we use the <code>slice(low, high)</code> Python function to evenly distribute LR between the first and last layers of the model.</p> <ul> <li>Run <code>learn.lr_find()</code> where the API will evaluate various learning rates and find the loss for each.</li> <li>Run <code>learn.recorder.plot(suggestion=True, show_grid=True)</code> to view the learning rate plot. It looks like below:</li> </ul> <p></p> <p>In general, we are not looking for learning at lowest loss, but for rate at the steepest segment of the loss curve. Using this suggestion, you can run</p> <pre><code>%%time\nlearn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(1e-6,1e-5))  # LR chosen from the suggestion in the image\n\n&gt;&gt;&gt;\nepoch   train_loss  valid_loss  error_rate  time\n    0   0.230512    0.231129    0.067659    02:13\n    1   0.246171    0.228836    0.064276    02:15\n\nWall time: 4min 28s\n</code></pre> <p>With an additional 4.5 minutes, we improved the accuracy to <code>93%</code>. You can then save that model as <code>learn.save('pets-lesson01-stage-2', return_path=True)</code> for later.</p>"},{"location":"projects/dl/fastai/vision/#predict-on-real-world-data","title":"Predict on real world data","text":"<p>To predict on any given image, use the <code>fastai.vision.image.open_image()</code> function to load an image. You get back an <code>fastai.vision.image.Image</code> object that can be passed to <code>learn.predict()</code> function.</p> <p>The prediction is a <code>tuple</code> of (<code>Category</code>, <code>category index</code>, <code>probabilities for each class</code>).</p>"},{"location":"projects/dl/fastai/vision/#hyper-parameter-tuning","title":"Hyper-parameter tuning","text":""},{"location":"projects/dl/fastai/vision/#learning-rate-too-high","title":"Learning rate too high","text":"<p>When the rate it too high, the validation loss gets obscenely high - like an impossible number. The default <code>max_lr</code> is <code>0.003</code>.</p>"},{"location":"projects/dl/fastai/vision/#learning-rate-too-low","title":"Learning rate too low","text":"<p>When the rate is too small, the model's validation drops, but very very slowly. The command <code>learn.recorder.plot_losses()</code> will plot the validation and training loss for visual interpretation. You can bump the rate by a factor of <code>10</code> or <code>100</code> and try again.</p>"},{"location":"projects/dl/fastai/vision/#training-loss-validation-loss","title":"Training loss &gt; Validation loss","text":"<p>When a model is properly trained, the training loss is typically lower than validation loss. If the training loss is greater, it means the model is not trained enough - try increasing number of epochs or increase the learning rate.</p>"},{"location":"projects/dl/fastai/vision/#too-few-epochs","title":"Too few epochs","text":"<p>Too few epochs and too low LR look alike. For instance when you train for just 1 epoch, the training loss might be greater than validation loss. Or, the difference between training and validation might be too high. Try increasing epochs or the LR.</p>"},{"location":"projects/dl/fastai/vision/#too-many-epochs","title":"Too many epochs","text":"<p>Too many epochs is too much training and can lead to <code>overfitting</code>. However, it is quite difficult to overfit in deep learning in practice. A sign of overfitting is when the model error starts increasing after a few epochs.</p>"},{"location":"projects/fun/","title":"Fun projects","text":"<p>Some uncategorized projects:</p> <ul> <li>Model complexity vs accuracy - an empirical analysis</li> <li>Verifying central limit theorem</li> <li>Verifying central limit theorem in regression</li> </ul>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/","title":"Model complexity vs accuracy - empirical anlaysis","text":"In\u00a0[101]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error <p>Create a linear stream of <code>10</code>million points between <code>-50</code> and <code>50</code>.</p> In\u00a0[102]: Copied! <pre>x = np.arange(-50,50,0.00001)\nx.shape\n</pre> x = np.arange(-50,50,0.00001) x.shape Out[102]: <pre>(10000000,)</pre> <p>Create random noise of same dimension</p> In\u00a0[103]: Copied! <pre>bias = np.random.standard_normal(x.shape)\n</pre> bias = np.random.standard_normal(x.shape) In\u00a0[104]: Copied! <pre>y2 = np.cos(x)**3 * (x**2/max(x)) + bias*5\n</pre> y2 = np.cos(x)**3 * (x**2/max(x)) + bias*5 In\u00a0[105]: Copied! <pre>x_train, x_test, y_train, y_test = train_test_split(x,y2, test_size=0.3)\n</pre> x_train, x_test, y_train, y_test = train_test_split(x,y2, test_size=0.3) In\u00a0[106]: Copied! <pre>x_train.shape\n</pre> x_train.shape Out[106]: <pre>(7000000,)</pre> <p>Plotting algorithms cannot work with millions of points, so you downsample just for plotting</p> In\u00a0[107]: Copied! <pre>stepper = int(x_train.shape[0]/1000)\nstepper\n</pre> stepper = int(x_train.shape[0]/1000) stepper Out[107]: <pre>7000</pre> In\u00a0[108]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(13,8))\nax.scatter(x[::stepper],y2[::stepper], marker='d')\nax.set_title('Distribution of training points')\n</pre> fig, ax = plt.subplots(1,1, figsize=(13,8)) ax.scatter(x[::stepper],y2[::stepper], marker='d') ax.set_title('Distribution of training points') Out[108]: <pre>Text(0.5,1,'Distribution of training points')</pre> In\u00a0[109]: Copied! <pre>def greedy_fitter(x_train, y_train, x_test, y_test, max_order=25):\n\"\"\"Fitter will try to find the best order of \n    polynomial curve fit for the given synthetic data\"\"\"\n    import time\n    train_predictions=[]\n    train_rmse=[]\n        \n    test_predictions=[]\n    test_rmse=[]\n    \n    for order in range(1,max_order+1):\n        t1 = time.time()\n        coeff = np.polyfit(x_train, y_train, deg=order)\n        n_order = order\n        count = 0\n        y_predict = np.zeros(x_train.shape)\n        while n_order &gt;=0:\n            y_predict += coeff[count]*x_train**n_order\n            count+=1\n            n_order = n_order-1\n        \n        # append to predictions\n        train_predictions.append(y_predict)\n        \n        # find training errors\n        current_train_rmse =np.sqrt(mean_squared_error(y_train, y_predict))\n        train_rmse.append(current_train_rmse)\n        \n        # predict and find test errors\n        n_order = order\n        count = 0\n        y_predict_test = np.zeros(x_test.shape)\n        while n_order &gt;=0:\n            y_predict_test += coeff[count]*x_test**n_order\n            count+=1\n            n_order = n_order-1\n        \n        # append test predictions\n        test_predictions.append(y_predict_test)\n        # find test errors\n        current_test_rmse =np.sqrt(mean_squared_error(y_test, y_predict_test))\n        test_rmse.append(current_test_rmse)\n        \n        t2 = time.time()\n        elapsed = round(t2-t1, 3)\n        print(\"Elapsed: \" + str(elapsed) + \\\n              \"s Order: \" + str(order) + \\\n              \" Train RMSE: \" + str(round(current_train_rmse, 4)) + \\\n              \" Test RMSE: \" + str(round(current_test_rmse, 4)))\n    \n    return (train_predictions, train_rmse, test_predictions, test_rmse)\n</pre> def greedy_fitter(x_train, y_train, x_test, y_test, max_order=25):     \"\"\"Fitter will try to find the best order of      polynomial curve fit for the given synthetic data\"\"\"     import time     train_predictions=[]     train_rmse=[]              test_predictions=[]     test_rmse=[]          for order in range(1,max_order+1):         t1 = time.time()         coeff = np.polyfit(x_train, y_train, deg=order)         n_order = order         count = 0         y_predict = np.zeros(x_train.shape)         while n_order &gt;=0:             y_predict += coeff[count]*x_train**n_order             count+=1             n_order = n_order-1                  # append to predictions         train_predictions.append(y_predict)                  # find training errors         current_train_rmse =np.sqrt(mean_squared_error(y_train, y_predict))         train_rmse.append(current_train_rmse)                  # predict and find test errors         n_order = order         count = 0         y_predict_test = np.zeros(x_test.shape)         while n_order &gt;=0:             y_predict_test += coeff[count]*x_test**n_order             count+=1             n_order = n_order-1                  # append test predictions         test_predictions.append(y_predict_test)         # find test errors         current_test_rmse =np.sqrt(mean_squared_error(y_test, y_predict_test))         test_rmse.append(current_test_rmse)                  t2 = time.time()         elapsed = round(t2-t1, 3)         print(\"Elapsed: \" + str(elapsed) + \\               \"s Order: \" + str(order) + \\               \" Train RMSE: \" + str(round(current_train_rmse, 4)) + \\               \" Test RMSE: \" + str(round(current_test_rmse, 4)))          return (train_predictions, train_rmse, test_predictions, test_rmse) <p>Run the model. Change the <code>max_order</code> to higher or lower if you wish</p> In\u00a0[110]: Copied! <pre>%%time\ncomplexity=50\ntrain_predictions, train_rmse, test_predictions, test_rmse = greedy_fitter(\n    x_train, y_train, x_test, y_test, max_order=complexity)\n</pre> %%time complexity=50 train_predictions, train_rmse, test_predictions, test_rmse = greedy_fitter(     x_train, y_train, x_test, y_test, max_order=complexity) <pre>Elapsed: 0.826s Order: 1 Train RMSE: 13.1708 Test RMSE: 13.1646\nElapsed: 1.264s Order: 2 Train RMSE: 13.1646 Test RMSE: 13.1582\nElapsed: 2.061s Order: 3 Train RMSE: 13.1646 Test RMSE: 13.1582\nElapsed: 2.727s Order: 4 Train RMSE: 13.1627 Test RMSE: 13.1564\nElapsed: 3.4s Order: 5 Train RMSE: 13.1627 Test RMSE: 13.1564\nElapsed: 4.144s Order: 6 Train RMSE: 13.1585 Test RMSE: 13.1519\nElapsed: 5.01s Order: 7 Train RMSE: 13.1585 Test RMSE: 13.1519\nElapsed: 5.749s Order: 8 Train RMSE: 13.0983 Test RMSE: 13.0891\nElapsed: 6.43s Order: 9 Train RMSE: 13.0983 Test RMSE: 13.0891\nElapsed: 7.193s Order: 10 Train RMSE: 12.876 Test RMSE: 12.865\nElapsed: 7.955s Order: 11 Train RMSE: 12.876 Test RMSE: 12.865\nElapsed: 8.777s Order: 12 Train RMSE: 12.4236 Test RMSE: 12.4185\nElapsed: 9.727s Order: 13 Train RMSE: 12.4236 Test RMSE: 12.4185\nElapsed: 10.495s Order: 14 Train RMSE: 11.9035 Test RMSE: 11.9015\nElapsed: 11.452s Order: 15 Train RMSE: 11.9035 Test RMSE: 11.9014\nElapsed: 11.929s Order: 16 Train RMSE: 11.6687 Test RMSE: 11.6657\nElapsed: 12.827s Order: 17 Train RMSE: 11.6687 Test RMSE: 11.6657\nElapsed: 13.863s Order: 18 Train RMSE: 11.6666 Test RMSE: 11.6638\nElapsed: 15.234s Order: 19 Train RMSE: 11.6666 Test RMSE: 11.6638\nElapsed: 15.793s Order: 20 Train RMSE: 11.2828 Test RMSE: 11.2825\nElapsed: 16.477s Order: 21 Train RMSE: 11.2828 Test RMSE: 11.2825\nElapsed: 18.752s Order: 22 Train RMSE: 10.6544 Test RMSE: 10.6509\nElapsed: 19.699s Order: 23 Train RMSE: 10.6544 Test RMSE: 10.6509\nElapsed: 20.26s Order: 24 Train RMSE: 10.6051 Test RMSE: 10.601\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 20.433s Order: 25 Train RMSE: 10.6051 Test RMSE: 10.601\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 20.777s Order: 26 Train RMSE: 10.6168 Test RMSE: 10.613\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 20.747s Order: 27 Train RMSE: 10.6168 Test RMSE: 10.613\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 22.231s Order: 28 Train RMSE: 9.7878 Test RMSE: 9.7872\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 23.836s Order: 29 Train RMSE: 9.7878 Test RMSE: 9.7872\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 25.725s Order: 30 Train RMSE: 9.5223 Test RMSE: 9.5227\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 25.587s Order: 31 Train RMSE: 9.5223 Test RMSE: 9.5227\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 25.041s Order: 32 Train RMSE: 9.3192 Test RMSE: 9.3201\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 26.645s Order: 33 Train RMSE: 9.3192 Test RMSE: 9.3201\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 27.387s Order: 34 Train RMSE: 9.2033 Test RMSE: 9.2045\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 28.049s Order: 35 Train RMSE: 9.2033 Test RMSE: 9.2045\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 29.866s Order: 36 Train RMSE: 9.1679 Test RMSE: 9.1692\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 31.415s Order: 37 Train RMSE: 9.1679 Test RMSE: 9.1692\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 33.605s Order: 38 Train RMSE: 9.1874 Test RMSE: 9.1887\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 33.52s Order: 39 Train RMSE: 9.1874 Test RMSE: 9.1886\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 33.863s Order: 40 Train RMSE: 9.1526 Test RMSE: 9.1539\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 34.658s Order: 41 Train RMSE: 9.1526 Test RMSE: 9.1539\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 35.006s Order: 42 Train RMSE: 9.0739 Test RMSE: 9.0755\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 35.865s Order: 43 Train RMSE: 9.0739 Test RMSE: 9.0755\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 36.595s Order: 44 Train RMSE: 8.3806 Test RMSE: 8.3852\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 39.269s Order: 45 Train RMSE: 8.3806 Test RMSE: 8.3852\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 38.545s Order: 46 Train RMSE: 8.4328 Test RMSE: 8.4372\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 42.502s Order: 47 Train RMSE: 8.4328 Test RMSE: 8.4372\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 41.427s Order: 48 Train RMSE: 8.5054 Test RMSE: 8.5096\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 43.643s Order: 49 Train RMSE: 8.5054 Test RMSE: 8.5096\n</pre> <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:13: RankWarning: Polyfit may be poorly conditioned\n  del sys.path[0]\n</pre> <pre>Elapsed: 43.055s Order: 50 Train RMSE: 8.5792 Test RMSE: 8.5831\nCPU times: user 41min 15s, sys: 4min 3s, total: 45min 19s\nWall time: 17min 31s\n</pre> In\u00a0[111]: Copied! <pre>%%time\nfig, axes = plt.subplots(1,1, figsize=(15,15))\naxes.scatter(x_train[::stepper], y_train[::stepper], \n             label='Original data', color='gray', marker='x')\norder=1\nfor p, r in zip(train_predictions, train_rmse):\n    axes.scatter(x_train[:stepper], p[:stepper], \n                 label='O: ' + str(order) + \" RMSE: \" + str(round(r,2)),\n                 marker='.')\n    order+=1\naxes.legend(loc=0)\naxes.set_title('Performance against training data')\n</pre> %%time fig, axes = plt.subplots(1,1, figsize=(15,15)) axes.scatter(x_train[::stepper], y_train[::stepper],               label='Original data', color='gray', marker='x') order=1 for p, r in zip(train_predictions, train_rmse):     axes.scatter(x_train[:stepper], p[:stepper],                   label='O: ' + str(order) + \" RMSE: \" + str(round(r,2)),                  marker='.')     order+=1 axes.legend(loc=0) axes.set_title('Performance against training data') <pre>CPU times: user 1.1 s, sys: 39.6 ms, total: 1.14 s\nWall time: 918 ms\n</pre> In\u00a0[112]: Copied! <pre>%%time\nfig, axes = plt.subplots(1,1, figsize=(15,15))\naxes.scatter(x_test[::stepper], y_test[::stepper], \n             label='Test data', color='gray', marker='x')\norder=1\nfor p, r in zip(test_predictions, test_rmse):\n    axes.scatter(x_test[:stepper], p[:stepper], \n                 label='O: ' + str(order) + \" RMSE: \" + str(round(r,2)),\n                 marker='.')\n    order+=1\naxes.legend(loc=0)\naxes.set_title('Performance against test data')\n</pre> %%time fig, axes = plt.subplots(1,1, figsize=(15,15)) axes.scatter(x_test[::stepper], y_test[::stepper],               label='Test data', color='gray', marker='x') order=1 for p, r in zip(test_predictions, test_rmse):     axes.scatter(x_test[:stepper], p[:stepper],                   label='O: ' + str(order) + \" RMSE: \" + str(round(r,2)),                  marker='.')     order+=1 axes.legend(loc=0) axes.set_title('Performance against test data') <pre>CPU times: user 893 ms, sys: 25.9 ms, total: 919 ms\nWall time: 901 ms\n</pre> In\u00a0[120]: Copied! <pre>ax = plt.plot(np.arange(1,complexity+1),test_rmse)\nplt.title('Bias vs Complexity'); plt.xlabel('Order of polynomial'); plt.ylabel('Test RMSE')\nax[0].axes.get_yaxis().get_major_formatter().set_useOffset(False)\nplt.savefig('Model efficiency.png')\n</pre> ax = plt.plot(np.arange(1,complexity+1),test_rmse) plt.title('Bias vs Complexity'); plt.xlabel('Order of polynomial'); plt.ylabel('Test RMSE') ax[0].axes.get_yaxis().get_major_formatter().set_useOffset(False) plt.savefig('Model efficiency.png') <p></p>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#model-complexity-vs-accuracy-empirical-anlaysis","title":"Model complexity vs accuracy - empirical anlaysis\u00b6","text":"<p>This notebook is intended as a moderate stress test for the DSX infrastructure. Notebook generates a known function, adds random noise to it and runs an ML algorithm on a wild goose chase asking it to fit and predict based on this data.</p> <p>Right now I am running this against 10 Million points. To increase the complexity, you can do two things</p> <ul> <li>Increase the number of points (direct hit)</li> <li>Increase the complexity of the function (indirect)</li> </ul>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#define-the-function","title":"Define the function\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#train-test-split","title":"Train test split\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#curve-fitting","title":"Curve fitting\u00b6","text":"<p>Let us define a function that will try to fit against the training data. It starts with lower order and sequentially increases the complexity of the model. The hope is, somewhere here is the sweet spot of low bias and variance. We will find it empirically</p>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#plot-results","title":"Plot results\u00b6","text":"<p>How well did the models fit against training data?</p>"},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#training-results","title":"Training results\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#test-results","title":"Test results\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#bias-vs-variance","title":"Bias vs Variance\u00b6","text":""},{"location":"projects/fun/curve_fitting_model-complexity-vs-accuracy/#cpu-usage-during-curve-fitting","title":"CPU usage during curve fitting\u00b6","text":""},{"location":"projects/fun/verifying_central_limit_theorem/","title":"Verifying Central Limit Theorem","text":"<p>The Central Limit Theorem states that the sampling distribution of the sampling means approaches a normal distribution as the sample size gets larger \u2014 no matter what the shape of the population distribution. This fact holds especially true for sample sizes over 30. All this is saying is that as you take more samples, especially large ones, your graph of the sample means will look more like a normal distribution.</p> In\u00a0[11]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline In\u00a0[2]: Copied! <pre>rand_1k = np.random.randint(0,100,1000)\n</pre> rand_1k = np.random.randint(0,100,1000) In\u00a0[3]: Copied! <pre>rand_1k.size\n</pre> rand_1k.size Out[3]: <pre>1000</pre> In\u00a0[12]: Copied! <pre>sns.distplot(rand_1k)\n</pre> sns.distplot(rand_1k) <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a19f2c048&gt;</pre> <p>Thus the population follows a <code>uniform</code> distribution, not a <code>normal</code> distribution. Still, we will see the distribution of our means will follow a <code>normal</code> distribution.</p> In\u00a0[4]: Copied! <pre>np.mean(rand_1k)\n</pre> np.mean(rand_1k) Out[4]: <pre>48.826</pre> In\u00a0[5]: Copied! <pre>subset_100 = np.random.choice(rand_1k, size=100, replace=False)\nsubset_100.size\n</pre> subset_100 = np.random.choice(rand_1k, size=100, replace=False) subset_100.size Out[5]: <pre>100</pre> In\u00a0[6]: Copied! <pre>np.mean(subset_100)\n</pre> np.mean(subset_100) Out[6]: <pre>43.2</pre> <p>The mean of this subset of <code>100</code> integers is <code>43.2</code>. Not close enough.</p> In\u00a0[7]: Copied! <pre># generate 50 random samples of size 100 each\nsubset_means = []\nfor i in range(0,50):\n    current_subset = np.random.choice(rand_1k, size=100, replace=False)\n    subset_means.append(np.mean(current_subset))\n</pre> # generate 50 random samples of size 100 each subset_means = [] for i in range(0,50):     current_subset = np.random.choice(rand_1k, size=100, replace=False)     subset_means.append(np.mean(current_subset)) <p>Calculate the mean of means (its meta :))</p> In\u00a0[33]: Copied! <pre>clt_mean = np.mean(subset_means)\nclt_mean\n</pre> clt_mean = np.mean(subset_means) clt_mean Out[33]: <pre>48.9768</pre> <p>Calculate the SD of the means</p> In\u00a0[34]: Copied! <pre>subset_sd = np.std(subset_means)\nsubset_sd\n</pre> subset_sd = np.std(subset_means) subset_sd Out[34]: <pre>2.657234983963594</pre> In\u00a0[37]: Copied! <pre>ax = sns.distplot(subset_means, bins=10)\n# draw mean in black\nax.axvline(clt_mean, color='black', linestyle='dashed')\n\n# draw mean +- 1 SD\nax.axvline(clt_mean + subset_sd, color='red', linestyle='dotted')\nax.axvline(clt_mean - subset_sd, color='red', linestyle='dotted')\n</pre> ax = sns.distplot(subset_means, bins=10) # draw mean in black ax.axvline(clt_mean, color='black', linestyle='dashed')  # draw mean +- 1 SD ax.axvline(clt_mean + subset_sd, color='red', linestyle='dotted') ax.axvline(clt_mean - subset_sd, color='red', linestyle='dotted') <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> Out[37]: <pre>&lt;matplotlib.lines.Line2D at 0x1a1ac5f908&gt;</pre> <p>Difference between mean of means and the population mean</p> In\u00a0[38]: Copied! <pre>np.mean(rand_1k) - clt_mean\n</pre> np.mean(rand_1k) - clt_mean Out[38]: <pre>-0.15079999999999671</pre>"},{"location":"projects/fun/verifying_central_limit_theorem/#verifying-central-limit-theorem","title":"Verifying Central Limit Theorem\u00b6","text":""},{"location":"projects/fun/verifying_central_limit_theorem/#generate-1k-random-integers","title":"Generate 1k random integers\u00b6","text":"<p>Let us use NumPy to generate <code>1000</code> random integers between the range <code>0-100</code>. Our objective is to calculate the population mean and verify if the mean obtained using CLT comes close to population mean.</p>"},{"location":"projects/fun/verifying_central_limit_theorem/#calculate-population-mean","title":"Calculate population mean\u00b6","text":""},{"location":"projects/fun/verifying_central_limit_theorem/#try-out-creating-a-subset-and-finding-its-mean","title":"Try out creating a subset and finding its mean\u00b6","text":""},{"location":"projects/fun/verifying_central_limit_theorem/#apply-clt","title":"Apply CLT.\u00b6","text":"<p>We will generate <code>50</code> samples with <code>100</code> items each and find their means.</p>"},{"location":"projects/fun/verifying_clt_in_regression/","title":"Verifying Central Limit Theorem in regression","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings('ignore') In\u00a0[80]: Copied! <pre>rand_1kx = np.random.randint(0,100,1000)\nx_mean = np.mean(rand_1kx)\nx_sd = np.std(rand_1kx)\nx_mean\n</pre> rand_1kx = np.random.randint(0,100,1000) x_mean = np.mean(rand_1kx) x_sd = np.std(rand_1kx) x_mean Out[80]: <pre>49.954</pre> In\u00a0[81]: Copied! <pre>pop_intercept = 30\npop_slope = 1.8\nerror_boost = 10\npop_error = np.random.standard_normal(size = rand_1kx.size) * error_boost\n# I added an error booster since without it, the correlation was too high.\n\ny = pop_intercept + pop_slope*rand_1kx + pop_error\ny_mean = np.mean(y)\ny_sd = np.std(y)\ny_mean\n</pre> pop_intercept = 30 pop_slope = 1.8 error_boost = 10 pop_error = np.random.standard_normal(size = rand_1kx.size) * error_boost # I added an error booster since without it, the correlation was too high.  y = pop_intercept + pop_slope*rand_1kx + pop_error y_mean = np.mean(y) y_sd = np.std(y) y_mean Out[81]: <pre>119.4183378140413</pre> <p>Make a scatter plot of <code>X</code> and <code>y</code> variables.</p> In\u00a0[82]: Copied! <pre>sns.jointplot(rand_1kx, y)\n</pre> sns.jointplot(rand_1kx, y) Out[82]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a1700b160&gt;</pre> <p><code>X</code> and <code>y</code> follow <code>uniform</code> distribution, but the error $\\epsilon$ is generated from <code>standard normal distribution</code> with a boosting factor. Let us plot its histogram to verify the distribution</p> In\u00a0[83]: Copied! <pre>sns.distplot(pop_error)\n</pre> sns.distplot(pop_error) Out[83]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a171de8d0&gt;</pre> In\u00a0[84]: Copied! <pre>from sklearn.linear_model import LinearRegression\nX_train_full = rand_1kx.reshape(-1,1)\ny_train_full = y.reshape(-1,1)\n</pre> from sklearn.linear_model import LinearRegression X_train_full = rand_1kx.reshape(-1,1) y_train_full = y.reshape(-1,1) In\u00a0[85]: Copied! <pre>y_train_full.shape\n</pre> y_train_full.shape Out[85]: <pre>(1000, 1)</pre> In\u00a0[86]: Copied! <pre>lm.fit(X_train, y_train)\n\n#print the linear model built\npredicted_pop_slope = lm.coef_[0][0]\npredicted_pop_intercept = lm.intercept_[0]\n\nprint(\"y = \" + str(predicted_pop_slope) + \"*X\" + \" + \" + str(predicted_pop_intercept))\n</pre> lm.fit(X_train, y_train)  #print the linear model built predicted_pop_slope = lm.coef_[0][0] predicted_pop_intercept = lm.intercept_[0]  print(\"y = \" + str(predicted_pop_slope) + \"*X\" + \" + \" + str(predicted_pop_intercept)) <pre>y = 1.795560991921382*X + 30.718916711669976\n</pre> In\u00a0[87]: Copied! <pre>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(rand_1kx, y, test_size=0.33)\nprint(X_train.size)\n\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n</pre> from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(rand_1kx, y, test_size=0.33) print(X_train.size)  from sklearn.linear_model import LinearRegression lm = LinearRegression() <pre>670\n</pre> In\u00a0[88]: Copied! <pre>X_train = X_train.reshape(-1,1)\nX_test = X_test.reshape(-1,1)\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n</pre> X_train = X_train.reshape(-1,1) X_test = X_test.reshape(-1,1) y_train = y_train.reshape(-1,1) y_test = y_test.reshape(-1,1) In\u00a0[89]: Copied! <pre>y_train.shape\n</pre> y_train.shape Out[89]: <pre>(670, 1)</pre> In\u00a0[90]: Copied! <pre>lm.fit(X_train, y_train)\n\n#print the linear model built\npredicted_subset_slope = lm.coef_[0][0]\npredicted_subset_intercept = lm.intercept_[0]\n\nprint(\"y = \" + str(predicted_subset_slope) + \"*X\" \n      + \" + \" + str(predicted_subset_intercept))\n</pre> lm.fit(X_train, y_train)  #print the linear model built predicted_subset_slope = lm.coef_[0][0] predicted_subset_intercept = lm.intercept_[0]  print(\"y = \" + str(predicted_subset_slope) + \"*X\"        + \" + \" + str(predicted_subset_intercept)) <pre>y = 1.794887898705644*X + 29.857924099881075\n</pre> In\u00a0[95]: Copied! <pre>y_predicted = lm.predict(X_test)\nresiduals = y_test - y_predicted\n</pre> y_predicted = lm.predict(X_test) residuals = y_test - y_predicted <p>Fitted vs Actual scatter</p> In\u00a0[96]: Copied! <pre>jax = sns.jointplot(y_test, y_predicted)\njax.set_axis_labels(xlabel='Y', ylabel='Predicted Y')\n</pre> jax = sns.jointplot(y_test, y_predicted) jax.set_axis_labels(xlabel='Y', ylabel='Predicted Y') Out[96]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a179c9048&gt;</pre> In\u00a0[98]: Copied! <pre>dax = sns.distplot(residuals)\ndax.set_title('Distribution of residuals')\n</pre> dax = sns.distplot(residuals) dax.set_title('Distribution of residuals') Out[98]: <pre>Text(0.5,1,'Distribution of residuals')</pre> In\u00a0[99]: Copied! <pre>jax = sns.jointplot(y_predicted, residuals)\njax.set_axis_labels(xlabel='Predicted Y', ylabel='Residuals')\n</pre> jax = sns.jointplot(y_predicted, residuals) jax.set_axis_labels(xlabel='Predicted Y', ylabel='Residuals') Out[99]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a1629f8d0&gt;</pre> In\u00a0[100]: Copied! <pre>jax = sns.jointplot(y_test, residuals)\njax.set_axis_labels(xlabel='Y', ylabel='Residuals')\n</pre> jax = sns.jointplot(y_test, residuals) jax.set_axis_labels(xlabel='Y', ylabel='Residuals') Out[100]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a16437eb8&gt;</pre> In\u00a0[101]: Copied! <pre>pop_df = pd.DataFrame(data={'x':rand_1kx, 'y':y})\npop_df.head()\n</pre> pop_df = pd.DataFrame(data={'x':rand_1kx, 'y':y}) pop_df.head() Out[101]: x y 0 38 85.149359 1 58 130.858406 2 15 67.280103 3 56 125.509595 4 19 55.793980 In\u00a0[102]: Copied! <pre>pop_df.shape\n</pre> pop_df.shape Out[102]: <pre>(1000, 2)</pre> In\u00a0[103]: Copied! <pre>sample_slopes = []\nsample_intercepts = []\n\nfor i in range(0,50):\n    # perform a choice on dataframe index\n    sample_index = np.random.choice(pop_df.index, size=50)\n    \n    # select the subset using that index\n    sample_df = pop_df.iloc[sample_index]\n    \n    # convert to numpy and reshape the matrix for lm.fit\n    sample_x = np.array(sample_df['x']).reshape(-1,1)\n    sample_y = np.array(sample_df['y']).reshape(-1,1)\n    \n    lm.fit(X=sample_x, y=sample_y)\n    \n    sample_slopes.append(lm.coef_[0][0])\n    sample_intercepts.append(lm.intercept_[0])\n</pre> sample_slopes = [] sample_intercepts = []  for i in range(0,50):     # perform a choice on dataframe index     sample_index = np.random.choice(pop_df.index, size=50)          # select the subset using that index     sample_df = pop_df.iloc[sample_index]          # convert to numpy and reshape the matrix for lm.fit     sample_x = np.array(sample_df['x']).reshape(-1,1)     sample_y = np.array(sample_df['y']).reshape(-1,1)          lm.fit(X=sample_x, y=sample_y)          sample_slopes.append(lm.coef_[0][0])     sample_intercepts.append(lm.intercept_[0]) <p>Plot the distribution of sample slopes and intercepts</p> In\u00a0[104]: Copied! <pre>mean_sample_slope = np.mean(sample_slopes)\nmean_sample_intercept = np.mean(sample_intercepts)\n\nfig, ax = plt.subplots(1,2, figsize=(15,6))\n\n# plot sample slopes\nsns.distplot(sample_slopes, ax=ax[0])\nax[0].set_title('Distribution of sample slopes. Mean: ' \n                + str(round(mean_sample_slope, 2)))\nax[0].axvline(mean_sample_slope, color='black')\n\n# plot sample slopes\nsns.distplot(sample_intercepts, ax=ax[1])\nax[1].set_title('Distribution of sample intercepts. Mean: ' \n                + str(round(mean_sample_intercept,2)))\nax[1].axvline(mean_sample_intercept, color='black')\n</pre> mean_sample_slope = np.mean(sample_slopes) mean_sample_intercept = np.mean(sample_intercepts)  fig, ax = plt.subplots(1,2, figsize=(15,6))  # plot sample slopes sns.distplot(sample_slopes, ax=ax[0]) ax[0].set_title('Distribution of sample slopes. Mean: '                  + str(round(mean_sample_slope, 2))) ax[0].axvline(mean_sample_slope, color='black')  # plot sample slopes sns.distplot(sample_intercepts, ax=ax[1]) ax[1].set_title('Distribution of sample intercepts. Mean: '                  + str(round(mean_sample_intercept,2))) ax[1].axvline(mean_sample_intercept, color='black') Out[104]: <pre>&lt;matplotlib.lines.Line2D at 0x1a17ed97b8&gt;</pre> In\u00a0[114]: Copied! <pre>print(\"Predicting using population\")\nprint(\"----------------------------\")\nprint(\"Error in intercept: {}\".format(pop_intercept - predicted_pop_intercept))\nprint(\"Error in slope: {}\".format(pop_slope - predicted_pop_slope))\n\nprint(\"\\n\\nPredicting using subset\")\nprint(\"----------------------------\")\nprint(\"Error in intercept: {}\".format(pop_intercept - predicted_subset_intercept))\nprint(\"Error in slope: {}\".format(pop_slope - predicted_subset_slope))\n\nprint(\"\\n\\nPredicting using a number of smaller samples\")\nprint(\"------------------------------------------------\")\nprint(\"Error in intercept: {}\".format(pop_intercept - mean_sample_intercept))\nprint(\"Error in slope: {}\".format(pop_slope - mean_sample_slope))\n</pre> print(\"Predicting using population\") print(\"----------------------------\") print(\"Error in intercept: {}\".format(pop_intercept - predicted_pop_intercept)) print(\"Error in slope: {}\".format(pop_slope - predicted_pop_slope))  print(\"\\n\\nPredicting using subset\") print(\"----------------------------\") print(\"Error in intercept: {}\".format(pop_intercept - predicted_subset_intercept)) print(\"Error in slope: {}\".format(pop_slope - predicted_subset_slope))  print(\"\\n\\nPredicting using a number of smaller samples\") print(\"------------------------------------------------\") print(\"Error in intercept: {}\".format(pop_intercept - mean_sample_intercept)) print(\"Error in slope: {}\".format(pop_slope - mean_sample_slope)) <pre>Predicting using population\n----------------------------\nError in intercept: -0.7189167116699764\nError in slope: 0.0044390080786180786\n\n\nPredicting using subset\n----------------------------\nError in intercept: 0.14207590011892535\nError in slope: 0.0051121012943560196\n\n\nPredicting using a number of smaller samples\n------------------------------------------------\nError in intercept: 0.4823977050074646\nError in slope: 0.002971759530004725\n</pre> <p>As we can see, error in quite small in all 3 cases, especially for <code>slope</code>. Prediction by averaging a number of smaller samples gives us much closer slope to population.</p> <p>For intercept, the least error was with prediction using subset, which is still interesting as prediction using the whole population yielded poorer intercept!</p> <p>In general, for really large datasets, that cannot be held in system memory, we can apply Central Limit Theorem for estimating slope and intercept by averaging over a number of smaller samples.</p>"},{"location":"projects/fun/verifying_clt_in_regression/#verifying-central-limit-theorem-in-regression","title":"Verifying Central Limit Theorem in regression\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#synthesize-the-dataset","title":"Synthesize the dataset\u00b6","text":"<p>Create <code>1000</code> random integers between <code>0</code>, <code>100</code> for <code>X</code> and create <code>y</code> such that $$ y = \\beta_{0} + \\beta_{1}X + \\epsilon $$ where $$ \\beta_{0} = 30 \\ and \\ \\beta_{1} = 1.8 \\ and \\ \\epsilon \\ = \\ standard \\ normal \\ error $$</p>"},{"location":"projects/fun/verifying_clt_in_regression/#predict-using-population","title":"Predict using population\u00b6","text":"<p>Let us predict the coefficients and intercept when using the whole dataset. We will compare this approach with CLT approach of breaking into multiple subsets and averaging the coefficients and intercepts</p>"},{"location":"projects/fun/verifying_clt_in_regression/#using-whole-population","title":"Using whole population\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#prediction-with-66-of-data","title":"Prediction with 66% of data\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#perform-predictions-and-plot-the-charts","title":"Perform predictions and plot the charts\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#predict-using-multiple-samples","title":"Predict using multiple samples\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#select-50-samples-of-size-200-and-perform-regression","title":"Select 50 samples of size 200 and perform regression\u00b6","text":""},{"location":"projects/fun/verifying_clt_in_regression/#conclusion","title":"Conclusion\u00b6","text":"<p>Here we compare the coefficients and intercepts obtained by different methods to see how CLT adds up.</p>"},{"location":"projects/math/","title":"Learn Mathematics with Python","text":"<p>One of my goals is to explain math using programming. Much has been said and done explaining programming using math, while that has led us to technological advancements, I strongly feel, math could be made more fun and elegant if we could use some high level languages to teach the beauty of it. Below are some topics needed to understand machine learning better.</p> <ul> <li>Set theory, probability, statistics</li> </ul>"},{"location":"projects/math/math-practical-odyssey-1/","title":"Mathematics - a practical odyssey","text":"<p>Complement of a set is the set of all elements not in that set, but in Universal set. It is represented as $$ A' = \\{x\\mid x \\in U ~and~ x \\not\\in A \\}$$ $$ n(A') = n(U) - n(A)$$</p> In\u00a0[2]: Copied! <pre># use itertools to calculate permutations in Python\nfrom itertools import permutations\n\n# create list of items in pool\nn = list(range(1,11))\n\nnpr_choices = list(permutations(n,3)) # since you get back a iterable\nprint(len(npr_choices))\n</pre> # use itertools to calculate permutations in Python from itertools import permutations  # create list of items in pool n = list(range(1,11))  npr_choices = list(permutations(n,3)) # since you get back a iterable print(len(npr_choices)) <pre>720\n</pre> <p>Combinations Selection without replacement and order is not important. If you change the order, that choice is not counted as a new choice. Thus, there are generally fewer combinations than permutations possible in a given scenario. Exceptions are narrow choices like choose 1 or 0 items from a pool of items. In this case, number of permutations and combinaions are the same.</p> <p>$$ _{n}C_{r} = \\frac{n!}{r! (n-r)!}$$</p> <p>Thus, the various combinations of choosing 3 balls from a bag of 10 balls are: $$ _{10}C_{3} = \\frac{10!}{3! \\cdot 7!} = 120$$</p> In\u00a0[3]: Copied! <pre># use combinations method from itertools module\nfrom itertools import combinations\n\nn = list(range(1,11))\n\nncr_choices = list(combinations(n,3))\nprint(len(ncr_choices))\n</pre> # use combinations method from itertools module from itertools import combinations  n = list(range(1,11))  ncr_choices = list(combinations(n,3)) print(len(ncr_choices)) <pre>120\n</pre> <p>Probability is the measure of likelihood of an event happening. In general, it is the ratio of number of outcomes in the event <code>E</code> to the total number of possible outcomes. Thus</p> <p>$$ p(E) = \\frac{n(E)}{n(S)}$$</p> <p>Probability is a computed value. If you are conducting an empirical study, you can measure probability of an event by measuring the relative frequency of that event. The law of large numbers states that</p> <pre><code>If an experiment is repeated a large number of times, the relative frequency of an outcome will tend to be close to the probability of the outcome.</code></pre> <p>Probability of being dealt 4 Aces in poker First find the sample space. Sample space includes all possible ways of selecting 5 cards from a pack of 52.</p> <p>Thus, n(S) = $_{52}C_{5} = 2,598,960$.</p> <p>Now, n(E) = number of ways of selecting 4 aces + 1 any other card.</p> <p>n(E) = $_{4}C_{4} \\cdot _{48}C_{1}$</p> <p>p(E) = n(E) / n(S) = (4*48)/2,598,960 = 0.00001847</p> <p>Probability of being dealt 4 of a kind.</p> <p>The, n(S) remains same. But n(E) is expanded 13 times as there are 13 different ways to select a 4 of a kind in a single pack of cards.</p> <p>p(E) = 13* (probability of 4 aces)</p> <p>Probability of getting 5 hearts.</p> <p>The n(S) remains same as $_{52}C_{5}$. Now the n(E) is written as $_{13}C_{5}$ as there are 13C5 number of ways to select a set of 5 hearts.</p> <p>Thus, p(E) = 13C5 / 52C5 = 0.0004951</p> <p>Conditional probability of event A given event B is written as $$p(A|B) = \\frac{n(A\\cap B)}{n(B)} $$</p> <p>dividing both numerator and demoninator of right side by n(S), we can rewrite this as</p>  $$p(A|B) = \\frac{p(A \\cap B)}{p(B)} $$  and, more commonly  <p>$$p(A \\cap B) = p(A|B) \\cdot p(B) $$</p> <p>The probability of selecting 1st heart is 13/52. Hence:</p> <p>$$p(B) = \\frac{13}{52}$$</p> <p>Since the 1st card is heart, you are left with 12 hearts and 51 total cards. So the probability of selecting another heart is: $$ p(A|B) = \\frac{12}{51} = 0.2352$$</p> <p>Thus:</p> <p>$$p(A \\cap B) = \\frac{12}{51} \\cdot \\frac{13}{52} = 0.0588$$</p> <p>We can also calculate probability of both hearts without conditional probability using combinatronics as below:</p> <p>$$p(A \\cap B) = \\frac{_{13}C_{2}}{_{52}C_{2}} $$</p> <p>$$p(A \\cap B) = \\frac{13}{52} \\cdot \\frac{12}{51} $$</p> <p>An example of independent events is, \"find the probability of getting a 6 when tossing a pair of die, given the first also yielded a 6\". Here the event of getting a 6 in one or first try does not affect getting 6 another time. Hence, they are simply independent events.</p> <p>Using cardinal number of sets, in set theory,</p>  $$ p(A \\cup B) = p(A) + p(B) - p(A \\cap B)$$  <p>for mutually exclusive A and B events, the $p(A \\cap B) = 0$ thus, $$ p(A \\cup B) = p(A) + p(B)$$</p> <p>Using De Morgan's laws,</p>  $$p(A' \\cap B') = p(A \\cup B)' $$  <p>and</p> <p>$$p(A' \\cup B') = p(A \\cap B)' $$</p> <p>The Area under the curve = 1. Here $\\mu$ is population mean and $\\sigma$ is population standard deviation. Thus, 68% values vall within $\\mu \\pm 1 \\sigma$ and 95% with $\\mu \\pm 2 \\sigma$ and 99.74% within $\\mu \\pm 3 \\sigma$. We use the Z table to find the area / probability for any other values of x.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/math/math-practical-odyssey-1/#mathematics-a-practical-odyssey","title":"Mathematics - a practical odyssey\u00b6","text":"<p>ToC</p> <ul> <li>set theory<ul> <li>notations</li> <li>De Morgans's laws</li> <li>Permutations and combinations</li> </ul> </li> <li>Probability<ul> <li>Combinatronics and probability</li> <li>Conditional probability</li> </ul> </li> <li>Statistics<ul> <li>Measures of central tendency</li> </ul> </li> </ul>"},{"location":"projects/math/math-practical-odyssey-1/#set-theory","title":"Set theory\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#notations","title":"Notations\u00b6","text":"<p>Set builder notation for representing a set: $$ G = \\{ x\\mid x &lt; 0 ~and~ x \\in R \\} $$ which is read as 'the set of all x, such that x is less than 0 and x is a real number'.</p> <p>Cardinal number of a set is the number of elements in it. It is denoted as <code>n(A)</code>. An empty set has 0 elements. It is represented as $\\phi$ or as <code>{}</code>. Universal set is always represented as <code>U</code>.</p> <p>A is a proper subset of B, if all elements of A are in B and B has more elements than A. It is represented as $A \\subset B$. A is an improper subset of B if A and B have the same elements. A not a subset is represented a $A  \\not\\subset B$. Thus $$ A \\cup B = \\{ x \\mid x \\in A ~or~ x\\in B\\}$$ $$ A \\cap B = \\{ x \\mid x \\in A ~and~ x\\in B\\}$$</p> <p>Intersection of sets represents the common elements and is written as $ A \\cap B$. Union of sets represents all elements, without repetition and is written as $A \\cup B$.</p> <p>Two sets are Mutually exclusive, if there are no common elements. Written as $ A \\cap B = \\phi $.</p>"},{"location":"projects/math/math-practical-odyssey-1/#cardinal-number-formula-for-union-of-two-sets","title":"Cardinal number formula for union of two sets\u00b6","text":"<p>$$n(A \\cup B) = n(A) + n(b) - n(A\\cap B)$$</p>"},{"location":"projects/math/math-practical-odyssey-1/#de-morgans-laws","title":"De Morgan's laws\u00b6","text":"<p>For any two sets A and B (no need to be independent or mutually exclusive) $$ (A\\cup B)' = A' \\cap B'$$ $$ (A\\cap B)' = A' \\cup B'$$</p>"},{"location":"projects/math/math-practical-odyssey-1/#permutations-and-combinations","title":"Permutations and combinations\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#fundamental-principle-of-counting","title":"Fundamental principle of counting\u00b6","text":"<p>The number of possible ways in which you can choose a 4 digit PIN number: Here, you can repeat numbers (or, your choices are replaced once you chose them so they can be chosen another time). Here, you simply raise the choices by number of selections.</p> <p>Number of ways to select 4 digit PIN = <code>10 x 10 x 10 x 10</code> = <code>10,000</code>.</p>"},{"location":"projects/math/math-practical-odyssey-1/#permutations","title":"Permutations\u00b6","text":"<p>Selection without replacement (no repetition) and order of selection is important, meaning, if you change the order, you can count that selection as a whole new choice. The formula is</p> <p>$$ _{n}P_{r} = \\frac{n!}{(n-r)!}$$ where <code>n</code> is number of items in the pool and <code>r</code> is the number of choices to be made.</p> <p>The number of ways to choose the first 3 places from a pool of 10 contestants: $$_{10}P_{3} = \\frac{10!}{7!} = 720$$</p>"},{"location":"projects/math/math-practical-odyssey-1/#probability","title":"Probability\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#probability-rules","title":"Probability rules\u00b6","text":"<p>$$  p(\\phi) = 0 \\\\ p(S) = 1 \\\\ 0 \\leq p \\leq 1 $$</p>"},{"location":"projects/math/math-practical-odyssey-1/#combinatronics-and-probability","title":"Combinatronics and probability\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#conditional-probability","title":"Conditional probability\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#understanding-the-difference-between-a-given-b-and-a-and-b","title":"Understanding the difference between 'A given B' and 'A and B'\u00b6","text":"<p>Two cards are dealt from a pack of cards, find the probability that</p> <ul> <li>both cards are hearts. This is $p(A \\cap B)$</li> <li>second card is heart, given first is a heart. This is looking for the odds of the 2nd card. It is $p(A|B)$ where A is second card and B is first card being heart.</li> </ul>"},{"location":"projects/math/math-practical-odyssey-1/#dependent-and-independent-events","title":"Dependent and Independent events\u00b6","text":"<p>If A and B are independent, then knowing B occurred does not affect the probability of A. Thus:</p>  $$p(A|B) = p(A)$$  <p>Conversely, if $p(A|B) \\neq p(A)$ then A and B are dependent.</p>  Thus, for A and B **independent** events, $$p(A \\cap B) = p(A) \\cdot p(B) $$"},{"location":"projects/math/math-practical-odyssey-1/#statistics","title":"Statistics\u00b6","text":""},{"location":"projects/math/math-practical-odyssey-1/#measures-of-central-tendency","title":"Measures of central tendency\u00b6","text":"<p>Population mean $\\mu$ and median cannot be calculated. We generally calculate the sample mean and median.</p> <p>$$Sample ~ mean: \\bar x = \\frac{\\sum x}{n} $$</p> <p>$$Sample ~ median: ~ L = \\frac{n+1}{2} $$</p>"},{"location":"projects/math/math-practical-odyssey-1/#measures-of-dispersion","title":"Measures of dispersion\u00b6","text":"<p>Variance is the squared deviation from the mean.</p> <p>$$Sample ~ variance: s^{2} = \\frac{\\sum{(x - \\bar x)^{2}}}{n-1}$$</p> <p>$$Sample ~ standard ~ deviation: s = \\sqrt{\\frac{\\sum{(x - \\bar x)^{2}}}{n-1}}$$</p>"},{"location":"projects/math/math-practical-odyssey-1/#normal-distribution","title":"Normal distribution\u00b6","text":"<p>If you plot the outcomes of a continuous random variable (of a process occurring in nature, like rainfall), it takes the shape of a bell curve, with values close to mean occurring more often than those farther from it. Since the curve represents the the probabilities of various outcomes, it is also a probability distribution.</p> <p></p>"},{"location":"projects/math/math-practical-odyssey-1/#transformation-to-standard-normal","title":"Transformation to standard normal\u00b6","text":"<p>The Z table gives probabilities for standard normal dist ($\\mu = 0$ and $\\sigma=1$). To transfrom a normal dist with any other mean and SD, use the formula</p> <p>$$ z= \\frac{x - \\mu}{\\sigma}$$</p> <p>where x is the value in the given distribution and z is the value in std. normal dist.</p>"},{"location":"projects/math/math-practical-odyssey-1/#central-limit-theorem","title":"Central limit theorem\u00b6","text":"<p>Since it is hard to find the population mean, SD, we use the CLT. According to CLT, means of a large number of samples is normally distributed around the population mean. Same applies for the SD and other measures.</p>"},{"location":"projects/math/math-practical-odyssey-1/#level-of-confidence","title":"Level of confidence\u00b6","text":"<p>\"We might say we are 95% confident the maximum error of a poll is plus or minus 3%\". This means, if 100 samples are analyzed, 95 of them would differ from population by under 0.03 of that measure, and 5 would be greater than 0.03.</p>"},{"location":"projects/ml/","title":"Machine Learning Projects","text":"<p>Today, we teach machines to learn. Tomorrow, we hope they'd return the favor ;-)</p>"},{"location":"projects/ml/#getting-started","title":"Getting started","text":"<ul> <li>Foundational ML concepts</li> <li>Understanding Scikit-Learn syntax</li> <li>Understanding Gradient Descent</li> <li>A primer on linear algebra</li> <li>Naive Bayes classification with <code>sklearn</code> - a work in progress</li> </ul>"},{"location":"projects/ml/#generalized-linear-models","title":"Generalized linear models","text":"<p>Theory</p> <ul> <li>Linear regression - stat concepts</li> <li>Solving multivariate linear regression using Gradient Descent</li> <li>Analytical vs Gradient Descent methods of solving linear regression</li> <li>Logistic regression, concepts</li> <li>Model regularization</li> </ul> <p>Applications</p> <ul> <li>Implementing linear regression using Gradient descent in Python</li> <li>Linear regression with <code>sklearn</code> and <code>statmodels</code></li> <li>Implementing logistic regression using gradient descent</li> <li>MNIST digits classification using Logistic regression in Scikit-Learn</li> </ul>"},{"location":"projects/ml/#ml-at-scale-with-pyspark","title":"ML at scale with PySpark","text":"<ul> <li>Getting started with PySpark</li> <li>CA housing price prediction with PySpark</li> </ul>"},{"location":"projects/ml/#house-hunting-the-data-scientist-way","title":"House hunting the data scientist way","text":"<ul> <li>Recording of this talk and the slide deck</li> <li>Technical write up</li> <li>Notebooks: Get my notebooks from: arcgis-python-api/talks/GeoDevPDX2018<ul> <li>Cleaning data</li> <li>Exploratory data analysis</li> <li>Feature engineering - neighboring facilities</li> <li>Feature engineering - batch</li> <li>Ranking properties</li> <li>Building a recommendation engine</li> </ul> </li> </ul>"},{"location":"projects/ml/#analyzing-over-a-century-of-global-hurricane-data","title":"Analyzing over a century of global hurricane data","text":"<p>This study showcases applying spatial data science techniques to analyze weather data and impacts of climate change on natural disasters. It is featured as a technology spotlight in the book GIS for Science. To get a high level overview of this study and its results, read the StoryMap webapp. For detailed analysis, read the analysis notebooks below:</p> <ul> <li>Part 1: Preparing larger-than-memory hurricane data using Dask and GeoAnalytics</li> <li>Part 2: EDA on hurricane tracks</li> <li>Part 3: Does intensity of hurricanes increase over time?</li> </ul>"},{"location":"projects/ml/gradient-descent-in-python/","title":"Implementing Gradient Descent for Linear Regression","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport plotly_express as px\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom mpl_toolkits import mplot3d\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode\nfrom plotly.offline import plot, iplot\n</pre> import pandas as pd import plotly_express as px import numpy as np import matplotlib.pyplot as plt import os from mpl_toolkits import mplot3d  from plotly.offline import download_plotlyjs, init_notebook_mode from plotly.offline import plot, iplot In\u00a0[13]: Copied! <pre>#set notebook mode\ninit_notebook_mode(connected=True)\n</pre> #set notebook mode init_notebook_mode(connected=True) In\u00a0[2]: Copied! <pre>path='/Users/atma6951/Documents/code/pychakras/pyChakras/ml/coursera-ml-matlabonline/machine-learning-ex/ex1'\ndata_df = pd.read_csv(os.path.join(path, 'ex1data1.txt'), header=None, names=['X','y'])\ndata_df.head()\n</pre> path='/Users/atma6951/Documents/code/pychakras/pyChakras/ml/coursera-ml-matlabonline/machine-learning-ex/ex1' data_df = pd.read_csv(os.path.join(path, 'ex1data1.txt'), header=None, names=['X','y']) data_df.head() Out[2]: X y 0 6.1101 17.5920 1 5.5277 9.1302 2 8.5186 13.6620 3 7.0032 11.8540 4 5.8598 6.8233 In\u00a0[3]: Copied! <pre>data_df.shape\n</pre> data_df.shape Out[3]: <pre>(97, 2)</pre> In\u00a0[4]: Copied! <pre>n_rows = data_df.shape[0]\n</pre> n_rows = data_df.shape[0] In\u00a0[5]: Copied! <pre>X=data_df['X'].to_numpy().reshape(n_rows,1)\n# Represent x_0 as a vector of 1s for vector computation\nones = np.ones((n_rows,1))\nX = np.concatenate((ones, X), axis=1)\ny=data_df['y'].to_numpy().reshape(n_rows,1)\n</pre> X=data_df['X'].to_numpy().reshape(n_rows,1) # Represent x_0 as a vector of 1s for vector computation ones = np.ones((n_rows,1)) X = np.concatenate((ones, X), axis=1) y=data_df['y'].to_numpy().reshape(n_rows,1) In\u00a0[6]: Copied! <pre>X.shape, y.shape\n</pre> X.shape, y.shape Out[6]: <pre>((97, 2), (97, 1))</pre> In\u00a0[7]: Copied! <pre>plt.scatter(x=data_df['X'], y=data_df['y'])\nplt.xlabel('X'); plt.ylabel('y');\nplt.title('Input dataset');\n</pre> plt.scatter(x=data_df['X'], y=data_df['y']) plt.xlabel('X'); plt.ylabel('y'); plt.title('Input dataset'); In\u00a0[7]: Copied! <pre>def compute_cost(X, y, theta=np.array([[0],[0]])):\n\"\"\"Given covariate matrix X, the prediction results y and coefficients theta\n    compute the loss\"\"\"\n    \n    m = len(y)\n    J=0 # initialize loss to zero\n    \n    # reshape theta\n    theta=theta.reshape(2,1)\n    \n    # calculate the hypothesis - y_hat\n    h_x = np.dot(X,theta)\n    \n    # subtract y from y_hat, square and sum\n    error_term = sum((h_x - y)**2)\n    \n    # divide by twice the number of samples - standard practice.\n    loss = error_term/(2*m)\n    \n    return loss\n</pre> def compute_cost(X, y, theta=np.array([[0],[0]])):     \"\"\"Given covariate matrix X, the prediction results y and coefficients theta     compute the loss\"\"\"          m = len(y)     J=0 # initialize loss to zero          # reshape theta     theta=theta.reshape(2,1)          # calculate the hypothesis - y_hat     h_x = np.dot(X,theta)          # subtract y from y_hat, square and sum     error_term = sum((h_x - y)**2)          # divide by twice the number of samples - standard practice.     loss = error_term/(2*m)          return loss In\u00a0[8]: Copied! <pre>compute_cost(X,y)\n</pre> compute_cost(X,y) Out[8]: <pre>array([32.07273388])</pre> In\u00a0[9]: Copied! <pre>def gradient_descent(X, y, theta=np.array([[0],[0]]),\n                    alpha=0.01, num_iterations=1500):\n\"\"\"\n    Solve for theta using Gradient Descent optimiztion technique. \n    Alpha is the learning rate\n    \"\"\"\n    m = len(y)\n    J_history = []\n    theta0_history = []\n    theta1_history = []\n    theta = theta.reshape(2,1)\n    \n    for i in range(num_iterations):\n        error = (np.dot(X, theta) - y)\n        \n        term0 = (alpha/m) * sum(error* X[:,0].reshape(m,1))\n        term1 = (alpha/m) * sum(error* X[:,1].reshape(m,1))\n        \n        # update theta\n        term_vector = np.array([[term0],[term1]])\n#         print(term_vector)\n        theta = theta - term_vector.reshape(2,1)\n        \n        # store history values\n        theta0_history.append(theta[0].tolist()[0])\n        theta1_history.append(theta[1].tolist()[0])\n        J_history.append(compute_cost(X,y,theta).tolist()[0])\n        \n    return (theta, J_history, theta0_history, theta1_history)\n</pre> def gradient_descent(X, y, theta=np.array([[0],[0]]),                     alpha=0.01, num_iterations=1500):     \"\"\"     Solve for theta using Gradient Descent optimiztion technique.      Alpha is the learning rate     \"\"\"     m = len(y)     J_history = []     theta0_history = []     theta1_history = []     theta = theta.reshape(2,1)          for i in range(num_iterations):         error = (np.dot(X, theta) - y)                  term0 = (alpha/m) * sum(error* X[:,0].reshape(m,1))         term1 = (alpha/m) * sum(error* X[:,1].reshape(m,1))                  # update theta         term_vector = np.array([[term0],[term1]]) #         print(term_vector)         theta = theta - term_vector.reshape(2,1)                  # store history values         theta0_history.append(theta[0].tolist()[0])         theta1_history.append(theta[1].tolist()[0])         J_history.append(compute_cost(X,y,theta).tolist()[0])              return (theta, J_history, theta0_history, theta1_history) In\u00a0[10]: Copied! <pre>%%time\nnum_iterations=1500\ntheta_init=np.array([[1],[1]])\nalpha=0.01\ntheta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,\n                                                                   alpha, num_iterations)\n</pre> %%time num_iterations=1500 theta_init=np.array([[1],[1]]) alpha=0.01 theta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,                                                                    alpha, num_iterations) <pre>CPU times: user 247 ms, sys: 2.52 ms, total: 250 ms\nWall time: 248 ms\n</pre> In\u00a0[11]: Copied! <pre>theta\n</pre> theta Out[11]: <pre>array([[-3.57081935],\n       [ 1.16038773]])</pre> In\u00a0[61]: Copied! <pre>fig, ax1 = plt.subplots()\n\n# plot thetas over time\ncolor='tab:blue'\nax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color)\nax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color)\n# ax1.legend()\nax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color);\nax1.tick_params(axis='y', labelcolor=color)\n\n# plot loss function over time\ncolor='tab:red'\nax2 = ax1.twinx()\nax2.plot(J_history, label='Loss function', color=color)\nax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations')\nax2.set_ylabel('Loss: $J(\\\\theta)$', color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\n# ax2.legend();\nfig.legend();\n</pre> fig, ax1 = plt.subplots()  # plot thetas over time color='tab:blue' ax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color) ax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color) # ax1.legend() ax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color); ax1.tick_params(axis='y', labelcolor=color)  # plot loss function over time color='tab:red' ax2 = ax1.twinx() ax2.plot(J_history, label='Loss function', color=color) ax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations') ax2.set_ylabel('Loss: $J(\\\\theta)$', color=color) ax1.tick_params(axis='y', labelcolor=color)  # ax2.legend(); fig.legend(); In\u00a0[12]: Copied! <pre>%%time\n# theta range\ntheta0_vals = np.linspace(-10,0,100)\ntheta1_vals = np.linspace(-1,4,100)\nJ_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n\n# compute cost for each combination of theta\nc1=0; c2=0\nfor i in theta0_vals:\n    for j in theta1_vals:\n        t = np.array([i, j])\n        J_vals[c1][c2] = compute_cost(X, y, t.transpose()).tolist()[0]\n        c2=c2+1\n    c1=c1+1\n    c2=0 # reinitialize to 0\n</pre> %%time # theta range theta0_vals = np.linspace(-10,0,100) theta1_vals = np.linspace(-1,4,100) J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))  # compute cost for each combination of theta c1=0; c2=0 for i in theta0_vals:     for j in theta1_vals:         t = np.array([i, j])         J_vals[c1][c2] = compute_cost(X, y, t.transpose()).tolist()[0]         c2=c2+1     c1=c1+1     c2=0 # reinitialize to 0 <pre>CPU times: user 566 ms, sys: 32.6 ms, total: 598 ms\nWall time: 569 ms\n</pre> In\u00a0[15]: Copied! <pre>import plotly.graph_objects as go\nfig = go.Figure(data=[go.Surface(x=theta0_vals, y=theta1_vals, z=J_vals)])\nfig.update_layout(title='Loss function for different thetas', autosize=True,\n                  width=600, height=600, xaxis_title='theta0', \n                 yaxis_title='theta1')\nfig.show()\n</pre> import plotly.graph_objects as go fig = go.Figure(data=[go.Surface(x=theta0_vals, y=theta1_vals, z=J_vals)]) fig.update_layout(title='Loss function for different thetas', autosize=True,                   width=600, height=600, xaxis_title='theta0',                   yaxis_title='theta1') fig.show() In\u00a0[93]: Copied! <pre>num_iterations=1500\ntheta_init=np.array([[-5],[4]])\nalpha=0.01\ntheta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,\n                                                                   alpha, num_iterations)\n</pre> num_iterations=1500 theta_init=np.array([[-5],[4]]) alpha=0.01 theta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,                                                                    alpha, num_iterations) In\u00a0[94]: Copied! <pre>plt.contour(theta0_vals, theta1_vals, J_vals, levels = np.logspace(-2,3,100))\nplt.xlabel('$\\\\theta_{0}$'); plt.ylabel(\"$\\\\theta_{1}$\")\nplt.title(\"Contour plot of loss function for different values of $\\\\theta$s\");\nplt.plot(theta0_history, theta1_history, 'r+');\n</pre> plt.contour(theta0_vals, theta1_vals, J_vals, levels = np.logspace(-2,3,100)) plt.xlabel('$\\\\theta_{0}$'); plt.ylabel(\"$\\\\theta_{1}$\") plt.title(\"Contour plot of loss function for different values of $\\\\theta$s\"); plt.plot(theta0_history, theta1_history, 'r+'); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/ml/gradient-descent-in-python/#implementing-gradient-descent-for-linear-regression","title":"Implementing Gradient Descent for Linear Regression\u00b6","text":"<p>For a theoretical understanding of Gradient Descent visit here. This page walks you through implementing gradient descent for a simple linear regression. Later, we also simulate a number of parameters, solve using GD and visualize the results in a 3D mesh to understand this process better.</p>"},{"location":"projects/ml/gradient-descent-in-python/#load-the-data","title":"Load the data\u00b6","text":""},{"location":"projects/ml/gradient-descent-in-python/#plot-the-dataset","title":"Plot the dataset\u00b6","text":""},{"location":"projects/ml/gradient-descent-in-python/#create-a-cost-function","title":"Create a cost function\u00b6","text":"<p>Here we will compute the cost function and code that into a Python function. Cost function is given by</p> <p>$$ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i})^2 $$</p> <p>where $h_{\\theta}(x_{i}) = \\theta^{T}X$</p>"},{"location":"projects/ml/gradient-descent-in-python/#solve-using-gradient-descent","title":"Solve using Gradient Descent\u00b6","text":"<p>Using GD, we simultaneously solve for theta0 and theta1 using the formula: $$ repeat \\; until \\; convergence $$ $$ \\theta_{0} := \\theta_{0} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} [(h_{\\theta}(x_{i}) - y_{i})x^{(0)}_{i}] $$ $$ \\theta_{1} := \\theta_{1} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} [(h_{theta}(x_{i}) - y_{i})x^{(1)}_{i}] $$</p>"},{"location":"projects/ml/gradient-descent-in-python/#plot-gradient-descent","title":"Plot Gradient Descent\u00b6","text":"<p>We visualize the process of reducing the loss function using gradient descent in this graph</p>"},{"location":"projects/ml/gradient-descent-in-python/#compute-cost-surface-for-an-array-of-input-thetas","title":"Compute cost surface for an array of input thetas\u00b6","text":"<p>Let us synthesize a range of theta values and compute the cost surface as a mesh. We will then overlay the path our GD algorithm took to reach the optima</p>"},{"location":"projects/ml/gradient-descent-in-python/#visualize-loss-function-as-contours","title":"Visualize loss function as contours\u00b6","text":""},{"location":"projects/ml/gradient-descent-in-python/#and-overlay-the-path-took-by-gd-to-seek-optima","title":"And overlay the path took by GD to seek optima\u00b6","text":""},{"location":"projects/ml/gradient-descent/","title":"Understanding Gradient Descent","text":""},{"location":"projects/ml/gradient-descent/#linear-regression","title":"Linear regression","text":""},{"location":"projects/ml/gradient-descent/#cost-functions","title":"Cost functions","text":"<p>The linear regression estimation function (hypothesis function) can be written as \\(h_{\\theta} (x) = \\theta_{0} + \\theta_{1}x\\). The cost function for this equation can be written as</p> \\[ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i})^2 \\] <p>The core of the cost function is the squared difference between prediction and truth in \\(y\\). In other words, this can be written as</p> \\[ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat y_{i} - y_{i})^2 \\] <p>We square the error as that is a common way of measuring loss. We sum the loss for each value of \\(y\\), get the average and then half the average. The squared error cost function is pretty common and works well for linear regressions. We halve the error function for convenience later when we take the derivative of the function and the <code>2</code> gets cancelled out. However, the objective of the estimation function is choosing values of \\(\\theta_{0} and \\theta_{1}\\) such that they minimize the cost function.</p>"},{"location":"projects/ml/gradient-descent/#minimizing-cost-functions","title":"Minimizing cost functions","text":"<p>To understand how to minimize the cost functions, let us simplify the above regression to a state where intercept is <code>0</code>. Thus the estimation / hypothesis function changes to</p> \\[ J(\\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m}(h_{\\theta}(x_{i}) - y_{i})^2 $$ which reduces to $$ J(\\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m}(\\theta_{1}x_{i} - y_{i})^2 \\] <p>Next, we solve for the cost function for different values of \\(\\theta_{1}\\) and plot them in a graph as shown below:</p> <p></p> <p>The cost function takes shape of a parabola, with a clear minima.</p>"},{"location":"projects/ml/gradient-descent/#minimizing-multidimensional-cost-functions","title":"Minimizing multidimensional cost functions","text":"<p>In the previous example, we assumed \\(\\theta_{0}=0\\). If that was not the case, then we need to minimize the residuals / cost function while changing values of both the variables. This leads to a 3D plot as shown below: </p> <p>Another way to represent the cost function is via contour plots as shown below:</p> <p></p> <p>Points along same contour have same values of error/loss for different values of \\(\\theta_{0}\\) and \\(\\theta_{1}\\). The objective is to find the lowest point in the contours - which has the lowest error/loss and find its parameters.</p> <p>In a multiple regression problem, there are several predictor variables. Thus the loss function is hard to visualize as there now multiple dimensions, one for coefficient of predictor variable + intercept term. An algorithmic way of minimizing the cost function is called gradient descent.</p>"},{"location":"projects/ml/gradient-descent/#gradient-descent","title":"Gradient descent","text":"<p>Gradient descent is an algorithm that is used to minimize the loss function. It is also used widely in many machine learning problems. The idea is, to start with arbitrary values for \\(\\theta_{0}\\) and \\(\\theta_{1}\\), keep changing them little by little until we reach minimal values for the loss function \\(J(\\theta_{0}, \\theta_{1})\\).</p> <p>The following graphic shows the distribution of the loss function. The GD algorithm starts at an arbitrary point for \\(\\theta_{0}\\) and \\(\\theta_{1}\\), takes small steps, at each step determining the direction of travel and stride length, and arrives as the local minima. The direction is determined by getting the slope of the tangent (derivative) at each point.</p> <p></p> <p>An interesting feature of GD is, if you started at a different point, you might end up at a different local minima. The definition of gradient descent for any arbitrary equation is:</p> <p>$$ \\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0}, \\theta_{1}) \\ (for \\; j=0 \\;and\\; j=1)  $$ We repeat the above equation until convergence. The \\(:=\\) is assignment operator, \\(\\partial\\) is partial differential operator, \\(\\alpha\\) is the learning rate. \\(\\alpha\\) controls the stride length in the descent graphic.</p> <p>Thus, when you have two coefficients, you would compute  $$ temp0 := \\theta_{0} - \\alpha\\frac{\\partial}{\\partial\\theta_{0}}J(\\theta_{0}, \\theta_{1}) $$ $$ temp1 := \\theta_{1} - \\alpha\\frac{\\partial}{\\partial\\theta_{1}}J(\\theta_{0}, \\theta_{1}) $$ $$ \\theta_{0} := temp0 $$ $$ \\theta_{1}:= temp1 $$ Note: It is important to compute \\(\\theta_{0}, \\theta_{1}\\) simultaneously (in parallel). You should not compute 1 and substitute its value when computing the next parameter. Intuitively, you are changing both \\(\\theta_{0}, \\theta_{1}\\) instead of changing just \\(\\theta_{0}\\), then the other. This is the principle behind partial differential equations -&gt; you are differentiating multiple parameters at the same time, as opposed to ordinary differential equations where you differentiate just one variable.</p>"},{"location":"projects/ml/gradient-descent/#gradient-descent-intuition","title":"Gradient descent intuition","text":"<p>For simplicity, let us simplify our solver to minimize over just one coefficient \\(J(\\theta_{1})\\). Now strictly speaking, this is a ordinary differential equation, not partial. The equation now becomes</p> \\[ repeat \\; until \\; convergence \\\\ \\theta_{1} := \\theta_{1} - \\alpha \\frac{d}{d\\theta_{1}} J(\\theta_{1}) \\] <p>The shape of \\(J(\\theta_{1})\\) looks like below:</p> <p></p> <p>The \\(\\frac{d}{d\\theta_{1}} J(\\theta_{1})\\) term is the derivative and gives the slope of the tangent at each point. The direction of the slope changes depending on the position on the curve and will lead the iteration to local minima.</p> <p>The learning rate \\(\\alpha\\) is multiplied by the slope / derivative term. A larger \\(\\alpha\\) will lead to an aggressive iteration which may overshoot the minima or even lead to run away divergence. A very small or conservative \\(\\alpha\\) will slow down the convergence or might settle for minor minimums as local minima.</p> <p></p> <p>Further, as we approach the local minima, the slope decreases. Thus even for a fixed \\(\\alpha\\), the rate of change will slow down in general, leading to a safe landing at minima. The slope at local minima is <code>0</code>. Thus, once reached, the second term of the equation (right of the minus sign) turns to <code>0</code> and the iteration as converged.</p>"},{"location":"projects/ml/gradient-descent/#gradient-descent-for-linear-regression","title":"Gradient descent for linear regression","text":"<p>The loss function \\(J(\\theta_{0}, \\theta_{1})\\) can be expanded out with actual loss function equation from earlier. Thus now the differential equations become:</p> \\[ repeat \\; until \\; convergence $$ $$ \\theta_{0} := \\theta_{0} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x_{i}) - y_{i}) $$ $$ \\theta_{1} := \\theta_{1} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} [(h_{\\theta}(x_{i}) - y_{i})x_{i}] \\] <p>Here, m is number of training samples, \\(\\theta_{0}\\) is intercept and \\(\\theta_{1}\\) is the coefficient 1, \\(x_{i}\\) and \\(y_{i}\\) are values of the training data.</p> <p>This process of using the full training sample for calculating the GD is called batch gradient descent. In the case of linear regression, the shape of the loss function is a convex function which looks like a bowl. There is only a global minima and no local minimas.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/","title":"Implementing Gradient Descent for Logistic Regression","text":"In\u00a0[\u00a0]: Copied! In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import pandas as pd import numpy as np import math import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[2]: Copied! <pre>vals = np.arange(-10,10,0.2)\ngz= 1/(1+np.e**(0-vals))\nplt.plot(vals, gz)\nplt.title('Sigmoid function');\n</pre> vals = np.arange(-10,10,0.2) gz= 1/(1+np.e**(0-vals)) plt.plot(vals, gz) plt.title('Sigmoid function'); In\u00a0[3]: Copied! <pre>xvals = np.arange(0,1,0.1)\ny1vals = 0-np.log(xvals)\ny0vals = 0-np.log(1-xvals)\nplt.plot(xvals, y1vals, 'b', label='y=1')\nplt.plot(xvals, y0vals, 'g', label='y=0')\nplt.title('Loss functions of logistic regression')\nplt.legend()\nplt.xlabel('Hypothesis: $h\\\\theta(x)$')\nplt.ylabel('Loss');\n</pre> xvals = np.arange(0,1,0.1) y1vals = 0-np.log(xvals) y0vals = 0-np.log(1-xvals) plt.plot(xvals, y1vals, 'b', label='y=1') plt.plot(xvals, y0vals, 'g', label='y=0') plt.title('Loss functions of logistic regression') plt.legend() plt.xlabel('Hypothesis: $h\\\\theta(x)$') plt.ylabel('Loss'); <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n  \n</pre> In\u00a0[4]: Copied! <pre>from numpy import loadtxt, where\n\n#load the dataset\ndata_path = '/Users/atma6951/Documents/code/pychakras/pyChakras/ml/coursera-ml-matlabonline/machine-learning-ex/ex2'\ndata = loadtxt(f'{data_path}/ex2data1.txt', delimiter=',')\ndata.shape\n</pre> from numpy import loadtxt, where  #load the dataset data_path = '/Users/atma6951/Documents/code/pychakras/pyChakras/ml/coursera-ml-matlabonline/machine-learning-ex/ex2' data = loadtxt(f'{data_path}/ex2data1.txt', delimiter=',') data.shape Out[4]: <pre>(100, 3)</pre> In\u00a0[224]: Copied! <pre># set dependent and independent variables.\nX = data[:, 0:2]\ny = data[:, 2]\ny = y.reshape(y.shape[0],1)\n\n# find positive and negative cases\npos = where(y == 1)\nneg = where(y == 0)\n\n# plot the data\nplt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b')\nplt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r')\nplt.xlabel('Exam 1 score')\nplt.ylabel('Exam 2 score')\nplt.legend(['Admitted', 'Not Admitted']);\n</pre> # set dependent and independent variables. X = data[:, 0:2] y = data[:, 2] y = y.reshape(y.shape[0],1)  # find positive and negative cases pos = where(y == 1) neg = where(y == 0)  # plot the data plt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b') plt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r') plt.xlabel('Exam 1 score') plt.ylabel('Exam 2 score') plt.legend(['Admitted', 'Not Admitted']); In\u00a0[225]: Copied! <pre>X.shape, y.shape\n</pre> X.shape, y.shape Out[225]: <pre>((100, 2), (100, 1))</pre> In\u00a0[6]: Copied! <pre>def sigmoid(z):\n    denom = 1+np.e**(0-z)\n    return 1/denom\n</pre> def sigmoid(z):     denom = 1+np.e**(0-z)     return 1/denom In\u00a0[28]: Copied! <pre># test sigmoid function for a range of values\n[sigmoid(-5), sigmoid(0), sigmoid(5)]\n</pre> # test sigmoid function for a range of values [sigmoid(-5), sigmoid(0), sigmoid(5)] Out[28]: <pre>[0.006692850924284857, 0.5, 0.9933071490757153]</pre> In\u00a0[33]: Copied! <pre>def cost_function(theta, X, y):\n    m=np.size(y)  # number of training samples\n    \n    h_theta_x = sigmoid(np.dot(X,theta))\n    term1 = (0-y)*np.log(h_theta_x)\n    term2 = (1-y)*np.log(1-h_theta_x)\n    J = (np.sum(term1-term2))/m\n    \n    grad = np.dot(np.transpose(h_theta_x - y),X)\n    grad = grad/m\n    \n    return (J, grad)\n</pre> def cost_function(theta, X, y):     m=np.size(y)  # number of training samples          h_theta_x = sigmoid(np.dot(X,theta))     term1 = (0-y)*np.log(h_theta_x)     term2 = (1-y)*np.log(1-h_theta_x)     J = (np.sum(term1-term2))/m          grad = np.dot(np.transpose(h_theta_x - y),X)     grad = grad/m          return (J, grad) In\u00a0[20]: Copied! <pre># Create an array for X_0\nx_0 = np.ones(X.shape[0]).reshape(X.shape[0],1)\nx_0.shape\n</pre> # Create an array for X_0 x_0 = np.ones(X.shape[0]).reshape(X.shape[0],1) x_0.shape Out[20]: <pre>(100, 1)</pre> In\u00a0[21]: Copied! <pre># Splice this with existing array X\nX2 = np.concatenate((x_0, X), axis=1)\nX2.shape\n</pre> # Splice this with existing array X X2 = np.concatenate((x_0, X), axis=1) X2.shape Out[21]: <pre>(100, 3)</pre> In\u00a0[201]: Copied! <pre># initialize the coefficients to 0\ninitial_theta = np.zeros(X.shape[1]+1).reshape(X.shape[1]+1,1)\ninitial_theta\n</pre> # initialize the coefficients to 0 initial_theta = np.zeros(X.shape[1]+1).reshape(X.shape[1]+1,1) initial_theta Out[201]: <pre>array([[0.],\n       [0.],\n       [0.]])</pre> In\u00a0[202]: Copied! <pre># compute cost and gradient\n(J, grad) = cost_function(initial_theta, X2, y)\nprint(f'Cost at initial theta: {J}')\nprint(f'Gradient at inital theta: {grad.flatten()}')\n</pre> # compute cost and gradient (J, grad) = cost_function(initial_theta, X2, y) print(f'Cost at initial theta: {J}') print(f'Gradient at inital theta: {grad.flatten()}') <pre>Cost at initial theta: 0.6931471805599453\nGradient at inital theta: [ -0.1        -12.00921659 -11.26284221]\n</pre> In\u00a0[109]: Copied! <pre>def gradient_descent(X, y, theta=initial_theta,\n                    alpha=0.01, num_iterations=1500):\n\"\"\"\n    Solve for theta using Gradient Descent optimiztion technique. \n    Alpha is the learning rate\n    \"\"\"\n    m = len(y)\n    J_history = []\n    theta0_history = []\n    theta1_history = []\n    theta2_history = []\n    theta = theta.reshape(3,1)\n    \n    for i in range(num_iterations):\n        error = (np.dot(X, theta) - y)\n        \n        term0 = (alpha/m) * np.sum(error* X[:,0].reshape(m,1))\n        term1 = (alpha/m) * np.sum(error* X[:,1].reshape(m,1))\n        term2 = (alpha/m) * np.sum(error* X[:,2].reshape(m,1))\n        \n        # update theta\n        term_vector = np.array([[term0],[term1], [term2]])\n#         print(term_vector)\n        theta = theta - term_vector.reshape(3,1)\n        \n        # store history values\n        theta0_history.append(theta[0].tolist()[0])\n        theta1_history.append(theta[1].tolist()[0])\n        theta2_history.append(theta[2].tolist()[0])\n        J_history.append(cost_function(theta,X,y)[0])\n        \n    return (theta, J_history, theta0_history, theta1_history, theta2_history)\n</pre> def gradient_descent(X, y, theta=initial_theta,                     alpha=0.01, num_iterations=1500):     \"\"\"     Solve for theta using Gradient Descent optimiztion technique.      Alpha is the learning rate     \"\"\"     m = len(y)     J_history = []     theta0_history = []     theta1_history = []     theta2_history = []     theta = theta.reshape(3,1)          for i in range(num_iterations):         error = (np.dot(X, theta) - y)                  term0 = (alpha/m) * np.sum(error* X[:,0].reshape(m,1))         term1 = (alpha/m) * np.sum(error* X[:,1].reshape(m,1))         term2 = (alpha/m) * np.sum(error* X[:,2].reshape(m,1))                  # update theta         term_vector = np.array([[term0],[term1], [term2]]) #         print(term_vector)         theta = theta - term_vector.reshape(3,1)                  # store history values         theta0_history.append(theta[0].tolist()[0])         theta1_history.append(theta[1].tolist()[0])         theta2_history.append(theta[2].tolist()[0])         J_history.append(cost_function(theta,X,y)[0])              return (theta, J_history, theta0_history, theta1_history, theta2_history) In\u00a0[216]: Copied! <pre>%%time\nnum_iterations=150\ninitial_theta2 = (np.ones(3)-0.5).reshape(3,1)\nalpha=0.0002\ntheta, J_history, theta0_history, \\\ntheta1_history, theta2_history = gradient_descent(X2,y,initial_theta,\n                                                  alpha,num_iterations)\n</pre> %%time num_iterations=150 initial_theta2 = (np.ones(3)-0.5).reshape(3,1) alpha=0.0002 theta, J_history, theta0_history, \\ theta1_history, theta2_history = gradient_descent(X2,y,initial_theta,                                                   alpha,num_iterations) <pre>CPU times: user 13.4 ms, sys: 2.6 ms, total: 16 ms\nWall time: 13.6 ms\n</pre> In\u00a0[217]: Copied! <pre>theta.flatten()\n</pre> theta.flatten() Out[217]: <pre>array([-0.00142996,  0.0058575 ,  0.00403084])</pre> In\u00a0[218]: Copied! <pre>fig, ax1 = plt.subplots(figsize=(8,5))\n\n# plot thetas over time\ncolor='tab:blue'\nax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color)\nax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color)\nax1.plot(theta2_history, label='$\\\\theta_{2}$', linestyle='-.', color=color)\n# ax1.legend()\nax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color);\nax1.tick_params(axis='y', labelcolor=color)\n\n# plot loss function over time\ncolor='tab:red'\nax2 = ax1.twinx()\nax2.plot(J_history, label='Loss function', color=color)\nax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations')\nax2.set_ylabel('Loss: $J(\\\\theta)$', color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\n# ax2.legend();\nfig.legend();\n</pre> fig, ax1 = plt.subplots(figsize=(8,5))  # plot thetas over time color='tab:blue' ax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color) ax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color) ax1.plot(theta2_history, label='$\\\\theta_{2}$', linestyle='-.', color=color) # ax1.legend() ax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color); ax1.tick_params(axis='y', labelcolor=color)  # plot loss function over time color='tab:red' ax2 = ax1.twinx() ax2.plot(J_history, label='Loss function', color=color) ax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations') ax2.set_ylabel('Loss: $J(\\\\theta)$', color=color) ax1.tick_params(axis='y', labelcolor=color)  # ax2.legend(); fig.legend(); In\u00a0[220]: Copied! <pre># when x1 = 30, find x2. x2= (-theta0 - theta1*30)/theta2\nt = theta.flatten()\nx2 = (0-t[0] - t[1]*40)/t[2]\nprint(x2)\n\n# when x1 = 100, find x2. x2= (-theta0 - theta1*100)/theta2\nt = theta.flatten()\nx2 = (0-t[0] - t[1]*100)/t[2]\nprint(x2)\n</pre> # when x1 = 30, find x2. x2= (-theta0 - theta1*30)/theta2 t = theta.flatten() x2 = (0-t[0] - t[1]*40)/t[2] print(x2)  # when x1 = 100, find x2. x2= (-theta0 - theta1*100)/theta2 t = theta.flatten() x2 = (0-t[0] - t[1]*100)/t[2] print(x2) <pre>-57.77211067097919\n-144.96240929648445\n</pre> <p>Note: At this point, I realize my gradient descent is not really optimizing well. The equation of the decision boundary line is way off. Hence I approach to solve this problem using Scikit-Learn and see what its parameters are.</p> In\u00a0[243]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X, y)\n</pre> from sklearn.linear_model import LogisticRegression clf = LogisticRegression(random_state=0).fit(X, y) <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/Users/atma6951/anaconda3/envs/pychakras/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n</pre> In\u00a0[244]: Copied! <pre>clf.coef_ # theta1, theta2\n</pre> clf.coef_ # theta1, theta2 Out[244]: <pre>array([[0.03844482, 0.03101855]])</pre> In\u00a0[246]: Copied! <pre># mean test accuracy\nclf.score(X, y)\n</pre> # mean test accuracy clf.score(X, y) Out[246]: <pre>0.87</pre> In\u00a0[247]: Copied! <pre>clf.predict(X)\n</pre> clf.predict(X) Out[247]: <pre>array([0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n       1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n       0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n       1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])</pre> In\u00a0[257]: Copied! <pre># plot function copied from https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nh = .02  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(8, 6))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\n# plt.scatter(X[:, 0], X[:, 1], edgecolors='k', cmap=plt.cm.Paired)\nplt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b')\nplt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r')\nplt.xlabel('Test 1 scores')\nplt.ylabel('Test 2 scores')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title('Logistic regression decision boundary');\n</pre> # plot function copied from https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5  h = .02  # step size in the mesh xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])  # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure(1, figsize=(8, 6)) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)  # Plot also the training points # plt.scatter(X[:, 0], X[:, 1], edgecolors='k', cmap=plt.cm.Paired) plt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b') plt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r') plt.xlabel('Test 1 scores') plt.ylabel('Test 2 scores')  plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title('Logistic regression decision boundary');"},{"location":"projects/ml/implementing-logistic-regression-in-python/#implementing-gradient-descent-for-logistic-regression","title":"Implementing Gradient Descent for Logistic Regression\u00b6","text":"<p>This notebook follows the topics discussed in logistic regression course notes. Please refer to that page for context. This notebook tries to implement the concepts in Python, instead of MatLab/Octave. I have borrowed some inspiration and code from this blog.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/#plot-sigmoid-function","title":"Plot sigmoid function\u00b6","text":"<p>To bound our probability predictions between <code>0-1</code>, we use a sigmoid function. Its definition is below.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/#plot-loss-function-for-logistic-regression","title":"Plot loss function for logistic regression\u00b6","text":""},{"location":"projects/ml/implementing-logistic-regression-in-python/#load-and-visualize-training-data","title":"Load and visualize training data\u00b6","text":""},{"location":"projects/ml/implementing-logistic-regression-in-python/#define-sigmoid-and-cost-functions","title":"Define sigmoid and cost functions\u00b6","text":""},{"location":"projects/ml/implementing-logistic-regression-in-python/#apply","title":"Apply\u00b6","text":""},{"location":"projects/ml/implementing-logistic-regression-in-python/#minimize-the-cost-function-using-gradient-descent","title":"Minimize the cost function using gradient descent\u00b6","text":"<p>Note: The implementation of gradient descent for logistic regression is the same as that for linear regression, as seen here.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/#prediction-and-plot-decision-boundary","title":"Prediction and plot decision boundary\u00b6","text":"<p>Using gradient descent, we found, the values of theta. The decision boundary exists where $h_{\\theta}(x) = 0$. Thus, we write the equation as</p> <p>$$ \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} = 0 \\\\ -0.04904473x_{0} + 0.00618754x_{1} + 0.00439495x_{2} = 0 \\\\ 0.00618754x_{1} + 0.00439495x_{2} = 0.04904473 $$</p> <p>substituting x1=0 and find x2, then vice versa. Thus, we get points <code>(0,11.15933),(7.92636,0)</code>. But these are out of bounds to plot. Instead, we calculate values within the range of <code>30-100</code> as these are exam scores.</p>"},{"location":"projects/ml/implementing-logistic-regression-in-python/#logistic-regression-using-scikit-learn","title":"Logistic regression using Scikit-Learn\u00b6","text":"<p>Using the logistic regression from SKlearn, we fit the same data and explore what the parameters are.</p>"},{"location":"projects/ml/linear-algebra/","title":"A Primer on Linear Algebra","text":"<p>A mxn matrix is given by </p> \\[ R^{mn} = \\begin{bmatrix}     a&amp;b&amp;c \\\\\\     f&amp;n&amp;i \\\\\\     c&amp;b&amp;w \\end{bmatrix}_{3\\times3} \\] <p>where \\(m\\) is number of rows and \\(n\\) is number of columns.</p> <p>A vector is a single column matrix and is given by </p> \\[ v = \\begin{bmatrix}     a\\\\\\     b\\\\\\     d\\\\\\     h\\\\\\     j \\end{bmatrix} \\] <p>with \\(5\\) rows.</p>"},{"location":"projects/ml/linear-algebra/#notations","title":"Notations","text":"<ol> <li>\\(A_{ij}\\) refers to element in \\(i\\)th row, \\(j\\)th column.</li> <li>In general matrixes are <code>1</code> indexed - both in math and in Matlab</li> <li>\\(v_{i}\\) refers to element in \\(i\\)th row of a vector</li> <li>A vector with <code>n</code> rows is considered an n-dimensional vector</li> <li>Matrices are denoted in uppercase and vectors and scalars in lower case.</li> </ol>"},{"location":"projects/ml/linear-algebra/#matrix-operations","title":"Matrix operations","text":""},{"location":"projects/ml/linear-algebra/#matrix-addition-and-subtraction","title":"Matrix addition and subtraction","text":"<p>You cannot add a scalar to a matrix. You can however add two matrices, they need to be of same dimensions. You add each element at corresponding positions.</p> \\[ \\begin{bmatrix}     3&amp;5\\\\\\     7&amp;8\\\\\\     -9&amp;0 \\end{bmatrix} + \\begin{bmatrix}     8&amp;0\\\\\\     5&amp;-2\\\\\\     2&amp;1 \\end{bmatrix} =  \\begin{bmatrix}     3+8&amp;5+0\\\\\\     7+5&amp;8-2\\\\\\     -9+2&amp;0+1 \\end{bmatrix} \\] <p>The same applies for subtraction.</p>"},{"location":"projects/ml/linear-algebra/#matrix-and-scalar-multiplication-and-division","title":"Matrix and scalar multiplication and division","text":"<p>You can multiply a scalar with a matrix, there are not restrictions with respect to dimensions. You multiply or divide each element with the same scalar.</p> \\[ 3 \\times \\begin{bmatrix}     3&amp;5\\\\\\     7&amp;8\\\\\\     9&amp;0 \\end{bmatrix} = \\begin{bmatrix}     9&amp;15\\\\\\     21&amp;24\\\\\\     27&amp;0 \\end{bmatrix} \\] <p>Division is similar.</p>"},{"location":"projects/ml/linear-algebra/#matrix-and-vector-multiplication","title":"Matrix and vector multiplication","text":"\\[ \\begin{bmatrix}     a&amp;b&amp;c\\\\\\     f&amp;n&amp;i\\\\\\     c&amp;b&amp;w \\end{bmatrix}^{\\rightarrow}_{3\\times3} \\times \\begin{bmatrix}     3\\\\\\     7\\\\\\     9 \\end{bmatrix}\\downarrow = \\begin{bmatrix}     3a + 7b + 9c\\\\\\     3f + 7n + 9i\\\\\\     3c + 7b + 9w \\end{bmatrix} \\]"},{"location":"projects/ml/linear-algebra/#solving-linear-equations-as-matrix-operations","title":"Solving linear equations as matrix operations","text":"<p>For optimization, you can represent linear equations as matrix operations. For instance, consider the hypothesis function \\(h_{\\theta}x = -40 + 0.45x_{i}\\). To compute the hypothesis for \\(n\\) different values of \\(x_{i}\\) (34,56,21,11,10), you can represent the calculation as a matrix operation:</p> <p>$$ \\begin{bmatrix}     1&amp; 34\\\\     1&amp; 56\\\\     1&amp; 21\\\\     1&amp; 11\\\\     1&amp; 10 \\end{bmatrix} \\times \\begin{bmatrix}     -40\\\\     0.45 \\end{bmatrix} = \\begin{bmatrix}     -24.7\\\\     -14.8\\\\     -30.55\\\\     -35.5\\\\     -35.5 \\end{bmatrix} $$ Such matrix computation is way faster than a loop. This is applicable for most language including java, c++, octave, python.</p>"},{"location":"projects/ml/linear-algebra/#matrix-x-matrix-multiplication","title":"Matrix x matrix multiplication","text":"<p>To multiply two matrices, the number of columns of first should match number of row of second =&gt; (mxn x nxp = mxp matrix).</p> \\[ \\begin{bmatrix}     a&amp;b&amp;c\\\\\\     f&amp;n&amp;i\\\\\\     c&amp;b&amp;w \\end{bmatrix} \\times \\begin{bmatrix}     3&amp;1\\\\\\     7&amp;2\\\\\\     9&amp;3 \\end{bmatrix} = \\begin{bmatrix}     (3a + 7b + 9c)&amp;(1a  + 2b+3c)\\\\\\     (3f + 7n + 9i)&amp;(1f+2n+3i)\\\\\\     (3c + 7b + 9w)&amp;(1c+2b+3w) \\end{bmatrix} \\] <p>Extending the former example, suppose you want to calculate the prediction for 3 different hypothesis functions, you can represent that problem as a matrix x matrix multiplication:</p> <p></p> <p>Representing these as matrix operations allows programming languages to compute them in parallel, allowing for great speedups.</p>"},{"location":"projects/ml/linear-algebra/#properties-of-matrix-multiplications","title":"Properties of matrix multiplications","text":"<ul> <li>Matrices are not commutative: \\(A\\times B \\ne B\\times A\\) </li> <li>Matrices are associative: \\((A \\times B) \\times C = A \\times (B \\times C)\\)</li> <li>Identity matrix is a matrix made of ones for diagonals of same dimension such that \\(A \\times I = I \\times A = A\\)</li> <li>Identity matrix is always a square matrix.</li> </ul>"},{"location":"projects/ml/linear-algebra/#matrix-inverse","title":"Matrix inverse","text":"<p>A matrix is said to be the inverse of another matrix, if you multiply that with the matrix, you get an identity matrix. \\(A \\times A^{-1} = I\\).</p> <p>Only certain square matrices have inverses. We typically compute inverse using software.</p>"},{"location":"projects/ml/linear-algebra/#matrix-transpose","title":"Matrix transpose","text":"<p>Transpose of a matrix can be created by flipping the rows and columns. For a matrix \\(A\\), matrix \\(B\\) is said to be its transpose \\(A^{T} = B\\) if \\(B_{ij} = A_{ji}\\). In other words, \\(A_{ij} = A^T_{ji}\\).</p>"},{"location":"projects/ml/linear-regression-analytical-solution/","title":"Analytical vs Gradient Descent methods of solving linear regression","text":"<p>The Gradient Descent offers an iterative method to solve linear models. However, there is a traditional and direct way of solving it called as normal equations. In normal equations, you build a matrix where each record of observation becomes a row (<code>m</code> rows) and each feature becomes a column. You prefix an additional column to represent the constant (<code>n+1</code> columns). This matrix, represented as <code>X</code> is of dimension <code>m x (n+1)</code>. You represent the response variable as a vector <code>y</code> of dimension <code>m x 1</code>.</p> <p></p> <p>The formula to calculate the optimal coefficients is given by \\(\\theta = (X^{T}X)^{-1}X^{T}y\\). Where \\(\\theta\\) is a vector of shape <code>n+1</code> containing \\([\\theta_{0}, \\theta_{1} ... \\theta_{n}]\\).</p>"},{"location":"projects/ml/linear-regression-analytical-solution/#caveats-when-applying-analytical-technique","title":"Caveats when applying analytical technique","text":"<ul> <li>In the analytical, normal equation method, there is no iteration to arrive at optimal \\(\\theta\\). You simply calculate it.</li> <li>You do not have to scale features. It is ok to have them in their native dimensions.</li> </ul>"},{"location":"projects/ml/linear-regression-analytical-solution/#guidelines-for-choosing-between-gd-and-normal-equation","title":"Guidelines for choosing between GD and Normal equation","text":"<ul> <li>GD needs you to play with \\(\\alpha\\) (learning rate), while normal equation does not.</li> <li>GD is an iterative process, while normal eq is not.</li> <li>GD shines well when you have a large number of attributes / features / independent variables. The order of GD is given by \\(O(kn^{2})\\) for <code>n</code> features.</li> <li>Normal equation needs to invert a matrix which is an expensive operation. Its time complexity is given by \\(O(n^{3})\\).</li> <li>If you have <code>&gt;10,000</code> independent variables, or if the number of observations / rows is less than number of independent variables (<code>m &lt; (n+1)</code>), then normal equation not produce a matrix that is invertible. You are better off with Gradient Descent regression.</li> <li>If you have highly correlated features (multi-collinearity) or when you have more features than observations, you might end up with a non-invertible matrix for the normal equation. In these cases, you can choose GD or you can delete some features or regularization techniques if you want to continue with normal equation.</li> <li>GD is an approximation technique, while normal equation is a deterministic approach. GD might settle in a local minima and not global minima. Although, for linear regressions, the shape of the loss function is such that there is no local but only a global minima.</li> </ul>"},{"location":"projects/ml/logistic-reg-concepts/","title":"Understanding logistic regression","text":"<p>A logistic regression is a binary classification algorithm. Its values are <code>0</code> for false cases and <code>1</code> for true cases. Before talking about logistic reg, let us consider why not to use linear regression for classification. We could hypothetically use linear reg to predict class probabilities and could theoretically set a threshold, (usually <code>0.5</code>) above which we group to Class 1 and below which we group to Class 2. However this system has flaws. Consider the graphic below:</p> <p></p> <p>The system works well with the hypothesis function in red, until a new valid but outlier data enters the training. Now the regression function is changed to blue and that changes how it classifies borderline cases. Further, regression does not lend well for multi-class problems. Further, regression is likely to predict <code>&gt;1</code> and <code>&lt;0</code> class probabilities which aren't true.</p>"},{"location":"projects/ml/logistic-reg-concepts/#logistic-regression-model","title":"Logistic regression model","text":"<p>We represent the hypothesis function of logistic regression as \\(0 \\le h_{\\theta}(x) \\le 1\\). The hypothesis function can be represented as:</p> \\[ h_{\\theta}(x) = g(\\theta^{T}x) $$ $$ where \\; g(z) = \\frac{1}{1+e^{-z}} \\] <p>\\(g(z)\\) is a <code>sigmoid</code> function, also called a <code>logistic</code> function, from which the regression gets its name. The hypothesis function looks similar to that of a linear regression, except for the product with the sigmoid function. The shape of the sigmoid function is given by:</p> <pre><code>vals = np.arange(-20,20,0.1)\ngz= 1/(1+np.e**(0-vals))\nplt.plot(vals, gz)\nplt.title('Sigmoid function');\n</code></pre> <p></p> <p>As <code>z</code> reaches \\(\\infty\\), \\(g(z)\\) asymptotes to 1. The values always range betwen <code>0 &lt; g(z) &lt; 1</code>. Thus, the hypothesis can be rewritten as </p> \\[ h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^{T}x}} \\] <p>\\(h_{\\theta}(x)\\) gives us the probability that output = 1. Thus, if \\(h_{\\theta}{x} = 0.7\\), that means a <code>70%</code> probability the output is <code>1</code> or a true case. Mathematically, this is represented as </p> <p>$$ h_{\\theta}(x) = P(y=1 | x;\\theta) = 0.7 $$ which is read as \"probability that y=1, given x, parametertized by \\(\\theta\\)\". Since probability adds to <code>1</code>, we can say, the inverse, probability of <code>y=0</code> is <code>0.3</code>: </p> \\[ h_{\\theta}(x) = P(y=0 | x;\\theta) = 0.3 \\]"},{"location":"projects/ml/logistic-reg-concepts/#decision-boundary-of-a-logistic-regression-model","title":"Decision boundary of a logistic regression model","text":"<p>The shape of the logistic function is such that, <code>g(z) &gt; 0.5</code> when <code>z &gt; 0</code> and <code>g(z) &lt; 0.5</code> when <code>z &lt; 0</code>, and <code>g(z) = 0.5</code> when <code>z=0</code>. The <code>z</code> can be expanded as \\(\\theta^{T}x\\). Additionally, we can simplify that <code>y=1</code> when \\(g(z) \\ge 0.5\\) and <code>y=0</code> when \\(g(z) &lt; 0.5\\).</p> <p>Now, consider this example dataset. The red cross show <code>y=1</code> case and blue circles show <code>y=0</code> case.:</p> <p></p> <p>We can represent this as \\(h_{\\theta}(x) = g(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2})\\). Solving for theta, say, we find that it is <code>[-3;1;1]</code>. Thus, from previous derivation, we know that <code>y=1</code> when \\(-3 + x_{1} + x_{2} \\ge 0\\) as <code>g(z) has to be &gt;= 0</code>. We can simplify it as shown in picture and derive the equation of the decision boundary which is sown in magenta on the pic. The following challenge will explain this better:</p> <p></p>"},{"location":"projects/ml/logistic-reg-concepts/#non-linear-decision-boundaries","title":"Non linear decision boundaries","text":"<p>We can represent non linearity in a linear model by adding additional parameters which are higher order representations of existing parameters, such as \\(h_{\\theta}(x) = g(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{1}^{2} + \\theta_{4}x_{2}^{2})\\) as shown here:</p> <p></p> <p>To get the decision boundary, we need to solve for <code>g(z)=0.5</code> case which happens when <code>z=0</code>. Thus, in pic we solve for \\(x_{1}^{2} + x_{2}^{2} = 1\\) which is the equation of a <code>circle</code>.</p> <p>Using this theory, decision boundaries that take complex shapes can be represented using a linear model and can be solved using logistic regression.</p>"},{"location":"projects/ml/logistic-reg-concepts/#cost-function-for-logistic-regression","title":"Cost function for logistic regression","text":"<p>The same cost function that we had for linear regression would apply for logistic regression, however, it leads to a non-convex loss function. The GD algorithm would fail to arrive at the global minima. Thus, we derive a new cost function as:</p> \\[ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}cost(h_{\\theta}(x),y) $$ where, $$ cost(h_{\\theta}(x), y) = \\begin{cases}                             -log(h_{\\theta}(x)) &amp; y=1 \\\\\\                             -log(1-h_{\\theta}(x)) &amp; y=0                         \\end{cases} \\] <p>Note, <code>log(0) = inf</code>, <code>log(-1) = nan</code>, <code>log(1) = 0</code>. Thus, we can plot this cost term as follows:</p> <pre><code>xvals = np.arange(0,1,0.1)\ny1vals = 0-np.log(xvals)\ny0vals = 0-np.log(1-xvals)\nplt.plot(xvals, y1vals, 'b', label='y=1')\nplt.plot(xvals, y0vals, 'g', label='y=0')\nplt.title('Loss functions of logistic regression')\nplt.legend(); plt.xlabel('Hypothesis: $h\\\\theta(x)$'); plt.ylabel('Loss');\n</code></pre> <p></p> <p>From the graph, when <code>h(x)=1</code> for <code>y=1</code>, the cost term is <code>0</code> (blue line). As predicted class reduces and approaches <code>0</code>, the cost raises to <code>inf</code>. Likewise, when <code>h(x)=0</code> for <code>y=0</code>, the cost is also <code>0</code>. However as the predicted value increases, the cost also increases penalizing the wrong prediction.</p> <p>Instead of having two equations, the cost term can be simplified into the following:</p> \\[ cost(h_{\\theta}x,y) = -ylog(h_{\\theta}(x)) - (1-y)log(1-h_{\\theta}(x)) \\] <p>plugging the cost term in the cost function, we get:</p> \\[ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y_{i}log(h_{\\theta}(x_{i})) + (1-y_{i})log(1-h_{\\theta}(x_{i})) \\]"},{"location":"projects/ml/logistic-reg-concepts/#gradient-descent-optimization-for-logistic-regression","title":"Gradient descent optimization for logistic regression","text":"<p>To find the values of \\(\\theta\\) at the global optima, we need to differentiate the cost function written earlier. This turns out to be</p> <p>repeat until convergence: $$ \\theta_{j} := \\theta_{j} - \\alpha\\sum_{i=1}^{m}(h_{\\theta}(x_{i}) - y_{i})x_{i,j} $$</p> <p>The above update rule, is just the same we had for linear regression. Thus, GD for linear and logistic regression is the same. What has changed is the hypothesis and the cost functions. A vectorized implementation of GD is</p> \\[ \\theta := \\theta - \\frac{\\alpha}{m}X^{T}(g(X\\theta) - \\vec{y}) \\]"},{"location":"projects/ml/logistic-reg-concepts/#using-logistic-regression-for-multiple-classes","title":"Using logistic regression for multiple classes","text":"<p>While the most use cases of logistic reg is to predict boolean classes, we can extend it to a multi-class problem using a technique called one-vs-all. </p> <p></p> <p>We essentially build multiple models, equalling the number of classes. Each model predicts the probability that given value will fall within the class it is trained to predict. Finally we find the class with max probability and assign it to the prediction.</p>"},{"location":"projects/ml/logistic-reg-concepts/#ocr-on-mnist-digits-database-using-multi-class-logistic-regression","title":"OCR on MNIST digits database using multi-class logistic regression","text":"<p>The MNIST database has <code>14</code> million images of handdrawn digits. We work with a subset of <code>5000</code> images. Each image is <code>20x20</code> pixels. When laid out as a column vector (which is how Neural Nets and log reg algorithms will read it), we get a <code>1x400</code> row vector. A sample of 100 images is below:</p> <p></p> <p>Here, we build a multi-class regularized logistic regression model to solve this classification problem. We train this multi-class logistic regression model by iterating a simple log reg model for each class. Each iteration will produce a set of \\(\\theta\\) values which predict the probability for that class. Finally we will assemble all theta into a 2D matrix.</p> <p>The weights / theta is a 2D matrix (unlike a vector for simple logistic reg), where each row is the weight vector for a particular class. Since each pixel is considered an input feature and since we have <code>10</code> classes to predict, we get theta as \\(\\Theta_{10x401}\\) matrix. <code>401</code> because we add <code>1</code> bias feature to <code>400</code> features which is obtained by flattening the <code>20x20</code> image into a <code>400x1</code> vector.</p> <p>During the training process, we need to represent the output class not as digits, but as one-hot encoded vector since that is what log reg understands. Finally during the prediction phase, we will translate the one-hot encoded prediction into the actual class label, which is the predicted digit.</p> <p>The full MATLAB/OCTAVE implementation can be found here.</p>"},{"location":"projects/ml/ml-concepts/","title":"Machine Learning Concepts","text":"<p>Machine Learning (ML) is the art and science of teaching machines with large amounts of data to perform a given task. Unlike typical programming, where the developer defines the methods, we leave it to the machines / algorithms to figure out the patterns from the data itself.</p>"},{"location":"projects/ml/ml-concepts/#classifications-of-ml-systems","title":"Classifications of ML systems","text":"<p>ML algorithms can be classified using the following categories:</p> <ul> <li> <p>Level of supervision - supervised, unsupervised, reinforcement learning</p> <ul> <li> <p>Supervised Learning - training data that is labeled is fed to the algorithm. Examples: KNN, Linear Regression, Logistic Regression, SVM, Decision Trees, RF, Neural Networks</p> </li> <li> <p>Unsupervised learning - unlabeled data is fed to the algorithm and it figures out natural groupings in the data. Some examples include - </p> <ul> <li>Clustering: K-means, Hierarchical cluster analysis (HCA), Expectation maximization. Unlike K-means, HCA can have subdivide each cluster into sub-clusters allowing for better grouping of data.</li> <li>Visualization &amp; Dimensionality reduction: PCA, Kernel PCA, Locally-Linear Embedding (LLE), t-distributed Stochastic Neighbor Embedding (t-SNE)</li> <li>Association rule learning: Apriori, Eclat. Here the goal is to dig into large amounts of data to discover relationships between attributes.</li> </ul> </li> <li>Reinforcement learning: RL is a type of learning where instead of providing labeled data, the algorithm is given either a reward or penalty for its output. The learner (called agent) will attempt multiple attempts (called policies) until it maximizes the reward. RL is used train machines to perform some highly complex tasks such as walking. AlphaGo is an example.</li> </ul> </li> <li> <p>Incremental learning - online vs batch learning</p> <ul> <li>Batch or offline learning: The system is trained using all available data and then put into production for inference. Here it does not learn any more. To retrain, it needs to be taken offline and fed the new data.</li> <li>Online learning: Data is fed incrementally in mini-batches and the model progressively gets refined. The concept of mini-batches allows th model to learn on data that is larger than what can be fit in memory. This is also called out-of-core learning. An important concept in online learning is Learning Rate - which controls how much should the model adapt to new data. A high LR will cause it to forget old data, a low LR will add inertia causing it to not fit new data well.</li> </ul> </li> <li> <p>Model generalization: Instance based vs Model based.</p> <ul> <li>Instance based learning: The model learns to find similarities in input data and uses a similarity score to predict on new data. Example: KNN.</li> <li>Model based learning: Here the algorithm generates a model (a math function) that minimizes the loss and uses that model to perform predictions. Example: Linear regression.</li> </ul> </li> </ul>"},{"location":"projects/ml/ml-concepts/#challenges-in-ml","title":"Challenges in ML","text":"<p>Below are come common challenges in machine learning.</p> <ul> <li>Poor quality data: Training data might be non-representative - with under or oversampled for certain classes. Certain times this is unavoidable, other times it could be due to sampling bias.</li> <li>Overfitting training data: Overfitting happens when the model is too complex relative to the amount and noise in training data. When a model overfits, it starts to use noise (such as coincidences) in data as predictors. A classic sign of overfitting is very low training error, but high test error.<ul> <li>Overfitting can be avoided by either increasing the size of training data (to include diverse data which averages out the noise) or by reducing the complexity of the model through regularization.</li> <li>The amount of regularization is controlled by a hyperparameter. A hyperparameter is a parameter which is set prior to training and remains constant for that training run. Unlike model weights, it is not updated as a result of the training itself.</li> <li>A high regularization value will oversimplify the model, leading to reduced Variance. But can result in a high error of Bias.</li> <li>A low regularization value can lead to a model that overfits - leading to high variability, but low training bias and high test bias.</li> <li>This relationship is called the bias-variance trade-off in machine learning.</li> </ul> </li> </ul>"},{"location":"projects/ml/ml-concepts/#a-machine-learning-checklist","title":"A Machine Learning checklist","text":"<p>Adapted from Hands-on ML from Scikit-Learn and TensorFlow.</p> <ol> <li>Frame the problem &amp; look at the big picture<ul> <li>Define the problem in business terms.</li> <li>Define it in ML terms (which type of ML will be used, what are the metrics)</li> <li>How should performance be measured, what's minimum performance</li> <li>Look for similar problems</li> <li>If you were to solve without ML, how would the solution be?</li> </ul> </li> <li>Get the data<ul> <li>Plan how much of the data is needed</li> <li>Plan where data will be stored and how it will be accessed (data workspace)</li> <li>Verify access privileges and permissions / licenses</li> <li>Remove PII</li> <li>Make test set and put it aside. Do everything else including EDA on remaining train set.</li> <li>Make a back-up copy that will not be touched at anytime. If you have to recover from back-up, make another back-up copy.</li> <li>Automate this process so it can be repeated if new data comes along.</li> </ul> </li> <li>EDA<ul> <li>Use an interface like Jupyter Notebooks for this process.</li> <li>Study each attribute and its characteristics (distribution, type, missing values, noise)</li> <li>Visualize using plots and maps</li> <li>Study correlations b/w attributes</li> <li>Identify useful transformations that can be applied</li> <li>Identify how data can be enriched (bringing in ancillary data sources)</li> </ul> </li> <li>Prepare the data<ul> <li>Write all transformations as functions that can be repeated on any data</li> <li>data cleaning - fix outliers, missing data</li> <li>feature selection - make new features, drop irrelevant features</li> <li>feature engineering - discretize continuous features, decompose complex features (datetime, categories etc.), apply transformations</li> <li>feature scaling - normalize / standardize features</li> </ul> </li> <li>Short-list promising models<ul> <li>Take a quick &amp; dirty approach and train several models on a subset</li> <li>Measure and compare the performance - for each model, use N-fold cross-validation and collect performance by computing mean and SD of performance metric</li> <li>Collect most significant variables for each model</li> <li>Iterate to feature selection / engineering</li> <li>Retrain</li> <li>Short-list top 3 models</li> </ul> </li> <li>Fine-tune the system<ul> <li>Bring back the full train set for this step</li> <li>Fine tune hyperparameters using cross-validation</li> <li>try ensemble methods</li> <li>measure performance on test set (generalization error). Don't tune the model beyond this step (if you tune to perform on test set, you end up with an overfit model)</li> </ul> </li> <li>Present the solution<ul> <li>Document and clean up the code</li> <li>Make a business presentation - explaining how business objective is being met / exceeded.</li> <li>Highlight interesting / hidden info that was discovered. Use as viz lavishly.</li> </ul> </li> </ol>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/","title":"MNIST digits classification using Logistic regression in Scikit-Learn","text":"In\u00a0[1]: Copied! <pre>from sklearn.datasets import load_digits\ndigits = load_digits()\n</pre> from sklearn.datasets import load_digits digits = load_digits() In\u00a0[2]: Copied! <pre>type(digits.data)\n</pre> type(digits.data) Out[2]: <pre>numpy.ndarray</pre> In\u00a0[3]: Copied! <pre>(digits.data.shape, digits.target.shape, digits.images.shape)\n</pre> (digits.data.shape, digits.target.shape, digits.images.shape) Out[3]: <pre>((1797, 64), (1797,), (1797, 8, 8))</pre> <p><code>1797</code> images, each <code>8x8</code> in dimension and <code>1797</code> labels.</p> In\u00a0[4]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[5]: Copied! <pre>plt.figure(figsize=(20,4))\nfor index, (image, label) in enumerate(zip(digits.data[0:5], \n                                           digits.target[0:5])):\n    plt.subplot(1, 5, index + 1)\n    plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n    plt.title('Training: %i\\n' % label, fontsize = 20);\n</pre> plt.figure(figsize=(20,4)) for index, (image, label) in enumerate(zip(digits.data[0:5],                                             digits.target[0:5])):     plt.subplot(1, 5, index + 1)     plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)     plt.title('Training: %i\\n' % label, fontsize = 20); In\u00a0[5]: Copied! <pre>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data, \n                                                    digits.target,\n                                                   test_size=0.25,\n                                                   random_state=0)\n</pre> from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(digits.data,                                                      digits.target,                                                    test_size=0.25,                                                    random_state=0) In\u00a0[7]: Copied! <pre>X_train.shape, X_test.shape\n</pre> X_train.shape, X_test.shape Out[7]: <pre>((1347, 64), (450, 64))</pre> In\u00a0[6]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(fit_intercept=True,\n                        multi_class='auto',\n                        penalty='l2', #ridge regression\n                        solver='saga',\n                        max_iter=10000,\n                        C=50)\nclf\n</pre> from sklearn.linear_model import LogisticRegression clf = LogisticRegression(fit_intercept=True,                         multi_class='auto',                         penalty='l2', #ridge regression                         solver='saga',                         max_iter=10000,                         C=50) clf Out[6]: <pre>LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n                   warm_start=False)</pre> In\u00a0[9]: Copied! <pre>%%time\nclf.fit(X_train, y_train)\n</pre> %%time clf.fit(X_train, y_train) <pre>CPU times: user 6.81 s, sys: 9.52 ms, total: 6.81 s\nWall time: 6.82 s\n</pre> Out[9]: <pre>LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n                   warm_start=False)</pre> <p>Let us see what the classifier has learned</p> In\u00a0[10]: Copied! <pre>clf.classes_\n</pre> clf.classes_ Out[10]: <pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[11]: Copied! <pre>clf.coef_.shape\n</pre> clf.coef_.shape Out[11]: <pre>(10, 64)</pre> In\u00a0[12]: Copied! <pre>clf.coef_[0].round(2) # prints weights for 8x8 image for class 0\n</pre> clf.coef_[0].round(2) # prints weights for 8x8 image for class 0 Out[12]: <pre>array([ 0.  , -0.  , -0.04,  0.1 ,  0.06, -0.14, -0.16, -0.02, -0.  ,\n       -0.03, -0.04,  0.2 ,  0.09,  0.08, -0.05, -0.01, -0.  ,  0.06,\n        0.15, -0.03, -0.39,  0.25,  0.09, -0.  , -0.  ,  0.13,  0.16,\n       -0.18, -0.57,  0.02,  0.12, -0.  ,  0.  ,  0.16,  0.11, -0.16,\n       -0.41,  0.05,  0.08,  0.  , -0.  , -0.06,  0.27, -0.11, -0.2 ,\n        0.15,  0.04, -0.  , -0.  , -0.12,  0.08, -0.05,  0.2 ,  0.1 ,\n       -0.04, -0.01, -0.  , -0.01, -0.09,  0.21, -0.04, -0.06, -0.1 ,\n       -0.05])</pre> In\u00a0[13]: Copied! <pre>clf.intercept_ # for 10 classes - this is a One-vs-All classification\n</pre> clf.intercept_ # for 10 classes - this is a One-vs-All classification Out[13]: <pre>array([ 0.0010181 , -0.07236521,  0.00379207,  0.00459855,  0.04585855,\n        0.00014299, -0.00442972,  0.01179654,  0.04413398, -0.03454583])</pre> In\u00a0[14]: Copied! <pre>clf.n_iter_[0] # num of iterations before tolerance was reached\n</pre> clf.n_iter_[0] # num of iterations before tolerance was reached Out[14]: <pre>1876</pre> In\u00a0[16]: Copied! <pre>coef = clf.coef_.copy()\nplt.imshow(coef[0].reshape(8,8).round(2));  # proof of concept\n</pre> coef = clf.coef_.copy() plt.imshow(coef[0].reshape(8,8).round(2));  # proof of concept In\u00a0[17]: Copied! <pre>coef = clf.coef_.copy()\nscale = np.abs(coef).max()\nplt.figure(figsize=(10,5))\n\nfor i in range(10): # 0-9\n    coef_plot = plt.subplot(2, 5, i + 1) # 2x5 plot\n\n    coef_plot.imshow(coef[i].reshape(8,8), \n                     cmap=plt.cm.RdBu,\n                     vmin=-scale, vmax=scale,\n                    interpolation='bilinear')\n    \n    coef_plot.set_xticks(()); coef_plot.set_yticks(()) # remove ticks\n    coef_plot.set_xlabel(f'Class {i}')\n\nplt.suptitle('Coefficients for various classes');\n</pre> coef = clf.coef_.copy() scale = np.abs(coef).max() plt.figure(figsize=(10,5))  for i in range(10): # 0-9     coef_plot = plt.subplot(2, 5, i + 1) # 2x5 plot      coef_plot.imshow(coef[i].reshape(8,8),                       cmap=plt.cm.RdBu,                      vmin=-scale, vmax=scale,                     interpolation='bilinear')          coef_plot.set_xticks(()); coef_plot.set_yticks(()) # remove ticks     coef_plot.set_xlabel(f'Class {i}')  plt.suptitle('Coefficients for various classes'); <p>Now predict on unknown dataset and compare with ground truth</p> In\u00a0[18]: Copied! <pre>print(clf.predict(X_test[0:9]))\nprint(y_test[0:9])\n</pre> print(clf.predict(X_test[0:9])) print(y_test[0:9]) <pre>[2 8 2 6 6 7 1 9 8]\n[2 8 2 6 6 7 1 9 8]\n</pre> <p>Score against training and test data</p> In\u00a0[19]: Copied! <pre>clf.score(X_train, y_train) # training score\n</pre> clf.score(X_train, y_train) # training score Out[19]: <pre>1.0</pre> In\u00a0[20]: Copied! <pre>score = clf.score(X_test, y_test) # test score\nscore\n</pre> score = clf.score(X_test, y_test) # test score score Out[20]: <pre>0.9555555555555556</pre> <p>Test score: <code>0.9555</code></p> In\u00a0[21]: Copied! <pre>from sklearn import metrics\n</pre> from sklearn import metrics In\u00a0[22]: Copied! <pre>predictions = clf.predict(X_test)\n\ncm = metrics.confusion_matrix(y_true=y_test, \n                         y_pred = predictions, \n                        labels = clf.classes_)\ncm\n</pre> predictions = clf.predict(X_test)  cm = metrics.confusion_matrix(y_true=y_test,                           y_pred = predictions,                          labels = clf.classes_) cm Out[22]: <pre>array([[37,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0, 40,  0,  0,  0,  0,  0,  0,  2,  1],\n       [ 0,  0, 42,  2,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0, 43,  0,  0,  0,  0,  1,  1],\n       [ 0,  0,  0,  0, 37,  0,  0,  1,  0,  0],\n       [ 0,  0,  0,  0,  0, 46,  0,  0,  0,  2],\n       [ 0,  1,  0,  0,  0,  0, 51,  0,  0,  0],\n       [ 0,  0,  0,  1,  1,  0,  0, 46,  0,  0],\n       [ 0,  3,  1,  0,  0,  0,  0,  0, 43,  1],\n       [ 0,  0,  0,  0,  0,  1,  0,  0,  1, 45]])</pre> <p>Visualize confusion matrix as a heatmap</p> In\u00a0[23]: Copied! <pre>import seaborn as sns\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot=True, \n            linewidths=.5, square = True, cmap = 'Blues_r');\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nall_sample_title = 'Accuracy Score: {0}'.format(score)\nplt.title(all_sample_title);\n</pre> import seaborn as sns  plt.figure(figsize=(10,10)) sns.heatmap(cm, annot=True,              linewidths=.5, square = True, cmap = 'Blues_r');  plt.ylabel('Actual label') plt.xlabel('Predicted label') all_sample_title = 'Accuracy Score: {0}'.format(score) plt.title(all_sample_title); In\u00a0[24]: Copied! <pre>index = 0\nmisclassified_images = []\nfor label, predict in zip(y_test, predictions):\n    if label != predict: \n        misclassified_images.append(index)\n    index +=1\n</pre> index = 0 misclassified_images = [] for label, predict in zip(y_test, predictions):     if label != predict:          misclassified_images.append(index)     index +=1 In\u00a0[25]: Copied! <pre>print(misclassified_images)\n</pre> print(misclassified_images) <pre>[56, 94, 118, 124, 130, 169, 181, 196, 213, 251, 315, 325, 331, 335, 378, 398, 425, 429, 430, 440]\n</pre> In\u00a0[26]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.suptitle('Misclassifications');\n\nfor plot_index, bad_index in enumerate(misclassified_images[0:20]):\n    p = plt.subplot(4,5, plot_index+1) # 4x5 plot\n    \n    p.imshow(X_test[bad_index].reshape(8,8), cmap=plt.cm.gray,\n            interpolation='bilinear')\n    p.set_xticks(()); p.set_yticks(()) # remove ticks\n    \n    p.set_title(f'Pred: {predictions[bad_index]}, Actual: {y_test[bad_index]}');\n</pre> plt.figure(figsize=(10,10)) plt.suptitle('Misclassifications');  for plot_index, bad_index in enumerate(misclassified_images[0:20]):     p = plt.subplot(4,5, plot_index+1) # 4x5 plot          p.imshow(X_test[bad_index].reshape(8,8), cmap=plt.cm.gray,             interpolation='bilinear')     p.set_xticks(()); p.set_yticks(()) # remove ticks          p.set_title(f'Pred: {predictions[bad_index]}, Actual: {y_test[bad_index]}'); In\u00a0[7]: Copied! <pre>%%time\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml(data_id=554) # https://www.openml.org/d/554\n</pre> %%time from sklearn.datasets import fetch_openml mnist = fetch_openml(data_id=554) # https://www.openml.org/d/554 <pre>CPU times: user 15.3 s, sys: 348 ms, total: 15.6 s\nWall time: 15.6 s\n</pre> In\u00a0[8]: Copied! <pre>type(mnist)\n</pre> type(mnist) Out[8]: <pre>sklearn.utils.Bunch</pre> In\u00a0[9]: Copied! <pre>type(mnist.data), type(mnist.categories), type(mnist.feature_names), type(mnist.target)\n</pre> type(mnist.data), type(mnist.categories), type(mnist.feature_names), type(mnist.target) Out[9]: <pre>(numpy.ndarray, dict, list, numpy.ndarray)</pre> In\u00a0[10]: Copied! <pre>mnist.data.shape, mnist.target.shape\n</pre> mnist.data.shape, mnist.target.shape Out[10]: <pre>((70000, 784), (70000,))</pre> <p>There are <code>70,000</code> images, each of dimension <code>28x28</code> pixels.</p> In\u00a0[11]: Copied! <pre>mnist.target[0]\n</pre> mnist.target[0] Out[11]: <pre>'5'</pre> In\u00a0[12]: Copied! <pre>plt.figure(figsize=(20,4))\nfor index, (image, label) in enumerate(zip(mnist.data[0:5], \n                                           mnist.target[0:5])):\n    plt.subplot(1, 5, index + 1)\n    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n    plt.title('Training: ' + label, fontsize = 20);\n</pre> plt.figure(figsize=(20,4)) for index, (image, label) in enumerate(zip(mnist.data[0:5],                                             mnist.target[0:5])):     plt.subplot(1, 5, index + 1)     plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)     plt.title('Training: ' + label, fontsize = 20); In\u00a0[13]: Copied! <pre>mnist.target.astype('int')\n</pre> mnist.target.astype('int') Out[13]: <pre>array([5, 0, 4, ..., 4, 5, 6])</pre> In\u00a0[14]: Copied! <pre>from sklearn.model_selection import train_test_split\nX2_train, X2_test, y2_train, y2_test = train_test_split(mnist.data, \n                                                    mnist.target.astype('int'), #targets str to int convert\n                                                   test_size=1/7.0,\n                                                   random_state=0)\n</pre> from sklearn.model_selection import train_test_split X2_train, X2_test, y2_train, y2_test = train_test_split(mnist.data,                                                      mnist.target.astype('int'), #targets str to int convert                                                    test_size=1/7.0,                                                    random_state=0) In\u00a0[15]: Copied! <pre>X2_train.shape, X2_test.shape\n</pre> X2_train.shape, X2_test.shape Out[15]: <pre>((60000, 784), (10000, 784))</pre> <p>Are the different classes evenly distributed? We can find this by plotting a histogram of the labels in both test and training datasets.</p> In\u00a0[77]: Copied! <pre>plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.hist(y2_train);\nplt.title('Frequency of different classes - Training data');\n\nplt.subplot(1,2,2)\nplt.hist(y2_test);\nplt.title('Frequency of different classes - Test data');\n</pre> plt.figure(figsize=(10,5)) plt.subplot(1,2,1) plt.hist(y2_train); plt.title('Frequency of different classes - Training data');  plt.subplot(1,2,2) plt.hist(y2_test); plt.title('Frequency of different classes - Test data'); In\u00a0[57]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nclf2 = LogisticRegression(fit_intercept=True,\n                        multi_class='auto',\n                        penalty='l1', #lasso regression\n                        solver='saga',\n                        max_iter=1000,\n                        C=50,\n                        verbose=2, # output progress\n                        n_jobs=5, # parallelize over 5 processes\n                        tol=0.01\n                         )\nclf2\n</pre> from sklearn.linear_model import LogisticRegression clf2 = LogisticRegression(fit_intercept=True,                         multi_class='auto',                         penalty='l1', #lasso regression                         solver='saga',                         max_iter=1000,                         C=50,                         verbose=2, # output progress                         n_jobs=5, # parallelize over 5 processes                         tol=0.01                          ) clf2 Out[57]: <pre>LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=5, penalty='l1',\n                   random_state=None, solver='saga', tol=0.01, verbose=2,\n                   warm_start=False)</pre> <p>Since there are <code>10</code> classes and <code>12</code> available cores, we will try to run the learning step in <code>5</code> jobs. Earlier, when I did not parallelize, the job did not finish within 1 hour, when I had to put the machine to sleep for a meeting.</p> In\u00a0[58]: Copied! <pre>%%time\nclf2.fit(X2_train, y2_train)\n</pre> %%time clf2.fit(X2_train, y2_train) <pre>[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n</pre> <pre>convergence after 47 epochs took 143 seconds\nCPU times: user 9min 30s, sys: 469 ms, total: 9min 30s\nWall time: 2min 22s\n</pre> <pre>[Parallel(n_jobs=5)]: Done   1 out of   1 | elapsed:  2.4min finished\n</pre> Out[58]: <pre>LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=5, penalty='l1',\n                   random_state=None, solver='saga', tol=0.01, verbose=2,\n                   warm_start=False)</pre> <p>Note: Since the verbosity is set <code>&gt;0</code>, the messages were printed, but they got printed on the terminal, not in the notebook.</p> <p>Let us see what the classifier has learned</p> In\u00a0[29]: Copied! <pre>clf2.classes_\n</pre> clf2.classes_ Out[29]: <pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[30]: Copied! <pre>clf2.coef_.shape\n</pre> clf2.coef_.shape Out[30]: <pre>(10, 784)</pre> <p>Get the coefficients for a single class, <code>1</code> in this case:</p> In\u00a0[59]: Copied! <pre>clf2.coef_[1].round(3) # prints weights for 8x8 image for class 0\n</pre> clf2.coef_[1].round(3) # prints weights for 8x8 image for class 0 Out[59]: <pre>array([ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   , -0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   ,\n       -0.001, -0.001, -0.001,  0.   ,  0.002,  0.004,  0.001,  0.002,\n        0.002,  0.001, -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n       -0.   , -0.   , -0.   , -0.001, -0.001, -0.002, -0.002, -0.003,\n        0.001,  0.002, -0.001,  0.001,  0.002,  0.   , -0.002,  0.   ,\n       -0.001, -0.001, -0.001, -0.001, -0.001, -0.   , -0.   ,  0.   ,\n        0.   ,  0.   ,  0.   , -0.   ,  0.   ,  0.001,  0.   , -0.   ,\n       -0.   , -0.   ,  0.   , -0.   ,  0.   ,  0.001,  0.   ,  0.   ,\n       -0.   ,  0.001, -0.001,  0.001,  0.   , -0.   , -0.001, -0.001,\n       -0.003, -0.002, -0.   , -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.003,  0.001, -0.002, -0.003, -0.002, -0.003, -0.003,\n       -0.002,  0.   , -0.001,  0.001, -0.001, -0.001,  0.   , -0.001,\n        0.   ,  0.   ,  0.002,  0.002, -0.001, -0.002, -0.   , -0.   ,\n        0.   ,  0.   , -0.   , -0.   ,  0.   ,  0.001,  0.   , -0.003,\n       -0.002, -0.001,  0.   ,  0.   , -0.002, -0.002, -0.001, -0.002,\n       -0.001, -0.003,  0.   , -0.001, -0.   , -0.   , -0.   ,  0.   ,\n       -0.001, -0.002, -0.   , -0.   ,  0.   , -0.   , -0.   , -0.   ,\n        0.   , -0.   , -0.002, -0.001, -0.003,  0.   , -0.   , -0.002,\n        0.001, -0.002,  0.001, -0.003,  0.   , -0.001, -0.002, -0.   ,\n        0.001,  0.   , -0.002, -0.001, -0.003, -0.002, -0.   , -0.   ,\n        0.   , -0.   , -0.   , -0.   ,  0.001, -0.001, -0.002,  0.001,\n       -0.002, -0.003, -0.001, -0.002, -0.   ,  0.001, -0.001,  0.   ,\n       -0.003,  0.001, -0.001,  0.001, -0.002, -0.001, -0.001, -0.003,\n       -0.004, -0.002, -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   , -0.002, -0.001,  0.001,  0.   , -0.002, -0.001, -0.001,\n       -0.001, -0.   , -0.   ,  0.003,  0.001, -0.001,  0.   , -0.002,\n       -0.002, -0.001, -0.001, -0.003, -0.002, -0.002, -0.   , -0.   ,\n       -0.   , -0.   , -0.   , -0.001, -0.001, -0.003, -0.002, -0.001,\n        0.   , -0.001, -0.001,  0.001, -0.   ,  0.002,  0.003,  0.003,\n        0.002,  0.001, -0.001, -0.   , -0.002,  0.   , -0.001, -0.001,\n       -0.002, -0.001, -0.   ,  0.   ,  0.   ,  0.   , -0.   , -0.   ,\n       -0.001, -0.002, -0.004, -0.003, -0.002, -0.001, -0.001, -0.001,\n       -0.001,  0.001,  0.003,  0.003,  0.   , -0.001,  0.   , -0.001,\n       -0.   , -0.002, -0.001, -0.001, -0.001, -0.   , -0.   ,  0.   ,\n        0.   , -0.   , -0.   , -0.   ,  0.   , -0.001, -0.001, -0.001,\n       -0.001,  0.   , -0.001, -0.002, -0.   ,  0.002,  0.005,  0.003,\n        0.   , -0.   , -0.001, -0.001, -0.001, -0.   , -0.   , -0.   ,\n       -0.001, -0.   , -0.   ,  0.   ,  0.   , -0.   , -0.   ,  0.   ,\n        0.001,  0.   ,  0.   , -0.001,  0.   ,  0.001, -0.004, -0.002,\n        0.   ,  0.001,  0.002,  0.002,  0.001,  0.003, -0.003, -0.002,\n        0.   ,  0.   , -0.001, -0.001, -0.001, -0.   , -0.   , -0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.001,  0.   , -0.001, -0.001,\n        0.001, -0.001, -0.003, -0.002, -0.   ,  0.001,  0.004,  0.002,\n        0.001, -0.   , -0.003, -0.003, -0.   , -0.001, -0.001, -0.001,\n       -0.   , -0.   , -0.   , -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n       -0.   , -0.001, -0.001, -0.   ,  0.001, -0.002, -0.003, -0.   ,\n        0.001,  0.002,  0.004, -0.001,  0.003, -0.001, -0.002, -0.005,\n       -0.002, -0.001, -0.001, -0.001, -0.   , -0.   , -0.   , -0.   ,\n        0.   ,  0.   ,  0.   , -0.   , -0.   , -0.001, -0.001, -0.001,\n       -0.001,  0.001, -0.   , -0.   , -0.001,  0.001,  0.003, -0.002,\n        0.001, -0.005, -0.003, -0.003, -0.001, -0.   , -0.001, -0.001,\n        0.001, -0.001, -0.   , -0.   ,  0.   ,  0.   ,  0.   , -0.   ,\n       -0.   , -0.001, -0.002, -0.002, -0.001, -0.001,  0.   , -0.001,\n        0.001,  0.003,  0.002,  0.001, -0.001, -0.005, -0.001, -0.   ,\n       -0.   ,  0.   ,  0.   , -0.   , -0.   , -0.001,  0.   , -0.   ,\n       -0.   ,  0.   , -0.   , -0.   , -0.   , -0.003, -0.005, -0.003,\n        0.   ,  0.   , -0.002, -0.001,  0.   , -0.   ,  0.002, -0.001,\n       -0.003,  0.   ,  0.002,  0.   ,  0.002,  0.   , -0.001, -0.   ,\n        0.001,  0.002,  0.001,  0.   ,  0.   ,  0.   , -0.   , -0.   ,\n       -0.001, -0.005, -0.002,  0.001, -0.001,  0.001, -0.001, -0.001,\n       -0.   ,  0.002, -0.002, -0.001,  0.002, -0.001, -0.001,  0.001,\n        0.001, -0.001, -0.001,  0.   ,  0.002,  0.001,  0.001,  0.   ,\n        0.   , -0.   , -0.   , -0.   , -0.   , -0.005, -0.   ,  0.   ,\n        0.   ,  0.002,  0.003,  0.002,  0.001, -0.001,  0.001, -0.   ,\n        0.001,  0.002,  0.003,  0.001,  0.   , -0.001, -0.   ,  0.001,\n        0.001,  0.001,  0.001,  0.   ,  0.   ,  0.   , -0.   ,  0.001,\n        0.002,  0.003,  0.003,  0.002,  0.002, -0.001,  0.001, -0.001,\n       -0.   ,  0.001,  0.002,  0.001,  0.001,  0.001,  0.003,  0.001,\n        0.003,  0.003, -0.001,  0.   ,  0.003,  0.001,  0.   ,  0.   ,\n        0.   ,  0.   , -0.   ,  0.   ,  0.001,  0.007,  0.003,  0.   ,\n       -0.   ,  0.001, -0.002, -0.001, -0.002, -0.004, -0.001, -0.   ,\n       -0.   ,  0.   ,  0.002,  0.001,  0.003,  0.002, -0.   , -0.   ,\n        0.001,  0.001,  0.   ,  0.   ,  0.   ,  0.   , -0.   , -0.001,\n       -0.002,  0.002,  0.001,  0.001,  0.   ,  0.001,  0.   ,  0.   ,\n        0.001,  0.001, -0.001,  0.002,  0.001,  0.002, -0.   ,  0.   ,\n       -0.001, -0.001, -0.001, -0.001, -0.   , -0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   , -0.001, -0.001, -0.001, -0.001, -0.   ,\n       -0.001,  0.001, -0.001, -0.001, -0.001, -0.001, -0.001, -0.   ,\n       -0.004,  0.001,  0.001,  0.   , -0.001, -0.001, -0.001, -0.   ,\n       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   , -0.   ,\n       -0.   , -0.001, -0.001, -0.002, -0.001, -0.003, -0.006, -0.004,\n       -0.001, -0.002, -0.002, -0.003, -0.004, -0.003, -0.002, -0.002,\n       -0.001, -0.   , -0.   , -0.   , -0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   ,  0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   , -0.   , -0.001, -0.001, -0.002, -0.002, -0.001, -0.001,\n       -0.   , -0.   , -0.001, -0.001, -0.   , -0.   , -0.   , -0.   ,\n        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n        0.   ,  0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   , -0.   ,\n       -0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ])</pre> <pre>convergence after 591 epochs took 1805 seconds\n</pre> In\u00a0[60]: Copied! <pre>clf2.intercept_ # for 10 classes - this is a One-vs-All classification\n</pre> clf2.intercept_ # for 10 classes - this is a One-vs-All classification Out[60]: <pre>array([-1.11398188e-04,  1.38709472e-04,  1.16909054e-04, -2.37842193e-04,\n        6.62466316e-05,  8.48133979e-04, -4.22181499e-05,  2.66499796e-04,\n       -8.62715013e-04, -1.82325388e-04])</pre> In\u00a0[78]: Copied! <pre>clf2.n_iter_[0] # num of iterations before tolerance was reached\n</pre> clf2.n_iter_[0] # num of iterations before tolerance was reached Out[78]: <pre>47</pre> In\u00a0[62]: Copied! <pre>coef = clf2.coef_.copy()\nscale = np.abs(coef).max()\nplt.figure(figsize=(13,7))\n\nfor i in range(10): # 0-9\n    coef_plot = plt.subplot(2, 5, i + 1) # 2x5 plot\n\n    coef_plot.imshow(coef[i].reshape(28,28), \n                     cmap=plt.cm.RdBu,\n                     vmin=-scale, vmax=scale,\n                    interpolation='bilinear')\n    \n    coef_plot.set_xticks(()); coef_plot.set_yticks(()) # remove ticks\n    coef_plot.set_xlabel(f'Class {i}')\n\nplt.suptitle('Coefficients for various classes');\n</pre> coef = clf2.coef_.copy() scale = np.abs(coef).max() plt.figure(figsize=(13,7))  for i in range(10): # 0-9     coef_plot = plt.subplot(2, 5, i + 1) # 2x5 plot      coef_plot.imshow(coef[i].reshape(28,28),                       cmap=plt.cm.RdBu,                      vmin=-scale, vmax=scale,                     interpolation='bilinear')          coef_plot.set_xticks(()); coef_plot.set_yticks(()) # remove ticks     coef_plot.set_xlabel(f'Class {i}')  plt.suptitle('Coefficients for various classes'); <p>Now predict on unknown dataset and compare with ground truth</p> In\u00a0[63]: Copied! <pre>print(clf2.predict(X2_test[0:9]))\nprint(y2_test[0:9])\n</pre> print(clf2.predict(X2_test[0:9])) print(y2_test[0:9]) <pre>[0 4 1 2 4 7 7 1 1]\n[0 4 1 2 7 9 7 1 1]\n</pre> <p>Score against training and test data</p> In\u00a0[64]: Copied! <pre>clf2.score(X2_train, y2_train) # training score\n</pre> clf2.score(X2_train, y2_train) # training score Out[64]: <pre>0.9374333333333333</pre> In\u00a0[65]: Copied! <pre>score2 = clf2.score(X2_test, y2_test) # test score\nscore2\n</pre> score2 = clf2.score(X2_test, y2_test) # test score score2 Out[65]: <pre>0.9191</pre> <p>Test Score: <code>0.9191</code> or 91%</p> In\u00a0[46]: Copied! <pre>from sklearn import metrics\n</pre> from sklearn import metrics In\u00a0[66]: Copied! <pre>predictions2 = clf2.predict(X2_test)\n\ncm = metrics.confusion_matrix(y_true=y2_test, \n                         y_pred = predictions2, \n                        labels = clf2.classes_)\ncm\n</pre> predictions2 = clf2.predict(X2_test)  cm = metrics.confusion_matrix(y_true=y2_test,                           y_pred = predictions2,                          labels = clf2.classes_) cm Out[66]: <pre>array([[ 967,    0,    1,    2,    1,    9,    9,    0,    7,    0],\n       [   0, 1114,    5,    3,    1,    5,    0,    4,    7,    2],\n       [   3,   13,  931,   18,   11,    1,   15,   10,   34,    4],\n       [   1,    5,   33,  894,    0,   26,    2,   12,   27,   13],\n       [   1,    2,    5,    1,  897,    1,   11,    9,    7,   28],\n       [  10,    2,    6,   30,    9,  747,   16,    6,   30,    7],\n       [   7,    3,    6,    0,   11,   18,  938,    1,    5,    0],\n       [   2,    5,   13,    2,   11,    2,    1,  982,    4,   42],\n       [   4,   18,    8,   18,    6,   25,    9,    2,  861,   12],\n       [   3,    5,    6,   10,   35,    7,    2,   32,    9,  860]])</pre> In\u00a0[71]: Copied! <pre>import seaborn as sns\n\nplt.figure(figsize=(12,12))\nsns.heatmap(cm, annot=True, \n            linewidths=.5, square = True, cmap = 'Blues_r', fmt='0.4g');\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nall_sample_title = 'Accuracy Score: {0}'.format(score2)\nplt.title(all_sample_title);\n</pre> import seaborn as sns  plt.figure(figsize=(12,12)) sns.heatmap(cm, annot=True,              linewidths=.5, square = True, cmap = 'Blues_r', fmt='0.4g');  plt.ylabel('Actual label') plt.xlabel('Predicted label') all_sample_title = 'Accuracy Score: {0}'.format(score2) plt.title(all_sample_title); <pre>convergence after 5470 epochs took 10856 seconds\n</pre>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#mnist-digits-classification-using-logistic-regression-in-scikit-learn","title":"MNIST digits classification using Logistic regression in Scikit-Learn\u00b6","text":"<p>This notebook is broadly adopted from this blog and this scikit-learn example</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#logistic-regression-on-smaller-built-in-subset","title":"Logistic regression on smaller built-in subset\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#load-the-dataset","title":"Load the dataset\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#display-sample-data","title":"Display sample data\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#split-into-training-and-test","title":"Split into training and test\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#learning","title":"Learning\u00b6","text":"<p>Refer to the Logistic reg API ref for these parameters and the guide for equations, particularly how penalties are applied.</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#viewing-coefficients-as-an-image","title":"Viewing coefficients as an image\u00b6","text":"<p>Since there is a coefficient for each pixel in the <code>8x8</code> image, we can view them as an image itself. The code below is similar to the original viz code, but runs on coeff.</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#prediction-and-scoring","title":"Prediction and scoring\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#confusion-matrix","title":"Confusion matrix\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#inspecting-misclassified-images","title":"Inspecting misclassified images\u00b6","text":"<p>We compare predictions with labels to find which images are wrongly classified, then display them.</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#predicting-on-full-mnist-database","title":"Predicting on full MNIST database\u00b6","text":"<p>In the previous section, we worked with as tiny subset. In this section, we will download and play with the full MNIST dataset. Downloading for the first time from open ml db takes me about half a minute. Since this dataset is cached locally, subsequent runs should not take as much.</p>"},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#preview-some-images","title":"Preview some images\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#split-into-training-and-test","title":"Split into training and test\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#learning","title":"Learning\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#visualize-coefficients-as-an-image","title":"Visualize coefficients as an image\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#prediction-and-scoring","title":"Prediction and scoring\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#confusion-matrix","title":"Confusion matrix\u00b6","text":""},{"location":"projects/ml/mnist-digits-classification-using-logistic-regression-scikit-learn/#conclusion","title":"Conclusion\u00b6","text":"<p>This notebook shows performing multi-class classification using logistic regression using one-vs-all technique. When run on MNIST DB, the best accuracy is still just 91%. There is still scope for improvement.</p>"},{"location":"projects/ml/model-regularization/","title":"Reducing overfitting using regularization","text":""},{"location":"projects/ml/model-regularization/#examples-of-overfitting","title":"Examples of overfitting","text":"<p>A model is set to be over fitting, when it performs exceedingly well on training data but poorly on validation/test data. Such a model typically is of higher order and having a high variance.</p> <p></p> <p>The image above shows 3 models, the one of right is over fitting and the one on left is under fitting (has a high bias). Under fitting can happen when the model is too simple or uses too few features to model the complexity. A overfitting model has high variance because if you change or shuffle the input training set slightly, the model changes dramatically. In other words, it has high variability depending on the input set.</p> <p></p> <p>The graphic above shows the different levels of fit for logistic regression.</p>"},{"location":"projects/ml/model-regularization/#addressing-overfitting","title":"Addressing overfitting","text":"<p>Some options include</p> <ol> <li>reducing number of features. However this leads to reducing useful information available.</li> <li>Regularization. Here, we will keep all the features, but limit or constrain the magnitude their coefficients \\(\\theta_{j}\\). This works well even when you have a lot of features.</li> </ol>"},{"location":"projects/ml/model-regularization/#regularization-for-linear-regression","title":"Regularization for linear regression","text":"<p>Regularization is the process of applying penalty to coefficients of higher order variables. The higher the coefficients are for those variables, the higher the cost/loss is. Thus, the optimization process will move toward fits where such coefficients are smaller, close to <code>0</code>. Intuitively, this leads to a simpler model, that is less prone to overfitting. In practice, we may not know which variables are higher order polynomials. Thus, we add penalties to all coefficients. Thus, the new cost function looks like</p> <p>$$ J(\\theta) = \\frac{1}{2m}[\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\lambda \\sum_{j=1}^{n}\\theta_{j}^{2}] $$ where \\(\\lambda\\) is the regularization parameter. The first part of the loss function fights to get the best fit while the second fights to keep the model simple and coefficients smaller. If \\(\\lambda\\) is too high, then model results in underfitting which has a high bias. If \\(\\lambda\\) is too low, then it results in overfitting which has a high variance.</p>"},{"location":"projects/ml/model-regularization/#l1-l2-lasso-ridge-regularizations","title":"L1, L2 (Lasso, Ridge) regularizations","text":"<p>In the equation above, the \\(\\theta\\) was squared in the regularization function. This is L2 regularization, aka. Ridge regression. Remember this as L2 squares the coefficients (\\(\\theta\\)) attached to \\(\\lambda\\). </p> <p>Whereas, in the case of L1, the absolute value of \\(\\theta\\) is used. L1 is also called Lasso regression which stands for Least Absolute Shrinkage and Selection Operator. The same cost function for lasso would look like the below:</p> \\[ J(\\theta) = \\frac{1}{2m}[\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\lambda \\sum_{j=1}^{n}|\\theta_{j}|] \\] <p>Mathematically, we don't penalize \\(\\theta_{0}\\). However in practice, it makes little difference if you penalize all coefficients or if you ignore the intercept.</p>"},{"location":"projects/ml/model-regularization/#computing-gradient-descent-with-regularization","title":"Computing gradient descent with regularization","text":"<p>Computing the gradient descent for this new loss function, we get:</p> <p>$$ \\theta_{0} := \\theta_{0} - \\alpha\\frac{1}{m} \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})x_{0i} $$ $$ \\theta_{j} := \\theta_{j} - \\alpha[\\frac{1}{m} \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})x_{ji} - \\frac{\\lambda}{m}\\theta_{j}] $$ which is rewritten as $$ \\theta_{j} := \\theta_{j}(1-\\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})x_{ji} $$ The term \\((1-\\alpha\\frac{\\lambda}{m})\\) is always <code>&lt;1</code>, which has a shrinking effect on \\(\\theta\\). Thus, for each iteration, it strives to keep it small.</p>"},{"location":"projects/ml/model-regularization/#regularization-for-logistic-regression","title":"Regularization for logistic regression","text":"<p>The cost function of a non-regularized logistic function looks like</p> <p>$$ J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^{m}[y_{i}log(h_{\\theta}(x_{i})) + (1-y_{i})log(1-h_{\\theta}(x_{i}))] $$ to this cost function, we add the regularization parameter to get: $$ J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^{m}[y_{i}log(h_{\\theta}(x_{i})) + (1-y_{i})log(1-h_{\\theta}(x_{i}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2} $$ The regularization term avoids penalty for bias term \\(\\theta_{0}\\).</p> <p>The gradient descent for logistic regression with regularization is identical to that of the linear regression explained earlier, except that, the hypothesis function is a sigmoid.</p>"},{"location":"projects/ml/multivariate-linear-regression/","title":"Solving multivariate linear regression using Gradient Descent","text":"<p>Note: This is a continuation of Gradient Descent topic. The context and equations used here derive from that article.</p> <p>When we regress for <code>y</code> using multiple predictors of <code>x</code>, the hypothesis function becomes:</p> \\[ h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + ... + \\theta_{n}x_{n} \\] <p>If we consider \\(x_{0} = 1\\), then the above can be represented as matrix multiplication using linear algebra.</p> \\[ x = \\begin{bmatrix}     x_{0}\\\\\\     x_{1}\\\\\\     \\vdots\\\\\\     x_{n} \\end{bmatrix} \\ and \\ \\theta=\\begin{bmatrix}     \\theta_{0}\\\\\\     \\theta_{1}\\\\\\     \\vdots\\\\\\     \\theta_{n} \\end{bmatrix} \\] <p>Thus,</p> \\[ h_{\\theta}(x) = \\theta^{T}x \\] <p>Here the dimensions of \\(\\theta\\) and <code>x</code> is <code>n+1</code> as this goes from <code>0</code> to <code>n</code>.</p>"},{"location":"projects/ml/multivariate-linear-regression/#loss-function-of-multivariate-linear-regression","title":"Loss function of multivariate linear regression","text":"<p>The loss function is given by</p> \\[ J(\\theta_{0}, \\theta_{1},...,\\theta_{n}) = \\frac{1}{2m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]^{2} \\] <p>which you can simplify to </p> \\[ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]^{2} \\] <p>The gradient descent of the loss function is now</p> <p>$$ \\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta) $$ Note: Here <code>j</code> represents the <code>n+1</code> features (attributes) and <code>i</code> goes from <code>1 -&gt; m</code> representing the <code>m</code> records.</p> <p>Simplifying the partial differential equation, we get the <code>n+1</code> update rules as follows</p> \\[ \\theta_{0} := \\theta_{0} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]x_{0}^{{(i)}} $$ $$ \\theta_{1} := \\theta_{1} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]x_{1}^{{(i)}} $$ $$ \\vdots $$ $$ \\theta_{n} := \\theta_{n} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}[h_{\\theta}(x^{(i)}) - y^{(i)}]x_{n}^{{(i)}} \\] <p>The equations above are very similar to ones from simple linear equations.</p>"},{"location":"projects/ml/multivariate-linear-regression/#impact-of-scaling-on-gradient-descent","title":"Impact of scaling on Gradient Descent","text":"<p>When the data ranges of features varies quite a bit from each other, the surface of GD is highly skewed as shown below:</p> <p></p> <p>This is because \\(\\theta\\) (which is our weights) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. When scaled, the surface takes a healthier shape, allowing the algorithm to converge faster. Ideally scale values so they fall within <code>-1 to 1</code>.</p>"},{"location":"projects/ml/multivariate-linear-regression/#scaling-methods","title":"Scaling methods","text":"<p>Feature Scaling is simply dividing values by range. Normalization is when you transform them to have a <code>mean = 0</code>.</p> <p>Mean normalization  $$ scaled \\ x_{j} = \\frac{(x_{j} - \\mu_{j})}{s_{j}} $$ where \\(\\mu\\) is mean and <code>s</code> is range.</p> <p>Standard normalization is similar to above, except, <code>s</code> is standard deviation.</p> <p>The exact range of normalization is less important than having all features follow a particular range.</p>"},{"location":"projects/ml/multivariate-linear-regression/#debugging-gradient-descent","title":"Debugging Gradient Descent","text":"<p>The general premise is, as number of iterations increase, the loss should reduce. You can also declare a threshold and if the loss reduces below that for <code>n</code> number of iterations, then you can declare convergence. However, Andrew Ng suggests against this and suggests visualizing the loss on a chart to pick LR.</p> <p>When LR is too high: If you have a diverging graph - loss increases steadily or if the loss is oscillating (pic below), it is likely the the rate is too high. In case of oscillation, the weights sporadically hit the local minima but continue to overshoot.</p> <p></p> <p>Iterating through a number of LRs: Andrew suggests picking a range of LRs <code>0.001, 0.01, 0.1, 1, ...</code> and iterating through them. He typically bumps rates by a factor of <code>10</code>. For convenience, he picks <code>..0.001, 0.003, 0.01, 0.03, 0.1, 0.3..</code> where he bumps by <code>~3</code> which is also effective.</p>"},{"location":"projects/ml/multivariate-linear-regression/#non-linear-functions-vs-non-linear-models","title":"Non-linear functions vs non-linear models","text":"<p>A linear function is one which produces a straight line. It is typically of the form \\(y = \\theta_{0} + \\theta_{1}x_{1}\\). A non-linear function is something that produces a curve. It is typically of the from \\(y = \\theta_{0} + \\theta_{1}x^{k}\\). A linear model is when the model parameters are additive, even though individual parameters are non-linear. It takes form \\(y = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{n}x_{n}\\). A non-linear model is when the model parameters are multiplicative even though they are of order <code>1</code>. It typically takes form \\(y = \\theta_{0}x_{1}theta_{1}x_{2}^{k}x_{3}\\) etc.</p>"},{"location":"projects/ml/multivariate-linear-regression/#representing-non-linearity-using-polynomial-regression","title":"Representing non-linearity using Polynomial Regression","text":"<p>Sometimes, when you plot the response variable with one of the predictors, it may not take a linear form. You might want an order <code>2</code> or <code>3</code> curve. You can still represent them using linear models. Consider the case where square footage is one of the parameters in predicting house price and you notice a non-linear relationship. From the graphic below, you might try a quadratic model as \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2}^{2}\\). But this model will eventually taper off. Instead, you may try a cubic model as \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2}^{2} + \\theta_{3}x_{3}^{3}\\).</p> <p></p> <p>The way to represent non-linearity is to sequentially raise the power / order of the parameter, represent them as additional features. This is a step in feature engineering. This method is called polynomial regression. When you raise the power, the range of that parameter also increases exponentially. Thus you model might become highly skewed. It is vital to scale features in a polynomial regression.</p> <p>Another option here is, instead of raising power, you take square roots or nth roots, such as: \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}\\sqrt{x_{2}} + \\theta_{3}\\sqrt[3]{x_{3}}\\)</p>"},{"location":"projects/ml/sklearn-1/","title":"Scikit Learn syntax","text":"In\u00a0[\u00a0]: Copied!"},{"location":"projects/ml/sklearn-1/#scikit-learn-syntax","title":"Scikit Learn syntax\u00b6","text":""},{"location":"projects/ml/sklearn-1/#library-constructs","title":"Library constructs\u00b6","text":""},{"location":"projects/ml/sklearn-1/#estimator","title":"Estimator\u00b6","text":"<p>Every algorithm is exposed via an <code>Estimator</code> which can be imported as</p> <pre>from sklearn.&lt;family&gt; import &lt;model&gt;\n</pre> <p>for linear regression</p> <pre>from sklearn.linear_model import LinearRegression\nlm_model = LinearRegression(&lt;estimator parameters&gt;)\n</pre> <p>Estimator parameters are provided as arguments when you instantiate an Estimator. Sklearn provides good defaults.</p> <p>In Scikit-learn, Estimators are designed such that</p> <ul> <li>consistency: all estimators share a common interface</li> <li>inspection: the hyperparameters you set when you instantiate an estimator is available for inspection as properties of that object</li> <li>limited hierarchy: only the algorithms are represented as Python objects. Training data, results, parameter names follow standard Python or Numpy / Pandas types</li> <li>composition: many workflows can be achieved as a series of more fundamental algorithms</li> <li>sensible defaults: you guessed it.</li> </ul>"},{"location":"projects/ml/sklearn-1/#general-steps-when-using-scikit-learn","title":"General steps when using Scikit-learn\u00b6","text":"<ul> <li>choose a class of model</li> <li>instantiate a model from the class by specifying hyperparameters to its constructor</li> <li>arrange data into <code>X</code> and <code>y</code> and split them for training and testing</li> <li>fit / learn the model on training data by calling <code>fit()</code> method</li> <li>predict new values by calling the <code>predict()</code> method</li> <li>evaluate results</li> </ul>"},{"location":"projects/ml/sklearn-1/#train-test-split","title":"Train-test split\u00b6","text":"<p>To split the input data into train and validation sets, use</p> <pre>from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n</pre> <p>to split it at 70% train and 30% test sets. This method splits both the dependent and independent attributes so as to validate the prediction.</p>"},{"location":"projects/ml/sklearn-1/#training","title":"Training\u00b6","text":"<p>The general syntax is <code>model.fit(independent_train, dependent_train)</code>. Thus</p> <pre>lm_model.fit(x_train, y_train)\n</pre> <p>In case of unsupervised models you only have a training data, no test data. Hence</p> <pre>model.fit(x_train)\n</pre> <p>In Scikit-Learn, by convention all model parameters that were learned during the <code>fit()</code> process have trailing underscores; for example in this linear model, we have <code>model.coef_</code>, <code>model.intercept_</code></p>"},{"location":"projects/ml/sklearn-1/#training-score","title":"Training score\u00b6","text":"<p>A <code>model.score()</code> method returns the a value <code>0-1</code> illustrating how well the model fitted the training data. Note this is useful to understand the influence of underfitting and overfitting of training data.</p>"},{"location":"projects/ml/sklearn-1/#prediction","title":"Prediction\u00b6","text":"<p>Use <code>model.predict(&lt;independent_test data&gt;)</code>. Thus for linear reg,</p> <pre>y_predicted = lm_model.predict(x_test)\n</pre>"},{"location":"projects/ml/sklearn-1/#prediction-probabilities","title":"Prediction probabilities\u00b6","text":"<p>In case of classification problems, you also get a <code>model.predict_proba()</code> method which will return the probabilities for each class. The <code>model.predict()</code> will return the class with highest probability.</p>"},{"location":"projects/ml/sklearn-1/#transformation","title":"Transformation\u00b6","text":"<p>Relevant in unsupervised models, <code>model.transform()</code> is used to transform input data to a new basis. Some models combine the fitting and transformation in one step using the <code>model.fit_transform()</code> method.</p>"},{"location":"projects/ml/sklearn-1/#validation","title":"Validation\u00b6","text":"<p>You can obtain the MAE (Mean Absolute Error), MSE (Mean Squared Error) and RMSE (Root Mean Squared Error) from the <code>metrics</code> module.</p> <pre>from sklearn import metrics\nimport numpy.np\n\nmetrics.mean_absolute_error(y_test, y_predicted)\nmetrics.mean_squared_error(y_test, y_predicted)\nnp.sqrt(metrics.mean_squared_error(y_test, y_predicted)\n</pre>"},{"location":"projects/ml/sklearn_naive_bayes_classifier/","title":"Naive Bayes classification - Sklearn","text":"<p>In machine learning, Naive Bayes is used to compute conditional probability of predicted class $y$ occuring given all the predictor variables $x$. In other words, Bayes theorem relates P(outcome/evidence) (what we want to predict) to P(evidence / outcome) (training set).</p> <p>The algorithm builds the conditional probability and applies prediction. The algorithm is called naive because it assumes independence between predictor variables, while in reality this may not be true in all cases.</p> <p>$$ P(A_i | B_j) = \\frac{P(B_j | A_i)P(A_i)}{P(B_j | A_1)P(A_1) + P(B_j | A_2)P(A_2) + ... + P(B_j | A_k)P(A_k)} $$</p> <p>Consider $A_1 ,... A_k$ as $k$ predictor variables in machine learning. The Naive Bayes classifier will build the conditional probabilities of $p(B_j|A_k)$ to later predict what would $p(A_i | B_j)$ be.</p> In\u00a0[1]: Copied! <pre>import seaborn as sns\niris = sns.load_dataset('iris')\n</pre> import seaborn as sns iris = sns.load_dataset('iris') In\u00a0[2]: Copied! <pre>iris.head()\n</pre> iris.head() Out[2]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa In\u00a0[3]: Copied! <pre>iris.shape\n</pre> iris.shape Out[3]: <pre>(150, 5)</pre> In\u00a0[9]: Copied! <pre>iris['species'].value_counts().plot(kind='bar')\n</pre> iris['species'].value_counts().plot(kind='bar') Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a0d0e03c8&gt;</pre> In\u00a0[13]: Copied! <pre>%matplotlib inline\nsns.pairplot(iris, hue='species')\n</pre> %matplotlib inline sns.pairplot(iris, hue='species') Out[13]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x1a17a970b8&gt;</pre> In\u00a0[2]: Copied! <pre>X_iris = iris.drop('species', axis=1)\nX_iris.shape\n</pre> X_iris = iris.drop('species', axis=1) X_iris.shape Out[2]: <pre>(150, 4)</pre> In\u00a0[3]: Copied! <pre>y_iris = iris['species']\ny_iris.shape\n</pre> y_iris = iris['species'] y_iris.shape Out[3]: <pre>(150,)</pre> <p><code>X_iris</code> is in caps as <code>X</code> is a vector of multiple features for each record, whereas <code>y_iris</code> is in small case as it is a scalar for each record.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/ml/sklearn_naive_bayes_classifier/#naive-bayes-classification-sklearn","title":"Naive Bayes classification - Sklearn\u00b6","text":"<p>Naive Bayes classifier builds directly on conditional probability and this</p> <p>$$ p(y|x) = \\frac{p(y \\cap x)}{p(x)} $$ from the above formula, $p(y \\cap x)$ can be written as</p> <p>$$ p(y \\cap x) = p(x | y).p(y) $$</p> <p>thus</p> <p>$$ p(y|x) = \\frac{p(x|y).p(y)}{p(x)} $$</p>"},{"location":"projects/ml/sklearn_naive_bayes_classifier/#naive-bayes-on-iris-dataset","title":"Naive Bayes on Iris dataset\u00b6","text":""},{"location":"projects/ml/sklearn_naive_bayes_classifier/#eda","title":"EDA\u00b6","text":""},{"location":"projects/ml/sklearn_naive_bayes_classifier/#create-features-and-labels","title":"Create features and labels\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/","title":"Applying Linear Regression with scikit-learn and statmodels","text":"<p><code>Scikit-learn</code> is one of the science kits for <code>SciPy</code> stack. Scikit has a collection of prediction and learning algorithms, grouped into</p> <ul> <li>classification</li> <li>clustering</li> <li>regression</li> <li>dimensionality reduction</li> </ul> <p>Each algorithm follows a typical pattern with a <code>fit</code>, <code>predict</code> method. In addition you get a set of utility methods that help with splitting datasets into train-test sets and for validating the outputs.</p> In\u00a0[3]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n</pre> import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns In\u00a0[4]: Copied! <pre>usa_house = pd.read_csv('../udemy_ml_bootcamp/Machine Learning Sections/Linear-Regression/USA_housing.csv')\nusa_house.head(5)\n</pre> usa_house = pd.read_csv('../udemy_ml_bootcamp/Machine Learning Sections/Linear-Regression/USA_housing.csv') usa_house.head(5) Out[4]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price Address 0 79545.45857 5.682861 7.009188 4.09 23086.80050 1.059034e+06 208 Michael Ferry Apt. 674\\nLaurabury, NE 3701... 1 79248.64245 6.002900 6.730821 3.09 40173.07217 1.505891e+06 188 Johnson Views Suite 079\\nLake Kathleen, CA... 2 61287.06718 5.865890 8.512727 5.13 36882.15940 1.058988e+06 9127 Elizabeth Stravenue\\nDanieltown, WI 06482... 3 63345.24005 7.188236 5.586729 3.26 34310.24283 1.260617e+06 USS Barnett\\nFPO AP 44820 4 59982.19723 5.040555 7.839388 4.23 26354.10947 6.309435e+05 USNS Raymond\\nFPO AE 09386 In\u00a0[4]: Copied! <pre>usa_house.info()\n</pre> usa_house.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 7 columns):\nAvg Income                 5000 non-null float64\nAvg House Age              5000 non-null float64\nAvg. Number of Rooms       5000 non-null float64\nAvg. Number of Bedrooms    5000 non-null float64\nArea Population            5000 non-null float64\nPrice                      5000 non-null float64\nAddress                    5000 non-null object\ndtypes: float64(6), object(1)\nmemory usage: 273.5+ KB\n</pre> In\u00a0[5]: Copied! <pre>usa_house.describe()\n</pre> usa_house.describe() Out[5]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price count 5000.000000 5000.000000 5000.000000 5000.000000 5000.000000 5.000000e+03 mean 68583.108984 5.977222 6.987792 3.981330 36163.516039 1.232073e+06 std 10657.991214 0.991456 1.005833 1.234137 9925.650114 3.531176e+05 min 17796.631190 2.644304 3.236194 2.000000 172.610686 1.593866e+04 25% 61480.562390 5.322283 6.299250 3.140000 29403.928700 9.975771e+05 50% 68804.286405 5.970429 7.002902 4.050000 36199.406690 1.232669e+06 75% 75783.338665 6.650808 7.665871 4.490000 42861.290770 1.471210e+06 max 107701.748400 9.519088 10.759588 6.500000 69621.713380 2.469066e+06 <p>Find the correlation between each of the numerical columns to the house price</p> In\u00a0[6]: Copied! <pre>sns.pairplot(usa_house)\n</pre> sns.pairplot(usa_house) Out[6]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x108258ef0&gt;</pre> <p>From this chart, we now,</p> <ul> <li>distribution of house price is normal (last chart)</li> <li>some scatters show a higher correlation, while some other show no correlation.</li> </ul> In\u00a0[7]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize=[15,5])\nsns.distplot(usa_house['Price'], ax=axs[0])\nsns.heatmap(usa_house.corr(), ax=axs[1], annot=True)\nfig.tight_layout()\n</pre> fig, axs = plt.subplots(1,2, figsize=[15,5]) sns.distplot(usa_house['Price'], ax=axs[0]) sns.heatmap(usa_house.corr(), ax=axs[1], annot=True) fig.tight_layout() <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> In\u00a0[8]: Copied! <pre>usa_house.columns\n</pre> usa_house.columns Out[8]: <pre>Index(['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population', 'Price', 'Address'],\n      dtype='object')</pre> In\u00a0[9]: Copied! <pre>X = usa_house[['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population']]\n\ny = usa_house[['Price']]\n</pre> X = usa_house[['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',        'Avg. Number of Bedrooms', 'Area Population']]  y = usa_house[['Price']] In\u00a0[10]: Copied! <pre>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n</pre> from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33) In\u00a0[11]: Copied! <pre>len(X_train)\n</pre> len(X_train) Out[11]: <pre>3350</pre> In\u00a0[12]: Copied! <pre>len(X_test)\n</pre> len(X_test) Out[12]: <pre>1650</pre> In\u00a0[13]: Copied! <pre>X_test.head()\n</pre> X_test.head() Out[13]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population 1066 64461.39215 7.949614 6.675121 2.04 34210.93608 4104 61687.39442 5.507913 6.995603 3.34 45279.16397 662 69333.68219 5.924392 6.542682 2.00 17187.11819 2960 74095.71281 5.908765 6.847362 3.00 32774.02197 1604 53066.37227 6.754571 8.062652 3.23 19103.12711 In\u00a0[14]: Copied! <pre>from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n</pre> from sklearn.linear_model import LinearRegression lm = LinearRegression() In\u00a0[15]: Copied! <pre>lm.fit(X_train, y_train) #no need to capture the return. All is stored in lm\n</pre> lm.fit(X_train, y_train) #no need to capture the return. All is stored in lm Out[15]: <pre>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</pre> <p>Create a table showing the coefficient (influence) of each of the columns</p> In\u00a0[16]: Copied! <pre>cdf = pd.DataFrame(lm.coef_[0], index=X_train.columns, columns=['coefficients'])\ncdf\n</pre> cdf = pd.DataFrame(lm.coef_[0], index=X_train.columns, columns=['coefficients']) cdf Out[16]: coefficients Avg Income 21.639515 Avg House Age 166094.788683 Avg. Number of Rooms 119855.858430 Avg. Number of Bedrooms 3037.782793 Area Population 15.215241 <p>Note, the coefficients for house age, number of rooms is pretty large. However that does not really mean they are more influential compared to income. It is simply because our dataset has not been normalized and the data range for each of these columns vary widely.</p> In\u00a0[17]: Copied! <pre>y_predicted = lm.predict(X_test)\nlen(y_predicted)\n</pre> y_predicted = lm.predict(X_test) len(y_predicted) Out[17]: <pre>1650</pre> In\u00a0[18]: Copied! <pre>plt.scatter(y_test, y_predicted) #actual vs predicted\n</pre> plt.scatter(y_test, y_predicted) #actual vs predicted Out[18]: <pre>&lt;matplotlib.collections.PathCollection at 0x1a1b4f4668&gt;</pre> In\u00a0[19]: Copied! <pre>sns.distplot((y_test - y_predicted))\n</pre> sns.distplot((y_test - y_predicted)) <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> Out[19]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1b519c88&gt;</pre> In\u00a0[20]: Copied! <pre>from sklearn import metrics\nmetrics.mean_absolute_error(y_test, y_predicted)\n</pre> from sklearn import metrics metrics.mean_absolute_error(y_test, y_predicted) Out[20]: <pre>80502.80373530561</pre> <p>RMSE</p> In\u00a0[21]: Copied! <pre>import numpy\nnumpy.sqrt(metrics.mean_squared_error(y_test, y_predicted))\n</pre> import numpy numpy.sqrt(metrics.mean_squared_error(y_test, y_predicted)) Out[21]: <pre>99779.47354600814</pre> In\u00a0[22]: Copied! <pre>X_test['predicted_price'] = y_predicted\n</pre> X_test['predicted_price'] = y_predicted <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"Entry point for launching an IPython kernel.\n</pre> In\u00a0[23]: Copied! <pre>X_test.head()\n</pre> X_test.head() Out[23]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population predicted_price 1066 64461.39215 7.949614 6.675121 2.04 34210.93608 1.396405e+06 4104 61687.39442 5.507913 6.995603 3.34 45279.16397 1.141590e+06 662 69333.68219 5.924392 6.542682 2.00 17187.11819 8.904433e+05 2960 74095.71281 5.908765 6.847362 3.00 32774.02197 1.267610e+06 1604 53066.37227 6.754571 8.062652 3.23 19103.12711 8.913813e+05 In\u00a0[24]: Copied! <pre>from sklearn.preprocessing import StandardScaler\ns_scaler = StandardScaler()\n</pre> from sklearn.preprocessing import StandardScaler s_scaler = StandardScaler() In\u00a0[25]: Copied! <pre># get all columns except 'Address' which is non numeric\nusa_house.columns[:-1]\n</pre> # get all columns except 'Address' which is non numeric usa_house.columns[:-1] Out[25]: <pre>Index(['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population', 'Price'],\n      dtype='object')</pre> In\u00a0[26]: Copied! <pre>usa_house_scaled = s_scaler.fit_transform(usa_house[usa_house.columns[:-1]])\nusa_house_scaled = pd.DataFrame(usa_house_scaled, columns=usa_house.columns[:-1])\nusa_house_scaled.head()\n</pre> usa_house_scaled = s_scaler.fit_transform(usa_house[usa_house.columns[:-1]]) usa_house_scaled = pd.DataFrame(usa_house_scaled, columns=usa_house.columns[:-1]) usa_house_scaled.head() Out[26]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price 0 1.028660 -0.296927 0.021274 0.088062 -1.317599 -0.490081 1 1.000808 0.025902 -0.255506 -0.722301 0.403999 0.775508 2 -0.684629 -0.112303 1.516243 0.930840 0.072410 -0.490211 3 -0.491499 1.221572 -1.393077 -0.584540 -0.186734 0.080843 4 -0.807073 -0.944834 0.846742 0.201513 -0.988387 -1.702518 In\u00a0[37]: Copied! <pre>usa_house_scaled.describe().round(3) # round the numbers for dispaly\n</pre> usa_house_scaled.describe().round(3) # round the numbers for dispaly Out[37]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price count 5000.000 5000.000 5000.000 5000.000 5000.000 5000.000 mean 0.000 -0.000 -0.000 -0.000 0.000 0.000 std 1.000 1.000 1.000 1.000 1.000 1.000 min -4.766 -3.362 -3.730 -1.606 -3.626 -3.444 25% -0.666 -0.661 -0.685 -0.682 -0.681 -0.664 50% 0.021 -0.007 0.015 0.056 0.004 0.002 75% 0.676 0.679 0.674 0.412 0.675 0.677 max 3.671 3.573 3.750 2.041 3.371 3.503 In\u00a0[27]: Copied! <pre>X_scaled = usa_house_scaled[['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population']]\n\ny_scaled = usa_house_scaled[['Price']]\n</pre> X_scaled = usa_house_scaled[['Avg Income', 'Avg House Age', 'Avg. Number of Rooms',        'Avg. Number of Bedrooms', 'Area Population']]  y_scaled = usa_house_scaled[['Price']] In\u00a0[28]: Copied! <pre>Xscaled_train, Xscaled_test, yscaled_train, yscaled_test = \\\ntrain_test_split(X_scaled, y_scaled, test_size=0.33)\n</pre> Xscaled_train, Xscaled_test, yscaled_train, yscaled_test = \\ train_test_split(X_scaled, y_scaled, test_size=0.33) In\u00a0[29]: Copied! <pre>lm_scaled = LinearRegression()\nlm_scaled.fit(Xscaled_train, yscaled_train)\n</pre> lm_scaled = LinearRegression() lm_scaled.fit(Xscaled_train, yscaled_train) Out[29]: <pre>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</pre> In\u00a0[30]: Copied! <pre>cdf_scaled = pd.DataFrame(lm_scaled.coef_[0], \n                          index=Xscaled_train.columns, columns=['coefficients'])\ncdf_scaled\n</pre> cdf_scaled = pd.DataFrame(lm_scaled.coef_[0],                            index=Xscaled_train.columns, columns=['coefficients']) cdf_scaled Out[30]: coefficients Avg Income 0.653151 Avg House Age 0.462883 Avg. Number of Rooms 0.341197 Avg. Number of Bedrooms 0.007156 Area Population 0.424653 In\u00a0[59]: Copied! <pre>lm_scaled.intercept_\n</pre> lm_scaled.intercept_ Out[59]: <pre>array([0.00215375])</pre> <p>From the table above, we notice <code>Avg Income</code> has more influence on the <code>Price</code> than other variables. This was not apparent before scaling the data. Further this corroborates the <code>correlation</code> matrix produced during exploratory data analysis.</p> In\u00a0[33]: Copied! <pre>import statsmodels.api as sm\nimport statsmodels\n</pre> import statsmodels.api as sm import statsmodels In\u00a0[32]: Copied! <pre>from statsmodels.regression import linear_model\n</pre> from statsmodels.regression import linear_model In\u00a0[34]: Copied! <pre>yscaled_train.shape\n</pre> yscaled_train.shape Out[34]: <pre>(3350, 1)</pre> In\u00a0[35]: Copied! <pre>Xscaled_train = sm.add_constant(Xscaled_train)\nsm_ols = linear_model.OLS(yscaled_train, Xscaled_train) # i know, the param order is inverse\nsm_model = sm_ols.fit()\n</pre> Xscaled_train = sm.add_constant(Xscaled_train) sm_ols = linear_model.OLS(yscaled_train, Xscaled_train) # i know, the param order is inverse sm_model = sm_ols.fit() In\u00a0[36]: Copied! <pre>sm_model.summary()\n</pre> sm_model.summary() Out[36]: OLS Regression Results Dep. Variable: Price   R-squared:             0.918 Model: OLS   Adj. R-squared:        0.917 Method: Least Squares   F-statistic:           7446. Date: Thu, 23 Aug 2018   Prob (F-statistic):   0.00 Time: 16:25:05   Log-Likelihood:      -552.85 No. Observations:   3350   AIC:                   1118. Df Residuals:   3344   BIC:                   1154. Df Model:      5 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const    -0.0015     0.005    -0.311  0.756    -0.011     0.008 Avg Income     0.6532     0.005   129.977  0.000     0.643     0.663 Avg House Age     0.4629     0.005    93.799  0.000     0.453     0.473 Avg. Number of Rooms     0.3412     0.006    61.197  0.000     0.330     0.352 Avg. Number of Bedrooms     0.0072     0.006     1.277  0.202    -0.004     0.018 Area Population     0.4247     0.005    86.506  0.000     0.415     0.434 Omnibus: 11.426   Durbin-Watson:         1.995 Prob(Omnibus):  0.003   Jarque-Bera (JB):      9.047 Skew:  0.023   Prob(JB):             0.0109 Kurtosis:  2.750   Cond. No.               1.66 <p>The regression coefficients are identical between <code>sklearn</code> and <code>statsmodels</code> libraries. The $R^{2}$ of <code>0.919</code> is as high as it gets. This indicates the predicted (train) <code>Price</code> varies similar to actual. Another measure of health is the <code>S</code> (std. error) and <code>p-value</code> of coefficients. The <code>S</code> of <code>Avg. Number of Bedrooms</code> is as low as other predictors, however it has a high <code>p-value</code> indicating a low confidence in predicting its coefficient.</p> <p>Similar is the <code>p-value</code> of the intercept.</p> In\u00a0[37]: Copied! <pre>yscaled_predicted = lm_scaled.predict(Xscaled_test)\nresiduals_scaled = yscaled_test - yscaled_predicted\n</pre> yscaled_predicted = lm_scaled.predict(Xscaled_test) residuals_scaled = yscaled_test - yscaled_predicted In\u00a0[42]: Copied! <pre>fig, axs = plt.subplots(2,2, figsize=(10,10))\n# plt.tight_layout()\n\nplt1 = axs[0][0].scatter(x=yscaled_test, y=yscaled_predicted)\naxs[0][0].set_title('Fitted vs Predicted')\naxs[0][0].set_xlabel('Price - test')\naxs[0][0].set_ylabel('Price - predicted')\n\nplt2 = axs[0][1].scatter(x=yscaled_test, y=residuals_scaled)\naxs[0][1].hlines(0, xmin=-3, xmax=3)\naxs[0][1].set_title('Fitted vs Residuals')\naxs[0][1].set_xlabel('Price - test (fitted)')\naxs[0][1].set_ylabel('Residuals')\n\nfrom numpy import random\naxs[1][0].scatter(x=sorted(random.randn(len(residuals_scaled))), \n                  y=sorted(residuals_scaled['Price']))\naxs[1][0].set_title('QQ plot of Residuals')\naxs[1][0].set_xlabel('Std. normal z scores')\naxs[1][0].set_ylabel('Residuals')\n\nsns.distplot(residuals_scaled, ax=axs[1][1])\naxs[1][1].set_title('Histogram of residuals')\nplt.tight_layout()\n</pre> fig, axs = plt.subplots(2,2, figsize=(10,10)) # plt.tight_layout()  plt1 = axs[0][0].scatter(x=yscaled_test, y=yscaled_predicted) axs[0][0].set_title('Fitted vs Predicted') axs[0][0].set_xlabel('Price - test') axs[0][0].set_ylabel('Price - predicted')  plt2 = axs[0][1].scatter(x=yscaled_test, y=residuals_scaled) axs[0][1].hlines(0, xmin=-3, xmax=3) axs[0][1].set_title('Fitted vs Residuals') axs[0][1].set_xlabel('Price - test (fitted)') axs[0][1].set_ylabel('Residuals')  from numpy import random axs[1][0].scatter(x=sorted(random.randn(len(residuals_scaled))),                    y=sorted(residuals_scaled['Price'])) axs[1][0].set_title('QQ plot of Residuals') axs[1][0].set_xlabel('Std. normal z scores') axs[1][0].set_ylabel('Residuals')  sns.distplot(residuals_scaled, ax=axs[1][1]) axs[1][1].set_title('Histogram of residuals') plt.tight_layout() <pre>/Users/atma6951/anaconda3/envs/pychakras/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n</pre> <p>From the charts above,</p> <ul> <li>Fitted vs predicted chart shows a strong correlation between the predictions and actual values</li> <li>Fitted vs Residuals chart shows an even distribution around the <code>0</code> mean line. There are not patterns evident, which means our model does not leak any systematic phenomena into the residuals (errors)</li> <li>Quantile-Quantile plot of residuals vs std. normal and the histogram of residual plots show a sufficiently normal distribution of residuals.</li> </ul> <p>Thus all assumptions hold good.</p> In\u00a0[45]: Copied! <pre>Xscaled_train.columns\n</pre> Xscaled_train.columns Out[45]: <pre>Index(['const', 'Avg Income', 'Avg House Age', 'Avg. Number of Rooms',\n       'Avg. Number of Bedrooms', 'Area Population'],\n      dtype='object')</pre> In\u00a0[55]: Copied! <pre>usa_house_fitted = Xscaled_test[Xscaled_test.columns[0:]]\nusa_house_fitted['Price'] = yscaled_test\nusa_house_fitted.head()\n</pre> usa_house_fitted = Xscaled_test[Xscaled_test.columns[0:]] usa_house_fitted['Price'] = yscaled_test usa_house_fitted.head() Out[55]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price 116 -0.572234 0.523958 0.333009 0.930840 -0.935132 -0.807544 1766 0.235111 -1.461858 -0.639184 -0.787131 -0.746915 -1.139124 318 -0.401314 -1.180810 0.886801 0.242031 -0.544216 -0.423747 1376 0.190042 -1.552636 0.291719 -0.503503 -1.186530 -1.552110 4699 -2.186060 -0.389114 -0.437924 -1.362489 0.903237 -1.453495 In\u00a0[59]: Copied! <pre>usa_house_fitted_inv = s_scaler.inverse_transform(usa_house_fitted)\nusa_house_fitted_inv = pd.DataFrame(usa_house_fitted_inv, \n                                    columns=usa_house_fitted.columns)\nusa_house_fitted_inv.head().round(3)\n</pre> usa_house_fitted_inv = s_scaler.inverse_transform(usa_house_fitted) usa_house_fitted_inv = pd.DataFrame(usa_house_fitted_inv,                                      columns=usa_house_fitted.columns) usa_house_fitted_inv.head().round(3) Out[59]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price 0 62484.855 6.497 7.323 5.13 26882.652 946943.036 1 71088.669 4.528 6.345 3.01 28750.641 829868.230 2 64306.339 4.807 7.880 4.28 30762.360 1082455.018 3 70608.372 4.438 7.281 3.36 24387.608 684049.919 4 45286.426 5.591 6.547 2.30 45127.832 718869.401 In\u00a0[65]: Copied! <pre>yinv_predicted = (yscaled_predicted * s_scaler.scale_[-1]) + s_scaler.mean_[-1]\n</pre> yinv_predicted = (yscaled_predicted * s_scaler.scale_[-1]) + s_scaler.mean_[-1] In\u00a0[66]: Copied! <pre>yinv_predicted.shape\n</pre> yinv_predicted.shape Out[66]: <pre>(1650, 1)</pre> In\u00a0[67]: Copied! <pre>usa_house_fitted_inv['Price predicted'] = yinv_predicted\nusa_house_fitted_inv.head().round(3)\n</pre> usa_house_fitted_inv['Price predicted'] = yinv_predicted usa_house_fitted_inv.head().round(3) Out[67]: Avg Income Avg House Age Avg. Number of Rooms Avg. Number of Bedrooms Area Population Price Price predicted 0 62484.855 6.497 7.323 5.13 26882.652 946943.036 1087455.848 1 71088.669 4.528 6.345 3.01 28750.641 829868.230 855848.404 2 64306.339 4.807 7.880 4.28 30762.360 1082455.018 971840.851 3 70608.372 4.438 7.281 3.36 24387.608 684049.919 877566.569 4 45286.426 5.591 6.547 2.30 45127.832 718869.401 743023.886 In\u00a0[68]: Copied! <pre>mse_scaled = metrics.mean_squared_error(usa_house_fitted_inv['Price'], \n                           usa_house_fitted_inv['Price predicted'])\nnumpy.sqrt(mse_scaled)\n</pre> mse_scaled = metrics.mean_squared_error(usa_house_fitted_inv['Price'],                             usa_house_fitted_inv['Price predicted']) numpy.sqrt(mse_scaled) Out[68]: <pre>101796.27442079457</pre> In\u00a0[69]: Copied! <pre>mae_scaled = metrics.mean_absolute_error(usa_house_fitted_inv['Price'], \n                           usa_house_fitted_inv['Price predicted'])\nmae_scaled\n</pre> mae_scaled = metrics.mean_absolute_error(usa_house_fitted_inv['Price'],                             usa_house_fitted_inv['Price predicted']) mae_scaled Out[69]: <pre>80765.04773907141</pre>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#applying-linear-regression-with-scikit-learn-and-statmodels","title":"Applying Linear Regression with <code>scikit-learn</code> and <code>statmodels</code>\u00b6","text":"<p>This notebook demonstrates how to conduct a valid regression analysis using a combination of Sklearn and statmodels libraries. While sklearn is popular and powerful from an operational point of view, it does not provide the detailed metrics required to statistically analyze your model, evaluate the importance of predictors, build or simplify your model.</p> <p>We use other libraries like <code>statmodels</code> or <code>scipy.stats</code> to bridge this gap.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#scikit-learn","title":"Scikit-learn\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#predicting-housing-prices-without-data-normalization","title":"Predicting housing prices without data normalization\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#exploratory-data-anslysis-eda","title":"Exploratory data anslysis (EDA)\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#data-cleaning","title":"Data cleaning\u00b6","text":"<p>Throw out the text column and split the data into predictor and predicted variables</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#train-test-split","title":"Train test split\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#multiple-regression","title":"Multiple regression\u00b6","text":"<p>We use a number of numerical columns to regress the house price. Each column's influence will vary, just like in real life, the number of bedrooms might not influence as much as population density. We can determine the influence from the correlation shown in the heatmap above</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#fit","title":"Fit\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#predict","title":"Predict\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#accuracy-assessment-model-validation","title":"Accuracy assessment  / Model validation\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#distribution-of-residuals","title":"Distribution of residuals\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#quantifying-errors","title":"Quantifying errors\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#combine-the-predicted-values-with-input","title":"Combine the predicted values with input\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#predicting-housing-prices-with-data-normalization-and-statmodels","title":"Predicting housing prices with data normalization and statmodels\u00b6","text":"<p>As seen earlier, even though sklearn will perform regression, it is hard to compare which of the predictor variables are influential in determining the house price. To answer this better, let us standardize our data to Std. Normal distribution using sklearn preprocessing.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#scale-the-housing-data-to-std-normal-distribution","title":"Scale the housing data to Std. Normal distribution\u00b6","text":"<p>We use <code>StandardScaler</code> from <code>sklearn.preprocessing</code> to normalize each predictor to mean <code>0</code> and unit variance. What we end up with is <code>z-score</code> for each record.</p> <p>$$ z-score = \\frac{x_{i} - \\mu}{\\sigma}   $$</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#train-test-split","title":"Train test split\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#train-the-model","title":"Train the model\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#evaluate-model-parameters-using-statsmodels","title":"Evaluate model parameters using statsmodels\u00b6","text":"<p><code>statmodels</code> is a different Python library built for and by statisticians. Thus it provides a lot more information on your model than <code>sklearn</code>. We use it here to refit against the data and evaluate the strength of fit.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#predict-for-unkown-values","title":"Predict for unkown values\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#evaluate-model-using-charts","title":"Evaluate model using charts\u00b6","text":"<p>In addition to the numerical metrics used above, we need to look at the distribution of residuals to evaluate if the model.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#inverse-transform-the-scaled-data-and-calculate-rmse","title":"Inverse Transform the scaled data and calculate RMSE\u00b6","text":""},{"location":"projects/ml/sklearn_statmodels_linear_regression/#calculate-rmse","title":"Calculate RMSE\u00b6","text":"<p>RMSE root mean squared error. This is useful as it tell you in terms of the dependent variable, what the mean error in prediction is.</p>"},{"location":"projects/ml/sklearn_statmodels_linear_regression/#conclusion","title":"Conclusion\u00b6","text":"<p>In this sample we observed two methods of predicting housing prices. The first involved applying linear regression on the dataset directly. The second involved scaling the features to standard normal distribution and applying a linear model using both <code>sklearn</code> and <code>statsmodels</code> packages. We thoroughly inspected the model parameters, vetted that assumptions hold good.</p> <p>In the end, the accuracy of the models did not increase much after scaling. However, we were able to better determine which predictor variables were influential in a truest sense, not being biased by the scale of the units.</p> <p>The allure of linear models is the explainability. They may not be the best when it comes to accurate predictions, however they help us answer basic questions better, such as \"which characteristics influence the cost of my home, is it # of bedrooms or average income of the residents\"?. The answer is the latter in this example.</p>"},{"location":"projects/mwrs/","title":"Microwave Remote Sensing","text":"<p>coming soon...</p>"},{"location":"projects/spatial/","title":"Spatial analysis","text":""},{"location":"projects/spatial/#getting-started","title":"Getting started","text":"<ul> <li>A guide to spatial analysis -1 - spatial data structures and understanding density</li> <li>A guide to spatial analysis -2 - spatial statistics</li> </ul>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/","title":"Guide to spatial analysis - Introduction","text":"<p>A set of notes explaining the core of spatial analysis. Most of the content in this page are excerpts from Esri Guide to Spatial Analysis volume 1</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/#basics-of-spatial-data-structures","title":"Basics of spatial data structures","text":""},{"location":"projects/spatial/guide-to-spatial-analysis-1/#2-fundamental-representations","title":"2 fundamental representations","text":"<ul> <li>vector - each entity /feature is a row in a table. Each feature can be queried, have one or more attributes. Features are generally discrete entities. Vectors can be represented using</li> <li>points</li> <li>lines</li> <li> <p>polygons</p> </li> <li> <p>raster - custinuous image surface made up of a matrix of cell values in continuous space. Each layer in general has just 1 attribute that it defines.</p> </li> </ul>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/#geographic-attributes","title":"Geographic Attributes","text":"<p>Attributes of entities / features can be classified into following categories  - Categorical - (political affiliation, age group, income group)  - ranks - (severity of poverty, risk prediction, severity of assault)  - counts and amounts - (number of employees at a business, number of people in a house)  - ratios    - proportions - generally the ratio of a measured value to another measured value. Eg: ratio of number of people in age 10-25 to total population    - density - a ratio where the denominator is a spatial measurement. Eg: ratio of number of people to area</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/#visualization","title":"Visualization","text":"<p>You can visualize continuous and discrete values by classes. You can create the classes manually, or you can use one of the following statistical techniques to determine the class intervals.</p> <ul> <li>Natural breaks (Jenks): breaks are chosen along breaking changes in frequency (on a histogram). Thus, each block group on the map is more likely to have similar values.</li> <li>Quantile: Each class has the same number of features. Thus higher frequency areas on the histogram have more classes assigned to them. Thus, the values on the ends of the histogram (which typically have low frequency) get clumbed into the same class.</li> <li>Equal interval: Equal interval is easy to comprehend when you look at the legend. It splits the histogram with a set number of classes that are equally spaced from one another. On the map, the most features fall under a few number of classes</li> <li>Standard Deviation: Breaks are calculated by how many standard deviations they are from the mean. On the map, each group shows how many std each is from the mean.</li> </ul> <p></p> <p>Most people can distinguish up to <code>7</code> classes on a map. <code>4</code> or <code>5</code> are great number of classes to reveal patterns.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-1/#mapping-density","title":"Mapping density","text":"<p>Density is a ratio of number of features to a fixed areal extent. Thus, mapping number of people per sq. mile is a measure of population density. Mapping the density of features allows you to see patterns of where things are concentrated. Density surfaces are useful when mapping areas such as census tracts or counties which vary greatly in size.</p> <p>There are two ways to map density - a. mapping for defined areas (zip codes, census tracts, etc.) b. creating a density surface. When mapping for a defined area, a dot density renderer is a good visualization choice. You do this by summing the number of features within each polygon. Then you visualize each polygon with a set of dots, where each dot represents a chosen number of features (1 dot = 1000 features). The location of dots does not represent actual features though.</p> <p></p> <p>Density surface is usually calculated as a <code>raster</code> layer. Each <code>cell</code> in the layer gets a value based on the number of features that fall within a given radius around the cell.</p> <p></p> <p>Calculation of the density surface depends on 1. cell size, 2. search radius, 3. calculation method and 4. units. Cell size will determine how coarse or smooth the patterns will appear. The smaller the cell size, the smoother the surface, the longer the computation. A larger cell size will make the surface coarser, but the calculation will be quicker.</p> <p></p> <p>Search radius determines whether you pick up local or global patterns. A larger search radius will generalize the patterns more than a smaller radius, which will show more local variations.</p> <p></p> <p>There are two types of Density calculation methods - a simple method which counts all features within a search radius vs a weighted method that uses a math function to give more importance to features closer to the cell center. The closer the features, higher their weight. The weight drops off for features at the edge and beyond the search radius.</p> <p>Density of defined areas (zip codes / some boundary) allows you to compare known boundaries. However, the location of actual features may be distributed unevenly within the area. This is exaggerated when areas of the boundaries vary greatly (such as counties). A good practice in such cases is to apply dot density renderer for smaller geographies (smaller than what you intend to measure), but overlay that on larger geographies that you are visualizing.</p> <p>The density surface can be visualized as contours or using graduated colors or using both. When using colors, avoid using more than 15 classes. A density surface will smooth out extreme high and low values since it is an interpolation algorithm. As with the dot density renderer, the density value of a cell and the actual number of features within the cell may be different. Thus, you need to overlay the actual features on the surface to ensure the patterns are correctly interpreted.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/","title":"Guide to spatial analysis - spatial statistics","text":""},{"location":"projects/spatial/guide-to-spatial-analysis-2/#1-why-study-spatial-statistics","title":"1. Why study spatial statistics?","text":"<p>Studying spatial statistics (or statistical geography) helps answer questions such as: \"How sure am I that the pattern I am seeing is not simply due to random occurrence? To what extent does the value of a feature depend on its neighbors?\". In addition, spatial stats can help answer questions such as</p> <ol> <li>How are the features distributed? (Spatial distribution)</li> <li>What is the pattern created by these features - are the locations spatially clustered?</li> <li>If clustered, where are the clusters?</li> <li>What are the relationships between sets of features or values? Once you establish the strength of relationship between two layers, then you can use one to predict the other.</li> </ol>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#11-geographic-analysis-with-statistics","title":"1.1 Geographic analysis with statistics","text":"<p>Framing the question: In descriptive statistics, the question usually takes the form: Where is the center of crimes? What is the overall direction of storm tracks?.</p> <p>In inferential statistics, the analysis is stated as a hypothesis and that takes the form: Burglaries are more clustered than auto thefts. To ensure impartiality, statisticians often frame the inverse of what they intend to prove as the hypothesis. Thus the same analysis would be framed as Burglaries are not more clustered than auto thefts (called <code>null hypothesis</code>) and they determine whether or not to reject this null hypothesis.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#12-understanding-geographic-data","title":"1.2 Understanding geographic data","text":"<p>Spatial data types: Geographic features are spread in space. Their location can be analyzed based on their location or using location influenced by an attribute value. Thus, geographic features are either discrete or spatially continuous. Discrete features can be locations of stores, weather stations etc. and are represented as <code>Points</code>. <code>Lines</code> can be disjunct (animal migration routes) or connected, as in a network (streets). Spatially continuous features such as temperature, precipitation are measured anywhere and everywhere. Such <code>continuous field</code> data are represented as a <code>Polygon</code> or as a surface.</p> <p>Now let's talk about types of attribute values: Attribute values can be</p> <ul> <li><code>nominal</code> (categorical) - ex: land cover types, </li> <li><code>ordinal</code> (ordered) - ex: soil suitability, landslide risk zone. In ordinal scale, you only know under which class a feature falls, but don't know by how much is it better or worse than features in other classes.</li> <li><code>interval</code> (quantities) - ex: house prices, population. Here, each class gets an upper and lower limit allowing you to understand how wide a class range is and compare different classes.</li> <li><code>ratio</code> types (proportions) - ex: population density, infection rate. </li> </ul>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#2-data-distributions","title":"2. Data distributions","text":"<p>Below are some of the common distributions seen in geography:</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#21-normal-distribution","title":"2.1 Normal distribution","text":"<p>This kind of distribution occurs for phenomena where values are similar, but some are higher and some are lower. The frequency curve of normal distribution takes shape of the classic symmetrical bell curve. Given enough readings over time, most values will cluster toward the mean.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#22-poisson-distribution","title":"2.2 Poisson distribution","text":"<p>This distribution occurs when extreme events like large magnitude earthquakes occur in time and space. When events are random, there will be a few periods when many events occur, followed by several periods where no or very few events occur. For this distribution, mean number of events is often \\(\\mu &lt; 1\\) and probability of no events occurring is higher.</p> <p></p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#3-spatial-distributions","title":"3. Spatial distributions","text":"<p>The notes above describe distributions of attribute values. But, how do we measure spatial distributions? Most often, your objective is to determine if features are evenly distributed (uniform distribution) or not, meaning there is a spatial phenomenon taking place. There are <code>3</code> ways of finding if features are non-randomly distributed across space:</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#311-method-1-overlay-grid-and-count","title":"3.1.1 Method 1 - overlay grid and count","text":"<p>You can overlay an imaginary grid over your features, count the number of points / features in each cell. Then you compare this metric against a hypothetical even distribution of same features and test for significance.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#312-method-2-frequency-distribution-of-distances","title":"3.1.2 Method 2 - frequency distribution of distances","text":"<p>You can measure the distances between each feature and its neighbors (after defining a neighborhood threshold) and plot the frequency distribution of the distances.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#313-method-3-weighting-attribute-values-by-distance-to-neighbors","title":"3.1.3 Method 3 - weighting attribute values by distance to neighbors","text":"<p>To analyze the spatial distribution of attribute values (not features itself, but certain attribute values), you can divide (weigh) the values by distance to neighbors and create a frequency distribution. You can then compare that against a uniform or random distribution.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#32-measuring-centrality-of-geographic-distributions","title":"3.2 Measuring centrality of geographic distributions","text":"<p>In traditional statistics, you define centrality using mean, median and mode. Similarly, you can define mean center, median center and central feature in a geographic distribution.</p> <p>Mean center is obtained by averaging the X and Y values. Median center is calculated by identifying the X and Y coordinate that has the shortest distance to all features. Central Feature is the feature that has the shortest distance to all other features. Note, mean and median center may or may not fall on the coordinates of an existing feature, but central feature will.</p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#321-measuring-centrality-weighted-by-some-attribute-value","title":"3.2.1 Measuring centrality weighted by some attribute value","text":"<p>In practice, you generally want to weight the features when measuring the centrality. This becomes particularly important with continuous area features. Since they are continuous, you are not as interested in centrality, but on centrality determined by some attribute value (weight).</p> <p></p> <p>Thus, the social service agency looking for a central location to service population over age 65 could calculate it by weighting the distances with the population over age 65.</p> <p></p> <p>Weighted mean center is calculated as a weighted average: \\(\\bar X = \\frac{\\sum_{i} (w_{i} X_{i})}{\\sum_{i} w_{i}}\\) and \\(\\bar Y = \\frac{\\sum_{i} (w_i Y_{i})}{\\sum_{i}w_{i}}\\)</p> <p>In case of lines and polygon features, the centers and centroids are used and the mean center is calculated against those point values.</p> <p>Median center does not have an exact formula. Instead, the GIS calculates the mean center first, then adjusts it iteratively with slight changes until it finds the median center. It performs gradient descent to find the appropriate median center. To find the weighted median center, it uses the same technique, except, it multiplies the distances with the weight of the corresponding features.</p> <p>Central feature is calculated by iterating over each feature, calculating the distance to each other feature and summing it up. The feature with least total sum is the central feature. Weighted central feature is calculated similarly, by multiplying the distance with weight.</p> <p>Reiterating the fact that weighted centers are better for area features, see the case below: When polygons are of varying sizes, the central measure is automatically pulled toward the direction with multiple smaller polygons. This is because, polygons are converted to centroids when calculating distances. To offset this, calculated weighted center</p> <p></p>"},{"location":"projects/spatial/guide-to-spatial-analysis-2/#33-measuring-compactness-of-geographic-distributions","title":"3.3 Measuring compactness of geographic distributions","text":"<p>For non-spatial datasets, the dispersion around the center is measured by variance and standard deviation. These values gives you the ability to compare compactness of distributions. Similarly, for spatial datasets, the standard distance, which is the average distance by which features vary from mean center gives a measure of compactness. It is also called the standard deviation distance.</p> <p></p> <p>When calculating the standard distance, you could calculate it just by distance to features or by multiplying the distance with a weight. When calculating the compactness of polygons, we use weighted distance of their centroids.</p> <p></p> <p>The formula to calculate standard distance with just distances of features to mean center is below:</p> <p></p> <p>Note that it looks very similar to calculating SD, except it sums the variance along <code>X</code> and <code>Y</code> and then takes the square root of their sum.</p> <p>When weights come into picture, you do a weighted average of the deviation along X and Y as shown below:</p> <p></p> <p>Weights are typically some numerical attribute (such as population if features are polygons, number of entities if features are lines or points). Further, the compactness measure works well if there no directionality in the dataset.</p>"},{"location":"projects/stats/","title":"Learn Statistics with Python","text":"<p>In these pages, I use Python and its rich stat analysis packages to teach the basics of probability and statistics. All those formulas which one could only apply on a limited number of problems (when calculating by hand) can now be applied across the entire dataset. You can literally verify if the central limit theorem holds good, without having to spend a week with your calculator. Enough said, checkout the pages below</p>"},{"location":"projects/stats/#introduction-to-statistics-with-python","title":"Introduction to statistics with Python","text":"<ul> <li>Collecting data</li> <li>Describing data</li> <li> <p>Conditional probability</p> </li> <li> <p>Probability distributions</p> <ul> <li>Normal distribution</li> <li>Binomial and Poisson distributions</li> </ul> </li> <li> <p>Hypothesis testing</p> </li> </ul>"},{"location":"projects/stats/#theory-of-statistical-learning","title":"Theory of Statistical learning","text":"<ul> <li>Statistical learning</li> <li>Linear regression concepts</li> </ul>"},{"location":"projects/stats/01_data_collection/","title":"Data collection methodologies","text":"<p>In scientific studies, data collection can fall under two broad categories</p> <ul> <li>observational study - where data is recorded without interfering with the process under study. These can be of 3 basic types</li> <li>sample survey - provides info about a population at a particular point in time</li> <li>prospective study - describes the population at present using sample survey and proceeds to follow forward in time to record specific outcomes</li> <li>retrospective study - describes the population at present using sample survey and records specific occurrence that have already taken place</li> <li>experimental study - where explanatory variables are actively manipulated and the effects on response variables are studied.</li> </ul>"},{"location":"projects/stats/01_data_collection/#data-collection-methodologies","title":"Data collection methodologies\u00b6","text":""},{"location":"projects/stats/01_data_collection/#sampling-designs-for-surveys-observational-study","title":"Sampling designs for surveys - observational study\u00b6","text":"<p>The various ways in which data collection experiments can be designed:</p> <ul> <li><p>Simple random sampling - Selecting a group of n units in such a way that each sample of size n has the equal probability of being selected.</p> </li> <li><p>Stratified random sampling - grouping the target population into strata based on a known auxiliary variable and then performing simple random sampling on each of the strata</p> </li> <li><p>cluster sampling - this involves selecting groups (based on convenience of study, such as buildings or city blocks) based on simple random sampling and then surveying or measuring all units within the groups.</p> </li> <li><p>systematic sampling - this involves selecting every nth value from a finite list as the sample. This technique is economical but can he heavily biased.</p> </li> </ul>"},{"location":"projects/stats/01_data_collection/#design-of-experiments-experimental-study","title":"Design of experiments - experimental study\u00b6","text":"<p>There are 2 types of variables in an experimental study - controlled variables (factors) and measured variables (response variables) which are under study</p> <ul> <li><p>factorial treatment - each of the various factors (m,n,o..) affecting the phenomena are combined with one-another. The resulting combinations (mno..) are studied individually</p> </li> <li><p>fractional factorial treatment - as the number of factors increase, factorial treatment becomes impossible to complete. Then, only a few selected combinations are picked for the study forming a fractional factorial treatment.</p> </li> </ul>"},{"location":"projects/stats/02_data_description/","title":"Describing Data Statistically","text":"In\u00a0[8]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsales_data = pd.read_csv('datasets_csv/CH03/ex3-14.txt')\nsales_data.columns = ['Year', 'Month', 'Sales'] #strip out the double quotes\nsales_data.head()\n</pre> import pandas as pd import matplotlib.pyplot as plt %matplotlib inline  sales_data = pd.read_csv('datasets_csv/CH03/ex3-14.txt') sales_data.columns = ['Year', 'Month', 'Sales'] #strip out the double quotes sales_data.head() Out[8]: Year Month Sales 0 1 1 101.9 1 2 1 109.0 2 3 1 115.5 3 4 1 122.0 4 5 1 128.1 In\u00a0[14]: Copied! <pre>print('Mean ' + str(sales_data.Sales.mean()))\nprint('Median ' + str(sales_data.Sales.median()))\nprint('Mode ' + str(sales_data.Sales.mode()))\n</pre> print('Mean ' + str(sales_data.Sales.mean())) print('Median ' + str(sales_data.Sales.median())) print('Mode ' + str(sales_data.Sales.mode())) <pre>Mean 118.7\nMedian 116.95\nMode 0    117.5\ndtype: float64\n</pre> In\u00a0[17]: Copied! <pre>quartiles = sales_data.Sales.quantile(q=[0.25, 0.5, 0.75])\nquartiles\n</pre> quartiles = sales_data.Sales.quantile(q=[0.25, 0.5, 0.75]) quartiles Out[17]: <pre>0.25    108.275\n0.50    116.950\n0.75    128.175\nName: Sales, dtype: float64</pre> In\u00a0[21]: Copied! <pre>IQR = quartiles[0.75] - quartiles[0.25]\nIQR\n</pre> IQR = quartiles[0.75] - quartiles[0.25] IQR Out[21]: <pre>19.899999999999991</pre> In\u00a0[88]: Copied! <pre>#create a subplot to show box plot and histogram next to each other\nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8,10))\n\n#plot a histogram using Pandas.\nhax = sales_data.Sales.hist(ax = axs[0], bins=15, grid=False)\nhax.set_title('Histogram of Sales data')\n\n#get the axis bounds\nhax_bounds = hax.axis()\n\n#plot the mean in black\nmean = sales_data.Sales.mean()\nhax.vlines(mean, hax_bounds[2], hax_bounds[3], label='Mean = ' + str(mean))\n\n#plot the median in yellow\nmedian= sales_data.Sales.median()\nhax.vlines(median, hax_bounds[2], hax_bounds[3], label='Median = ' + str(median), colors='yellow')\n\n#plot the mode in red\nmode= sales_data.Sales.mode()[0]\nhax.vlines(mode, hax_bounds[2], hax_bounds[3], label='Mode = ' + str(mode), colors='red')\n\n#Get the standard deviation\nsd = sales_data.Sales.std()\n\n#get mean +- 1SD lines\nm1sd = mean + 1*sd\nm1negsd = mean - 1*sd\nhax.vlines(m1sd, hax_bounds[2], hax_bounds[3], label='Mean + 1SD = ' + str(m1sd), colors='cyan')\nhax.vlines(m1negsd, hax_bounds[2], hax_bounds[3], label='SD = ' + str(sd), colors='cyan')\n\nhax.legend()\n\n\n############## plot 2\n#now plot the box plot\nbax = sales_data.Sales.plot(kind='box', ax = axs[1], title = 'Boxplot of Sales data', vert=False)\n#vert False to make it horizontal\n\n#Get the quartiles\nquartiles = sales_data.Sales.quantile([0.25, 0.5, 0.75])\nbax.text(quartiles[0.25], 0.75, r'$Q_{0.25}= ' + str(quartiles[0.25])+'$')\nbax.text(quartiles[0.75], 0.75, r'$Q_{0.75}= ' + str(quartiles[0.75])+'$')\n\n#Calculate the IQR\niqr = quartiles[0.75] - quartiles[0.25]\nbax.text(x=150, y=1.25, s='IQR = ' + str(iqr))\n\n#Get the Left inner quartile\nliq = quartiles[0.25] - 1.5*iqr\nbax.text(x=liq, y=0.85, s=str(liq))\n\n#Get the right inner quartile\nriq = quartiles[0.75] + 1.5*iqr\nbax.text(x=riq, y=0.85, s=str(riq))\n</pre> #create a subplot to show box plot and histogram next to each other fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8,10))  #plot a histogram using Pandas. hax = sales_data.Sales.hist(ax = axs[0], bins=15, grid=False) hax.set_title('Histogram of Sales data')  #get the axis bounds hax_bounds = hax.axis()  #plot the mean in black mean = sales_data.Sales.mean() hax.vlines(mean, hax_bounds[2], hax_bounds[3], label='Mean = ' + str(mean))  #plot the median in yellow median= sales_data.Sales.median() hax.vlines(median, hax_bounds[2], hax_bounds[3], label='Median = ' + str(median), colors='yellow')  #plot the mode in red mode= sales_data.Sales.mode()[0] hax.vlines(mode, hax_bounds[2], hax_bounds[3], label='Mode = ' + str(mode), colors='red')  #Get the standard deviation sd = sales_data.Sales.std()  #get mean +- 1SD lines m1sd = mean + 1*sd m1negsd = mean - 1*sd hax.vlines(m1sd, hax_bounds[2], hax_bounds[3], label='Mean + 1SD = ' + str(m1sd), colors='cyan') hax.vlines(m1negsd, hax_bounds[2], hax_bounds[3], label='SD = ' + str(sd), colors='cyan')  hax.legend()   ############## plot 2 #now plot the box plot bax = sales_data.Sales.plot(kind='box', ax = axs[1], title = 'Boxplot of Sales data', vert=False) #vert False to make it horizontal  #Get the quartiles quartiles = sales_data.Sales.quantile([0.25, 0.5, 0.75]) bax.text(quartiles[0.25], 0.75, r'$Q_{0.25}= ' + str(quartiles[0.25])+'$') bax.text(quartiles[0.75], 0.75, r'$Q_{0.75}= ' + str(quartiles[0.75])+'$')  #Calculate the IQR iqr = quartiles[0.75] - quartiles[0.25] bax.text(x=150, y=1.25, s='IQR = ' + str(iqr))  #Get the Left inner quartile liq = quartiles[0.25] - 1.5*iqr bax.text(x=liq, y=0.85, s=str(liq))  #Get the right inner quartile riq = quartiles[0.75] + 1.5*iqr bax.text(x=riq, y=0.85, s=str(riq))   Out[88]: <pre>&lt;matplotlib.text.Text at 0x120d504a8&gt;</pre> <p><code>Variance</code> is the mean squared deviation from mean. Thus $\\sigma^2$ is <code>variance of population</code> and $s^2$ is <code>variance of sample</code></p> <p>$$ s^2 = \\frac{\\sum_{i}^{n}(y_i - \\bar y)^2}{(n-1)} $$</p> <p><code>s</code> = <code>standard deviation</code> is square root of variance. $$ s = \\sqrt{\\frac{\\sum_{i}^{n}(y_i - \\bar y)^2}{(n-1)}} $$</p> <p>We divide the sum of squared deviation by <code>(n-1)</code> because we don't want the sample SD to be underestimated. This is because, we can estimate the SD of population by averaging the SDs of multiple samples. During this process, if we divided by <code>n</code> instead of <code>n-1</code>, then the SD of population is underestimated.</p> <p>For a <code>normal</code>ly distributed population, empirically, $$ \\bar y \\pm 1s = 68\\% \\verb ! of data! \\\\ \\bar y \\pm 2s = 95\\% \\verb ! of data! \\\\ \\bar y \\pm 3s = 97.7\\% \\verb ! of data! $$</p> <p>Thus range is max - min value. Hence $range = 6s$</p> <p>where $s_x$ is standard deviation of $x$ and so on. $r$ is called Pearson's r</p> <p><code>r</code> ranges from <code>-1 to 1</code>. A value of <code>-1</code> indicates strong negative relationship and vice versa. A value close to <code>0</code> might represent no relationship or presence of a non-linear relationship.</p> <p>The numerator, is called the <code>covariance</code> of <code>x</code> and <code>y</code>, which is the combined deviation from their corresponding means.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/stats/02_data_description/#describing-data-statistically","title":"Describing Data Statistically\u00b6","text":""},{"location":"projects/stats/02_data_description/#measures-of-central-tendancy","title":"Measures of central tendancy\u00b6","text":""},{"location":"projects/stats/02_data_description/#measuring-dispersion-inter-quartile-ranges","title":"Measuring dispersion - Inter Quartile Ranges\u00b6","text":""},{"location":"projects/stats/02_data_description/#histogram-of-sales-data","title":"Histogram of sales data\u00b6","text":""},{"location":"projects/stats/02_data_description/#measures-of-dispersion-standard-deviation","title":"Measures of dispersion - Standard Deviation\u00b6","text":"<p>Range (max val - min val), IQR - inter quartile range are a few measures to find the spread of data and outliers. Another quantitative way is variance and standard deviation.</p> <p>$$ \\mu = \\verb !population mean (cannot be calculated in most cases)! \\\\ \\bar y = \\verb !(y bar) sample mean! \\\\ \\bar y = \\frac{\\sum_{i}^{n}y_i}{n} $$</p>"},{"location":"projects/stats/02_data_description/#coefficient-of-variability","title":"Coefficient of variability\u00b6","text":"<p>To compare the dispersion of two different variables (of different ranges and units), we can normalize them to a common unitless measure called <code>CV</code>.</p> <p>$$ CV = \\frac{s}{|\\bar y|} $$</p>"},{"location":"projects/stats/02_data_description/#dispersion-based-on-percentiles","title":"Dispersion based on percentiles\u00b6","text":"<p>A <code>percentile</code> : <code>p</code>th percentile is the value such that <code>p</code>% of values are less than this and <code>100-p</code>% are higher than that value. Thus <code>80</code>th percentile means, <code>79.9%</code> of data points are less than this and <code>19.9%</code> values are higher than this.</p> <p>Quantile is the same as percentile but expressed in decimals. Thus <code>80</code>th percentile = <code>0.8</code> quantile.</p> <p>Quartile is quantile at every quarter. Thus <code>0.25</code>, <code>0.5</code>, <code>0.75</code> quantiles are quartiles. Note: <code>0.5</code> quantile is same as Median.</p> <p><code>IQR</code> InterQuartile Range is the difference between <code>75</code>th and <code>25</code>th percentiles. (0.75 and 0.25 quartiles).</p>"},{"location":"projects/stats/02_data_description/#correlation-coefficient","title":"Correlation coefficient\u00b6","text":"<p>To determine the linear relationship between two variables, we determine how each of the measurement pairs deviate from their corresponding means.</p> <p>$$ r = \\frac{\\sum_{i}^{n}(x_i - \\bar x)(y_i - \\bar y)}{(n-1)s_x s_y} $$</p>"},{"location":"projects/stats/02_data_description/#when-is-big-data-needed","title":"When is big data needed?\u00b6","text":"<p>Are all problems a big data problem? Can problems be solved with sampling and handling a subset of data? Yes, most often, quality and representiveness of data is more important than quantity.</p> <p>The cases where big-data is needed is when data is sparse, when thousands of predictors is required over millions of data points, where values of most predictors is 0. This is a very sparse data set. For such cases, big data improves the accuracy and random sampling cannot just produce a representative sample.</p>"},{"location":"projects/stats/04_conditional_probability/","title":"Conditional Probability","text":"<p>Probability ranges from 0 to 1. The sum of P(A) and the opposite of A occuring is 1. For mutually exclusive events A and B, the Probability of either A or B ocurring is sum of their probabilities.</p> <p>Mutually exclusive: Two events are considered mutually exclusive, if when an event is performed once, the occurrence of one of the events excludes the possibility of another.</p> <p>For two independent events, probabilities of their union and intersection can be represented as</p> <p>$$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) $$</p> <p>If A and B are mutually exclusive, then $P(A \\cap B) = 0$</p> <p>The reason we negate <code>P(A intersection B)</code> can be seen from the venn diagram below. Probabilities of A and B are (0.5 and 0.2). The probability of both A and B ocurring is 0.05. Thus to not double count the intersection, we negate it.</p> In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib_venn import venn2\nvenn2(subsets = (0.45, 0.15, 0.05), set_labels = ('A', 'B'))\n</pre> import matplotlib.pyplot as plt %matplotlib inline from matplotlib_venn import venn2 venn2(subsets = (0.45, 0.15, 0.05), set_labels = ('A', 'B')) Out[4]: <pre>&lt;matplotlib_venn._common.VennDiagram at 0x11a436ef0&gt;</pre> <p>The $P(A/B)$ probability of A given that B occurs, is the probability of A and B occurring $P(A \\cap B)$ to the probability of B occurring $P(B)$. Thus if <code>A</code> and <code>B</code> are <code>mutually exclusive</code>, then there is no conditional probability.</p> <p>Example Consider the case of insurance fraud. In table below, you are given insurance type and what rate of them are fraud claims.</p> In\u00a0[2]: Copied! <pre>import pandas as pd\ndf = pd.DataFrame([[6,1,3,'Fradulent'],[14,29,47,'Not Fradulent']], \n                  columns=['Fire', 'Auto','Other','Status'])\n</pre> import pandas as pd df = pd.DataFrame([[6,1,3,'Fradulent'],[14,29,47,'Not Fradulent']],                    columns=['Fire', 'Auto','Other','Status']) In\u00a0[3]: Copied! <pre>df\n</pre> df Out[3]: Fire Auto Other Status 0 6 1 3 Fradulent 1 14 29 47 Not Fradulent <p>The total number of claims: 100, number of fraud claims: 10. Thus 10% of all claims are fraud. However, with additional information about type of claims, we can fine grain whether a given claim is fraud, if we knew the type of claim (predictor variable).</p> <p>To answer the question, what is the probability that a claim is fraud, given that it is a Fire claim?:</p> <p>$$ p(Fraud \\ |\\ fire \\ policy) = \\frac{p(fire \\cap fraud)}{p(fire \\ policy)} $$ $$ p(Fraud \\ | \\ fire \\ policy) = \\frac{0.06}{0.20} = .30 $$ or 30% of claims are fraud, given that they are of type fire.</p> <p>Here, <code>30%</code> is called the <code>conditional probability</code> and the general <code>10%</code> is called the <code>unconditional</code> or <code>marginal</code> probability. Clearly, knowing the conditional probability is of much higher value than knowing the unconditional probability.</p> <p>In reality, we expand the $\\bar A$ case. Thus if $A_1 ,... A_k$ are mutually exclusive <code>states of nature</code> and if $B_1 .. B_m$ are <code>m</code> possible mutually exclusive observable events, then,</p> <p>$$ P(A_i | B_j) = \\frac{P(B_j | A_i)P(A_i)}{P(B_j | A_1)P(A_1) + P(B_j | A_2)P(A_2) + ... + P(B_j | A_k)P(A_k)} $$</p> <p>Consider $A_1 ,... A_k$ as $k$ predictor variables in machine learning. The Naive Bayes classifier will build the conditional probabilities of $p(B_j|A_k)$ to later predict what would $p(A_i | B_j)$ be.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/stats/04_conditional_probability/#conditional-probability","title":"Conditional Probability\u00b6","text":""},{"location":"projects/stats/04_conditional_probability/#axioms-of-probability","title":"Axioms of probability\u00b6","text":"<p>$$ 0 \\leq P(A) \\leq 1 \\\\ P(A) + P(\\bar A) = 1 \\\\ P(A \\verb ! or ! B) = P(A) + P(B) $$</p>"},{"location":"projects/stats/04_conditional_probability/#conditional-probability","title":"Conditional Probability\u00b6","text":"<p>Generally, conditional probability is more helpful in explaining a situtation than general probabilities.</p> <p>Given two events <code>A</code> and <code>B</code> with non zero probabilities, then the probability of A occurring, given that B has occurs is</p> <p>$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$</p> <p>and $$ P(B|A) = \\frac{P(A \\cap B)}{P(A)} $$</p>"},{"location":"projects/stats/04_conditional_probability/#bayesian-conditional-probability","title":"Bayesian conditional probability\u00b6","text":"<p>The Bayesian theorem builds on conditional probability, specifically on prior and posterior probabilities. It states that, if <code>A</code> and <code>B</code> are any events whose probabilities are not 0 or 1, then:</p> <p>$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\\bar A)P(\\bar A)} $$</p>"},{"location":"projects/stats/04_normal_distribution/","title":"Normal distribution","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[10]: Copied! <pre>vals = np.random.standard_normal(100000)\nlen(vals)\n</pre> vals = np.random.standard_normal(100000) len(vals) Out[10]: <pre>100000</pre> In\u00a0[19]: Copied! <pre>fig, ax = plt.subplots(1,1)\nhist_vals = ax.hist(vals, bins=200, color='red', density=True)\n</pre> fig, ax = plt.subplots(1,1) hist_vals = ax.hist(vals, bins=200, color='red', density=True) <p>The above is the standard normal distribution. Its mean is 0 and SD is 1. About <code>95%</code> values fall within $\\mu \\pm 2 SD$ and <code>98%</code> within $\\mu \\pm 3 SD$</p> <p>The area under this curve is <code>1</code> which gives the probability of values falling within the range of standard normal.</p> <p>A common use is to find the probability of a value falling at a particular range. For instance, find $p(-2 \\le z \\le 2)$ which is the probability of a value falling within $\\mu \\pm 2SD$. This calculated by summing the area under the curve between these bounds.</p> <p>$$p(-2 \\le z \\le 2) = 0.9544$$ which is <code>95.44%</code> probability. Its <code>z</code> score is <code>0.9544</code>.</p> <p>Similarly $$p(z \\ge 5.1) = 0.00000029$$</p> <p>Example Let X be age of US presidents at inaugration. $X \\in N(\\mu = 54.8, \\sigma=6.2)$. What is the probability of choosing a president at random that is less than <code>44</code> years of age.</p> <p>We need to find $p(x&lt;44)$. First we need to transform to standard normal.</p> <p>$$p(z&lt; \\frac{44-54.8}{6.2})$$ $$p(z&lt;-1.741) = 0.0409 \\approx 4\\%$$</p> In\u00a0[2]: Copied! <pre>import scipy.stats as st\n\n# compute the p value for a z score\nst.norm.cdf(-1.741)\n</pre> import scipy.stats as st  # compute the p value for a z score st.norm.cdf(-1.741) Out[2]: <pre>0.04084178926110883</pre> <p>Let us try for some common <code>z scores</code>:</p> In\u00a0[4]: Copied! <pre>[st.norm.cdf(-3), st.norm.cdf(-1), st.norm.cdf(0), st.norm.cdf(1), st.norm.cdf(2)]\n</pre> [st.norm.cdf(-3), st.norm.cdf(-1), st.norm.cdf(0), st.norm.cdf(1), st.norm.cdf(2)] Out[4]: <pre>[0.0013498980316300933,\n 0.15865525393145707,\n 0.5,\n 0.8413447460685429,\n 0.9772498680518208]</pre> <p>As you noticed, the <code>norm.cdf()</code> function gives the cumulative probability (left tail) from <code>-3</code> to <code>3</code> approx. If you need right tailed distribution, you simply subtract this value from <code>1</code>.</p> In\u00a0[7]: Copied! <pre># Find Z score for a probability of 0.97 (2sd)\nst.norm.ppf(0.97)\n</pre> # Find Z score for a probability of 0.97 (2sd) st.norm.ppf(0.97) Out[7]: <pre>1.8807936081512509</pre> In\u00a0[8]: Copied! <pre>[st.norm.ppf(0.95), st.norm.ppf(0.97), st.norm.ppf(0.98), st.norm.ppf(0.99)]\n</pre> [st.norm.ppf(0.95), st.norm.ppf(0.97), st.norm.ppf(0.98), st.norm.ppf(0.99)] Out[8]: <pre>[1.6448536269514722,\n 1.8807936081512509,\n 2.0537489106318225,\n 2.3263478740408408]</pre> <p>As is the <code>ppf()</code> function gives only positive <code>z</code> scores, you need to apply $\\pm$ to it.</p> In\u00a0[3]: Copied! <pre>demo_dist = 55 + np.random.randn(200) * 3.4\nstd_normal = np.random.randn(200)\n</pre> demo_dist = 55 + np.random.randn(200) * 3.4 std_normal = np.random.randn(200) In\u00a0[6]: Copied! <pre>[demo_dist.mean(), demo_dist.std(), demo_dist.min(), demo_dist.max()]\n</pre> [demo_dist.mean(), demo_dist.std(), demo_dist.min(), demo_dist.max()] Out[6]: <pre>[55.11294611274521, 3.29635155084324, 46.56508960658229, 65.69499942017563]</pre> In\u00a0[7]: Copied! <pre>[std_normal.mean(), std_normal.std(), std_normal.min(), std_normal.max()]\n</pre> [std_normal.mean(), std_normal.std(), std_normal.min(), std_normal.max()] Out[7]: <pre>[0.08344701747941835,\n 0.9782741158088577,\n -1.9692101150067682,\n 2.94001634796817]</pre> <p>Now let us use scikit-learn to easily transform this dataset</p> In\u00a0[8]: Copied! <pre>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n</pre> from sklearn.preprocessing import StandardScaler scaler = StandardScaler() In\u00a0[22]: Copied! <pre>demo_dist = demo_dist.reshape(200,1)\ndemo_dist_scaled = scaler.fit_transform(demo_dist)\n</pre> demo_dist = demo_dist.reshape(200,1) demo_dist_scaled = scaler.fit_transform(demo_dist) In\u00a0[23]: Copied! <pre>[round(demo_dist_scaled.mean(),3), demo_dist_scaled.std(), demo_dist_scaled.min(), demo_dist_scaled.max()]\n</pre> [round(demo_dist_scaled.mean(),3), demo_dist_scaled.std(), demo_dist_scaled.min(), demo_dist_scaled.max()] Out[23]: <pre>[0.0, 1.0, -2.5931264837260137, 3.2102320229538095]</pre> In\u00a0[49]: Copied! <pre>fig, axs = plt.subplots(2,2, figsize=(15,8))\np1 = axs[0][0].scatter(sorted(demo_dist), sorted(std_normal))\naxs[0][0].set_title(\"Scatter of original dataset against standard normal\")\n\np1 = axs[0][1].scatter(sorted(demo_dist_scaled), sorted(std_normal))\naxs[0][1].set_title(\"Scatter of scaled dataset against standard normal\")\n\np2 = axs[1][0].hist(demo_dist, bins=50)\naxs[1][0].set_title(\"Histogram of original dataset against standard normal\")\n\np3 = axs[1][1].hist(demo_dist_scaled, bins=50)\naxs[1][1].set_title(\"Histogram of scaled dataset against standard normal\")\n</pre> fig, axs = plt.subplots(2,2, figsize=(15,8)) p1 = axs[0][0].scatter(sorted(demo_dist), sorted(std_normal)) axs[0][0].set_title(\"Scatter of original dataset against standard normal\")  p1 = axs[0][1].scatter(sorted(demo_dist_scaled), sorted(std_normal)) axs[0][1].set_title(\"Scatter of scaled dataset against standard normal\")  p2 = axs[1][0].hist(demo_dist, bins=50) axs[1][0].set_title(\"Histogram of original dataset against standard normal\")  p3 = axs[1][1].hist(demo_dist_scaled, bins=50) axs[1][1].set_title(\"Histogram of scaled dataset against standard normal\") Out[49]: <pre>Text(0.5,1,'Histogram of scaled dataset against standard normal')</pre> <p>As you see above, the shape of distribution is the same, just the values are scaled.</p> In\u00a0[20]: Copied! <pre>demo_dist = 55 + np.random.randn(200) * 3.4\nstd_normal = np.random.randn(200)\n</pre> demo_dist = 55 + np.random.randn(200) * 3.4 std_normal = np.random.randn(200) In\u00a0[21]: Copied! <pre>demo_dist = sorted(demo_dist)\nstd_normal = sorted(std_normal)\n</pre> demo_dist = sorted(demo_dist) std_normal = sorted(std_normal) In\u00a0[22]: Copied! <pre>plt.scatter(demo_dist, std_normal)\n</pre> plt.scatter(demo_dist, std_normal) Out[22]: <pre>&lt;matplotlib.collections.PathCollection at 0x111e56dd8&gt;</pre> <p>For the most part, the values fall on a straight line, except in the fringes. Thus, the demo distribution is fairly normal.</p> <p>The <code>z</code> table and normal distribution are used to derive confidence intervals. Popular intervals and their corresponding <code>z</code> scores are</p> interval z-value 99% $\\pm 2.576$ 98% $\\pm 2.326$ 95% $\\pm 1.96$ 90% $\\pm 1.645$ <p>As you imagine, these are the values of <code>z</code> on X axis of the standard normal distribution and the area they cover.</p> <p>For a normal distribution, confidence intervals for an estimate (such as mean) can be given as $$CI = \\bar x \\pm z\\frac{s}{\\sqrt{n}}$$ where $s$ is sample SD that is substituted in place of population SD, if sample size is larger than 30.</p> <p>Example The average TV viewing times of <code>40</code> adults sampled in Iowa is <code>7.75</code> hours per week. The SD of this sample is <code>12.5</code>. Find the <code>95%</code> CI population's average TV viewing times.</p> <p>$\\bar x = 40$, $s=12.5$, $n=40$, $Z=1.96$ for 95% CI. Thus $$95\\%CI = 7.75 \\pm 1.96\\frac{12.5}{\\sqrt{40}}$$ $$95\\%CI = (3.877 | 11.623)$$</p> <p>Thus the <code>95</code>% CI is pretty wide. Intuitively, if SD of sample is smaller, then so is the CI.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/stats/04_normal_distribution/#normal-distribution","title":"Normal distribution\u00b6","text":"<p>Standard normal distribution takes a bell curve. It is also called as gaussian distribution. Values in nature are believed to take a normal distribution. The equation for normal distribution is</p> <p>$$y = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ where $\\mu$ is mean</p> <p>$\\sigma$ is standard deviation</p> <p>$\\pi$ = 3.14159..</p> <p>$e$ = 2.71828.. (natural log)</p>"},{"location":"projects/stats/04_normal_distribution/#finding-z-score-and-p-values-using-scipy","title":"Finding z score and p values using SciPy\u00b6","text":"<p>The standard normal is useful as a z table to look up the probability of a z score (x axis). You can use Scipy to accomplish this.</p>"},{"location":"projects/stats/04_normal_distribution/#levels-of-significance","title":"Levels of significance\u00b6","text":"<p>By rule of thumb, a <code>z</code> score greater than <code>0.005</code> is considered significant as such a value has a very low probability of occuring. Thus, there is less chance of it occurring randomly and hence, there is probably a force acting on it (significant force, not random chance).</p>"},{"location":"projects/stats/04_normal_distribution/#transformation-to-standard-normal","title":"Transformation to standard normal\u00b6","text":"<p>If the distribution of a phenomena follows normal dist, then you can transform it to standard normal, so you can measure the <code>z</code> scores. To do so, $$std normal value = \\frac{observed - \\mu}{\\sigma}$$ You subtract the mean and divide by SD of the distribution.</p>"},{"location":"projects/stats/04_normal_distribution/#finding-z-score-and-p-values-using-scipy","title":"Finding z score and p values using SciPy\u00b6","text":"<p>The standard normal is useful as a z table to look up the probability of a z score (x axis). You can use Scipy to accomplish this.</p>"},{"location":"projects/stats/04_normal_distribution/#finding-z-score-from-a-p-value","title":"Finding z score from a p value\u00b6","text":"<p>Sometimes, you have the probability (p value), but want to find the <code>z score</code> or how many SD does this value fall from mean. You can do this inverse using <code>ppf()</code>.</p>"},{"location":"projects/stats/04_normal_distribution/#transformation-to-standard-normal-and-machine-learning","title":"Transformation to standard normal and machine learning\u00b6","text":"<p>Transforming features to standard normal has applications in machine learning. As each feature has a different unit, their range, standard deviation vary. Hence we scale them all to standard normal distribution with mean=0 and SD=1. This way a learner finds those variables that are truly influencial and not simply because it has a larger range.</p> <p>To accomplish this easily, we use <code>scikit-learn</code>'s <code>StandardScaler</code> object as shown below:</p>"},{"location":"projects/stats/04_normal_distribution/#assessing-normality-of-a-distribution","title":"Assessing normality of a distribution\u00b6","text":"<p>To assess how normal a distribution of values is, we sort the values, then plot them against sorted values of standard normal distribution. If the values fall on a straight line, then they are normally distributed, else they exhibit skewness and or kurtosis.</p>"},{"location":"projects/stats/04_normal_distribution/#standard-error","title":"Standard error\u00b6","text":"<p>As can be seen, we use statistics to estimate population mean $\\mu$ from sample mean $\\bar x$. Standard error of $\\bar x$ represents on average, how far will it be from $\\mu$.</p> <p>As you suspect, the quality of $\\bar x$, or its standard error will depend on the sample size. In addition, it also depends on population standard deviation. Thus for a tighter population, it is much easy to estimate mean from a small sample as there are fewer outliers.</p> <p>Nonetheless, $$SE(\\bar x) = \\frac{\\sigma}{\\sqrt{n}}$$ where $\\sigma$ is population SD and $n$ is sample size.</p> <p>Empirically, $SE(\\bar x)$ is same as the SD of a distribution of sample means. If you were to collect a number of samples, find their means to form a distribution, the SD of this distribution represents the standard error of that estimate (mean in this case).</p>"},{"location":"projects/stats/04_normal_distribution/#confidence-intervals","title":"Confidence intervals\u00b6","text":"<p>From a population, many samples of size &gt; <code>30</code> is drawn and their means are computed and plotted, then with $\\bar x$ or $\\bar y$ -&gt; mean of a sample and $n$ -&gt; size of 1 sample, $\\sigma_{\\bar x}$ or $\\sigma_{\\bar y}$ is SD of distribution of samples, you can observe that</p> <ul> <li>$\\mu_{\\bar x} = \\mu$ (mean of distribution of sample means equals population mean)</li> <li>$\\frac{\\sigma}{\\sqrt n} = \\sigma_{\\bar x}$ (SD of population over sqrt of sample size equals SD of sampling distribution)</li> <li>relationship between population mean and mean of a single sample is</li> <li>$$ \\mu = \\bar y \\pm z(\\frac{\\sigma}{\\sqrt n})$$</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/","title":"Binomial and Poisson distributions","text":"<p>The probability of observing <code>y</code> success in <code>n</code> trials of a binomial experiment is $$ P(y) = \\frac{n!}{y!(n-y)!}\\pi^y (1-\\pi)^{n-y} $$</p> <p>where</p> <ul> <li>n = number of trials</li> <li>$\\pi$ = probability of success in a single trial</li> <li>$1-\\pi$ = probability of failure in a single tiral</li> <li><code>y</code> = number of successes in <code>n</code> trials</li> <li>$n!$ (n factorial) = $n(n-1)(n-2)..(n-(n-1))$</li> </ul> <p>We can build a simple Python function to calculate the binomial probability as shown below:</p> In\u00a0[1]: Copied! <pre>import math\n\ndef bin_prob(n,y,pi):\n    a = math.factorial(n)/(math.factorial(y)*math.factorial(n-y))\n    b = math.pow(pi, y) * math.pow((1-pi), (n-y))\n    p_y = a*b\n    return p_y\n</pre> import math  def bin_prob(n,y,pi):     a = math.factorial(n)/(math.factorial(y)*math.factorial(n-y))     b = math.pow(pi, y) * math.pow((1-pi), (n-y))     p_y = a*b     return p_y In\u00a0[5]: Copied! <pre>utmost_80 = bin_prob(100,80,0.85)\nprint(\"utmost 80: \" + str(utmost_80))\n\nutmost_50 = bin_prob(100,50,0.85)\nprint(\"utmost 50: \" + str(utmost_50))\n\nutmost_10 = bin_prob(100,10,0.85)\nprint(\"utmost 10: \" + str(utmost_10))\n\nutmost_95 = bin_prob(100, 95, 0.85)\nprint(\"utmost 95: \" + str(utmost_95))\n</pre> utmost_80 = bin_prob(100,80,0.85) print(\"utmost 80: \" + str(utmost_80))  utmost_50 = bin_prob(100,50,0.85) print(\"utmost 50: \" + str(utmost_50))  utmost_10 = bin_prob(100,10,0.85) print(\"utmost 10: \" + str(utmost_10))  utmost_95 = bin_prob(100, 95, 0.85) print(\"utmost 95: \" + str(utmost_95)) <pre>utmost 80: 0.04022449066141771\nutmost 50: 1.9026685879668748e-16\nutmost 10: 2.4027434608795305e-62\nutmost 95: 0.0011271383580980794\n</pre> <p>We could calculate the probability for all possible values of the discrete random varibale in a loop and plot the probabilities as shown below:</p> In\u00a0[20]: Copied! <pre>x =[]\ny =[]\ncum_prob = []\nfor i in range(1,101):\n    x.append(i)\n    p_y = bin_prob(100,i,0.85)\n#     print(str(i) + \"  \" + str(p_y))\n    y.append(p_y)\n    \n    if i==1:\n        cum_prob.append(p_y)\n    else:\n        cum_prob.append(cum_prob[i-2] + p_y)\n</pre> x =[] y =[] cum_prob = [] for i in range(1,101):     x.append(i)     p_y = bin_prob(100,i,0.85) #     print(str(i) + \"  \" + str(p_y))     y.append(p_y)          if i==1:         cum_prob.append(p_y)     else:         cum_prob.append(cum_prob[i-2] + p_y) In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n\nfig,ax = plt.subplots(1,2, figsize=(13,5))\nax[0].plot(x,y)\nax[0].set_title('Probability of y successes')\nax[0].set_xlabel('num of successes in 100 trials')\nax[0].set_ylabel('probability of successes')\n\nax[1].plot(x,cum_prob)\nax[1].set_title('Cumulative Probability of y successes')\nax[1].set_xlabel('num of successes in 100 trials')\nax[1].set_ylabel('cumulative probability of successes')\n</pre> import matplotlib.pyplot as plt %matplotlib inline  fig,ax = plt.subplots(1,2, figsize=(13,5)) ax[0].plot(x,y) ax[0].set_title('Probability of y successes') ax[0].set_xlabel('num of successes in 100 trials') ax[0].set_ylabel('probability of successes')  ax[1].plot(x,cum_prob) ax[1].set_title('Cumulative Probability of y successes') ax[1].set_xlabel('num of successes in 100 trials') ax[1].set_ylabel('cumulative probability of successes')  Out[7]: <pre>&lt;matplotlib.text.Text at 0x1126d2b00&gt;</pre> <p>As we can see in the graph above, the probability that <code>x</code> number of seeds will germinate peaks around <code>85</code>, matching the germination rate of <code>0.85</code>.</p> In\u00a0[24]: Copied! <pre>#find x corresponding to the max probability value\ny.index(max(y)) + 1\n</pre> #find x corresponding to the max probability value y.index(max(y)) + 1 Out[24]: <pre>85</pre> <p>The probability falls steeply before and after 85. Using the <code>cumulative probability</code>, we can answer the question of <code>atleast</code>. Find the probability that</p> <ul> <li>atleast 20 seeds will germinate = prob(that 21 + 22 + 23 ... 100) will germinate</li> </ul> In\u00a0[30]: Copied! <pre>atleast_20 = cum_prob[99] - cum_prob[19]\nprint(\"atleast 20 = \" + str(atleast_20))\n\natleast_85 = cum_prob[99] - cum_prob[84]\nprint(\"atleast 85 = \" + str(atleast_85))\n\natleast_95 = cum_prob[99] - cum_prob[94]\nprint(\"atleast 95 = \" + str(atleast_95))\n</pre> atleast_20 = cum_prob[99] - cum_prob[19] print(\"atleast 20 = \" + str(atleast_20))  atleast_85 = cum_prob[99] - cum_prob[84] print(\"atleast 85 = \" + str(atleast_85))  atleast_95 = cum_prob[99] - cum_prob[94] print(\"atleast 95 = \" + str(atleast_95)) <pre>atleast 20 = 1.0\natleast 85 = 0.45722420577595013\natleast 95 = 0.00042551381703914704\n</pre> <p>We can repeat the experiment with a sample size of <code>20</code> and plot the results</p> In\u00a0[31]: Copied! <pre>x =[]\ny =[]\ncum_prob = []\nfor i in range(1,21):\n    x.append(i)\n    p_y = bin_prob(20,i,0.85)\n#     print(str(i) + \"  \" + str(p_y))\n    y.append(p_y)\n    \n    if i==1:\n        cum_prob.append(p_y)\n    else:\n        cum_prob.append(cum_prob[i-2] + p_y)\n</pre> x =[] y =[] cum_prob = [] for i in range(1,21):     x.append(i)     p_y = bin_prob(20,i,0.85) #     print(str(i) + \"  \" + str(p_y))     y.append(p_y)          if i==1:         cum_prob.append(p_y)     else:         cum_prob.append(cum_prob[i-2] + p_y) In\u00a0[32]: Copied! <pre>#find x corresponding to the max probability value\ny.index(max(y)) + 1\n</pre> #find x corresponding to the max probability value y.index(max(y)) + 1 Out[32]: <pre>17</pre> In\u00a0[33]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n\nfig,ax = plt.subplots(1,2, figsize=(13,5))\nax[0].plot(x,y)\nax[0].set_title('Probability of y successes')\nax[0].set_xlabel('num of successes in 20 trials')\nax[0].set_ylabel('probability of successes')\n\nax[1].plot(x,cum_prob)\nax[1].set_title('Cumulative Probability of y successes')\nax[1].set_xlabel('num of successes in 20 trials')\nax[1].set_ylabel('cumulative probability of successes')\n</pre> import matplotlib.pyplot as plt %matplotlib inline  fig,ax = plt.subplots(1,2, figsize=(13,5)) ax[0].plot(x,y) ax[0].set_title('Probability of y successes') ax[0].set_xlabel('num of successes in 20 trials') ax[0].set_ylabel('probability of successes')  ax[1].plot(x,cum_prob) ax[1].set_title('Cumulative Probability of y successes') ax[1].set_xlabel('num of successes in 20 trials') ax[1].set_ylabel('cumulative probability of successes')  Out[33]: <pre>&lt;matplotlib.text.Text at 0x112aaa630&gt;</pre> <p>Example Let y denote number of field mice captured in a trap in 24 hour period. The average value of y is <code>2.3</code>. What is the probability of capturing exactly <code>4</code> mice in a randomly selected trap?</p> <p>Ans: $$ \\mu=2.3 $$ $$ P(y=4)=? $$</p> In\u00a0[1]: Copied! <pre>import math\ndef poisson_prob(y,mu):\n    e = 2.71828\n    numerator = math.pow(mu, y) * math.pow(e, 0-mu)\n    denomenator = math.factorial(y)\n    \n    return numerator/denomenator\n</pre> import math def poisson_prob(y,mu):     e = 2.71828     numerator = math.pow(mu, y) * math.pow(e, 0-mu)     denomenator = math.factorial(y)          return numerator/denomenator In\u00a0[2]: Copied! <pre>#calculate p(4)\np_4 = poisson_prob(4, 2.3)\np_4\n</pre> #calculate p(4) p_4 = poisson_prob(4, 2.3) p_4 Out[2]: <pre>0.1169024103856968</pre> <p>Lets plot the distribution of y for values 0 to 10</p> In\u00a0[11]: Copied! <pre>y=list(range(0,11))\np_y = []\ncum_y = []\nmu = 2.3\n\nfor yi in y:\n    prob = poisson_prob(yi, mu)\n    p_y.append(prob)\n\n    if yi==0:\n        cum_y.append(prob)\n    else:\n        cum_y.append(cum_y[yi-1] + prob)\n</pre> y=list(range(0,11)) p_y = [] cum_y = [] mu = 2.3  for yi in y:     prob = poisson_prob(yi, mu)     p_y.append(prob)      if yi==0:         cum_y.append(prob)     else:         cum_y.append(cum_y[yi-1] + prob) In\u00a0[13]: Copied! <pre>#plot this\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig,ax = plt.subplots(1,2, figsize=(13,5))\nax[0].plot(y, p_y)\nax[0].set_title('Probability of finding y mice in 24 hours')\nax[0].set_xlabel('Probability of finding exactly y mice in 24 hours')\nax[0].set_ylabel('Probability')\n\nax[1].plot(y,cum_y)\nax[1].set_title('Cumulative Probability of y successes')\nax[1].set_xlabel('Probability of finding atleast y mice in 24 hours')\nax[1].set_ylabel('Cumulative probability')\n</pre> #plot this import matplotlib.pyplot as plt %matplotlib inline  fig,ax = plt.subplots(1,2, figsize=(13,5)) ax[0].plot(y, p_y) ax[0].set_title('Probability of finding y mice in 24 hours') ax[0].set_xlabel('Probability of finding exactly y mice in 24 hours') ax[0].set_ylabel('Probability')  ax[1].plot(y,cum_y) ax[1].set_title('Cumulative Probability of y successes') ax[1].set_xlabel('Probability of finding atleast y mice in 24 hours') ax[1].set_ylabel('Cumulative probability') Out[13]: <pre>&lt;matplotlib.text.Text at 0x115375eb8&gt;</pre>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#random-variables","title":"Random variables\u00b6","text":"<p>When the objective is to predict the category (qualitative, such as predicting political party affiliation), we term the it as predicting a <code>qualitative random variable</code>. On the other hand, if we are predicting a quantitative value (number of cars sold), we term it a <code>quantitative random variable</code>.</p> <p>When the observations of a <code>quantitative random variable</code> can assume values in a continuous interval (such as predicting temperature), it is called a <code>continuous random variable</code>.</p>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#properties-of-discrete-random-variable","title":"Properties of discrete random variable\u00b6","text":"<p>Say, we are predicting the probability of getting heads in two coin tosses P(y). Then</p> <ul> <li>probability of y ranges from 0 and 1</li> <li>sum of probabilities of all values of y = 1</li> <li>probabilities of outcomes of discrete random variable is additive. Thus probability of y = 1 or 2 is P(1) + P(2)</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#binomial-and-poisson-discrete-random-variables","title":"Binomial and Poisson discrete random variables\u00b6","text":""},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#binomial-probability-distribution","title":"Binomial probability distribution\u00b6","text":"<p>A binomial experiment is one in which the outcome is one of two possible outcomes. Coin tosses, accept / reject, pass / fail, infected / uninfected, these are the kinds of studies that involve a binomial experiment. Thus an experiment is of binomial in nature if</p> <ul> <li>experiment has <code>n</code> identical trials</li> <li>each trial results in 1 of 2 outcomes ( success and failure )</li> <li>probability of one of the outcome, say success remains the same for all trials</li> <li>trials are independent of each other</li> <li>the random variable <code>y</code> is the number of successes observed in <code>n</code> trials.</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#mean-and-standard-deviation-of-binomial-probability-distribution","title":"Mean and Standard Deviation of Binomial probability distribution\u00b6","text":"<p>$$ \\mu = n\\pi $$ $$ \\sigma = \\sqrt{n\\pi(1-\\pi)} $$</p> <p>where</p> <ul> <li>$\\mu$ is mean</li> <li>$\\sigma$ is standard deviation</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#binomial-probability-of-germination","title":"Binomial probability of germination\u00b6","text":"<p>Let us consider a problem where 100 seeds are drawn at random. The germination rate of each seed is <code>85%</code>. Or in other words, the probability that a seed will germinate is <code>0.85</code>, derived from experiment that <code>85</code> out of <code>100</code> seeds would germinate in a nursery. Now we want to calculate what is the probability</p> <ul> <li>that utmost only 80 seeds will germinate</li> <li>that utmost only 50 seeds will germinate</li> <li>that utmost only 10 seeds will germinate</li> <li>that utmost only 95 seeds will germinate</li> </ul>"},{"location":"projects/stats/04_probability_distributions_binomial_poisson/#poisson-probability-distribution","title":"Poisson probability distribution\u00b6","text":"<p>Poisson is used for modeling the events of a particular time over a period of time or region of space. An example is the number of vehicles passing through a security checkpoint in a 5 min interval.</p> <p>Conditions</p> <p>The probability distribution of a discrete random variable y is Poisson, if:</p> <ul> <li>Events occur one at a time. Two or more events do not occur precisely at the same time or space</li> <li>Events are independent - occurrence of an event at a time is independent of any other event in during a non-overlapping period of time or space</li> <li>The expected number of events during one period or region $\\mu$ is the same as the expected number of events in any other period or region</li> </ul> <p>Thus the probability of observing y events in a unit of time or space is given by</p> <p>$$ P(y) = \\frac{\\mu^{y}e^{-\\mu}}{y!} $$</p> <p>where</p> <ul> <li>$\\mu$ is average value of y</li> <li>e is naturally occurring constant. <code>e = 2.71828</code></li> </ul>"},{"location":"projects/stats/05_hypothesis_testing/","title":"Hypothesis testing","text":"<p>Example A sample of <code>49</code> batteries are tested for their limetimes. The SD is <code>15.0</code>, mean longivity is <code>1006.2</code>. Is it possible to claim the batteries last longer than <code>1000</code> hours on average?</p> <p>$\\bar x = 1006.2$, $n=49$, $s=15$, $\\alpha = 0.01$ assumed. $$H_{0} =&gt; \\mu \\le 1000$$ $$H_{A} =&gt; \\mu &gt; 1000$$</p> <p>Find Test Statistic $$TS = \\frac{1006.2-1000}{15/\\sqrt{49}}$$ $$TS=2.89$$ This is a right tailed hypothesis as we test if test statistic is &gt; z score for the said alpha.</p> <p>z score for $\\alpha=0.01$ = 2.576 (for 99% CI) The TS is &gt; z score. Hence reject $H_{0}$. Thus mean battery life &gt; 1000 hours by significance.</p> In\u00a0[\u00a0]: Copied!"},{"location":"projects/stats/05_hypothesis_testing/#hypothesis-testing","title":"Hypothesis testing\u00b6","text":"<p>The goal of hypothesis testing is to answer a simple yes / no question about a population parameter. There are two types of hypothesis, $H_{0}$ the null hypothesis and $H_{A}$ the Alternate hypothesis.</p> <p>The steps followed are:</p> <ul> <li>set up the hypothesis (null, alternate)</li> <li>choose $\\alpha$ level (confidence interval)</li> <li>determine rejection region (on the z curve)</li> <li>compute the test statistic (p value based on z score)</li> <li>make a decision</li> </ul> <p>Rules in hypothesis testing</p> <ol> <li>No equal sign in $H_{A}$. Only $\\ne, &lt;, &gt;$ signs</li> <li>Put what you want to test in $H_{A}$, unless you violate rule 1, then you put that in $H_{0}$</li> <li>Believe $H_{0}$ unless the dataset shows otherwise</li> <li>when we make our decision, we either reject $H_{0}$ or fail to reject it.</li> </ol>"},{"location":"projects/stats/05_hypothesis_testing/#examples-of-formulating-hypothesis","title":"Examples of formulating hypothesis\u00b6","text":"<ol> <li>Nitrate levels are unsafe if &gt; 10ppm. Test if out water is unsafe on average.</li> </ol> <ul> <li>$H_{0} =&gt; \\mu \\le 10ppm$</li> <li>$H_{A} =&gt; \\mu &gt; 10 ppm$</li> </ul> <ol> <li>Test if a coin is fair.</li> </ol> <ul> <li>$H_{0} =&gt; p(h)=0.5$</li> <li>$H_{A} =&gt; p(h)\\ne 0.5$ This is because alt hypothesis should not have equal sign.</li> </ul>"},{"location":"projects/stats/05_hypothesis_testing/#type-1-2-errors","title":"Type 1, 2 errors\u00b6","text":"<p>For a jury trial, our motto is innocent until proven guilty. Hence</p> <ul> <li><p>$H_{0} =&gt; innocent$ as we reject H0 or fail to do so</p> </li> <li><p>$H_{A} =&gt; guilty$</p> </li> <li><p>Type 1 error: False positive</p> <ul> <li>we reject $H_{0}$ when it is still true</li> <li>$\\alpha$ = p(type 1 error) = p(rejecting $H_{0}$ when it is still true)</li> </ul> </li> <li><p>Type 2 error: False negative</p> <ul> <li>$\\beta$ = p(type 2 error) = p(failing to reject $H_{0}$ when $H_{A}$ is true)</li> </ul> </li> </ul> <p>In practice, we fix $\\alpha = 0.5$ and calculate $\\beta' = (1-\\beta)$</p>"},{"location":"projects/stats/05_hypothesis_testing/#testing-your-hypothesis","title":"Testing your hypothesis\u00b6","text":"<p>You calculate the test statistic as $$TS = \\frac{\\bar x - \\mu}{\\frac{s}{\\sqrt n}}$$ You either reject or approve the $H_{0}$ based on the value of the TS compared against the p value for the said $\\alpha$ value (confidence interval)</p>"},{"location":"projects/stats/islr/02_stat_learning/","title":"Statistical Learning","text":"<p>Notes</p> <ul> <li>parametric methods are an approximation of the true functional form of f.</li> <li>simpler (lower order, less flexible) models may lead to poorer estimates of f</li> <li>more flexible (higher order, complex) models may lead to overfitting.</li> <li>Since the model is trained on a subset of values, it might be very different from true nature of f. Hence the model developed is only valid for the range of data it was trained on.</li> </ul> <p>When K=1, the decision boundary is of highest flexibility and overfits with low bias and high variance. When K is very large, it underfits with low flexibility. It has high bias and low variance. As in regression, with classification, increasing flexibility reduces the training error, but does not affect test error rate.</p>"},{"location":"projects/stats/islr/02_stat_learning/#statistical-learning","title":"Statistical Learning\u00b6","text":""},{"location":"projects/stats/islr/02_stat_learning/#prediction","title":"Prediction\u00b6","text":"<p>In a prediction / regression problem, the inputs (denoted by <code>X</code>) are called as <code>predictors</code>, <code>independent variables</code>, <code>features</code> and the predicted variable is called as <code>response</code>, <code>dependent variable</code> and is denoted by <code>Y</code>.</p> <p>The relationship betwen input and predicted is represented as</p> <p>$$ Y = f(X) + \\epsilon $$</p> <p>where $f$ is some fixed, unknown function that is to be determined. $\\epsilon$ is random error term that is independent of <code>X</code> and has zero mean.</p> <p>In reality, $f$ may depend on more than 1 input variable $X$, for instance 2. In this case, $f$ is a <code>2D</code> surface that is fit. In general, the process of estimating $f$ is statistical learning.</p>"},{"location":"projects/stats/islr/02_stat_learning/#reducible-and-irreducible-errors","title":"Reducible and Irreducible errors\u00b6","text":"<p>Since $f$ and $Y$ cannot be calculated, the best we can get is to estimate them. Thus, the estimates are called $\\hat f$ and $\\hat Y$</p> <p>$$ \\hat Y = \\hat f(X) $$</p> <p>The accuracy of $\\hat Y$ depends on reducible and irreducible errors. The error in prediction of $\\hat f$ is reduible and can be improved wth more data and better models. However, $\\hat Y$ is also a function of $\\epsilon$ which is irreducible. Thus, the best our predictions can get is</p> <p>$$ \\hat Y = f(X) $$</p> <p>Focus of Statistical learning is to estimating $f$ as $\\hat f$ with least reducible error. However, the accuracy of $\\hat Y$ will always be controlled by irreducible and unknown error $\\epsilon$.</p> <p>In prediction problems, $\\hat f$ can be treated as blackbox as we are only interested in predicting $Y$.</p>"},{"location":"projects/stats/islr/02_stat_learning/#inference","title":"Inference\u00b6","text":"<p>We are interested in understanding how each of the different $X_{1}... X_{p}$ affect the dependent variable $Y$, hence the name inference. Here, $\\hat f$ cannot be treated as blackbox and we need to know its exact form. Some questions that are sought to be answered through inference:</p> <ul> <li>which predictor variables are associated with the response?</li> <li>what is the relationship b/w response and each predictor?</li> <li>is the relationship linear or is more complicated?</li> </ul>"},{"location":"projects/stats/islr/02_stat_learning/#parametric-and-unparametric-methods-for-estimating-f","title":"Parametric and Unparametric methods for Estimating f\u00b6","text":"<p>The observations for X and Y can be written as ${(x_{1}, y_{1}),(x_{2}, y_{2}),...,(x_{n}, y_{n})}$ where each x has many predictor variables that can be written as $x_{i} = (x_{i1},x_{i2},..,x_{ip})^{T}$. The goal is to find $\\hat f$ such that $Y \\approx \\hat f (X)$</p>"},{"location":"projects/stats/islr/02_stat_learning/#parametric-methods-for-estimating-f","title":"Parametric methods for estimating f\u00b6","text":"<p>Parametric methods take a model based approach (deterministic). We make an assumption about the functional form of f (whether it is linear, non linear, higher order, logistic etc). For instance, if we assume that f is linear, then</p> <p>$$ Y \\approx f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + .. + \\beta_{p}X_{p} $$ we only need to find $p+1$ coefficients. Through training or fitting (using methods like ordinary least squares), we can estimate the coefficients.</p>"},{"location":"projects/stats/islr/02_stat_learning/#non-parametric-methods-for-estimating-f","title":"Non parametric methods for estimating f\u00b6","text":"<p>Non parametric methods avoid assuming the functional form of f. However, these methods require a very large number of observations since they do not try to reduce the phenomenon to a model.</p>"},{"location":"projects/stats/islr/02_stat_learning/#general-concepts","title":"General concepts\u00b6","text":"<p>Model interpretability and complexity: The more complex a model is (higher order more flexible models, decision trees..), the less interpretable it is.</p> <p>Supervised vs Unsupervised algorithms: Supervised methods are used when both the <code>predictor</code> and <code>response</code> variables can be measured and data is available. Unsupervised methods are used when little is known about the data and only <code>predictor</code> variables are available. Unsupervised are best when put to classification / clustering problems.</p> <p>Regression vs Classification: When the <code>response</code> variable is <code>quantitative</code> and continuous, the problem is considered a regression. When the <code>response</code> is <code>qualitative</code> and falls within categories, then the problem is a classification problem. Howerver, this distinction is not really solid as many algorithms can be used for both.</p>"},{"location":"projects/stats/islr/02_stat_learning/#model-accuracy-regression-problems","title":"Model accuracy - Regression problems\u00b6","text":""},{"location":"projects/stats/islr/02_stat_learning/#measuring-quality-of-fit","title":"Measuring quality of fit\u00b6","text":"<p>Here the deviation between <code>predicted</code> and actual values is measured. In regression, <code>Mean Squared Error (MSE)</code> is commonly used.</p> <p>$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_{i} - \\hat f(x_{i}))^2 $$</p> <p>The MSE obtained is called training MSE. When used against unseen test data, we get test MSE. Our objective is to choose the method with lowest test MSE. There is no guarantee that a low training MSE will yield a low test MSE.</p> <p>A fundamental property in statistical learning is as model flexibility increases, training MSE might decrease, but test MSE might not. When a given learning method yields a small training MSE but a large test MSE, we are overfitting the data. This is because, our data might have noise from irreducible error and the model is trying to fit it.</p>"},{"location":"projects/stats/islr/02_stat_learning/#bias-variance-trade-off","title":"Bias variance trade-off\u00b6","text":"<p>If you plot the test MSE against model flexibility, it follows a U shaped curve. Thus it first reduces then increases. The expected test MSE for observation $x_{0}$ $E(y_{0} - \\hat f(x_{0}))$ can be decomposed to <code>3</code> fundamental quantities: (a) the variance of $\\hat f(x_{0})$, (b) the squared bias of $\\hat f(x_{0})$ and (c) variance of irreducible error $\\epsilon$. Thus:</p> <p>$$ E(y_{0} - \\hat f(x_{0}))^{2} = Var(\\hat f(x_{0})) + [Bias(\\hat f(x_{0}))]^{2} + Var(\\epsilon) $$</p> <p>Thus, to reduce the expected test MSE, we need to reduce both the variance of $\\hat f$ and bias of $\\hat f$.</p> <p>Variance refers to the amount by which $\\hat f$ would change if we estimated it using a different training dataset. Ideally, $\\hat f$ should not change much if a slightly different data set is used. A statistical learning method with high variance would yield a very different $\\hat f$ for different training data sets. Higher the model flexibility, the higher is its variance as the model closely fits the training data.</p> <p>Bias refers to the error introduced by approximating a real-life problem. Generally, higher model flexibility, the lower is the bias. Thus as we use more flexible methods, the variance will increase and bias would decrease.</p> <p>As model flexibility increases, the bias reduces faster than the rate at which variance increases. Thus, the test MSE drops initially before increasing (<code>U shape</code>). This relationship is called the bias variance trade-off and the objective is to pick the model flexibility that has the least of both.</p>"},{"location":"projects/stats/islr/02_stat_learning/#model-accuracy-classification-problems","title":"Model accuracy - Classification problems\u00b6","text":""},{"location":"projects/stats/islr/02_stat_learning/#measuring-quality-of-classification","title":"Measuring quality of classification\u00b6","text":"<p><code>error rate</code> is used to quantify the errors in classification. It is the ratio of <code>sum of misclassifications</code> to <code>number of observations</code>.</p> <p>$$ error rate = \\frac{1}{n}\\sum_{i=1}^{n}I(y_{i} \\ne \\hat y_{i}) $$</p> <p>where $\\hat y_{i}$ is predicted class label for ith observation using $\\hat f$. When computed against training data, this yields training error rate. When computed for test data, this yields test error rate.</p>"},{"location":"projects/stats/islr/02_stat_learning/#bayes-classifier","title":"Bayes Classifier\u00b6","text":"<p>Bayes classifier is a simple but idealistic classifier. It assigns each observation to the most likely class given its predictor class. This can be written using conditional probability as below:</p> <p>$$ P(Y=j \\ | \\ X=x_{0}) $$</p> <p>Thus, the error rate with Bayes classifier becomes the average of (1 - max probability for different classes). The Bayes error rate is analogous to **irreducible error*.</p>"},{"location":"projects/stats/islr/02_stat_learning/#knn-classifier","title":"KNN classifier\u00b6","text":"<p>In reality, Bayes classifier is not possible as the conditional probability is unknown. Instead, algorithms attempt to derive the conditional probability. One such is KNN.</p> <p>The KNN classifier identifies K points in training data that are closest to test observation $x_{0}$, represented at $N_{0}$. It then estimates conditional proabability for class $j$ as the fraction of points in $N_{0}$ whose classes equal $j$. This can be written as:</p> <p>$$ P(Y = j \\ | \\ X = x_{0}) = \\frac{1}{K} \\sum_{i \\in N_{0}} I(y_{i} = j) $$</p>"},{"location":"projects/stats/islr/03_linear_regression/","title":"Linear regression concepts","text":"<p>Mathematically, a linear relationship between <code>X</code> and <code>Y</code> can be written as $$Y \\approx \\beta_{0} + \\beta_{1}X$$</p> <p>$\\beta_{0}$ and $\\beta_{1}$ represent the intercept and slope. They are the model coefficients or parameters. Through regression we estimate these parameters. The estimates are represented as $\\hat\\beta_{0}$ and $\\hat\\beta_{1}$. Thus,</p> <p>$$\\hat y = \\hat\\beta_{0} + \\hat\\beta_{1}x$$</p> <p>We estimate $\\hat\\beta_{0}$ and $\\hat\\beta_{1}$ using <code>least squared regression</code>. This technique forms a line that minimizes average squared error for all data points. Each point is weighed equally. If</p> <p>$$\\hat y_{i} = \\hat\\beta_{0} + \\hat\\beta_{1}x_{i}$$ is the prediction for <code>i</code>th value pair of x, y, then the error is calculated as $$e_{i} = y_{i} - \\hat y_{i}$$. This error is also called a <code>residual</code>. This the residual sum of squares (RSS) is calculated as $$RSS = e_{1}^{2} + e_{2}^{2}... + e_{i}^{2}$$</p> <p>Thus if the relationship between $X$ and $Y$ is approximately linear, then we can write: $$Y = \\beta_{0} + \\beta_{1}X + \\epsilon$$</p> <p>where $\\epsilon$ is the catch-all error that is introduced in forcing a linear fit for the model. The above equation is the population regression line. In reality, this is not known (unless you synthesize data using this model). In practice, you estimate the population regression with a smaller subset of datasets.</p> <p>Using Central Limit Theorem, we know the average of a number of sample regression coefficients, predict the population coefficients pretty closely. Proof is availble here.</p> <p>As you see, quadratic produces 1 curve while cubic produces 2 curves.</p> In\u00a0[\u00a0]: Copied!"},{"location":"projects/stats/islr/03_linear_regression/#linear-regression-concepts","title":"Linear regression concepts\u00b6","text":""},{"location":"projects/stats/islr/03_linear_regression/#standard-error","title":"Standard error\u00b6","text":"<p>By averaging a number of estimations of $\\hat\\beta_{0}$ and $\\hat\\beta_{1}$, we are able to estimate the population coefficients in an unbiased manner. Averaging will greatly reduce any systematic over or under estimations when choosing a small sample.</p> <p>Now, how far will a single estimate of $\\hat\\beta_{0}$ be from the actual $\\beta_{0}$? We can calculate it using standard error.</p> <p>To understand standard error let us consider the simple case of estimating population mean using a number of smaller samples. The standard error in a statistic (mean in this case) can be written as:</p> <p>$$ SE(\\hat\\mu) = \\frac{\\sigma}{\\sqrt{n}}$$ where $\\hat\\mu$ is the estimate for which we calculate the standard error for (sample mean in this case), $\\sigma$ is the standard deviation of the population and $n$ is the size of the sample you draw each time.</p> <p>The SE of $\\hat\\mu$ is the same as the Standard Deviation of the sampling distribution of a number of sample means. Thus, the above equation gives the relationship between sample mean and population mean and sample size and how far will the sample mean be off. Thus:</p> <p>$$ SD(\\hat\\mu) = SE(\\hat\\mu) = \\frac{\\sigma}{\\sqrt n}$$</p> <p>Note, SE is likely to be smaller if you have a large sample. The value of SE is the average amount your $\\hat\\mu$ deviates from $\\mu$.</p> <p>In reality, you don't have $\\sigma$ or $\\mu$. Thus, using the SE formula, we can calculate the SD of population as:</p> <p>$$\\sigma = \\sigma_{\\hat\\mu}*\\sqrt n $$</p>"},{"location":"projects/stats/islr/03_linear_regression/#applications-of-standard-error","title":"Applications of Standard Error\u00b6","text":"<p>Confidence Intervals: SE is used to compute <code>CI</code>. A <code>95</code>% CI is defined as the range of values such that with <code>95</code>% probability, the range will contain the true unknown value of the parameter. Similar is <code>99</code>% CI. Thus, <code>95</code>% CI for $\\beta_{1}$ is written as</p> <p>$$ \\hat \\beta_{1} \\pm 2 * SE(\\hat \\beta_{1}) $$ Same for $\\beta_{0}$.</p> <p>Hypothesis tests: SE is also used to perform hypothesis tests on coefficients. Commonly,</p> <ul> <li>null hypothesis: $H_{0}$: there is no relationship between X and Y - meaning $\\beta_{1} \\ = \\ 0$</li> <li>alternate hypothesis: $H_{a}$: there is some significant relationship</li> </ul> <p>To disprove $H_{0}$, we need</p> <ul> <li>$\\hat \\beta_{1}$ to be sufficiently large: (either positive or negative), or,</li> <li>$SE(\\hat \\beta_{1})$ to be small, then even relatively small values of $\\hat \\beta_{1}$ would be statistically significant. Else, slope has to be really large.</li> </ul> <p>We compute <code>t-statistic</code> to evaluate the significance of $\\beta$, which is similar to computing <code>z scores</code>. $$ t \\ = \\ \\frac{\\hat \\beta_{1} - 0}{SE(\\hat \\beta_{1})} $$ We get scores for <code>t</code> distribution. t follows standard normal for <code>n&gt;30</code>. The <code>p-value</code> we get is used to evaluate the significance of the estimate. A small <code>p-value</code> indicates the relationship between predictor and response is unlikely to be by chance.</p> <p>To reject the null hypothesis, we need a <code>p-value &lt; 0.005</code> and <code>t-statistic &lt; 2</code></p>"},{"location":"projects/stats/islr/03_linear_regression/#multiple-linear-regression","title":"Multiple Linear Regression\u00b6","text":"<p>To fit the effects of multiple predictors, we extend the simple linear model by providing a coefficient for each predictor. Thus:</p> <p>$$ Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{p}X_{p} + \\epsilon $$</p> <p>we interpret $\\beta_{p}$ as the average effect on $Y$ that predictor variable has when holding all other variables constant.</p>"},{"location":"projects/stats/islr/03_linear_regression/#what-to-look-for-in-multiple-linear-regression","title":"What to look for in multiple linear regression\u00b6","text":""},{"location":"projects/stats/islr/03_linear_regression/#1-is-there-a-relationship-between-response-and-predictor-variables","title":"1. Is there a relationship between response and predictor variables\u00b6","text":"<p>We run hypothesis tests, just that, $H_{0}$ checks for all coefficients to be <code>0</code> and $H_{a}$ checks for at least one of $\\beta_{p}$ is non zero.</p> <p>$$ H_{0} = \\beta_{1} = \\beta_{2} = \\beta_{p} = 0 $$</p> <p>In simple linear regression, the hypothesis tests were conducted against a t distribution, where as in multiple linear regression, we compute a F distribution. The <code>p-value</code>s are against this <code>F</code> distribution.</p>"},{"location":"projects/stats/islr/03_linear_regression/#hallmarks-of-a-valid-regression-analysis","title":"Hallmarks of a valid regression analysis\u00b6","text":"<p>Source: Regression tutorial from Minitab</p> <p>Applications of regression analysis:</p> <ul> <li>multiple predictor variables</li> <li>continuous and categorical variables</li> <li>higher-order terms to model curvature</li> <li>interaction terms to see if the effect of one predictor depends upon the value of another</li> </ul>"},{"location":"projects/stats/islr/03_linear_regression/#what-to-put-in-a-regression-output","title":"What to put in a regression output\u00b6","text":"<ul> <li>Coefficients of each predictors. The coefficients indicate how influential a predictor is.</li> <li>Std. Error $S$ of coefficients</li> <li><code>p-value</code> of each coefficient. A low <code>p-value</code>, (lower than <code>0.05</code>) indicates the confidence in predicting the coefficient.</li> <li><code>95%</code> CI of each coefficient prediction</li> <li>residual plots<ul> <li>Fitted vs Residual plot to ensure the residuals don't follow any pattern. Residuals should be random around the prediction line, with <code>mean=0</code>). If residuals follow a pattern, then the model is missing or leaking some phenomena into the error term. You may be missing an important predictor variable.</li> <li>QQ plot of residuals against standard normal to assess normalcy of residuals</li> <li>histograms of residuals can also be used to assess normalcy.</li> </ul> </li> </ul> <p>Example from Minitab </p>"},{"location":"projects/stats/islr/03_linear_regression/#r-squared","title":"R-squared\u00b6","text":"<p>$R^2$ is calculated as the ratio of variance in predicted <code>Y</code> to actual <code>Y</code>. In other words, it compares the distance between actual values to mean vs predicted values to mean. $$ R^2 = \\frac{(\\hat y - \\bar y)^2}{(y-\\bar y)^2} $$</p> <p>$R^2$ measures the strength of the relationship between predictors and response. It ranges from <code>0-100%</code> or <code>0-1</code>.</p> <p>$R^2$ is limited, it cannot quite tell you if the model is systematically under or over predicting values. Since it compares deviation from mean, if values fall farther from regression line, yet keep the same variation from mean as actual, then $R^2$ is high. However, this does not mean the model is a good fit.</p> <p>Since it is inherently biased, some researchers don't use this at all.</p>"},{"location":"projects/stats/islr/03_linear_regression/#s-standard-error-in-regression","title":"<code>S</code> - Standard error in Regression\u00b6","text":"<p>A better estimate of regression is Standard Error (also called RSE - residual standard error) which measures average distance each actual value falls from regression line. It is calculated as below:</p> <p>$$ S = \\sqrt{\\frac{\\sum (\\hat y -y)^2}{n-2}} $$</p> <p><code>S</code> represents, in the units of predicted variable, on average how far the actual values fall from prediction. </p>"},{"location":"projects/stats/islr/03_linear_regression/#confidence-intervals","title":"Confidence intervals\u00b6","text":"<p>Approximately <code>95%</code> of predictions fall within $\\pm 2 standard error$ of regression from regression line.</p>"},{"location":"projects/stats/islr/03_linear_regression/#problems","title":"Problems\u00b6","text":"<p>Multicollinearity is when predictors are correlated. In ideal case, each predictor should be independent which helps us properly assess their individual influence on the phenomena.</p>"},{"location":"projects/stats/islr/03_linear_regression/#how-to-interpret-a-regression-analysis","title":"How to interpret a regression analysis\u00b6","text":""},{"location":"projects/stats/islr/03_linear_regression/#p-values","title":"P-values\u00b6","text":"<p>The <code>p-value</code> of a coefficient is the probability of</p> <ul> <li>$H_{0} \\ = \\ 0$ : Null hypothesis that coefficient is 0. That is, it has no effect on the model / phenomena</li> <li>$\\alpha$ is the threshold you set to determine what is statistically significant. By convention it is <code>0.95</code> for <code>p-values</code>. Thus, those coefficents whose <code>p-value</code>s are less than $\\alpha$ have a significant coefficient. The size of the coefficient determines their influence on the model itself, not the <code>p-value</code>.</li> </ul> <p>When you simplify your model (step-wise forward or hierarchical), you start by removing those predictors whose <code>p-value</code> is greater than $\\alpha$.</p>"},{"location":"projects/stats/islr/03_linear_regression/#coefficients","title":"Coefficients\u00b6","text":"<p>Regression coefficients represent the mean value by which the predicted variable will change for <code>1</code> unit change in that particular predictor variable while holding all other variables constant. Thus, regression provides a high level of explainability associated with each variable and its influence on the phenomena.</p>"},{"location":"projects/stats/islr/03_linear_regression/#r-squared","title":"R-squared\u00b6","text":"<p>As said before, $R^{2}$ measures the variability of predicted data vs actual data with mean. It can be interpreted as proportion of variability in Y that can be explained using X. A regression with high <code>p-value</code> and low $R^{2}$ is problematic. What this means is the trend (slope) is significant where as the data has a lot of inherent variability that the model does not explain well.</p> <p>In the case of simple linear regression, $R^{2}$ equals the correlation coefficient $r$. However, in multiple linear regression, this relationship does not extend.</p>"},{"location":"projects/stats/islr/03_linear_regression/#s","title":"S\u00b6","text":"<p>The Std. Error in regression (also called as <code>RSE</code> (residual standard error)) quantifies how far the points are spread from the regression line. This also helps build the prediction interval and understand what the error would be on average when you predict unknown dataset.</p> <p><code>RSE</code> takes the unit of predicted variable. Determining whether or not an <code>RSE</code> is good depends on the problem context.</p>"},{"location":"projects/stats/islr/03_linear_regression/#explaining-a-regression-analysis","title":"Explaining a regression analysis\u00b6","text":"<p>Resource Follow the axioms below when you try to explain a regression analysis</p> <ul> <li>do not use <code>p-value</code> to explain influence of a predictor. <code>p-value</code> only suggests the significance of the coefficient not being <code>0</code></li> <li>do not compare the importance of two predictors based on their coefficients (slopes). The variables might be in different units and hence may not be a fair comparison</li> <li>To compare predictors, standardize them (subtract mean and divide by SD -&gt; transform to std. normal Z scores)</li> <li>Now compare the coefficients for influence.</li> <li>When you add more variables to the model, the $R^{2}$ increases, but this does not mean the model improves. In this case, calculate the adj.$R^{2}$ which accounts for this.</li> <li>Another approach is, when you build the model step-wise, find the variable that accounts for greatest increase in $R^{2}$ value. That is the most influential predictor.</li> <li>Another adavantage of standardization is it reduces multicollinearity</li> </ul>"},{"location":"projects/stats/islr/03_linear_regression/#types-of-regression-analysis","title":"Types of regression analysis\u00b6","text":"<p>Resource - minitab blog</p>"},{"location":"projects/stats/islr/03_linear_regression/#predicting-categorical-response-variable","title":"Predicting categorical response variable:\u00b6","text":"<ul> <li>binary logistic regression: response falls into one of two categories (Yes or No)</li> <li>ordinal logistic regression: response is in categories (discrete) and can be ordered from least to greatest</li> <li>nominal logistic regression: response is in categories (discrete) and not follow any order.</li> </ul>"},{"location":"projects/stats/islr/03_linear_regression/#regressing-a-continuous-response","title":"Regressing a continuous response:\u00b6","text":"<p>The response is on a continuous scale.</p> <ul> <li>OLS - ordinary least squares regression: Linear and multiple regressions. One or more continuous predictors and a continuous response. Note: here, the predictors are also continuous, not categorical.</li> <li>Stepwise regession: This is a technique to find influential predictors, not a type of regression. Methods include forward selection, backward elimination.</li> <li>Nonlinear regression: use a non-linear function to model a set of continuous predictors and predict continuous response variable.</li> </ul>"},{"location":"projects/stats/islr/03_linear_regression/#linear-vs-non-linear-regression-vs-transformation","title":"Linear vs Non linear regression vs Transformation\u00b6","text":"<p>Contrary to popular belief, linear regression can produce curved fits! An equation is considered non linear when the predictor variables are multiplicative rather than additive. Using log, quadratic, cubic functions, you can produce a curved linear fit.</p> <p>In general, you choose the order of the equation based on number of curves you need in the fit.</p> <p></p>"},{"location":"projects/stats/islr/03_linear_regression/#reciprocal-transformation","title":"Reciprocal transformation\u00b6","text":"<p>As X increases, if your data (Y) descends to floor or ascends to a ceiling and flat lines, then the effect of X flattens out as it increases. In these cases, you can do a reciprocal of X and fit against it. Sometimes a quadratic fit of reciprocal of X would be a good fit.</p>"},{"location":"projects/stats/islr/03_linear_regression/#log-transformation","title":"Log transformation\u00b6","text":"<p>Log transform can rescue a model from non-linear to linear territory. When you take log on both sides of equation, multiplication signs become additions and exponential signs become multiplications.</p> <p>For instance: $$ Y = e^{B_{0}}X_{1}^{B_{1}}X_{2}^{B_{2}} $$ transforms to below when you take log on both sides (double-log form): $$ Ln Y = B_{0} + B_{1}lnX_{1} + B_{2}lnX_{2} $$</p>"},{"location":"projects/stats/islr/03_linear_regression/#non-linear-models","title":"Non-linear models\u00b6","text":"<p>Non linear models have predictors that in product with each other. In general, nonlinear models can take any number of formats. The trick is to finding one that will fit the data. Some limits of non-linear models</p> <ul> <li>lack of interpretability. You cannot clearly state the influence each predictors have on the phenomena</li> <li>You cannot find <code>p-values</code> and <code>CI</code> for the coefficients.</li> <li>You don't have $R^2$ statistic to evaluate the goodness of fit.</li> </ul>"},{"location":"projects/thermal/","title":"Thermal Remote Sensing","text":"<p>coming soon...</p>"},{"location":"talks/","title":"Atma's talks","text":""},{"location":"talks/#talks-in-public-conferences","title":"Talks in public conferences","text":"<ul> <li> <p>2019 - Explore How to Use ArcGIS and Jupyter for Geospatial Data Science </p> <ul> <li></li> </ul> </li> <li> <p>2019 FOSS4GNA</p> <ul> <li>Let's take the machines house hunting</li> </ul> </li> <li> <p>2018 Portland GeoDev Meetup</p> <ul> <li></li> </ul> </li> </ul>"},{"location":"talks/#talks-at-esri-sponsored-events","title":"Talks at Esri sponsored events","text":""},{"location":"talks/#2021-esri-developer-summit","title":"2021 Esri Developer Summit","text":"<ul> <li>ArcGIS Pro: Your spatial data science workstation</li> <li>Deploying apps and services with ArcGIS API for Python</li> </ul>"},{"location":"talks/#2020-esri-developer-summit","title":"2020 Esri Developer Summit","text":"<ul> <li>Imagery and raster analysis on your Web GIS</li> <li>Spatial data science in ArcGIS - a Tour</li> <li>ArcGIS API for Python for analysts and data scientists</li> </ul>"},{"location":"talks/#spring-2020-spatial-data-science-mooc","title":"Spring 2020 Spatial Data Science MOOC","text":""},{"location":"talks/#feb-2020-python-libraries-for-spatial-data-science","title":"Feb 2020 Python libraries for Spatial Data Science","text":""},{"location":"talks/#2019-esri-developer-summit","title":"2019 Esri Developer Summit","text":"<ul> <li>Python across the ArcGIS platform</li> <li>Imagery in ArcGIS</li> <li>Spatial Data Science with ArcGIS</li> </ul>"},{"location":"talks/#2018-esri-partner-conference-and-developer-summit","title":"2018 Esri Partner conference and Developer Summit","text":"<ul> <li>Plenary: Automation and analytics in the ArcGIS Platform with Python</li> <li>ArcGIS API for Python for developers, administrators and data scientists</li> <li>Plenary: Automation in the ArcGIS platform</li> <li>ArcGIS Python API: Advanced Scripting</li> <li>Mapping visualization and analysis with ArcGIS API for Python</li> </ul>"},{"location":"talks/#2017-esri-user-conference-talks","title":"2017 Esri User Conference talks","text":"<ul> <li>Introduction to scripting your WebGIS with ArcGIS API for Python</li> <li>Advanced scripting with ArcGIS API for Python</li> <li>ArcGIS API for Python for administrators and content publishers</li> <li>ArcGIS Python API for GIS Analysts and Data Scientists</li> <li>ArcGIS Python API: Introduction to Scripting your Web GIS</li> <li>Cloning Your Portal Users, Groups and Content using ArcGIS API for Python</li> <li>Mapping, Visualization, and Analysis Using ArcGIS API for Python</li> </ul>"},{"location":"talks/#2017-geodev-webinar-python","title":"2017 GeoDev Webinar - Python","text":"<ul> <li>Explore the power of the ArcGIS API for Python</li> </ul>"},{"location":"talks/#2017-esri-developer-summit-talks","title":"2017 Esri Developer Summit talks","text":"<ul> <li>A 2 day Dev Summit precon workshop on ArcGIS API for Python</li> <li>Introduction to scripting your WebGIS with ArcGIS API for Python</li> <li>Advanced scripting with ArcGIS API for Python</li> </ul>"},{"location":"talks/#2015-esri-user-conference-talks","title":"2015 Esri User Conference talks","text":"<ul> <li>Publishing GIS Services to ArcGIS for Server</li> </ul>"},{"location":"talks/2015-esri-uc/","title":"2015 Esri User Conference","text":""},{"location":"talks/2015-esri-uc/#publishing-gis-services-to-arcgis-for-server","title":"Publishing GIS Services to ArcGIS for Server","text":"<p> Get the slide deck here: Esri proceedings</p>"},{"location":"talks/2017-esri-devsummit/","title":"2017 Esri Developer Summit","text":""},{"location":"talks/2017-esri-devsummit/#2017-dev-summit-precon-workshop","title":"2017 Dev Summit precon workshop","text":"<p>This was a 2 day hands-on workshop that I ran. Find the tutorial notebooks here</p>"},{"location":"talks/2017-esri-devsummit/#introduction-to-scripting-your-webgis-with-arcgis-api-for-python","title":"Introduction to scripting your WebGIS with ArcGIS API for Python","text":""},{"location":"talks/2017-esri-devsummit/#advanced-scripting-with-arcgis-api-for-python","title":"Advanced scripting with ArcGIS API for Python","text":""},{"location":"talks/2017-esri-uc/","title":"2017 Esri User Conference talks","text":""},{"location":"talks/2017-esri-uc/#1-introduction-to-scripting-your-webgis-with-arcgis-api-for-python","title":"1. Introduction to scripting your WebGIS with ArcGIS API for Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-esri-uc/#2-advanced-scripting-with-arcgis-api-for-python","title":"2. Advanced scripting with ArcGIS API for Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-esri-uc/#3-arcgis-api-for-python-for-administrators-and-content-publishers","title":"3. ArcGIS API for Python for administrators and content publishers","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-esri-uc/#4-arcgis-python-api-for-gis-analysts-and-data-scientists","title":"4. ArcGIS Python API for GIS Analysts and Data Scientists","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-esri-uc/#5-arcgis-python-api-introduction-to-scripting-your-web-gis","title":"5. ArcGIS Python API: Introduction to Scripting your Web GIS","text":"<p>Get your notebooks from here</p>"},{"location":"talks/2017-esri-uc/#6-cloning-your-portal-users-groups-and-content-using-arcgis-api-for-python","title":"6. Cloning Your Portal Users, Groups and Content using ArcGIS API for Python","text":"<p>Get your notebooks from here</p>"},{"location":"talks/2017-esri-uc/#7-mapping-visualization-and-analysis-using-arcgis-api-for-python","title":"7. Mapping, Visualization, and Analysis Using ArcGIS API for Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2017-geodev-webinar/","title":"2017 Esri GeoDev webinar","text":""},{"location":"talks/2018-esri-devsummit/","title":"2018 Esri Partner conference and Developer Summit","text":""},{"location":"talks/2018-esri-devsummit/#2018-esri-partner-conference","title":"2018 Esri Partner conference","text":""},{"location":"talks/2018-esri-devsummit/#plenary-automation-and-analytics-in-the-arcgis-platform-with-python","title":"Plenary: Automation and analytics in the ArcGIS Platform with Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2018-esri-devsummit/#arcgis-api-for-python-for-developers-administrators-and-data-scientists","title":"ArcGIS API for Python for developers, administrators and data scientists","text":"<p>This was a closed session. The slide deck and notebooks are not made public.</p>"},{"location":"talks/2018-esri-devsummit/#2018-esri-developer-summit","title":"2018 Esri Developer summit","text":""},{"location":"talks/2018-esri-devsummit/#plenary-automation-in-the-arcgis-platform","title":"Plenary: Automation in the ArcGIS platform","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2018-esri-devsummit/#arcgis-python-api-advanced-scripting","title":"ArcGIS Python API: Advanced Scripting","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2018-esri-devsummit/#mapping-visualization-and-analysis-with-arcgis-api-for-python","title":"Mapping visualization and analysis with ArcGIS API for Python","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2018-portland-geodev-meetup/","title":"2018 Portland GeoDev Meetup","text":"<p>House hunting the data scientist way </p> <p>Buying a house is a huge financial and personal undertaking for most people. Whether we realize or not, a lot of decisions we make are heavily influenced by the location of the houses. In this talk, I show how Python's data analysis and geospatial analysis packages can be used to analyze the whole gamut of available listings in a market, evaluate and score properties based on various attribute and spatial parameters and arrive at a shortlist. I extend by showing how this process can be used to build a machine learning model that will understand our preferences and continue to learn as more data is fed. I conclude with ideas for future work and how rest of the industry is progressing in this field.</p> <p>This talk was presented at a Python MeetUp. Find the link here. This talk was published as a blog on Medium and ArcUser magazine. </p>"},{"location":"talks/2018-portland-geodev-meetup/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>If you are interested in the technical details of this study, you can view the notebooks below:</p> <ul> <li>Cleaning data</li> <li>Exploratory data analysis</li> <li>Feature engineering - neighboring facilities</li> <li>Feature engineering - batch</li> <li>Ranking properties</li> <li>Building a recommendation engine</li> </ul>"},{"location":"talks/2018-portland-geodev-meetup/#slides","title":"Slides","text":""},{"location":"talks/2018-portland-geodev-meetup/#talk-screencast","title":"Talk screencast:","text":""},{"location":"talks/2019-dirmag-webinar/","title":"Explore How to Use ArcGIS and Jupyter for Geospatial Data Science","text":"<p>  Watch the talk here: https://www.directionsmag.com/webinar/9311</p>"},{"location":"talks/2019-esri-devsummit/","title":"2019 Esri Developer Summit talks","text":""},{"location":"talks/2019-esri-devsummit/#python-across-the-arcgis-platform","title":"Python across the ArcGIS Platform","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2019-esri-devsummit/#imagery-in-arcgis","title":"Imagery in ArcGIS","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2019-esri-devsummit/#spatial-data-science-with-arcgis","title":"Spatial Data Science with ArcGIS","text":"<p>Get the notebooks from here</p>"},{"location":"talks/2019-foss4gna/","title":"Free &amp; Open Source Software 4 Geospatial - North America (FOSS4G-NA), 2019","text":"<p>Visit the program: 2019 FOSS4GNA</p> <p>Lookup the talk: Let's take the machines house hunting</p>"},{"location":"talks/2020-esri-devsummit/","title":"2020 Esri Developer Summit talks","text":"<p>This was in March 2020, the beginning of the pandemic. All speakers had to return back home and cast their talks live from their home offices. We were all learning the ropes of remote work and remote conferences back then.</p>"},{"location":"talks/2020-esri-devsummit/#imagery-and-raster-analysis-on-your-web-gis","title":"Imagery and Raster Analysis on your Web GIS","text":""},{"location":"talks/2020-esri-devsummit/#spatial-data-science-with-arcgis","title":"Spatial Data Science with ArcGIS","text":""},{"location":"talks/2020-esri-devsummit/#arcgis-api-for-python-for-data-scientists-and-analysts","title":"ArcGIS API for Python for Data Scientists and Analysts","text":""},{"location":"talks/2020-esri-seminar/","title":"Python libraries for Spatial Data Science","text":""},{"location":"talks/2020-esri-seminar/#esri-live-training-seminar-feb-2020","title":"Esri, Live Training Seminar, Feb 2020","text":"<p>Register for the seminar here: https://www.esri.com/training/catalog/5e2750afea39935a53625340/python-libraries-for-spatial-data-science/</p>"},{"location":"talks/2020-spatial-ds-mooc/","title":"Spatial Data Science, The New Frontier in Analytics, Esri, 2020","text":"<p>I was lucky to design, review and teach a few chapters in this highly successful and popular MOOC from Esri.</p> <p></p> <p>Visit the course page: https://www.esri.com/training/catalog/5d76dcf7e9ccda09bef61294/</p>"},{"location":"talks/2021-esri-devsummit/","title":"2021 Esri Developer Summit","text":""},{"location":"talks/2021-esri-devsummit/#speaker-profile","title":"Speaker profile:","text":"<p>Atma Mani, Esri</p>"},{"location":"talks/2021-esri-devsummit/#arcgis-pro-your-spatial-data-science-workstation","title":"ArcGIS Pro: Your Spatial Data Science Workstation","text":"<p>If the video does not play, click here</p> <p>Slide deck for this talk can be found here</p>"},{"location":"talks/2021-esri-devsummit/#deploying-apps-and-services-with-arcgis-api-for-python","title":"Deploying Apps and Services with ArcGIS API for Python","text":"<p>Slide deck fo this talk can be found here</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/","title":"Clean housing data","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import pandas as pd import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[2]: Copied! <pre>csv1_path = 'resources/file1_2018-09-20-14-42-11_4k.csv'\ncsv2_path = 'resources/file2_2018-09-20-15-04-20_g.csv'\ncsv3_path = 'resources/file3_2018-09-20-16-02-01_b.csv'\n\nprop_df1 = pd.read_csv(csv1_path)\nprop_df2 = pd.read_csv(csv2_path)\nprop_df3 = pd.read_csv(csv3_path)\nprop_df1.head(3)\n</pre> csv1_path = 'resources/file1_2018-09-20-14-42-11_4k.csv' csv2_path = 'resources/file2_2018-09-20-15-04-20_g.csv' csv3_path = 'resources/file3_2018-09-20-16-02-01_b.csv'  prop_df1 = pd.read_csv(csv1_path) prop_df2 = pd.read_csv(csv2_path) prop_df3 = pd.read_csv(csv3_path) prop_df1.head(3) Out[2]: SALE TYPE SOLD DATE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS ... STATUS NEXT OPEN HOUSE START TIME NEXT OPEN HOUSE END TIME URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING) SOURCE MLS# FAVORITE INTERESTED LATITUDE LONGITUDE 0 MLS Listing NaN Single Family Residential 3445 NE Marine Dr Unit BH04 Portland OR 97211.0 27500.0 0.0 1.0 ... Active NaN NaN http://www.redfin.com/OR/Portland/3445-NE-Mari... RMLS 18567126 N Y 45.600583 -122.628508 1 MLS Listing NaN Vacant Land NW Thurman St Portland OR 97210.0 30000.0 NaN NaN ... Active NaN NaN http://www.redfin.com/OR/Portland/NW-Thurman-S... RMLS 18118897 N Y 45.537928 -122.718082 2 MLS Listing NaN Vacant Land NE Rocky Butte Rd Portland OR 97220.0 34777.0 NaN NaN ... Active NaN NaN http://www.redfin.com/OR/Portland/NE-Rocky-But... RMLS 18454531 N Y 45.542424 -122.565746 <p>3 rows \u00d7 27 columns</p> In\u00a0[3]: Copied! <pre>(prop_df1.shape, prop_df2.shape, prop_df3.shape)\n</pre> (prop_df1.shape, prop_df2.shape, prop_df3.shape) Out[3]: <pre>((3716, 27), (387, 27), (133, 27))</pre> In\u00a0[4]: Copied! <pre>prop_df = prop_df1.append(prop_df2)\nprop_df = prop_df.append(prop_df3)\nprop_df.shape\n</pre> prop_df = prop_df1.append(prop_df2) prop_df = prop_df.append(prop_df3) prop_df.shape Out[4]: <pre>(4236, 27)</pre> In\u00a0[5]: Copied! <pre>prop_dup_index = prop_df.duplicated(['MLS#'])\nprop_dup = prop_df[prop_dup_index]\nprop_dup.shape\n</pre> prop_dup_index = prop_df.duplicated(['MLS#']) prop_dup = prop_df[prop_dup_index] prop_dup.shape Out[5]: <pre>(209, 27)</pre> In\u00a0[6]: Copied! <pre>prop_df.drop_duplicates(subset=['MLS#'], inplace=True)\nprop_df.shape\n</pre> prop_df.drop_duplicates(subset=['MLS#'], inplace=True) prop_df.shape Out[6]: <pre>(4027, 27)</pre> In\u00a0[7]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[7]: <pre>Index(['SALE TYPE', 'SOLD DATE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH', 'STATUS',\n       'NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME',\n       'URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)',\n       'SOURCE', 'MLS#', 'FAVORITE', 'INTERESTED', 'LATITUDE', 'LONGITUDE'],\n      dtype='object')</pre> In\u00a0[8]: Copied! <pre>prop_df.rename(index=str, columns={'$/SQUARE FEET':'PRICE PER SQFT',\n                                  'HOA/MONTH':'HOA PER MONTH',\n                                  'URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)':'URL',\n                                  'MLS#':'MLS'}, inplace=True)\nprop_df.columns\n</pre> prop_df.rename(index=str, columns={'$/SQUARE FEET':'PRICE PER SQFT',                                   'HOA/MONTH':'HOA PER MONTH',                                   'URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)':'URL',                                   'MLS#':'MLS'}, inplace=True) prop_df.columns Out[8]: <pre>Index(['SALE TYPE', 'SOLD DATE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME',\n       'URL', 'SOURCE', 'MLS', 'FAVORITE', 'INTERESTED', 'LATITUDE',\n       'LONGITUDE'],\n      dtype='object')</pre> In\u00a0[9]: Copied! <pre>prop_df.drop(columns=['SOLD DATE','NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME',\n                     'FAVORITE', 'INTERESTED'], inplace=True)\n</pre> prop_df.drop(columns=['SOLD DATE','NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME',                      'FAVORITE', 'INTERESTED'], inplace=True) In\u00a0[10]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[10]: <pre>Index(['SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE', 'ZIP',\n       'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE'],\n      dtype='object')</pre> In\u00a0[13]: Copied! <pre># explore distribution of numeric columns\nax_list = prop_df.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> # explore distribution of numeric columns ax_list = prop_df.hist(bins=25, layout=(4,4), figsize=(15,15)) In\u00a0[11]: Copied! <pre># drop rows with missing values in critical columns\nprop_df_nona = prop_df.dropna(axis=0, how='any', # if any of these cols are empty, remove row\n                              subset=['BEDS','BATHS', 'PRICE', 'YEAR BUILT', 'LATITUDE','LONGITUDE'])\nprop_df_nona.shape\n</pre> # drop rows with missing values in critical columns prop_df_nona = prop_df.dropna(axis=0, how='any', # if any of these cols are empty, remove row                               subset=['BEDS','BATHS', 'PRICE', 'YEAR BUILT', 'LATITUDE','LONGITUDE']) prop_df_nona.shape Out[11]: <pre>(3653, 22)</pre> <p>Let us impute for missing values using different strategies for different columns.</p> In\u00a0[23]: Copied! <pre>prop_df_nona['HOA PER MONTH'].fillna(value=0, inplace=True)\nprop_df_nona['LOT SIZE'].fillna(value=prop_df_nona['LOT SIZE'].median(), inplace=True)\nprop_df_nona['PRICE PER SQFT'].fillna(value=prop_df_nona['PRICE PER SQFT'].median(), inplace=True)\nprop_df_nona['SQUARE FEET'].fillna(value=prop_df_nona['SQUARE FEET'].median(), inplace=True)\nprop_df_nona['YEAR BUILT'].fillna(value=prop_df_nona['YEAR BUILT'].mode(), inplace=True)\nprop_df_nona['ZIP'].fillna(value=prop_df_nona['SQUARE FEET'].mode(), inplace=True)\n</pre> prop_df_nona['HOA PER MONTH'].fillna(value=0, inplace=True) prop_df_nona['LOT SIZE'].fillna(value=prop_df_nona['LOT SIZE'].median(), inplace=True) prop_df_nona['PRICE PER SQFT'].fillna(value=prop_df_nona['PRICE PER SQFT'].median(), inplace=True) prop_df_nona['SQUARE FEET'].fillna(value=prop_df_nona['SQUARE FEET'].median(), inplace=True) prop_df_nona['YEAR BUILT'].fillna(value=prop_df_nona['YEAR BUILT'].mode(), inplace=True) prop_df_nona['ZIP'].fillna(value=prop_df_nona['SQUARE FEET'].mode(), inplace=True) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/pandas/core/generic.py:5434: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._update_inplace(new_data)\n</pre> In\u00a0[25]: Copied! <pre># explore distribution of numeric columns\nax_list = prop_df_nona.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> # explore distribution of numeric columns ax_list = prop_df_nona.hist(bins=25, layout=(4,4), figsize=(15,15)) <p>As can be seen from histogram, some numeric columns are heavily sqewed by outliers. Let us pull up statistics for each of these columns</p> In\u00a0[26]: Copied! <pre>prop_df_nona.describe().round(3)\n</pre> prop_df_nona.describe().round(3) Out[26]: ZIP PRICE BEDS BATHS SQUARE FEET LOT SIZE YEAR BUILT DAYS ON MARKET PRICE PER SQFT HOA PER MONTH LATITUDE LONGITUDE count 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 3653.000 mean 97210.091 609851.635 3.316 2.428 2291.079 13448.155 1973.970 62.268 281.446 105.865 45.513 -122.648 std 93.873 458104.003 1.762 1.340 1342.049 72390.700 36.279 72.488 118.235 223.111 0.046 0.107 min 97002.000 35000.000 0.000 0.500 212.000 25.000 1878.000 1.000 44.000 0.000 45.382 -122.902 25% 97206.000 359900.000 2.000 2.000 1400.000 5227.000 1948.000 15.000 206.000 0.000 45.483 -122.712 50% 97217.000 495000.000 3.000 2.500 2062.000 6969.000 1980.000 42.000 251.000 0.000 45.516 -122.661 75% 97229.000 699934.000 4.000 3.000 2862.000 8712.000 2006.000 84.000 321.000 74.000 45.541 -122.584 max 98664.000 8650000.000 43.000 21.000 14500.000 3167247.000 2019.000 1080.000 1505.000 2091.000 45.703 -122.306 In\u00a0[27]: Copied! <pre>def six_sigma_filter(df, column):\n    sigma = df[column].std()\n    mu = df[column].mean()\n    three_sigma = [mu-(3*sigma), mu+(3*sigma)]\n    print(\"Column:{}, Mean:{}, Sigma:{}, 3sigma_range: {}:{}\".format(column,mu.round(3),\n                                                                       sigma.round(3),\n                                                                       three_sigma[0].round(2),\n                                                                       three_sigma[1].round(2)))\n    \n    # filter\n    df_to_keep = df[(df[column] &gt; three_sigma[0]) &amp; (df[column] &lt; three_sigma[1])]\n    \n    # prints\n    num_rows_dropped = prop_df.shape[0] - df_to_keep.shape[0]\n    print(\"Number of rows dropped: \" + str(num_rows_dropped))\n    \n    return df_to_keep\n</pre> def six_sigma_filter(df, column):     sigma = df[column].std()     mu = df[column].mean()     three_sigma = [mu-(3*sigma), mu+(3*sigma)]     print(\"Column:{}, Mean:{}, Sigma:{}, 3sigma_range: {}:{}\".format(column,mu.round(3),                                                                        sigma.round(3),                                                                        three_sigma[0].round(2),                                                                        three_sigma[1].round(2)))          # filter     df_to_keep = df[(df[column] &gt; three_sigma[0]) &amp; (df[column] &lt; three_sigma[1])]          # prints     num_rows_dropped = prop_df.shape[0] - df_to_keep.shape[0]     print(\"Number of rows dropped: \" + str(num_rows_dropped))          return df_to_keep In\u00a0[28]: Copied! <pre>def iqr_filter(df, column):\n    med = df[column].median()\n    p_25 = df[column].quantile(q=0.25)\n    p_75 = df[column].quantile(q=0.75)\n    \n    # find valid range\n    iqr_range = [med-(2*p_25), med+(2*p_75)]\n    print(\"Column: {}, Median:{}, 25%:{}, 75%:{}, IQR:{}:{}\".format(column,med,\n                                                                    p_25,p_75,\n                                                                    iqr_range[0].round(2),\n                                                                    iqr_range[1].round(2)))\n    \n    # filter\n    df_to_keep = df[(df[column] &gt; iqr_range[0]) &amp; (df[column] &lt; iqr_range[1])]\n    \n    #prints\n    num_rows_dropped = prop_df.shape[0] - df_to_keep.shape[0]\n    print(\"Number of rows dropped: \" + str(num_rows_dropped))\n    \n    return df_to_keep\n</pre> def iqr_filter(df, column):     med = df[column].median()     p_25 = df[column].quantile(q=0.25)     p_75 = df[column].quantile(q=0.75)          # find valid range     iqr_range = [med-(2*p_25), med+(2*p_75)]     print(\"Column: {}, Median:{}, 25%:{}, 75%:{}, IQR:{}:{}\".format(column,med,                                                                     p_25,p_75,                                                                     iqr_range[0].round(2),                                                                     iqr_range[1].round(2)))          # filter     df_to_keep = df[(df[column] &gt; iqr_range[0]) &amp; (df[column] &lt; iqr_range[1])]          #prints     num_rows_dropped = prop_df.shape[0] - df_to_keep.shape[0]     print(\"Number of rows dropped: \" + str(num_rows_dropped))          return df_to_keep In\u00a0[37]: Copied! <pre>prop_df2 = six_sigma_filter(prop_df_nona, 'BATHS')\n</pre> prop_df2 = six_sigma_filter(prop_df_nona, 'BATHS') <pre>Column:BATHS, Mean:2.428, Sigma:1.34, 3sigma_range: -1.59:6.45\nNumber of rows dropped: 422\n</pre> In\u00a0[38]: Copied! <pre>prop_df2_iqr = iqr_filter(prop_df_nona, 'BATHS')\n</pre> prop_df2_iqr = iqr_filter(prop_df_nona, 'BATHS') <pre>Column: BATHS, Median:2.5, 25%:2.0, 75%:3.0, IQR:-1.5:8.5\nNumber of rows dropped: 396\n</pre> In\u00a0[39]: Copied! <pre>prop_df4 = six_sigma_filter(prop_df2, 'BEDS')\n</pre> prop_df4 = six_sigma_filter(prop_df2, 'BEDS') <pre>Column:BEDS, Mean:3.206, Sigma:1.263, 3sigma_range: -0.58:7.0\nNumber of rows dropped: 459\n</pre> In\u00a0[40]: Copied! <pre>prop_df4_iqr = iqr_filter(prop_df2_iqr, 'BEDS')\n</pre> prop_df4_iqr = iqr_filter(prop_df2_iqr, 'BEDS') <pre>Column: BEDS, Median:3.0, 25%:2.0, 75%:4.0, IQR:-1.0:11.0\nNumber of rows dropped: 403\n</pre> In\u00a0[42]: Copied! <pre>(prop_df4.shape, prop_df4_iqr.shape)\n</pre> (prop_df4.shape, prop_df4_iqr.shape) Out[42]: <pre>((3539, 22), (3624, 22))</pre> In\u00a0[43]: Copied! <pre>ax_list = prop_df4.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> ax_list = prop_df4.hist(bins=25, layout=(4,4), figsize=(15,15)) In\u00a0[44]: Copied! <pre>ax_list = prop_df4_iqr.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> ax_list = prop_df4_iqr.hist(bins=25, layout=(4,4), figsize=(15,15)) <p>The <code>IQR</code> filter yields a better result in our case as the resulting histograms of numeric columns show a nice normal distribution. We proceed with this dataset and write that to disk.</p> In\u00a0[45]: Copied! <pre>prop_df4_iqr.to_csv('resources/houses_for_sale_filtered.csv')\n</pre> prop_df4_iqr.to_csv('resources/houses_for_sale_filtered.csv')"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#clean-housing-data","title":"Clean housing data\u00b6","text":"<p>In this notebook, we download housing data from a popular MLS website, merge, clean, filter and prepare it for subsequent spatial and statistical analysis.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#read-data-from-all-csv-files","title":"Read data from all CSV files\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#merge-all-csv-into-a-single-sheet","title":"Merge all CSV into a single sheet\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#find-if-there-are-duplicates","title":"Find if there are duplicates\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#clean-column-names","title":"Clean column names\u00b6","text":"<p>Column names contain illegal characters. Let us rename them</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#drop-unnecessary-columns","title":"Drop unnecessary columns\u00b6","text":"<p>While it is beneficial to have a lot of features (columns) in our dataset, it is also important to drop those that are highly correlated to an existing field or a derivative of one or fields that we will never use. In the following, we drop fields that we will never use in this analysis.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#find-and-fill-missing-values","title":"Find and fill missing values\u00b6","text":"<p>Missing values in certain columns are critical - <code>beds</code>, <code>bath</code>, <code>price</code>, <code>latitude</code>, <code>longitude</code>. We will drop these rows. Missing values in remaining columns are not so critical, we will fill them with <code>0</code> or average values.</p> <p>Before we fill, let us explore the histograms of numerical columns. With pandas, this can be accomplished with a single command as shown below:</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#explore-distribution-of-numeric-columns","title":"Explore distribution of numeric columns\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#build-statistical-filters-to-remove-outliers","title":"Build statistical filters to remove outliers\u00b6","text":"<p>In this segment, we build $6\\sigma$ and Inter Quartile Range (<code>IQR</code>) filters to remove outliers from our dataset.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#6-sigma-filter","title":"6 sigma filter\u00b6","text":"<p> Statistically, about <code>99.5%</code> of data falls within $\\pm 3 \\sigma$ from $\\mu$ (mean). Thus, for fairly normally distributed data, $6\\sigma$ filter yields a good resultant dataset.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#iqr-filter","title":"IQR filter\u00b6","text":"<p> Howerver, certain columns such as <code>BEDS</code>, <code>LOT SIZE</code> are heavily sqewed. For such, the Inter Quartile Range filter (which uses median as a measure of centrality unlike 6sigma which uses mean) yields a robust filter as it is unaffected by outliers like the 6 sigma filter.</p>"},{"location":"talks/portland-house-hunting/01_clean-housing-data/#filter-columns-using-both-filters-and-compare","title":"Filter columns using both filters and compare\u00b6","text":""},{"location":"talks/portland-house-hunting/01_clean-housing-data/#write-cleaned-appended-table-to-disk","title":"Write cleaned, appended table to disk\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/","title":"Portland housing data - exploratory data analysis","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom arcgis.gis import GIS\nfrom arcgis.features import GeoAccessor, GeoSeriesAccessor\ngis = GIS()\n</pre> import pandas as pd import matplotlib.pyplot as plt %matplotlib inline  from arcgis.gis import GIS from arcgis.features import GeoAccessor, GeoSeriesAccessor gis = GIS() In\u00a0[2]: Copied! <pre>csv_path = 'resources/houses_for_sale_filtered.csv'\nprop_df = pd.read_csv(csv_path)\nprop_df.head(3)\n</pre> csv_path = 'resources/houses_for_sale_filtered.csv' prop_df = pd.read_csv(csv_path) prop_df.head(3) Out[2]: Unnamed: 0 SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS ... YEAR BUILT DAYS ON MARKET PRICE PER SQFT HOA PER MONTH STATUS URL SOURCE MLS LATITUDE LONGITUDE 0 3 MLS Listing Mobile/Manufactured Home 6112 SE Clatsop St Portland OR 97206.0 35000.0 2.0 1.0 ... 1981.0 93.0 44.0 0.0 Active http://www.redfin.com/OR/Portland/6112-SE-Clat... RMLS 18331169 45.461282 -122.600153 1 8 For-Sale-by-Owner Listing Single Family Residential 6901 SE Oaks Park Way Slip 32 Portland OR 97202.0 60000.0 1.0 1.0 ... 1939.0 359.0 55.0 0.0 Active http://www.redfin.com/OR/Portland/6901-SE-Oaks... Fizber.com 4933081 45.474369 -122.662307 2 14 For-Sale-by-Owner Listing Mobile/Manufactured Home 11187 SW Royal Villa Dr Portland OR 97224.0 69950.0 2.0 2.0 ... 1976.0 44.0 47.0 0.0 Active http://www.redfin.com/OR/Portland/11187-SW-Roy... Fizber.com 4974567 45.398486 -122.796175 <p>3 rows \u00d7 23 columns</p> In\u00a0[7]: Copied! <pre>prop_df.shape\n</pre> prop_df.shape Out[7]: <pre>(3624, 23)</pre> In\u00a0[8]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[8]: <pre>Index(['Unnamed: 0', 'SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE'],\n      dtype='object')</pre> <p>Drop redundant columns</p> In\u00a0[3]: Copied! <pre>try:\n    prop_df.drop(columns=['Unnamed: 0'], inplace=True)\n    prop_df.drop(columns=['Unnamed: 0.1'], inplace=True)\nexcept:\n    pass\n</pre> try:     prop_df.drop(columns=['Unnamed: 0'], inplace=True)     prop_df.drop(columns=['Unnamed: 0.1'], inplace=True) except:     pass In\u00a0[4]: Copied! <pre>prop_sdf = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE')\ntype(prop_sdf)\n</pre> prop_sdf = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE') type(prop_sdf) Out[4]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_map = gis.map('Portland, OR')\npdx_map.basemap = 'streets'\npdx_map\n</pre> pdx_map = gis.map('Portland, OR') pdx_map.basemap = 'streets' pdx_map <p></p> In\u00a0[6]: Copied! <pre>prop_sdf.spatial.plot(map_widget=pdx_map)\n</pre> prop_sdf.spatial.plot(map_widget=pdx_map) Out[6]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_density_map = gis.map('Portland, OR')\npdx_density_map.basemap='gray'\npdx_density_map\n</pre> pdx_density_map = gis.map('Portland, OR') pdx_density_map.basemap='gray' pdx_density_map <p></p> In\u00a0[10]: Copied! <pre>prop_sdf.spatial.plot(map_widget=pdx_density_map, renderer_type='h')\n</pre> prop_sdf.spatial.plot(map_widget=pdx_density_map, renderer_type='h') Out[10]: <pre>True</pre> <p>There is a hotspot near downtown Portland. There are few other isolated hotspots around the downtown as well. These could be pockets of communities that are spread around a major city.</p> In\u00a0[\u00a0]: Copied! <pre>pdx_age_map = gis.map(\"Portland, OR\")\npdx_age_map.basemap = 'gray-vector'\npdx_age_map\n</pre> pdx_age_map = gis.map(\"Portland, OR\") pdx_age_map.basemap = 'gray-vector' pdx_age_map <p></p> In\u00a0[12]: Copied! <pre>prop_sdf.spatial.plot(map_widget = pdx_age_map, \n                      renderer_type='c', # for classs breaks renderer\n                     method='esriClassifyNaturalBreaks',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='YEAR BUILT',\n                     cmap='Blues',  # matplotlib color map\n                     alpha=0.7,\n                     outline_color=[0,0,0,0])\n</pre> prop_sdf.spatial.plot(map_widget = pdx_age_map,                        renderer_type='c', # for classs breaks renderer                      method='esriClassifyNaturalBreaks',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='YEAR BUILT',                      cmap='Blues',  # matplotlib color map                      alpha=0.7,                      outline_color=[0,0,0,0]) Out[12]: <pre>True</pre> <p>If you notice the syntax above, it mirrors the general syntax of plotting a DataFrame as a chart. It uses the same color map and symbols of <code>matplotlib</code>. Internally, the ArcGIS API for Python converts this syntax to ArcGIS symbology and renders it on a map.</p> <p>Since the colormap and symbols is that of <code>matplotlib</code>, you can collect the class breaks from the map above and use that to plot the same data as a bar chart as shown below. Combining the map and chart gives you a powerful way to interactively explore the spatial and statistical distribution of your dataset.</p> In\u00a0[13]: Copied! <pre>age_class_breaks = pdx_age_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos\n# print(len(age_class_breaks))\ncbs_list = []\ncmap_list = []\nfor cb in age_class_breaks:\n#     print(cb.description)  # print the class break labels\n    cbs_list.append(cb.classMaxValue)\n    cmap_list.append([x/255.0 for x in cb.symbol.color])\n    \n# build a histogram for the same class breaks\nn, bins, patches = plt.hist(prop_sdf['YEAR BUILT'], bins=cbs_list)\n\n# apply the same color for each class to match the map\nidx = 0\nfor c, p in zip(bins, patches):\n    plt.setp(p, 'facecolor', cmap_list[idx])\n    idx+=1\n\nplt.title('Histogram of YEAR BUILT column')\n</pre> age_class_breaks = pdx_age_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos # print(len(age_class_breaks)) cbs_list = [] cmap_list = [] for cb in age_class_breaks: #     print(cb.description)  # print the class break labels     cbs_list.append(cb.classMaxValue)     cmap_list.append([x/255.0 for x in cb.symbol.color])      # build a histogram for the same class breaks n, bins, patches = plt.hist(prop_sdf['YEAR BUILT'], bins=cbs_list)  # apply the same color for each class to match the map idx = 0 for c, p in zip(bins, patches):     plt.setp(p, 'facecolor', cmap_list[idx])     idx+=1  plt.title('Histogram of YEAR BUILT column') Out[13]: <pre>Text(0.5,1,'Histogram of YEAR BUILT column')</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_price_map = gis.map(\"Portland, OR\")\npdx_price_map.basemap = 'gray-vector'\npdx_price_map\n</pre> pdx_price_map = gis.map(\"Portland, OR\") pdx_price_map.basemap = 'gray-vector' pdx_price_map <p></p> In\u00a0[18]: Copied! <pre>prop_sdf.spatial.plot(map_widget = pdx_price_map, \n                      renderer_type='c', # for classs breaks renderer\n                     method='esriClassifyQuantile',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='PRICE',\n                     cmap='BuPu_r',  # matplotlib color map\n                     alpha=0.5,\n                     outline_color=[50,0,0,50], line_width=1)\n</pre> prop_sdf.spatial.plot(map_widget = pdx_price_map,                        renderer_type='c', # for classs breaks renderer                      method='esriClassifyQuantile',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='PRICE',                      cmap='BuPu_r',  # matplotlib color map                      alpha=0.5,                      outline_color=[50,0,0,50], line_width=1) Out[18]: <pre>True</pre> In\u00a0[19]: Copied! <pre>price_class_breaks = pdx_price_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos\n# print(len(age_class_breaks))\ncbs_list = []\ncmap_list = []\nfor cb in price_class_breaks:\n#     print(cb.description)  # print the class break labels\n    cbs_list.append(cb.classMaxValue)\n    cmap_list.append([x/255.0 for x in cb.symbol.color])\n    \n# build a histogram for the same class breaks\nn, bins, patches = plt.hist(prop_sdf['PRICE'], bins=cbs_list)\n\n# apply the same color for each class to match the map\nidx = 0\nfor c, p in zip(bins, patches):\n    plt.setp(p, 'facecolor', cmap_list[idx])\n    idx+=1\n\nplt.title('Histogram of PRICE column')\n</pre> price_class_breaks = pdx_price_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos # print(len(age_class_breaks)) cbs_list = [] cmap_list = [] for cb in price_class_breaks: #     print(cb.description)  # print the class break labels     cbs_list.append(cb.classMaxValue)     cmap_list.append([x/255.0 for x in cb.symbol.color])      # build a histogram for the same class breaks n, bins, patches = plt.hist(prop_sdf['PRICE'], bins=cbs_list)  # apply the same color for each class to match the map idx = 0 for c, p in zip(bins, patches):     plt.setp(p, 'facecolor', cmap_list[idx])     idx+=1  plt.title('Histogram of PRICE column') Out[19]: <pre>Text(0.5,1,'Histogram of PRICE column')</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_size_map = gis.map(\"Portland, OR\")\npdx_size_map.basemap = 'gray-vector'\npdx_size_map\n</pre> pdx_size_map = gis.map(\"Portland, OR\") pdx_size_map.basemap = 'gray-vector' pdx_size_map <p></p> In\u00a0[21]: Copied! <pre>prop_sdf.spatial.plot(map_widget = pdx_size_map, \n                      renderer_type='c', # for classs breaks renderer\n                     method='esriClassifyNaturalBreaks',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='SQUARE FEET',\n                     cmap='RdBu',  # matplotlib color map\n                     alpha=0.7,\n                     outline_color=[50,0,0,50], line_width=1)\n</pre> prop_sdf.spatial.plot(map_widget = pdx_size_map,                        renderer_type='c', # for classs breaks renderer                      method='esriClassifyNaturalBreaks',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='SQUARE FEET',                      cmap='RdBu',  # matplotlib color map                      alpha=0.7,                      outline_color=[50,0,0,50], line_width=1) Out[21]: <pre>True</pre> In\u00a0[22]: Copied! <pre>size_class_breaks = pdx_size_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos\n# print(len(age_class_breaks))\ncbs_list = []\ncmap_list = []\nfor cb in size_class_breaks:\n#     print(cb.description)  # print the class break labels\n    cbs_list.append(cb.classMaxValue)\n    cmap_list.append([x/255.0 for x in cb.symbol.color])\n    \n# build a histogram for the same class breaks\nn, bins, patches = plt.hist(prop_sdf['SQUARE FEET'], bins=cbs_list)\n\n# apply the same color for each class to match the map\nidx = 0\nfor c, p in zip(bins, patches):\n    plt.setp(p, 'facecolor', cmap_list[idx])\n    idx+=1\n\nplt.title('Histogram of SQUARE FEET column')\n</pre> size_class_breaks = pdx_size_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos # print(len(age_class_breaks)) cbs_list = [] cmap_list = [] for cb in size_class_breaks: #     print(cb.description)  # print the class break labels     cbs_list.append(cb.classMaxValue)     cmap_list.append([x/255.0 for x in cb.symbol.color])      # build a histogram for the same class breaks n, bins, patches = plt.hist(prop_sdf['SQUARE FEET'], bins=cbs_list)  # apply the same color for each class to match the map idx = 0 for c, p in zip(bins, patches):     plt.setp(p, 'facecolor', cmap_list[idx])     idx+=1  plt.title('Histogram of SQUARE FEET column') Out[22]: <pre>Text(0.5,1,'Histogram of SQUARE FEET column')</pre> In\u00a0[\u00a0]: Copied! <pre>pdx_hoa_map = gis.map(\"Portland, OR\")\npdx_hoa_map.basemap = 'gray-vector'\npdx_hoa_map\n</pre> pdx_hoa_map = gis.map(\"Portland, OR\") pdx_hoa_map.basemap = 'gray-vector' pdx_hoa_map <p></p> In\u00a0[42]: Copied! <pre>#plot properties without HOA as hollow\n# prop_sdf_hoa_f = prop_df[prop_df['HOA PER MONTH']==0]\n# prop_sdf_hoa_f.spatial.plot(map_widget=pdx_hoa_map, symbol_type='simple',\n#                             symbol_style='+',outline_color='Blues',\n#                             marker_size=7)\n\nprop_sdf_hoa_2 = prop_df[prop_df['HOA PER MONTH']&gt;0]\n\nprop_sdf_hoa_2.spatial.plot(map_widget = pdx_hoa_map, \n                      renderer_type='c', # for classs breaks renderer\n                     method='esriClassifyQuantile',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='HOA PER MONTH',\n                     cmap='RdBu',  # matplotlib color map\n                     alpha=0.7,\n                     outline_color=[0,0,0,0], line_width=0)\n</pre> #plot properties without HOA as hollow # prop_sdf_hoa_f = prop_df[prop_df['HOA PER MONTH']==0] # prop_sdf_hoa_f.spatial.plot(map_widget=pdx_hoa_map, symbol_type='simple', #                             symbol_style='+',outline_color='Blues', #                             marker_size=7)  prop_sdf_hoa_2 = prop_df[prop_df['HOA PER MONTH']&gt;0]  prop_sdf_hoa_2.spatial.plot(map_widget = pdx_hoa_map,                        renderer_type='c', # for classs breaks renderer                      method='esriClassifyQuantile',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='HOA PER MONTH',                      cmap='RdBu',  # matplotlib color map                      alpha=0.7,                      outline_color=[0,0,0,0], line_width=0) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/arcgis/features/geo/_accessor.py:861: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._data[col] = self._data[col]\n/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/arcgis/features/geo/_accessor.py:1968: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._data['OBJECTID'] = list(range(1, self._data.shape[0] + 1))\n</pre> Out[42]: <pre>True</pre> In\u00a0[43]: Copied! <pre>hoa_class_breaks = pdx_hoa_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos\n# print(len(age_class_breaks))\ncbs_list = []\ncmap_list = []\nfor cb in hoa_class_breaks:\n#     print(cb.description)  # print the class break labels\n    cbs_list.append(cb.classMaxValue)\n    cmap_list.append([x/255.0 for x in cb.symbol.color])\n    \n# build a histogram for the same class breaks\nn, bins, patches = plt.hist(prop_sdf['HOA PER MONTH'], bins=cbs_list)\n\n# apply the same color for each class to match the map\nidx = 0\nfor c, p in zip(bins, patches):\n    plt.setp(p, 'facecolor', cmap_list[idx])\n    idx+=1\n\nplt.title('Histogram of HOA PER MONTH column')\n</pre> hoa_class_breaks = pdx_hoa_map.layers[0].layer.layerDefinition.drawingInfo.renderer.classBreakInfos # print(len(age_class_breaks)) cbs_list = [] cmap_list = [] for cb in hoa_class_breaks: #     print(cb.description)  # print the class break labels     cbs_list.append(cb.classMaxValue)     cmap_list.append([x/255.0 for x in cb.symbol.color])      # build a histogram for the same class breaks n, bins, patches = plt.hist(prop_sdf['HOA PER MONTH'], bins=cbs_list)  # apply the same color for each class to match the map idx = 0 for c, p in zip(bins, patches):     plt.setp(p, 'facecolor', cmap_list[idx])     idx+=1  plt.title('Histogram of HOA PER MONTH column') Out[43]: <pre>Text(0.5,1,'Histogram of HOA PER MONTH column')</pre> <p></p> In\u00a0[46]: Copied! <pre>ax_list = prop_df.hist(bins=25, layout=(4,4), figsize=(15,12))\n</pre> ax_list = prop_df.hist(bins=25, layout=(4,4), figsize=(15,12)) <p>Explore the frequency of categorical columns</p> In\u00a0[47]: Copied! <pre>fig2, ax2 = plt.subplots(1,2, figsize=(10,5))\n\nprop_df['CITY'].value_counts().plot(kind='bar', ax=ax2[0], \n                                             title='City name frequency')\nax2[0].tick_params(labelrotation=45)\n\nprop_df['PROPERTY TYPE'].value_counts().plot(kind='bar', ax=ax2[1], \n                                             title='Property type frequency')\nax2[1].tick_params(labelrotation=45)\nplt.tight_layout()\n</pre> fig2, ax2 = plt.subplots(1,2, figsize=(10,5))  prop_df['CITY'].value_counts().plot(kind='bar', ax=ax2[0],                                               title='City name frequency') ax2[0].tick_params(labelrotation=45)  prop_df['PROPERTY TYPE'].value_counts().plot(kind='bar', ax=ax2[1],                                               title='Property type frequency') ax2[1].tick_params(labelrotation=45) plt.tight_layout() In\u00a0[48]: Copied! <pre>filtered_df = prop_sdf[(prop_df['BEDS']&gt;=2) &amp; \n                       (prop_df['BATHS']&gt;1)&amp; \n                       (prop_df['HOA PER MONTH']&lt;=200) &amp; \n                       (prop_df['YEAR BUILT']&gt;=2000) &amp; \n                       (prop_df['SQUARE FEET'] &gt; 2000) &amp; \n                       (prop_df['PRICE']&lt;=700000)]\nfiltered_df.shape\n</pre> filtered_df = prop_sdf[(prop_df['BEDS']&gt;=2) &amp;                         (prop_df['BATHS']&gt;1)&amp;                         (prop_df['HOA PER MONTH']&lt;=200) &amp;                         (prop_df['YEAR BUILT']&gt;=2000) &amp;                         (prop_df['SQUARE FEET'] &gt; 2000) &amp;                         (prop_df['PRICE']&lt;=700000)] filtered_df.shape Out[48]: <pre>(331, 23)</pre> In\u00a0[49]: Copied! <pre>(prop_sdf.shape, filtered_df.shape)\n</pre> (prop_sdf.shape, filtered_df.shape) Out[49]: <pre>((3624, 23), (331, 23))</pre> <p>From <code>3624</code> houses, we shortlisted <code>331</code> of them. Below, let us visualize the statistical distribution of this shortlist.</p> In\u00a0[50]: Copied! <pre>ax_list2 = filtered_df.hist(bins=25, layout=(4,4), figsize=(15,15))\n</pre> ax_list2 = filtered_df.hist(bins=25, layout=(4,4), figsize=(15,15)) <p>From the histograms above, we notice most of the houses have <code>4</code> beds, while we requested for at least <code>2</code>. Majority of them are newly built and skewed toward upper end of the price spectrum.</p> In\u00a0[\u00a0]: Copied! <pre>pdx_filtered_map = gis.map(\"Portland, OR\")\npdx_filtered_map.basemap = 'gray-vector'\npdx_filtered_map\n</pre> pdx_filtered_map = gis.map(\"Portland, OR\") pdx_filtered_map.basemap = 'gray-vector' pdx_filtered_map <p></p> In\u00a0[\u00a0]: Copied! <pre>filtered_df.spatial.plot(map_widget=pdx_filtered_map, \n                         renderer_type='c',\n                         method='esriClassifyNaturalBreaks',  # classification scheme\n                         class_count=10,\n                         col='PRICE',\n                         cmap='Blues',  # matplotlib color map\n                        alpha=0.7,outline_color=[0,0,0,0])\n</pre> filtered_df.spatial.plot(map_widget=pdx_filtered_map,                           renderer_type='c',                          method='esriClassifyNaturalBreaks',  # classification scheme                          class_count=10,                          col='PRICE',                          cmap='Blues',  # matplotlib color map                         alpha=0.7,outline_color=[0,0,0,0]) <p>The houses in the shortlist are well spread across the Portland market. We notice spatial clustering in the distribution of property prices. Higher priced houses are mostly to the west of downtown while moderately and lower priced are spread across east and south.</p> In\u00a0[53]: Copied! <pre>filtered_df.to_csv('resources/houses_for_sale_att_filtered.csv')\n</pre> filtered_df.to_csv('resources/houses_for_sale_att_filtered.csv')"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#portland-housing-data-exploratory-data-analysis","title":"Portland housing data - exploratory data analysis\u00b6","text":"<p>In this second notebook, we use the cleaned and filtered data produced from notebook 1. Here we explore the spatial distribution of our dataset.</p> <p>So far, we worked with pandas <code>DataFrame</code> objects, to map the properties, we import the ArcGIS API for Python. This adds a <code>GeoAccessor</code> and spatially enables your <code>DataFrame</code> objects. As you will see in this notebook, you can then easily plot your data both as charts and as maps.</p>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#read-data","title":"Read data\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatially","title":"Visualize spatially\u00b6","text":"<p>Convert to Spatially Enabled DataFrame</p>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#explore-density-of-properties-for-sale","title":"Explore density of properties for sale\u00b6","text":"<p>You can render the same data using a heatmap renderer, which visualizes the spatial density of the properties (houses) using a heatmap.</p>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-geographic-distributions-of-housing-properties","title":"Visualize geographic distributions of housing properties\u00b6","text":"<p>Using different renderers such as 'class colored renderer', you can visualize and investigate if there is any spatial phenomena observable for the following columns.</p> <ul> <li>How is property age spatially distributed?</li> <li>Does property price drop outward from city center?</li> <li>Does property size increase in suburbs?</li> <li>hoa vs no hoa</li> </ul>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatial-distribution-by-property-age","title":"Visualize spatial distribution by property age\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatial-distribution-by-price","title":"Visualize spatial distribution by price\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatial-distribution-of-property-size","title":"Visualize spatial distribution of property size\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#spatial-distribution-of-hoa","title":"Spatial distribution of HoA\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#explore-distribution-of-numeric-columns","title":"Explore distribution of numeric columns:\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#filter-based-on-your-criteria","title":"Filter based on your criteria\u00b6","text":"<p>Let us define a few rules based on the intrinsic properties of these houses and shortlist them.</p>"},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-statistical-distribution-of-shortlisted-properties","title":"Visualize statistical distribution of shortlisted properties\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#visualize-spatial-distribution-of-shortlisted-properties","title":"Visualize spatial distribution of shortlisted properties\u00b6","text":""},{"location":"talks/portland-house-hunting/02_housing-exploratory-data-analysis/#write-the-shortlisted-properties-to-disk","title":"Write the shortlisted properties to disk\u00b6","text":"<p>So far, we used attribute queries to explore and filter out properties. We have not yet used GIS analysis to narrow them further. Before that, let us save our work to disk.</p>"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/","title":"Feature engineering - quantifying access to facilities","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\n\nfrom arcgis.gis import GIS\nfrom arcgis.geocoding import geocode\nfrom arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor\nfrom arcgis.features import SpatialDataFrame\nfrom arcgis.geometry import Geometry, Point\nfrom arcgis.geometry.functions import buffer\nfrom arcgis.network import RouteLayer\n</pre> import pandas as pd import matplotlib.pyplot as plt from pprint import pprint %matplotlib inline  from arcgis.gis import GIS from arcgis.geocoding import geocode from arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor from arcgis.features import SpatialDataFrame from arcgis.geometry import Geometry, Point from arcgis.geometry.functions import buffer from arcgis.network import RouteLayer <p>Connect to GIS</p> In\u00a0[2]: Copied! <pre>gis = GIS(profile='')\n</pre> gis = GIS(profile='') In\u00a0[3]: Copied! <pre>prop_list_df = pd.read_csv('resources/houses_for_sale_att_filtered.csv')\nprop_list_df.shape\n</pre> prop_list_df = pd.read_csv('resources/houses_for_sale_att_filtered.csv') prop_list_df.shape Out[3]: <pre>(331, 24)</pre> In\u00a0[4]: Copied! <pre>prop_list_df = pd.DataFrame.spatial.from_xy(prop_list_df, 'LONGITUDE','LATITUDE')\ntype(prop_list_df)\n</pre> prop_list_df = pd.DataFrame.spatial.from_xy(prop_list_df, 'LONGITUDE','LATITUDE') type(prop_list_df) Out[4]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[5]: Copied! <pre>prop1 = prop_list_df[prop_list_df['MLS']==18389440]\nprop1\n</pre> prop1 = prop_list_df[prop_list_df['MLS']==18389440] prop1 Out[5]: Unnamed: 0 SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS ... DAYS ON MARKET PRICE PER SQFT HOA PER MONTH STATUS URL SOURCE MLS LATITUDE LONGITUDE SHAPE 203 2328 MLS Listing Single Family Residential 3775 NW Hilton Head Ter Portland OR 97229.0 649900.0 4.0 2.5 ... 21.0 221.0 25.0 Active http://www.redfin.com/OR/Portland/3775-NW-Hilt... RMLS 18389440 45.546644 -122.815658 {\"x\": -122.8156584, \"y\": 45.546644, \"spatialRe... <p>1 rows \u00d7 24 columns</p> In\u00a0[6]: Copied! <pre>house_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Shapes/RedStarLargeB.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24}\ngrocery_symbol = symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Shopping.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\nhospital_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/SafetyHealth/Hospital.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24}\ncoffee_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Coffee.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\nrestaurant_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Dining.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\nbar_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Bar.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\ngas_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Transportation/esriBusinessMarker_72.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\nshops_service_symbol={\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/esriBusinessMarker_58_Red.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10}\ntransport_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Transportation/esriDefaultMarker_195_White.png\",\"contentType\":\"image/png\",\"width\":15,\"height\":15}\nprofessional_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/esriBusinessMarker_64_Yellow.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10}\nparks_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/OutdoorRecreation/RestArea.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10}\neducation_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Note.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10}\narts_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/LiveShow.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12}\ndestination_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":12,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Basic/RedStickpin.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24}\nfill_symbol = {\"type\": \"esriSFS\",\"style\": \"esriSFSNull\",\n               \"outline\":{\"color\": [255,0,0,255]}}\n\nfill_symbol2 = {\"type\": \"esriSFS\",\"style\": \"esriSFSNull\",\n               \"outline\":{\"color\": [0,0,0,255]}}\n\nroute_symbol = {\"type\": \"esriSLS\",\"style\": \"esriSLSSolid\",\n                \"color\": [0, 120, 255, 255],\"width\": 1.5}\n</pre> house_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Shapes/RedStarLargeB.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24} grocery_symbol = symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Shopping.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} hospital_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/SafetyHealth/Hospital.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24} coffee_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Coffee.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} restaurant_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Dining.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} bar_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Bar.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} gas_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Transportation/esriBusinessMarker_72.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} shops_service_symbol={\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/esriBusinessMarker_58_Red.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10} transport_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Transportation/esriDefaultMarker_195_White.png\",\"contentType\":\"image/png\",\"width\":15,\"height\":15} professional_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/esriBusinessMarker_64_Yellow.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10} parks_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/OutdoorRecreation/RestArea.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10} education_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/Note.png\",\"contentType\":\"image/png\",\"width\":10,\"height\":10} arts_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":0,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/PeoplePlaces/LiveShow.png\",\"contentType\":\"image/png\",\"width\":12,\"height\":12} destination_symbol = {\"angle\":0,\"xoffset\":0,\"yoffset\":12,\"type\":\"esriPMS\",\"url\":\"http://static.arcgis.com/images/Symbols/Basic/RedStickpin.png\",\"contentType\":\"image/png\",\"width\":24,\"height\":24} fill_symbol = {\"type\": \"esriSFS\",\"style\": \"esriSFSNull\",                \"outline\":{\"color\": [255,0,0,255]}}  fill_symbol2 = {\"type\": \"esriSFS\",\"style\": \"esriSFSNull\",                \"outline\":{\"color\": [0,0,0,255]}}  route_symbol = {\"type\": \"esriSLS\",\"style\": \"esriSLSSolid\",                 \"color\": [0, 120, 255, 255],\"width\": 1.5} In\u00a0[7]: Copied! <pre>paddress = prop1.ADDRESS + \", \" + prop1.CITY + \", \" + prop1.STATE\nprop_geom_fset = geocode(paddress.values[0], as_featureset=True)\n</pre> paddress = prop1.ADDRESS + \", \" + prop1.CITY + \", \" + prop1.STATE prop_geom_fset = geocode(paddress.values[0], as_featureset=True) <p>Create an envelope around the property using its extent</p> In\u00a0[8]: Copied! <pre>prop_geom = prop_geom_fset.features[0]\nprop_geom.geometry\n</pre> prop_geom = prop_geom_fset.features[0] prop_geom.geometry Out[8]: <pre>{'x': -122.81532305026238,\n 'y': 45.54674878544642,\n 'spatialReference': {'wkid': 4326, 'latestWkid': 4326}}</pre> In\u00a0[9]: Copied! <pre>prop_geom = prop_geom_fset.features[0]\nprop_buffer = buffer([prop_geom.geometry], \n                     in_sr = 102100, buffer_sr=102100,\n                     distances=0.05, unit=9001)[0]\n\nprop_buffer_f = Feature(geometry=prop_buffer)\nprop_buffer_fset = FeatureSet([prop_buffer_f])\n</pre> prop_geom = prop_geom_fset.features[0] prop_buffer = buffer([prop_geom.geometry],                       in_sr = 102100, buffer_sr=102100,                      distances=0.05, unit=9001)[0]  prop_buffer_f = Feature(geometry=prop_buffer) prop_buffer_fset = FeatureSet([prop_buffer_f]) In\u00a0[10]: Copied! <pre>pdx_map = gis.map('Portland, OR')\npdx_map.basemap='gray'\npdx_map\n</pre> pdx_map = gis.map('Portland, OR') pdx_map.basemap='gray' pdx_map <pre>MapView(basemap='gray', layout=Layout(height='400px', width='100%'))</pre> <p></p> In\u00a0[11]: Copied! <pre>pdx_map.draw(prop_buffer_fset, symbol=fill_symbol2)\npdx_map.draw(prop_geom_fset, symbol=house_symbol)\n</pre> pdx_map.draw(prop_buffer_fset, symbol=fill_symbol2) pdx_map.draw(prop_geom_fset, symbol=house_symbol) In\u00a0[15]: Copied! <pre>neighborhood_data_dict = {}\n</pre> neighborhood_data_dict = {} In\u00a0[16]: Copied! <pre>groceries = geocode('groceries', search_extent=prop_buffer.extent, \n                    max_locations=20, as_featureset=True)\nneighborhood_data_dict['groceries'] = []\n\nfor place in groceries:\n    popup={\"title\" : place.attributes['PlaceName'], \n    \"content\" : place.attributes['Place_addr']}\n    pdx_map.draw(place.geometry, symbol=grocery_symbol, popup=popup)\n    neighborhood_data_dict['groceries'].append(place.attributes['PlaceName'])\n</pre> groceries = geocode('groceries', search_extent=prop_buffer.extent,                      max_locations=20, as_featureset=True) neighborhood_data_dict['groceries'] = []  for place in groceries:     popup={\"title\" : place.attributes['PlaceName'],      \"content\" : place.attributes['Place_addr']}     pdx_map.draw(place.geometry, symbol=grocery_symbol, popup=popup)     neighborhood_data_dict['groceries'].append(place.attributes['PlaceName']) <p>We will geocode for the following facilities within the said <code>5</code> mile buffer.</p> <pre><code>Groceries\nRestaurants\nHospitals\nCoffee shops\nBars\nGas stations\nShops and service\nTravel and transport\nParks and outdoors\nEducation</code></pre> In\u00a0[48]: Copied! <pre>pdx_map2 = gis.map('Portland, OR')\npdx_map2.basemap='gray'\npdx_map2\n</pre> pdx_map2 = gis.map('Portland, OR') pdx_map2.basemap='gray' pdx_map2 <pre>MapView(basemap='gray', layout=Layout(height='400px', width='100%'))</pre> <p></p> <p></p> <p></p> In\u00a0[59]: Copied! <pre>pdx_map2.draw(prop_buffer_fset, symbol=fill_symbol2)\npdx_map2.draw(prop_geom_fset, symbol=house_symbol)\n</pre> pdx_map2.draw(prop_buffer_fset, symbol=fill_symbol2) pdx_map2.draw(prop_geom_fset, symbol=house_symbol) In\u00a0[21]: Copied! <pre>restaurants = geocode('restaurant', search_extent=prop_buffer.extent, max_locations=200)\nneighborhood_data_dict['restauruants'] = []\n\nfor place in restaurants:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=restaurant_symbol, popup=popup)\n    neighborhood_data_dict['restauruants'].append(place['attributes']['PlaceName'])\n</pre> restaurants = geocode('restaurant', search_extent=prop_buffer.extent, max_locations=200) neighborhood_data_dict['restauruants'] = []  for place in restaurants:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=restaurant_symbol, popup=popup)     neighborhood_data_dict['restauruants'].append(place['attributes']['PlaceName']) In\u00a0[51]: Copied! <pre>hospitals = geocode('hospital', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['hospitals'] = []\n\nfor place in hospitals:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=hospital_symbol, popup=popup)\n    neighborhood_data_dict['hospitals'].append(place['attributes']['PlaceName'])\n</pre> hospitals = geocode('hospital', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['hospitals'] = []  for place in hospitals:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=hospital_symbol, popup=popup)     neighborhood_data_dict['hospitals'].append(place['attributes']['PlaceName']) In\u00a0[52]: Copied! <pre>coffees = geocode('coffee', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['coffees'] = []\n\nfor place in coffees:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=coffee_symbol, popup=popup)\n    neighborhood_data_dict['coffees'].append(place['attributes']['PlaceName'])\n</pre> coffees = geocode('coffee', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['coffees'] = []  for place in coffees:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=coffee_symbol, popup=popup)     neighborhood_data_dict['coffees'].append(place['attributes']['PlaceName']) In\u00a0[53]: Copied! <pre>bars = geocode('bar', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['bars'] = []\n\nfor place in bars:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=bar_symbol, popup=popup)\n    neighborhood_data_dict['bars'].append(place['attributes']['PlaceName'])\n</pre> bars = geocode('bar', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['bars'] = []  for place in bars:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=bar_symbol, popup=popup)     neighborhood_data_dict['bars'].append(place['attributes']['PlaceName']) In\u00a0[54]: Copied! <pre>gas = geocode('gas station', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['gas'] = []\n\nfor place in gas:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=gas_symbol, popup=popup)\n    neighborhood_data_dict['gas'].append(place['attributes']['PlaceName'])\n</pre> gas = geocode('gas station', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['gas'] = []  for place in gas:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=gas_symbol, popup=popup)     neighborhood_data_dict['gas'].append(place['attributes']['PlaceName']) In\u00a0[55]: Copied! <pre>shops_service = geocode(\"\",category='shops and service', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['shops'] = []\n\nfor place in shops_service:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=shops_service_symbol, popup=popup)\n    neighborhood_data_dict['shops'].append(place['attributes']['PlaceName'])\n</pre> shops_service = geocode(\"\",category='shops and service', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['shops'] = []  for place in shops_service:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=shops_service_symbol, popup=popup)     neighborhood_data_dict['shops'].append(place['attributes']['PlaceName']) In\u00a0[56]: Copied! <pre>transport = geocode(\"\",category='travel and transport', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['transport'] = []\n\nfor place in transport:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=transport_symbol, popup=popup)\n    neighborhood_data_dict['transport'].append(place['attributes']['PlaceName'])\n</pre> transport = geocode(\"\",category='travel and transport', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['transport'] = []  for place in transport:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=transport_symbol, popup=popup)     neighborhood_data_dict['transport'].append(place['attributes']['PlaceName']) In\u00a0[57]: Copied! <pre>parks = geocode(\"\",category='parks and outdoors', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['parks'] = []\n\nfor place in parks:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=parks_symbol, popup=popup)\n    neighborhood_data_dict['parks'].append(place['attributes']['PlaceName'])\n</pre> parks = geocode(\"\",category='parks and outdoors', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['parks'] = []  for place in parks:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=parks_symbol, popup=popup)     neighborhood_data_dict['parks'].append(place['attributes']['PlaceName']) In\u00a0[58]: Copied! <pre>education = geocode(\"\",category='education', search_extent=prop_buffer.extent, max_locations=50)\nneighborhood_data_dict['education'] = []\n\nfor place in education:\n    popup={\"title\" : place['attributes']['PlaceName'], \n    \"content\" : place['attributes']['Place_addr']}\n    pdx_map2.draw(place['location'], symbol=education_symbol, popup=popup)\n    neighborhood_data_dict['education'].append(place['attributes']['PlaceName'])\n</pre> education = geocode(\"\",category='education', search_extent=prop_buffer.extent, max_locations=50) neighborhood_data_dict['education'] = []  for place in education:     popup={\"title\" : place['attributes']['PlaceName'],      \"content\" : place['attributes']['Place_addr']}     pdx_map2.draw(place['location'], symbol=education_symbol, popup=popup)     neighborhood_data_dict['education'].append(place['attributes']['PlaceName']) In\u00a0[75]: Copied! <pre>neighborhood_df = pd.DataFrame.from_dict(neighborhood_data_dict, orient='index')\nneighborhood_df = neighborhood_df.transpose()\nneighborhood_df\n</pre> neighborhood_df = pd.DataFrame.from_dict(neighborhood_data_dict, orient='index') neighborhood_df = neighborhood_df.transpose() neighborhood_df Out[75]: groceries restauruants hospitals coffees bars gas shops transport parks education 0 Bales Market Place Coffee. Cup Providence St Vincent Medical Center-ER Coffee. Cup None Shell Anderson Towing &amp; Recovery MAX-Elmonica &amp; SW 170th Ave Jqay House Park Portland Community College-Rock Creek 1 Safeway Papa Murphy's Providence St Vincent Medical Center Starbucks None ARCO Bassitt Auto Co Powder Lodging Jackie Husen Park Cedar Mill Elementary School 2 QFC Tilly's Gelato None Starbucks None 76 Cottman Transmission Homestead Studio Suites-Beaverton The Bluffs Goddard School 3 Dinihanian's Farm Market Oak Hills Brew Pub None Poppa's Haven None Costco Powell Paint Center MAX-Sunset TC Bonny Slope Park St Pius X Elementary School 4 Walmart Neighborhood Market Starbucks None Tazza Cafe None 76 Retied MAX-Merlo &amp; SW 158th Ave Burton Park Terra Linda Elementary School 5 India Supermarket Starbucks None Laughing Planet Cafe None Fred Meyer Fuel Center Chrisman's Picture Frame &amp; Gallery MAX-Beaverton Creek Lost Park Montessori School of Beaverton 6 Apna Bazaar Chiam None Coffee Renaissance None Chevron Team Uniforms Rodeway Inn &amp; Suites-Portland Terra Linda Park A Childs Way Kindergarten 7 Fred Meyer Bandito Taco None Starbucks None Chevron T-Mobile Doubletree-Beaverton Jordan Park Christ United Methodist Preschool 8 Albertsons Chipotle None Bowl &amp; Berry None 76 Holistic Pet Hilton Garden Inn-Beaverton Cedar Mill Woods Park Cornell Children's Village Day Sch 9 WinCo Foods Bollywood Bites None Taiwan Eats None Shell Shell Fairfield Inn &amp; Suites-Beaverton Cedar Mill Park Catlin Gabel School 10 Plaid Pantry Pizza Schmizza None Starbucks None 76 Mike's Auto Parts Homewood Suites-Hillsboro Beaverton Roger Tilbury Memorial Park West Tualatin View Elementary Sch 11 Target Wan Q Restaurant None Bethany Public House None Shell Kaady Car Wash Poehler Airport Merritt Orchard Park Beaver Acres Elementary School 12 None Shari's None Starbucks None 76 7-Eleven Park &amp; Ride-Elmonica/SW 170th Ave Commonwealth Park Ridgewood Elementary School 13 None Ct Bistro None Starbucks None None Motor Sports International Park &amp; Ride-NW Cornell Pioneer Park William Walker Elementary School 14 None McDonald's None Creatures of Habit Espresso &amp; Deli None None Bank of America Park &amp; Ride-Sunset TRANSIT Center Peppertree Park Meadow Park Middle School 15 None Pizzicato Westside None Starbucks None None Bales Market Place Park &amp; Ride-Beaverton Creek Claremont Golf Course Pasitos Spanish School 16 None Lejoi Cafe None Coffee Rush None None US Bank Park &amp; Ride-Cedar Hills Church Kaiser Woods Park Holy Trinity Elementary School 17 None Ichiban Japanese None Starbucks None None Dufresne's Auto Service Dock Bethany Meadows Park Barnes Elementary School 18 None Mac's Market &amp; Deli None None None None US Bank Dock Kaiser Ridge Park Shiny Sparkles Montessori 19 None SUBWAY None None None None Bank of America Fred Meyer Morgans Run Park Stoller Middle School 20 None Salars Mediterranean Grill None None None None Ace Hardware Dock West Union Estates Park Findley Elementary 21 None Poppa's Haven None None None None Du Fresne's Auto Repair Dock College Park Kindercare Learning Center 22 None Papa John's None None None None Jiffy Lube Dock Northeast Neighborhood Park Westview Senior High School 23 None Tazza Cafe None None None None Wells Fargo Dock Ben Graf Park Beaverton School 24 None Teriyaki Beef Bowl None None None None Walgreens 1-1 Spyglass Park Sweet Peas Kidzone 25 None China Rim None None None None Wells Fargo Exit 67/Murray Blvd/E Stoller Farms Park La Petite Academy 26 None SUBWAY None None None None Safeway Exit 65/Bethany Blvd/E Emerald Estates Park Oak Hills Elementary School 27 None Dairy Queen None None None None Team Uniforms Exit 67/Murray Blvd/W Serah Lindsay Estates Park Bethany Elementary School 28 None Si Senor Mexican Restaurant None None None None Xpressolube Exit 69B/Park Way/W Quarry Park Kids of the Kingdom 29 None Laughing Planet Cafe None None None None Mike's Auto Parts Exit 68/Cedar Hills Blvd/W Springville Meadows Park Elmonica Elementary School 30 None Pizza Schmizza None None None None SHERWIN-WILLIAMS Tillamook/N Skyview Park Touchstone School 31 None Bleachers None None None None Pet Barn Exit 65/Cornell Rd/W John Marty Park Five Oaks Middle School 32 None Tazza Cafe None None None None Rock It Resell Exit 69B/Cedar Hills/E George W Otten Park Pacific Academy 33 None Bowl &amp; Berry None None None None Baby &amp; Me Exit 68/Cedar Hills Blvd/E Somerset Meadows Park Merlo Station High School 34 None Sweet Lemon Vegetarian Bistro None None None None Cedar Mill Home Theater Exit 1/Walker Rd/S Oak Hills Park Kinder Prep Private Preschool 35 None Juan Colorado None None None None Dollar Tree Exit 64/185th Ave/W Bronson Creek Park Prince of Peace Lutheran Preschool 36 None Taiwan Eats None None None None Sunset Science Park Federal CU None Autumn Ridge Park Sunset High School 37 None Taiwan Eats None None None None Sunset Science Park Federal CU None Apollo Ridge Park Cedar Hills Kindercare 38 None Cackalack's Hot Chicken Shack None None None None Dennis Market None Willow Creek Nature Park Northwest Montessori School 39 None Chen's Dynasty Restaurant None None None None Laughing Planet Cafe None Dwight S Parr Jr Park Agia Sophia Academy 40 None Bethany Sushi None None None None None None Moshofsky Woods Park Learning Years Day School 41 None SUBWAY None None None None None None Stonegate at Willow Creek ABC Children's Academy 42 None Starbucks None None None None None None Waterhouse Park Arco Iris Spanish Immersion School 43 None Bethany Public House None None None None None None Sunset Park Forest Park Elementary School 44 None Bethany Public House None None None None None None Foege Park None 45 None Bliss Bake Shoppe None None None None None None Peterkort Village Park None 46 None Bliss Bake Shoppe None None None None None None Wanda L Peck Memorial Park None 47 None Biscuits Cafe None None None None None None Howard M Terpenning Complex None 48 None Koi Fusion None None None None None None Forest Heights Park None 49 None Starbucks None None None None None None Forest Heights City Park None In\u00a0[76]: Copied! <pre>neighborhood_df.count().plot(kind='bar')\nplt.title('Facilities within 5 miles of {}'.format(prop1.ADDRESS.values[0]))\n</pre> neighborhood_df.count().plot(kind='bar') plt.title('Facilities within 5 miles of {}'.format(prop1.ADDRESS.values[0])) Out[76]: <pre>Text(0.5,1,'Facilities within 5 miles of 3775 NW Hilton Head Ter')</pre> In\u00a0[32]: Copied! <pre>route_service_url = gis.properties.helperServices.route.url\nroute_service = RouteLayer(route_service_url, gis=gis)\n</pre> route_service_url = gis.properties.helperServices.route.url route_service = RouteLayer(route_service_url, gis=gis) In\u00a0[33]: Copied! <pre>stops = [paddress.values[0], '309 SW 6th Ave #600, Portland, OR 97204']\nfrom arcgis.geocoding import geocode, batch_geocode\nstops_geocoded = batch_geocode(stops)\n\nstops_geocoded = [item['location'] for item in stops_geocoded]\nstops_geocoded2 = '{},{};{},{}'.format(stops_geocoded[0]['x'],stops_geocoded[0]['y'],\n                                       stops_geocoded[1]['x'],stops_geocoded[1]['y'])\nstops_geocoded2\n</pre> stops = [paddress.values[0], '309 SW 6th Ave #600, Portland, OR 97204'] from arcgis.geocoding import geocode, batch_geocode stops_geocoded = batch_geocode(stops)  stops_geocoded = [item['location'] for item in stops_geocoded] stops_geocoded2 = '{},{};{},{}'.format(stops_geocoded[0]['x'],stops_geocoded[0]['y'],                                        stops_geocoded[1]['x'],stops_geocoded[1]['y']) stops_geocoded2 Out[33]: <pre>'-122.81532304999996,45.546748785000034;-122.67727209699996,45.52153932300007'</pre> In\u00a0[34]: Copied! <pre>modes = route_service.retrieve_travel_modes()['supportedTravelModes']\nfor mode in modes:\n    print(mode['name'])\n</pre> modes = route_service.retrieve_travel_modes()['supportedTravelModes'] for mode in modes:     print(mode['name']) <pre>Walking Time\nRural Driving Distance\nDriving Time\nDriving Distance\nWalking Distance\nRural Driving Time\nTrucking Time\nTrucking Distance\n</pre> In\u00a0[35]: Copied! <pre>route_service.properties.impedance\n</pre> route_service.properties.impedance Out[35]: <pre>'TravelTime'</pre> <p>Calculate time it takes to get to work. Set start time as <code>8:00 AM</code> on Mondays. ArcGIS routing service will use historic averages, so we provide this time as <code>8:00 AM, Monday, June 4 1990</code> in Unix epoch time. Read more about this here</p> In\u00a0[36]: Copied! <pre>route_result = route_service.solve(stops_geocoded2, return_routes=True, \n                             return_stops=True, return_directions=True,\n                             impedance_attribute_name='TravelTime',\n                             start_time=644511600000,\n                             return_barriers=False, return_polygon_barriers=False,\n                             return_polyline_barriers=False)\n</pre> route_result = route_service.solve(stops_geocoded2, return_routes=True,                               return_stops=True, return_directions=True,                              impedance_attribute_name='TravelTime',                              start_time=644511600000,                              return_barriers=False, return_polygon_barriers=False,                              return_polyline_barriers=False) In\u00a0[37]: Copied! <pre>route_length = route_result['directions'][0]['summary']['totalLength']\nroute_duration = route_result['directions'][0]['summary']['totalTime']\nroute_duration_str = \"{}m, {}s\".format(int(route_duration), \n                                       round((route_duration %1)*60,2))\nprint(\"route length: {} miles, route duration: {}\".format(round(route_length,3),\n                                                         route_duration_str))\n</pre> route_length = route_result['directions'][0]['summary']['totalLength'] route_duration = route_result['directions'][0]['summary']['totalTime'] route_duration_str = \"{}m, {}s\".format(int(route_duration),                                         round((route_duration %1)*60,2)) print(\"route length: {} miles, route duration: {}\".format(round(route_length,3),                                                          route_duration_str)) <pre>route length: 10.273 miles, route duration: 27m, 48.39s\n</pre> In\u00a0[49]: Copied! <pre>route_features = route_result['routes']['features']\nroute_fset = FeatureSet(route_features)\nstop_features = route_result['stops']['features']\nstop_fset = FeatureSet(stop_features)\n\nroute_pop_up = {'title':'Name',\n               'content':'Total_Miles'}\npdx_map2.draw(route_fset, symbol=route_symbol, popup=route_pop_up)\n</pre> route_features = route_result['routes']['features'] route_fset = FeatureSet(route_features) stop_features = route_result['stops']['features'] stop_fset = FeatureSet(stop_features)  route_pop_up = {'title':'Name',                'content':'Total_Miles'} pdx_map2.draw(route_fset, symbol=route_symbol, popup=route_pop_up) In\u00a0[50]: Copied! <pre>pdx_map2.draw(stop_fset, symbol=destination_symbol)\n</pre> pdx_map2.draw(stop_fset, symbol=destination_symbol)"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#feature-engineering-quantifying-access-to-facilities","title":"Feature engineering - quantifying access to facilities\u00b6","text":"<p>Often, when shortlisting facilities buyers look for access to facilities such as groceries, restaurants, schools, emergency and health care in thier neighborhood. In this notebook, we use the <code>geocoding</code> module to search for such facilities and build a table for each property.</p>"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#read-one-of-the-shortlisted-properties","title":"Read one of the shortlisted properties\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#create-symbols-for-facilities","title":"Create symbols for facilities\u00b6","text":"<p>Get your symbols using this online tool: http://esri.github.io/arcgis-python-api/tools/symbol.html</p>"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#get-5-mile-extent-around-the-property-of-interest","title":"Get 5 mile extent around the property of interest\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#plot-house-and-buffer-on-map","title":"Plot house and buffer on map\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#geocode-for-facilities","title":"Geocode for facilities\u00b6","text":"<p>We use the ArcGIS Geocoding service to search for facilities around this house</p>"},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#groceries","title":"Groceries\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#restaurants","title":"Restaurants\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#hospitals","title":"Hospitals\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#coffee-shops","title":"Coffee shops\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#bars","title":"Bars\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#gas-stations","title":"Gas stations\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#shops-and-service","title":"Shops and service\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#travel-and-transport","title":"Travel and transport\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#parks-and-outdoors","title":"Parks and outdoors\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#education","title":"Education\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#present-the-results-in-a-table","title":"Present the results in a table\u00b6","text":""},{"location":"talks/portland-house-hunting/03_feature-engineering-neighboring-facilities/#find-duration-to-commute-to-work","title":"Find duration to commute to work\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/","title":"Feature engineering - quantifying access to facilities - batch mode","text":"In\u00a0[9]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\n\nfrom arcgis.gis import GIS\nfrom arcgis.geocoding import geocode, batch_geocode\nfrom arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor\nfrom arcgis.features import SpatialDataFrame\nfrom arcgis.geometry import Geometry, Point\nfrom arcgis.geometry.functions import buffer\nfrom arcgis.network import RouteLayer\n</pre> import pandas as pd import matplotlib.pyplot as plt from pprint import pprint %matplotlib inline  from arcgis.gis import GIS from arcgis.geocoding import geocode, batch_geocode from arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor from arcgis.features import SpatialDataFrame from arcgis.geometry import Geometry, Point from arcgis.geometry.functions import buffer from arcgis.network import RouteLayer <p>Connect to GIS</p> In\u00a0[7]: Copied! <pre>gis = GIS(profile='')\nroute_service_url = gis.properties.helperServices.route.url\nroute_service = RouteLayer(route_service_url, gis=gis)\n</pre> gis = GIS(profile='') route_service_url = gis.properties.helperServices.route.url route_service = RouteLayer(route_service_url, gis=gis) In\u00a0[3]: Copied! <pre>prop_list_df = pd.read_csv('resources/houses_for_sale_att_filtered.csv')\nprop_list_df.shape\n</pre> prop_list_df = pd.read_csv('resources/houses_for_sale_att_filtered.csv') prop_list_df.shape Out[3]: <pre>(331, 24)</pre> In\u00a0[4]: Copied! <pre>prop_list_df = pd.DataFrame.spatial.from_xy(prop_list_df, 'LONGITUDE','LATITUDE')\ntype(prop_list_df)\n</pre> prop_list_df = pd.DataFrame.spatial.from_xy(prop_list_df, 'LONGITUDE','LATITUDE') type(prop_list_df) Out[4]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[10]: Copied! <pre>groceries_count = []\nrestaurants_count = []\nhospitals_count = []\ncoffee_count = []\nbars_count = []\ngas_count = []\nshops_service_count = []\ntravel_transport_count = []\nparks_count = []\neducation_count = []\nroute_length = []\nroute_duration = []\n\ndestination_address = '309 SW 6th Ave #600, Portland, OR 97204'\n</pre> groceries_count = [] restaurants_count = [] hospitals_count = [] coffee_count = [] bars_count = [] gas_count = [] shops_service_count = [] travel_transport_count = [] parks_count = [] education_count = [] route_length = [] route_duration = []  destination_address = '309 SW 6th Ave #600, Portland, OR 97204' In\u00a0[12]: Copied! <pre>count=0\nfor index, prop in prop_list_df.iterrows():\n    count+=1\n    print(str(count), end=\": \")\n    # geocode the property\n    paddress = prop['ADDRESS'] + \", \" + prop['CITY'] + \", \" + prop['STATE']\n    prop_geom_fset = geocode(paddress, as_featureset=True)\n    \n    print(prop['MLS'], end=\" : \")\n    \n    # create an envelope around each property\n    prop_geom = prop_geom_fset.features[0]\n    \n    # create buffer of 5 miles\n    prop_buffer = buffer([prop_geom.geometry], \n                     in_sr = 102100, buffer_sr=102100,\n                     distances=0.05, unit=9001)[0]\n\n    prop_buffer_f = Feature(geometry=prop_buffer)\n    prop_buffer_fset = FeatureSet([prop_buffer_f])\n    \n    # geocode for Groceries\n    groceries = geocode('groceries', search_extent=prop_buffer.extent, \n                    max_locations=20, as_featureset=True)\n    groceries_count.append(len(groceries.features))\n    print('Groc', end=\" : \")\n    \n    # restaurants\n    restaurants = geocode('restaurant', search_extent=prop_buffer.extent, max_locations=200)\n    restaurants_count.append(len(restaurants))\n    print('Rest', end=\" : \")\n    \n    # hospitals\n    hospitals = geocode('hospital', search_extent=prop_buffer.extent, max_locations=50)\n    hospitals_count.append(len(hospitals))\n    print('Hosp', end =\" : \")\n    \n    # coffee shop\n    coffees = geocode('coffee', search_extent=prop_buffer.extent, max_locations=50)\n    coffee_count.append(len(coffees))\n    print('Coffee', end=\" : \")\n    \n    # bars\n    bars = geocode('bar', search_extent=prop_buffer.extent, max_locations=50)\n    bars_count.append(len(bars))\n    print('Bars', end=\" : \")\n    \n    # gas stations\n    gas = geocode('gas station', search_extent=prop_buffer.extent, max_locations=50)\n    gas_count.append(len(gas))\n    print('Gas', end=\" : \")\n    \n    # shops\n    shops_service = geocode(\"\",category='shops and service', \n                            search_extent=prop_buffer.extent, max_locations=50)\n    shops_service_count.append(len(shops_service))\n    print('Shops', end=\" : \")\n    \n    # travel &amp; transport\n    transport = geocode(\"\",category='travel and transport', \n                        search_extent=prop_buffer.extent, max_locations=50)\n    travel_transport_count.append(len(transport))\n    print(\"Travel\", end =\" : \")\n    \n    # parks\n    parks = geocode(\"\",category='parks and outdoors', \n                    search_extent=prop_buffer.extent, max_locations=50)\n    parks_count.append(len(parks))\n    print('Parks', end=\" : \")\n    \n    # education\n    education = geocode(\"\",category='education', search_extent=prop_buffer.extent, \n                        max_locations=50)\n    education_count.append(len(education))\n    print(\"Edu\", end=\" : \")\n    \n    # get route\n    stops = [paddress, destination_address]\n    stops_geocoded = batch_geocode(stops)\n\n    stops_geocoded = [item['location'] for item in stops_geocoded]\n    stops_geocoded2 = '{},{};{},{}'.format(stops_geocoded[0]['x'],stops_geocoded[0]['y'],\n                                           stops_geocoded[1]['x'],stops_geocoded[1]['y'])\n\n    route_result = route_service.solve(stops_geocoded2, return_routes=True, \n                             return_stops=False, return_directions=True,\n                             impedance_attribute_name='TravelTime',\n                             start_time=644511600000,\n                             return_barriers=False, return_polygon_barriers=False,\n                             return_polyline_barriers=False)\n    route_length.append(route_result['directions'][0]['summary']['totalLength'])\n    route_duration.append(route_result['directions'][0]['summary']['totalTime'])\n    print(\"Route\")\n</pre> count=0 for index, prop in prop_list_df.iterrows():     count+=1     print(str(count), end=\": \")     # geocode the property     paddress = prop['ADDRESS'] + \", \" + prop['CITY'] + \", \" + prop['STATE']     prop_geom_fset = geocode(paddress, as_featureset=True)          print(prop['MLS'], end=\" : \")          # create an envelope around each property     prop_geom = prop_geom_fset.features[0]          # create buffer of 5 miles     prop_buffer = buffer([prop_geom.geometry],                       in_sr = 102100, buffer_sr=102100,                      distances=0.05, unit=9001)[0]      prop_buffer_f = Feature(geometry=prop_buffer)     prop_buffer_fset = FeatureSet([prop_buffer_f])          # geocode for Groceries     groceries = geocode('groceries', search_extent=prop_buffer.extent,                      max_locations=20, as_featureset=True)     groceries_count.append(len(groceries.features))     print('Groc', end=\" : \")          # restaurants     restaurants = geocode('restaurant', search_extent=prop_buffer.extent, max_locations=200)     restaurants_count.append(len(restaurants))     print('Rest', end=\" : \")          # hospitals     hospitals = geocode('hospital', search_extent=prop_buffer.extent, max_locations=50)     hospitals_count.append(len(hospitals))     print('Hosp', end =\" : \")          # coffee shop     coffees = geocode('coffee', search_extent=prop_buffer.extent, max_locations=50)     coffee_count.append(len(coffees))     print('Coffee', end=\" : \")          # bars     bars = geocode('bar', search_extent=prop_buffer.extent, max_locations=50)     bars_count.append(len(bars))     print('Bars', end=\" : \")          # gas stations     gas = geocode('gas station', search_extent=prop_buffer.extent, max_locations=50)     gas_count.append(len(gas))     print('Gas', end=\" : \")          # shops     shops_service = geocode(\"\",category='shops and service',                              search_extent=prop_buffer.extent, max_locations=50)     shops_service_count.append(len(shops_service))     print('Shops', end=\" : \")          # travel &amp; transport     transport = geocode(\"\",category='travel and transport',                          search_extent=prop_buffer.extent, max_locations=50)     travel_transport_count.append(len(transport))     print(\"Travel\", end =\" : \")          # parks     parks = geocode(\"\",category='parks and outdoors',                      search_extent=prop_buffer.extent, max_locations=50)     parks_count.append(len(parks))     print('Parks', end=\" : \")          # education     education = geocode(\"\",category='education', search_extent=prop_buffer.extent,                          max_locations=50)     education_count.append(len(education))     print(\"Edu\", end=\" : \")          # get route     stops = [paddress, destination_address]     stops_geocoded = batch_geocode(stops)      stops_geocoded = [item['location'] for item in stops_geocoded]     stops_geocoded2 = '{},{};{},{}'.format(stops_geocoded[0]['x'],stops_geocoded[0]['y'],                                            stops_geocoded[1]['x'],stops_geocoded[1]['y'])      route_result = route_service.solve(stops_geocoded2, return_routes=True,                               return_stops=False, return_directions=True,                              impedance_attribute_name='TravelTime',                              start_time=644511600000,                              return_barriers=False, return_polygon_barriers=False,                              return_polyline_barriers=False)     route_length.append(route_result['directions'][0]['summary']['totalLength'])     route_duration.append(route_result['directions'][0]['summary']['totalTime'])     print(\"Route\") <pre>1: 18517652 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n2: 18465613 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n3: 18005102 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n4: 18216924 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n5: 18647164 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n6: 18229660 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n7: 18586790 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n8: 18314898 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n9: 18085278 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n10: 18406186 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n11: 18020972 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n12: 18283940 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n13: 18166839 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n14: 18390189 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n15: 18281614 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n16: 18159838 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n17: 18697929 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n18: 18184111 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n19: 18381383 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n20: 18036264 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n21: 18352241 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n22: 18415529 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n23: 18052500 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n24: 18615156 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n25: 18663618 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n26: 18524346 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n27: 18287995 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n28: 18496603 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n29: 18306005 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n30: 18017989 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n31: 18630734 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n32: 18362577 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n33: 18185462 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n34: 18525026 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n35: 18268485 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n36: 18077776 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n37: 18404645 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n38: 18543295 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n39: 18268007 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n40: 18565385 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n41: 18327093 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n42: 18300182 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n43: 18130222 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n44: 18390288 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n45: 18244519 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n46: 18533145 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n47: 18317832 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n48: 18625148 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n49: 18046552 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n50: 18613304 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n51: 18035240 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n52: 18170798 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n53: 18479918 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n54: 18134679 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n55: 18175979 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n56: 18489535 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n57: 18136667 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n58: 18330192 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n59: 18035177 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n60: 18117942 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n61: 18467170 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n62: 18021854 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n63: 18453150 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n64: 17228335 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n65: 18385412 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n66: 18303159 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n67: 18260962 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n68: 18077788 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n69: 1443106 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n70: 18363380 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n71: 18038013 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n72: 18592639 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n73: 18031108 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n74: 18529842 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n75: 18346063 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n76: 18058515 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n77: 18312442 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n78: 18062133 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n79: 18292112 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n80: 18021426 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n81: 18243995 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n82: 18225445 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n83: 18294835 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n84: 18036942 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n85: 18164206 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n86: 18112429 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n87: 18401204 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n88: 18185257 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n89: 18056844 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n90: 18158997 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n91: 18089346 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n92: 18432056 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n93: 18248238 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n94: 18042235 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n95: 18376736 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n96: 17468803 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n97: 18002685 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n98: 18609084 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n99: 18115705 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n100: 18197366 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n101: 18367807 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n102: 18289907 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n103: 1456193 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n104: 18047073 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n105: 18379344 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n106: 18487228 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n107: 18123653 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n108: 18310961 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n109: 18671543 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n110: 18384430 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n111: 18292179 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n112: 18545728 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n113: 18317933 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n114: 18131614 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n115: 18359207 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n116: 18188909 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n117: 18257295 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n118: 18454489 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n119: 18644320 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n120: 18103930 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n121: 18213089 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n122: 18436725 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n123: 18217493 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n124: 18534351 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n125: 18391773 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n126: 17138243 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n127: 18580547 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n128: 18385584 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n129: 1516748 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n130: 18108097 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n131: 18343244 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n132: 18518261 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n133: 18496090 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n134: 18084407 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n135: 18622197 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n136: 18590953 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n137: 18548575 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n138: 18361209 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n139: 18190102 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n140: 18306032 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n141: 18546456 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n142: 17218625 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n143: 18360477 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n144: 18447320 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n145: 18127206 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n146: 18489822 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n147: 18218285 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n148: 18647011 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n149: 18193927 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n150: 18595550 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n151: 18317847 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n152: 18108604 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n153: 18410442 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n154: 17515218 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n155: 18578497 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n156: 18293042 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n157: 18401105 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n158: 18095097 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n159: 18030852 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n160: 18299685 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n161: 18583483 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n162: 18171268 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n163: 18053604 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n164: 18622831 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n165: 18586722 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n166: 18467321 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n167: 18383920 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n168: 18599482 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n169: 18500611 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n170: 18345364 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n171: 18116411 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n172: 18187676 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n173: 18521135 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n174: 18661709 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n175: 18196732 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n176: 18047304 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n177: 18202505 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n178: 18090694 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n179: 18572038 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n180: 18641153 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n181: 18231117 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n182: 18069841 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n183: 18599386 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n184: 18160313 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n185: 18377250 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n186: 18301272 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n187: 18094956 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n188: 1465894 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n189: 18525186 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n190: 18144594 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n191: 18462595 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n192: 18597525 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n193: 18436901 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n194: 18166594 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n195: 18164582 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n196: 18258361 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n197: 18326643 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n198: 18681920 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n199: 18123789 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n200: 18080838 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n201: 18551307 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n202: 18142190 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n203: 18333327 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n204: 18389440 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n205: 18232692 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n206: 18147756 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n207: 18439349 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n208: 18319633 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n209: 18400923 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n210: 18108336 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n211: 18270738 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n212: 18045246 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n213: 18158354 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n214: 18225617 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n215: 17626574 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n216: 18285532 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n217: 18256758 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n218: 18659423 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n219: 18384499 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n220: 18126242 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n221: 18151979 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n222: 18323631 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n223: 18309110 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n224: 18669250 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n225: 18020854 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n226: 18412385 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n227: 18590261 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n228: 18197963 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n229: 18138336 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n230: 18100325 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n231: 18047136 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n232: 18073472 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n233: 18115891 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n234: 18486871 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n235: 18537072 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n236: 611481 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n237: 18664192 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n238: 18392231 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n239: 18318834 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n240: 18373029 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n241: 18207743 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n242: 18310301 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n243: 18032546 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n244: 18034065 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n245: 18698609 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n246: 18504270 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n247: 18662489 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n248: 18522209 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n249: 18189976 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n250: 18120748 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n251: 18571434 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n252: 18360370 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n253: 18535877 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n254: 18614773 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n255: 18213231 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n256: 18156948 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n257: 18409009 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n258: 18491281 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n259: 632347 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n260: 18638831 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n261: 18423090 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n262: 18076378 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n263: 18277192 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n264: 18492677 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n265: 18299358 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n266: 18404327 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n267: 18316280 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n268: 18495278 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n269: 18588520 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n270: 18421975 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n271: 18328827 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n272: 18683443 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n273: 18171061 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n274: 18034056 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n275: 18427656 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n276: 18251129 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n277: 18611981 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n278: 18430420 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n279: 18407305 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n280: 18396804 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n281: 18003220 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n282: 18043220 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n283: 18126322 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n284: 18561064 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n285: 18053995 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n286: 18413031 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n287: 18186050 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n288: 18372039 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n289: 18496459 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n290: 18456842 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n291: 18344814 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n292: 18070961 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n293: 18545517 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n294: 18089153 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n295: 18472667 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n296: 18041807 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n297: 18599599 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n298: 18206648 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n299: 18027309 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n300: 18498791 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n301: 18296852 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n302: 18309208 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n303: 18220910 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n304: 18370645 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n305: 18161028 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n306: 18043776 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n307: 18007267 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n308: 18334430 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n309: 18639725 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n310: 18476873 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n311: 18620161 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n312: 18280624 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n313: 18439556 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n314: 18186450 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n315: 18359555 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n316: 18695406 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n317: 18175562 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n318: 18074557 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n319: 18679919 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n320: 18355910 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n321: 18474192 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n322: 18672376 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n323: 18510796 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n324: 18247970 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n325: 18538109 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n326: 18570032 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n327: 18584969 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n328: 18427782 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n329: 17071766 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n330: 1516761 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n331: 1516762 : Groc : Rest : Hosp : Coffee : Bars : Gas : Shops : Travel : Parks : Edu : Route\n</pre> In\u00a0[29]: Copied! <pre>prop_list_df['grocery_count'] = groceries_count\nprop_list_df['restaurant_count']= restaurants_count\nprop_list_df['hospitals_count']= hospitals_count\nprop_list_df['coffee_count']= coffee_count\nprop_list_df['bars_count']=bars_count\nprop_list_df['gas_count']=gas_count\nprop_list_df['shops_count']=shops_service_count\nprop_list_df['travel_count']=travel_transport_count\nprop_list_df['parks_count']=parks_count\nprop_list_df['edu_count']=education_count\nprop_list_df['commute_length']=route_length\nprop_list_df['commute_duration']=route_duration\nprop_list_df.head()\n</pre> prop_list_df['grocery_count'] = groceries_count prop_list_df['restaurant_count']= restaurants_count prop_list_df['hospitals_count']= hospitals_count prop_list_df['coffee_count']= coffee_count prop_list_df['bars_count']=bars_count prop_list_df['gas_count']=gas_count prop_list_df['shops_count']=shops_service_count prop_list_df['travel_count']=travel_transport_count prop_list_df['parks_count']=parks_count prop_list_df['edu_count']=education_count prop_list_df['commute_length']=route_length prop_list_df['commute_duration']=route_duration prop_list_df.head() Out[29]: Unnamed: 0 SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS ... hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration 0 618 MLS Listing Single Family Residential 144 SE 191st Pl Portland OR 97233.0 334900.0 4.0 2.5 ... 0 19 0 28 41 50 50 50 13.978589 25.621265 1 776 MLS Listing Single Family Residential 20028 NE Hoyt St Portland OR 97230.0 349950.0 3.0 2.5 ... 2 23 0 31 38 50 50 50 13.913465 24.726944 2 808 MLS Listing Single Family Residential 711 SE 160th Ave Portland OR 97233.0 358999.0 4.0 2.5 ... 0 16 0 34 47 50 50 50 14.644863 27.830024 3 881 MLS Listing Single Family Residential 5536 SE 142nd Pl Portland OR 97236.0 369800.0 4.0 2.5 ... 2 18 0 29 45 48 50 50 13.097669 25.943438 4 955 MLS Listing Single Family Residential 15707 SE Flavel Dr Portland OR 97236.0 379900.0 3.0 2.0 ... 0 13 0 17 44 10 50 41 14.254413 28.403598 <p>5 rows \u00d7 36 columns</p> In\u00a0[33]: Copied! <pre>prop_list_df.columns\n</pre> prop_list_df.columns Out[33]: <pre>Index(['Unnamed: 0', 'SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration'],\n      dtype='object')</pre> In\u00a0[37]: Copied! <pre>facility_list = ['grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration']\n\naxes = prop_list_df[facility_list].hist(bins=25, layout=(3,4), figsize=(15,10))\n</pre> facility_list = ['grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',        'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',        'edu_count', 'commute_length', 'commute_duration']  axes = prop_list_df[facility_list].hist(bins=25, layout=(3,4), figsize=(15,10)) <p>From the histograms above, most houses don't have very many bars in 5 miles around them. The commute length and duration appears to be tightly clustered around the lower end of the spectrum. Most houses have at least 1 hospital or medical center near them and a large number of parks, restaurants, educational institutions.</p> In\u00a0[30]: Copied! <pre>prop_list_df.to_csv('resources/houses_facility_counts.csv')\nprop_list_df.spatial.to_featureclass('resources/shp/houses_facility_counts.shp')\n</pre> prop_list_df.to_csv('resources/houses_facility_counts.csv') prop_list_df.spatial.to_featureclass('resources/shp/houses_facility_counts.shp') Out[30]: <pre>'resources/shp/houses_facility_counts.shp'</pre>"},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#feature-engineering-quantifying-access-to-facilities-batch-mode","title":"Feature engineering - quantifying access to facilities - batch mode\u00b6","text":"<p>This notebook is similar to previous (04_feature-engineering-neighboring-facilities), except, this one runs for all shortlisted facilities and adds these as features.</p>"},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#read-the-shortlisted-properties","title":"Read the shortlisted properties\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#loop-through-each-property-and-build-the-neighborhood-facility-table","title":"Loop through each property and build the neighborhood facility table\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#feature-engineer-with-access-to-amenities","title":"Feature engineer with access to amenities\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#plot-the-distribution-of-facility-access","title":"Plot the distribution of facility access\u00b6","text":""},{"location":"talks/portland-house-hunting/04_feature-engineering-neighboring-facilities-batch/#store-to-disk","title":"Store to disk\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/","title":"Score and rank properties using intrinsic and spatial features","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\nimport seaborn as sns\n\nfrom arcgis.gis import GIS\nfrom arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor\n</pre> import pandas as pd import matplotlib.pyplot as plt from pprint import pprint %matplotlib inline import seaborn as sns  from arcgis.gis import GIS from arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor In\u00a0[2]: Copied! <pre>gis = GIS(profile='')\n</pre> gis = GIS(profile='') In\u00a0[3]: Copied! <pre>prop_df = pd.read_csv('resources/houses_facility_counts.csv')\nprop_df = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE')\n</pre> prop_df = pd.read_csv('resources/houses_facility_counts.csv') prop_df = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE') In\u00a0[19]: Copied! <pre>prop_df.head()\n</pre> prop_df.head() Out[19]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration 0 MLS Listing Single Family Residential 144 SE 191st Pl Portland OR 97233.0 334900.0 4.0 2.5 Gresham, Sandy, Troutdale, Corbett ... 0 19 0 28 41 50 50 50 13.978589 25.621265 1 MLS Listing Single Family Residential 20028 NE Hoyt St Portland OR 97230.0 349950.0 3.0 2.5 Reeds Place ... 2 23 0 31 38 50 50 50 13.913465 24.726944 2 MLS Listing Single Family Residential 711 SE 160th Ave Portland OR 97233.0 358999.0 4.0 2.5 Portland Southeast ... 0 16 0 34 47 50 50 50 14.644863 27.830024 3 MLS Listing Single Family Residential 5536 SE 142nd Pl Portland OR 97236.0 369800.0 4.0 2.5 Portland Southeast ... 2 18 0 29 45 48 50 50 13.097669 25.943438 4 MLS Listing Single Family Residential 15707 SE Flavel Dr Portland OR 97236.0 379900.0 3.0 2.0 Portland Southeast ... 0 13 0 17 44 10 50 41 14.254413 28.403598 <p>5 rows \u00d7 35 columns</p> In\u00a0[20]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[20]: <pre>Index(['SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE', 'ZIP',\n       'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration'],\n      dtype='object')</pre> <p>Drop unnecessary columns</p> In\u00a0[4]: Copied! <pre>try:\n    prop_df.drop(columns=['Unnamed: 0'], inplace=True)\n    prop_df.drop(columns=['Unnamed: 0.1'], inplace=True)\nexcept:\n    pass\n</pre> try:     prop_df.drop(columns=['Unnamed: 0'], inplace=True)     prop_df.drop(columns=['Unnamed: 0.1'], inplace=True) except:     pass In\u00a0[5]: Copied! <pre>def set_scores(row):\n    score = ((row['PRICE']*-1.5) + # penalize by 1.5 times\n             (row['BEDS']*1)+\n             (row['BATHS']*1)+\n             (row['SQUARE FEET']*1)+\n             (row['LOT SIZE']*1)+\n             (row['YEAR BUILT']*1)+\n             (row['HOA PER MONTH']*-1)+  # penalize by 1 times\n             (row['grocery_count']*1)+\n             (row['restaurant_count']*1)+\n             (row['hospitals_count']*1.5)+  # reward by 1.5 times\n             (row['coffee_count']*1)+\n             (row['bars_count']*1)+\n             (row['shops_count']*1)+\n             (row['travel_count']*1.5)+  # reward by 1.5 times\n             (row['parks_count']*1)+\n             (row['edu_count']*1)+\n             (row['commute_length']*-1)+  # penalize by 1 times\n             (row['commute_duration']*-2)  # penalize by 2 times\n            )\n    return score\n</pre> def set_scores(row):     score = ((row['PRICE']*-1.5) + # penalize by 1.5 times              (row['BEDS']*1)+              (row['BATHS']*1)+              (row['SQUARE FEET']*1)+              (row['LOT SIZE']*1)+              (row['YEAR BUILT']*1)+              (row['HOA PER MONTH']*-1)+  # penalize by 1 times              (row['grocery_count']*1)+              (row['restaurant_count']*1)+              (row['hospitals_count']*1.5)+  # reward by 1.5 times              (row['coffee_count']*1)+              (row['bars_count']*1)+              (row['shops_count']*1)+              (row['travel_count']*1.5)+  # reward by 1.5 times              (row['parks_count']*1)+              (row['edu_count']*1)+              (row['commute_length']*-1)+  # penalize by 1 times              (row['commute_duration']*-2)  # penalize by 2 times             )     return score In\u00a0[29]: Copied! <pre>%%time\nprop_df['scores'] = prop_df.apply(set_scores, axis=1)\n</pre> %%time prop_df['scores'] = prop_df.apply(set_scores, axis=1) <pre>CPU times: user 78 ms, sys: 1.98 ms, total: 80 ms\nWall time: 101 ms\n</pre> In\u00a0[30]: Copied! <pre>prop_df.head()\n</pre> prop_df.head() Out[30]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores 0 MLS Listing Single Family Residential 144 SE 191st Pl Portland OR 97233.0 334900.0 4.0 2.5 Gresham, Sandy, Troutdale, Corbett ... 19 0 28 41 50 50 50 13.978589 25.621265 -495439.721119 1 MLS Listing Single Family Residential 20028 NE Hoyt St Portland OR 97230.0 349950.0 3.0 2.5 Reeds Place ... 23 0 31 38 50 50 50 13.913465 24.726944 -516614.867353 2 MLS Listing Single Family Residential 711 SE 160th Ave Portland OR 97233.0 358999.0 4.0 2.5 Portland Southeast ... 16 0 34 47 50 50 50 14.644863 27.830024 -528948.304911 3 MLS Listing Single Family Residential 5536 SE 142nd Pl Portland OR 97236.0 369800.0 4.0 2.5 Portland Southeast ... 18 0 29 45 48 50 50 13.097669 25.943438 -545169.484546 4 MLS Listing Single Family Residential 15707 SE Flavel Dr Portland OR 97236.0 379900.0 3.0 2.0 Portland Southeast ... 13 0 17 44 10 50 41 14.254413 28.403598 -559115.061609 <p>5 rows \u00d7 36 columns</p> In\u00a0[39]: Copied! <pre>prop_df['scores'].hist(bins=50);   # trailing ; suppresses matplotlib's prints\n</pre> prop_df['scores'].hist(bins=50);   # trailing ; suppresses matplotlib's prints In\u00a0[37]: Copied! <pre>sns.jointplot('PRICE','scores', data=prop_df);\n</pre> sns.jointplot('PRICE','scores', data=prop_df); <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n</pre> <p>From the chart above, it appears numeric columns with large values (such as Price) weild an undue importance on our model. To rectify this, we need to scale all numeric columns and normalize them with each other. The section below performs this and then tries to generate scores based on the scaled values.</p> In\u00a0[38]: Copied! <pre>prop_df.to_csv('resources/houses_scores_unscaled.csv')\n</pre> prop_df.to_csv('resources/houses_scores_unscaled.csv') In\u00a0[6]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()\n</pre> from sklearn.preprocessing import MinMaxScaler mm_scaler = MinMaxScaler() In\u00a0[7]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[7]: <pre>Index(['SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE', 'ZIP',\n       'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration'],\n      dtype='object')</pre> In\u00a0[8]: Copied! <pre>columns_to_scale = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'HOA PER MONTH','grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration']\n</pre> columns_to_scale = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',        'YEAR BUILT', 'HOA PER MONTH','grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',        'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',        'edu_count', 'commute_length', 'commute_duration'] In\u00a0[9]: Copied! <pre>scaled_array = mm_scaler.fit_transform(prop_df[columns_to_scale])\nprop_scaled = pd.DataFrame(scaled_array, columns=columns_to_scale)\nprop_scaled.head()\n</pre> scaled_array = mm_scaler.fit_transform(prop_df[columns_to_scale]) prop_scaled = pd.DataFrame(scaled_array, columns=columns_to_scale) prop_scaled.head() Out[9]: PRICE BEDS BATHS SQUARE FEET LOT SIZE YEAR BUILT HOA PER MONTH grocery_count restaurant_count hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration 0 0.000000 0.333333 0.1 0.016024 0.023259 0.368421 0.00 1.0 1.0 0.000000 0.38 0.0 0.56 0.82 1.00 1.0 1.00 0.005581 0.009332 1 0.041222 0.166667 0.1 0.049981 0.046518 0.315789 0.00 1.0 1.0 0.166667 0.46 0.0 0.62 0.76 1.00 1.0 1.00 0.005553 0.008905 2 0.066007 0.333333 0.1 0.028233 0.069777 0.157895 0.00 1.0 1.0 0.000000 0.32 0.0 0.68 0.94 1.00 1.0 1.00 0.005865 0.010386 3 0.095590 0.333333 0.1 0.014498 0.069777 0.736842 0.00 1.0 1.0 0.166667 0.36 0.0 0.58 0.90 0.96 1.0 1.00 0.005204 0.009486 4 0.123254 0.166667 0.0 0.025181 0.093036 0.000000 0.17 1.0 1.0 0.000000 0.26 0.0 0.34 0.88 0.20 1.0 0.82 0.005698 0.010660 In\u00a0[10]: Copied! <pre>prop_scaled['scores_scaled'] = prop_scaled.apply(set_scores, axis=1)\n</pre> prop_scaled['scores_scaled'] = prop_scaled.apply(set_scores, axis=1) <p>Check the influence of price on scores</p> In\u00a0[12]: Copied! <pre>sns.distplot(prop_scaled['scores_scaled'], bins=50)\n</pre> sns.distplot(prop_scaled['scores_scaled'], bins=50) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n</pre> Out[12]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a216e6198&gt;</pre> In\u00a0[13]: Copied! <pre>sns.jointplot(x='PRICE', y='scores_scaled', data=prop_scaled)\n</pre> sns.jointplot(x='PRICE', y='scores_scaled', data=prop_scaled) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n</pre> Out[13]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a206eab00&gt;</pre> <p></p> <p>After scaling, the scatter plot of <code>PRICE</code> VS <code>SCORES</code> appears random without much correlation. The histogram of scores also looks normally distributed. This is important as we want to consider all aspects of a house and weigh them according to their relative importance to the buyer. It is not much of a model if the scores are entirely based off one attribute (such as PRICE).</p> In\u00a0[13]: Copied! <pre>prop_df['scores_scaled'] = prop_scaled['scores_scaled']\nprop_df.head()\n</pre> prop_df['scores_scaled'] = prop_scaled['scores_scaled'] prop_df.head() Out[13]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores_scaled 0 MLS Listing Single Family Residential 144 SE 191st Pl Portland OR 97233.0 334900.0 4.0 2.5 Gresham, Sandy, Troutdale, Corbett ... 19 0 28 41 50 50 50 13.978589 25.621265 7.516793 1 MLS Listing Single Family Residential 20028 NE Hoyt St Portland OR 97230.0 349950.0 3.0 2.5 Reeds Place ... 23 0 31 38 50 50 50 13.913465 24.726944 7.563760 2 MLS Listing Single Family Residential 711 SE 160th Ave Portland OR 97233.0 358999.0 4.0 2.5 Portland Southeast ... 16 0 34 47 50 50 50 14.644863 27.830024 7.323591 3 MLS Listing Single Family Residential 5536 SE 142nd Pl Portland OR 97236.0 369800.0 4.0 2.5 Portland Southeast ... 18 0 29 45 48 50 50 13.097669 25.943438 8.036890 4 MLS Listing Single Family Residential 15707 SE Flavel Dr Portland OR 97236.0 379900.0 3.0 2.0 Portland Southeast ... 13 0 17 44 10 50 41 14.254413 28.403598 5.162985 <p>5 rows \u00d7 36 columns</p> In\u00a0[14]: Copied! <pre>sns.jointplot('PRICE', 'scores_scaled', data=prop_df)\n</pre> sns.jointplot('PRICE', 'scores_scaled', data=prop_df) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n</pre> Out[14]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1a21a4d7b8&gt;</pre> In\u00a0[15]: Copied! <pre>prop_df_sorted = prop_df.sort_values(by='scores_scaled', ascending=False)\nprop_df_sorted.head(3)\n</pre> prop_df_sorted = prop_df.sort_values(by='scores_scaled', ascending=False) prop_df_sorted.head(3) Out[15]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores_scaled 87 MLS Listing Single Family Residential 15986 SE Spokane Ct. Ave Portland OR 97236.0 543900.0 4.0 3.5 Portland Southeast ... 50 2 34 46 50 50 50 5.796321 16.509734 10.179443 170 MLS Listing Multi-Family (2-4 Unit) SE Henderson St Portland OR 97206.0 625000.0 6.0 6.0 LENTS ... 50 1 50 40 50 50 50 8.380589 23.087985 10.174764 101 MLS Listing Single Family Residential 8268 SE Yamhill St Portland OR 97216.0 550000.0 7.0 4.0 Portland Southeast ... 50 1 50 43 50 50 50 6.330796 16.910622 10.140356 <p>3 rows \u00d7 36 columns</p> In\u00a0[16]: Copied! <pre>prop_df_sorted.reset_index(drop=True, inplace=True)\nprop_df_sorted.head()\n</pre> prop_df_sorted.reset_index(drop=True, inplace=True) prop_df_sorted.head() Out[16]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores_scaled 0 MLS Listing Single Family Residential 15986 SE Spokane Ct. Ave Portland OR 97236.0 543900.0 4.0 3.5 Portland Southeast ... 50 2 34 46 50 50 50 5.796321 16.509734 10.179443 1 MLS Listing Multi-Family (2-4 Unit) SE Henderson St Portland OR 97206.0 625000.0 6.0 6.0 LENTS ... 50 1 50 40 50 50 50 8.380589 23.087985 10.174764 2 MLS Listing Single Family Residential 8268 SE Yamhill St Portland OR 97216.0 550000.0 7.0 4.0 Portland Southeast ... 50 1 50 43 50 50 50 6.330796 16.910622 10.140356 3 MLS Listing Single Family Residential 6311 SE Tenino St Portland OR 97206.0 479900.0 4.0 2.5 Portland Southeast ... 50 2 44 48 50 50 50 7.299694 20.389635 10.035397 4 MLS Listing Multi-Family (2-4 Unit) 2028 SE Harold St Portland OR 97202.0 699900.0 5.0 4.0 SELLWOOD - WEST MORELAND ... 50 2 40 45 50 50 50 3.710354 11.486135 9.958218 <p>5 rows \u00d7 36 columns</p> In\u00a0[18]: Copied! <pre>prop_df_sorted['rank'] = prop_df_sorted.index\nprop_df_sorted.head(10)\n</pre> prop_df_sorted['rank'] = prop_df_sorted.index prop_df_sorted.head(10) Out[18]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration scores_scaled rank 0 MLS Listing Single Family Residential 15986 SE Spokane Ct. Ave Portland OR 97236.0 543900.0 4.0 3.5 Portland Southeast ... 2 34 46 50 50 50 5.796321 16.509734 10.179443 0 1 MLS Listing Multi-Family (2-4 Unit) SE Henderson St Portland OR 97206.0 625000.0 6.0 6.0 LENTS ... 1 50 40 50 50 50 8.380589 23.087985 10.174764 1 2 MLS Listing Single Family Residential 8268 SE Yamhill St Portland OR 97216.0 550000.0 7.0 4.0 Portland Southeast ... 1 50 43 50 50 50 6.330796 16.910622 10.140356 2 3 MLS Listing Single Family Residential 6311 SE Tenino St Portland OR 97206.0 479900.0 4.0 2.5 Portland Southeast ... 2 44 48 50 50 50 7.299694 20.389635 10.035397 3 4 MLS Listing Multi-Family (2-4 Unit) 2028 SE Harold St Portland OR 97202.0 699900.0 5.0 4.0 SELLWOOD - WEST MORELAND ... 2 40 45 50 50 50 3.710354 11.486135 9.958218 4 5 New Construction Home Single Family Residential 16010 SE Spokane Ct Portland OR 97236.0 518750.0 3.0 2.5 Peach Tree Meadows ... 2 34 46 50 50 50 5.796321 16.509734 9.929172 5 6 MLS Listing Multi-Family (2-4 Unit) 5850 SE 86th Ave Portland OR 97266.0 699000.0 6.0 6.0 LENTS ... 1 50 38 50 50 50 10.899918 21.144505 9.924611 6 7 MLS Listing Townhouse 2022 SE Harold St Portland OR 97202.0 699900.0 5.0 4.0 SELLWOOD - WEST MORELAND ... 2 40 45 50 50 50 3.713178 11.513058 9.880673 7 8 MLS Listing Single Family Residential 15965 SE Spokane Ct Portland OR 97236.0 488750.0 3.0 2.5 Portland Southeast ... 2 34 46 50 50 50 5.796321 16.509734 9.794684 8 9 MLS Listing Single Family Residential 16136 SE Tenino St Portland OR 97236.0 479900.0 3.0 2.5 Portland Southeast ... 2 36 46 50 50 50 4.998670 13.217221 9.761623 9 <p>10 rows \u00d7 37 columns</p> In\u00a0[29]: Copied! <pre>top_10_map = gis.map('Portland, OR')\ntop_10_map.basemap = 'gray'\ntop_10_map\n</pre> top_10_map = gis.map('Portland, OR') top_10_map.basemap = 'gray' top_10_map <pre>MapView(basemap='gray', layout=Layout(height='400px', width='100%'))</pre> <p></p> In\u00a0[39]: Copied! <pre>prop_df_sorted.head(50).spatial.plot(map_widget = top_10_map, \n                      renderer_type='c',\n                     method='esriClassifyNaturalBreaks',  # classification scheme\n                     class_count=10,  # between 1900 - 2000, each decade in a class\n                     col='scores_scaled',\n                     cmap='autumn',  # matplotlib color map\n                     alpha=0.7,\n                     outline_color=[0,0,0,0])\n</pre> prop_df_sorted.head(50).spatial.plot(map_widget = top_10_map,                        renderer_type='c',                      method='esriClassifyNaturalBreaks',  # classification scheme                      class_count=10,  # between 1900 - 2000, each decade in a class                      col='scores_scaled',                      cmap='autumn',  # matplotlib color map                      alpha=0.7,                      outline_color=[0,0,0,0]) <pre>/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/arcgis/features/geo/_accessor.py:861: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._data[col] = self._data[col]\n/Users/atma6951/anaconda3/envs/geosaurus_gold/lib/python3.6/site-packages/arcgis/features/geo/_accessor.py:1968: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._data['OBJECTID'] = list(range(1, self._data.shape[0] + 1))\n</pre> Out[39]: <pre>True</pre> In\u00a0[19]: Copied! <pre>interesting_columns=['grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration','PRICE','BEDS','BATHS','YEAR BUILT','SQUARE FEET']\n\nax_list = prop_df_sorted[interesting_columns].iloc[:49].hist(bins=25, layout=(5,4), figsize=(15,15))\nplt.tight_layout()\n</pre> interesting_columns=['grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',        'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',        'edu_count', 'commute_length', 'commute_duration','PRICE','BEDS','BATHS','YEAR BUILT','SQUARE FEET']  ax_list = prop_df_sorted[interesting_columns].iloc[:49].hist(bins=25, layout=(5,4), figsize=(15,15)) plt.tight_layout() In\u00a0[54]: Copied! <pre>column_set1 = ['hospitals_count','commute_length','commute_duration',\n               'gas_count', 'shops_count', 'rank']\ncolumn_set2 = ['rank','PRICE','BEDS','BATHS','YEAR BUILT','SQUARE FEET']\n\ngrd = sns.pairplot(data=prop_df_sorted[column_set1].iloc[:49])\nplt.title('How is rank correlated with location properties of a house?')\n</pre> column_set1 = ['hospitals_count','commute_length','commute_duration',                'gas_count', 'shops_count', 'rank'] column_set2 = ['rank','PRICE','BEDS','BATHS','YEAR BUILT','SQUARE FEET']  grd = sns.pairplot(data=prop_df_sorted[column_set1].iloc[:49]) plt.title('How is rank correlated with location properties of a house?') Out[54]: <pre>Text(0.5,1,'How is rank correlated with location properties of a house?')</pre> In\u00a0[55]: Copied! <pre>grd = sns.pairplot(data=prop_df_sorted[column_set2].iloc[:49])\nplt.title('How is rank correlated with intrinsic properties of a house?')\n</pre> grd = sns.pairplot(data=prop_df_sorted[column_set2].iloc[:49]) plt.title('How is rank correlated with intrinsic properties of a house?') Out[55]: <pre>Text(0.5,1,'How is rank correlated with intrinsic properties of a house?')</pre> In\u00a0[40]: Copied! <pre>prop_df_sorted.to_csv('resources/houses_ranked.csv')\nprop_df_sorted.spatial.to_featureclass('resources/shp/houses_ranked.shp')\n</pre> prop_df_sorted.to_csv('resources/houses_ranked.csv') prop_df_sorted.spatial.to_featureclass('resources/shp/houses_ranked.shp') Out[40]: <pre>'resources/shp/houses_ranked.shp'</pre>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#score-and-rank-properties-using-intrinsic-and-spatial-features","title":"Score and rank properties using intrinsic and spatial features\u00b6","text":"<p>So far we have shortlisted properties based on their intrinsic properties such as number of rooms, price, HoA etc. We have enriched them with spatial attributes such as access to facilities and distance to a point of interest (such as work). In this notebook, we weigh and sum these attributes to produce a score for each property. We finally sort them to pick the top few.</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#read-spatially-enriched-properties","title":"Read spatially enriched properties\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#apply-weights","title":"Apply weights\u00b6","text":"<p>In the following section, we determine the relative importance of each attribute and set weights accordingly</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#generate-scores-for-all-properties","title":"Generate scores for all properties\u00b6","text":"<p>We calculate the score for each property using the formula we defined in previous section</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#write-unscaled-data-to-disk","title":"Write unscaled data to disk\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#scale-data","title":"Scale data\u00b6","text":"<p>Although price plays an important role while selecting properties, the range and valuds of the <code>PRICE</code> column is much larger than any other. Thus, it influences more than its fair share. To resolve this, we scale the data using different techniques.</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#apply-scores-on-scaled-data","title":"Apply scores on scaled data\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#inverse-transform-data","title":"Inverse transform data\u00b6","text":"<p>Since we made a copy of the DataFrame when we scaled, we don't need to inverse transfrom the scaled data. We can simply copy the <code>scaled_scores</code> from this DataFrame and apply that to the original DataFrame</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#sort-by-scores","title":"Sort by scores\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#apply-a-rank-for-each-property-based-on-the-scores","title":"Apply a rank for each property based on the scores\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#visualize-the-top-50-properties","title":"Visualize the top 50 properties\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#investigate-statistical-distribution-facilities-in-the-top-50-shortlist","title":"Investigate statistical distribution facilities in the top 50 shortlist\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#how-is-rank-correlated-with-location-properties-of-houses","title":"How is rank correlated with location properties of houses?\u00b6","text":"<p>In the charts below we attempt to find if there exists a correlation between different facilities.</p>"},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#how-is-rank-correlated-with-intrinsic-properties-of-houses","title":"How is rank correlated with intrinsic properties of houses?\u00b6","text":""},{"location":"talks/portland-house-hunting/05-rank-properties-using-features/#save-to-disk","title":"Save to disk\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/","title":"Building a housing recommendation engine","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n%matplotlib inline\nimport seaborn as sns\n\nfrom arcgis.gis import GIS\nfrom arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor\n</pre> import pandas as pd import matplotlib.pyplot as plt from pprint import pprint %matplotlib inline import seaborn as sns  from arcgis.gis import GIS from arcgis.features import Feature, FeatureLayer, FeatureSet, GeoAccessor, GeoSeriesAccessor In\u00a0[2]: Copied! <pre>prop_df = pd.read_csv('resources/houses_ranked.csv')\nprop_df = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE')\n</pre> prop_df = pd.read_csv('resources/houses_ranked.csv') prop_df = pd.DataFrame.spatial.from_xy(prop_df, 'LONGITUDE','LATITUDE') In\u00a0[3]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[3]: <pre>Index(['Unnamed: 0', 'SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE',\n       'ZIP', 'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration', 'scores_scaled',\n       'rank'],\n      dtype='object')</pre> In\u00a0[4]: Copied! <pre>prop_df.shape\n</pre> prop_df.shape Out[4]: <pre>(331, 38)</pre> <p>Generate a prefernce list that is <code>331</code> records long. This list has <code>1</code> for first <code>50</code> records followed by <code>0</code>.</p> In\u00a0[5]: Copied! <pre>preference_list = [1]*50\npreference_list.extend([0]*(331-50))\nlen(preference_list)\n</pre> preference_list = [1]*50 preference_list.extend([0]*(331-50)) len(preference_list) Out[5]: <pre>331</pre> In\u00a0[6]: Copied! <pre>prop_df['favorite'] = preference_list\n</pre> prop_df['favorite'] = preference_list In\u00a0[7]: Copied! <pre>prop_df.drop(columns=['Unnamed: 0','scores_scaled','rank'], inplace=True)\nprop_df.head()\n</pre> prop_df.drop(columns=['Unnamed: 0','scores_scaled','rank'], inplace=True) prop_df.head() Out[7]: SALE TYPE PROPERTY TYPE ADDRESS CITY STATE ZIP PRICE BEDS BATHS LOCATION ... coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration favorite 0 MLS Listing Single Family Residential 15986 SE Spokane Ct. Ave Portland OR 97236.0 543900.0 4.0 3.5 Portland Southeast ... 50 2 34 46 50 50 50 5.796321 16.509734 1 1 MLS Listing Multi-Family (2-4 Unit) SE Henderson St Portland OR 97206.0 625000.0 6.0 6.0 LENTS ... 50 1 50 40 50 50 50 8.380589 23.087985 1 2 MLS Listing Single Family Residential 8268 SE Yamhill St Portland OR 97216.0 550000.0 7.0 4.0 Portland Southeast ... 50 1 50 43 50 50 50 6.330796 16.910622 1 3 MLS Listing Single Family Residential 6311 SE Tenino St Portland OR 97206.0 479900.0 4.0 2.5 Portland Southeast ... 50 2 44 48 50 50 50 7.299694 20.389635 1 4 MLS Listing Multi-Family (2-4 Unit) 2028 SE Harold St Portland OR 97202.0 699900.0 5.0 4.0 SELLWOOD - WEST MORELAND ... 50 2 40 45 50 50 50 3.710354 11.486135 1 <p>5 rows \u00d7 36 columns</p> In\u00a0[8]: Copied! <pre>prop_df.columns\n</pre> prop_df.columns Out[8]: <pre>Index(['SALE TYPE', 'PROPERTY TYPE', 'ADDRESS', 'CITY', 'STATE', 'ZIP',\n       'PRICE', 'BEDS', 'BATHS', 'LOCATION', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'DAYS ON MARKET', 'PRICE PER SQFT', 'HOA PER MONTH',\n       'STATUS', 'URL', 'SOURCE', 'MLS', 'LATITUDE', 'LONGITUDE', 'SHAPE',\n       'grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration', 'favorite'],\n      dtype='object')</pre> In\u00a0[11]: Copied! <pre>train_df = prop_df.drop(columns=['SALE TYPE','PROPERTY TYPE','ADDRESS', 'CITY', 'STATE', 'ZIP','LOCATION', \n                                'DAYS ON MARKET','PRICE PER SQFT','STATUS',\n                                 'URL', 'SOURCE', 'MLS', 'SHAPE','LATITUDE', 'LONGITUDE'])\ntrain_df.head()\n</pre> train_df = prop_df.drop(columns=['SALE TYPE','PROPERTY TYPE','ADDRESS', 'CITY', 'STATE', 'ZIP','LOCATION',                                  'DAYS ON MARKET','PRICE PER SQFT','STATUS',                                  'URL', 'SOURCE', 'MLS', 'SHAPE','LATITUDE', 'LONGITUDE']) train_df.head() Out[11]: PRICE BEDS BATHS SQUARE FEET LOT SIZE YEAR BUILT HOA PER MONTH grocery_count restaurant_count hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration favorite 0 543900.0 4.0 3.5 3178.0 6969.0 2018.0 50.0 20 50 6 50 2 34 46 50 50 50 5.796321 16.509734 1 1 625000.0 6.0 6.0 2844.0 6969.0 2018.0 0.0 20 50 6 50 1 50 40 50 50 50 8.380589 23.087985 1 2 550000.0 7.0 4.0 3038.0 6969.0 2018.0 0.0 20 50 4 50 1 50 43 50 50 50 6.330796 16.910622 1 3 479900.0 4.0 2.5 2029.0 3920.0 2018.0 0.0 20 50 6 50 2 44 48 50 50 50 7.299694 20.389635 1 4 699900.0 5.0 4.0 2582.0 6969.0 2016.0 0.0 20 50 8 50 2 40 45 50 50 50 3.710354 11.486135 1 In\u00a0[12]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()\n</pre> from sklearn.preprocessing import MinMaxScaler mm_scaler = MinMaxScaler() In\u00a0[13]: Copied! <pre>columns_to_scale = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n       'YEAR BUILT', 'HOA PER MONTH','grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',\n       'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',\n       'edu_count', 'commute_length', 'commute_duration']\n</pre> columns_to_scale = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',        'YEAR BUILT', 'HOA PER MONTH','grocery_count', 'restaurant_count', 'hospitals_count', 'coffee_count',        'bars_count', 'gas_count', 'shops_count', 'travel_count', 'parks_count',        'edu_count', 'commute_length', 'commute_duration'] In\u00a0[15]: Copied! <pre>scaled_array = mm_scaler.fit_transform(train_df[columns_to_scale])\nprop_scaled = pd.DataFrame(scaled_array, columns=columns_to_scale)\nprop_scaled.head()\n</pre> scaled_array = mm_scaler.fit_transform(train_df[columns_to_scale]) prop_scaled = pd.DataFrame(scaled_array, columns=columns_to_scale) prop_scaled.head() Out[15]: PRICE BEDS BATHS SQUARE FEET LOT SIZE YEAR BUILT HOA PER MONTH grocery_count restaurant_count hospitals_count coffee_count bars_count gas_count shops_count travel_count parks_count edu_count commute_length commute_duration 0 0.572446 0.333333 0.3 0.448684 0.100778 0.947368 0.25 1.0 1.0 0.500000 1.0 1.0 0.68 0.92 1.0 1.0 1.0 0.002085 0.004983 1 0.794577 0.666667 0.8 0.321251 0.100778 0.947368 0.00 1.0 1.0 0.500000 1.0 0.5 1.00 0.80 1.0 1.0 1.0 0.003189 0.008123 2 0.589154 0.833333 0.4 0.395269 0.100778 0.947368 0.00 1.0 1.0 0.333333 1.0 0.5 1.00 0.86 1.0 1.0 1.0 0.002313 0.005175 3 0.397151 0.333333 0.1 0.010301 0.046518 0.947368 0.00 1.0 1.0 0.500000 1.0 1.0 0.88 0.96 1.0 1.0 1.0 0.002727 0.006835 4 0.999726 0.500000 0.4 0.221290 0.100778 0.842105 0.00 1.0 1.0 0.666667 1.0 1.0 0.80 0.90 1.0 1.0 1.0 0.001193 0.002586 In\u00a0[29]: Copied! <pre>prop_scaled.columns\n</pre> prop_scaled.columns Out[29]: <pre>Index(['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE', 'YEAR BUILT',\n       'HOA PER MONTH', 'grocery_count', 'restaurant_count', 'hospitals_count',\n       'coffee_count', 'bars_count', 'gas_count', 'shops_count',\n       'travel_count', 'parks_count', 'edu_count', 'commute_length',\n       'commute_duration'],\n      dtype='object')</pre> In\u00a0[30]: Copied! <pre>prop_scaled.info()\n</pre> prop_scaled.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 331 entries, 0 to 330\nData columns (total 19 columns):\nPRICE               331 non-null float64\nBEDS                331 non-null float64\nBATHS               331 non-null float64\nSQUARE FEET         331 non-null float64\nLOT SIZE            331 non-null float64\nYEAR BUILT          331 non-null float64\nHOA PER MONTH       331 non-null float64\ngrocery_count       331 non-null float64\nrestaurant_count    331 non-null float64\nhospitals_count     331 non-null float64\ncoffee_count        331 non-null float64\nbars_count          331 non-null float64\ngas_count           331 non-null float64\nshops_count         331 non-null float64\ntravel_count        331 non-null float64\nparks_count         331 non-null float64\nedu_count           331 non-null float64\ncommute_length      331 non-null float64\ncommute_duration    331 non-null float64\ndtypes: float64(19)\nmemory usage: 49.2 KB\n</pre> In\u00a0[31]: Copied! <pre>X = prop_scaled\ny = train_df['favorite']\n</pre> X = prop_scaled y = train_df['favorite'] In\u00a0[32]: Copied! <pre>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33)\n\n(len(X_train), len(X_test))\n</pre> from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33)  (len(X_train), len(X_test)) Out[32]: <pre>(221, 110)</pre> In\u00a0[33]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(verbose=1)\n</pre> from sklearn.linear_model import LogisticRegression log_model = LogisticRegression(verbose=1) In\u00a0[34]: Copied! <pre>log_model.fit(X_train, y_train)\n</pre> log_model.fit(X_train, y_train) <pre>[LibLinear]</pre> Out[34]: <pre>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=1, warm_start=False)</pre> In\u00a0[35]: Copied! <pre>test_predictions = log_model.predict(X_test)\n</pre> test_predictions = log_model.predict(X_test) In\u00a0[36]: Copied! <pre>test_predictions\n</pre> test_predictions Out[36]: <pre>array([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0])</pre> In\u00a0[37]: Copied! <pre>from sklearn.metrics import classification_report, confusion_matrix\n</pre> from sklearn.metrics import classification_report, confusion_matrix In\u00a0[39]: Copied! <pre>from pprint import pprint\npprint(classification_report(y_test, test_predictions, target_names=['not fav','fav']))\n</pre> from pprint import pprint pprint(classification_report(y_test, test_predictions, target_names=['not fav','fav'])) <pre>('             precision    recall  f1-score   support\\n'\n '\\n'\n '    not fav       0.94      0.98      0.96        89\\n'\n '        fav       0.88      0.71      0.79        21\\n'\n '\\n'\n 'avg / total       0.93      0.93      0.92       110\\n')\n</pre> In\u00a0[40]: Copied! <pre>tn, fp, fn, tp = confusion_matrix(y_test, test_predictions).ravel()\ntn, fp, fn, tp\n</pre> tn, fp, fn, tp = confusion_matrix(y_test, test_predictions).ravel() tn, fp, fn, tp Out[40]: <pre>(87, 2, 6, 15)</pre> In\u00a0[41]: Copied! <pre>coeff = log_model.coef_.round(5).tolist()[0]\nlist(zip(X_train.columns, coeff))\n</pre> coeff = log_model.coef_.round(5).tolist()[0] list(zip(X_train.columns, coeff)) Out[41]: <pre>[('PRICE', -0.4817),\n ('BEDS', 0.56799),\n ('BATHS', 0.65258),\n ('SQUARE FEET', 0.09618),\n ('LOT SIZE', -0.10108),\n ('YEAR BUILT', 0.86107),\n ('HOA PER MONTH', 0.02129),\n ('grocery_count', -0.7736),\n ('restaurant_count', -1.22493),\n ('hospitals_count', 1.38967),\n ('coffee_count', 1.27494),\n ('bars_count', 2.9728),\n ('gas_count', 1.16501),\n ('shops_count', -0.71489),\n ('travel_count', 0.24195),\n ('parks_count', -1.02031),\n ('edu_count', -0.45057),\n ('commute_length', -0.58029),\n ('commute_duration', -0.59949)]</pre> In\u00a0[44]: Copied! <pre>log_model.intercept_\n</pre> log_model.intercept_ Out[44]: <pre>array([-1.46905595])</pre>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#building-a-housing-recommendation-engine","title":"Building a housing recommendation engine\u00b6","text":"<p>So far, we have feature engineered our data set with location specific features. We explicitly defined weights for different attributes and arrived at a rank. Instead, we could simply like and dislike a few houses and let a machine learning model infer our preferences based on that. That is what this notebook tries to do.</p> <p>Since it is time consuming to like and dislike a large number of properties, we pick the top 50 notebooks from our previous rank and like them all. We dislike the remaining ones.</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#read-ranked-dataset","title":"Read ranked dataset\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#generate-preference-column","title":"Generate preference column\u00b6","text":"<p>We will pick the top 50 records and provide a positive preference to them. Then we will drop the score and rank columns and let the machine learning algorithm learn our preferences.</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#drop-rank-scores_scaled-columns-from-dataframe","title":"Drop <code>rank</code>, <code>scores_scaled</code> columns from DataFrame\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#one-hot-encoding","title":"One hot encoding\u00b6","text":"<p>We drop more columns that don't really determine a buyer's preference</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#scale-numeric-columns","title":"Scale numeric columns\u00b6","text":"<p>We use the same <code>MinMaxScaler</code> we used earlier to scale the data.</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#split-dataset-into-training-and-test","title":"Split dataset into training and test\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#logistic-regression","title":"Logistic Regression\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#model-evaluation","title":"Model evaluation\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#model-inference","title":"Model inference\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#recommendation-engines","title":"Recommendation engines\u00b6","text":"<p>From the example above, we could build a recommendation engine that runs on periodically on a newer set of properties and determines which ones are worth your time (one's it predicts you would 'like'). The ML model's weights appear similar to what we defined manually. In some cases, it goes way off.</p> <p>This type of recommendation is called 'content based filtering' and for this to work, we need a really large training set. In reality nobody can sit and generate such a large set. In practice, another type of recommendations called 'community based filtering' is used. Based on the features identified for the properties, it tries to find similarity between buyers and pools the training set for all similar buyers together to create a really large training set and learns from that.</p>"},{"location":"talks/portland-house-hunting/06-build-recommendation-engine-scaled/#overall","title":"Overall\u00b6","text":"<p>In these sets of notebooks, we observed how data science and machine learning approaches can be employed in the real estate industry. Buying houses is a very personal process, however a lot of decisions are heavily influenced by the location of the houses. We showed how Python libraries such as Pandas can be used to statistically analyze the properties. We also showed how the ArcGIS API for Python adds spatial capabilities to Pandas allowing to perform spatial data analysis. We enriched the data with information on access to different facilities and used that to compare, score and rank the properties. The shortlist we arrived at can be used for field visits.</p> <p>We conclude with a forward thinking approach to turn this into a recommendation engine and suggest scope for future work in this area.</p>"},{"location":"blog/category/parallel-processing/","title":"parallel processing","text":""},{"location":"blog/category/python/","title":"python","text":""},{"location":"blog/category/threads/","title":"threads","text":""},{"location":"blog/category/processes/","title":"processes","text":""},{"location":"blog/category/serverless/","title":"serverless","text":""},{"location":"blog/category/aws/","title":"aws","text":""},{"location":"blog/category/cloud/","title":"cloud","text":""},{"location":"blog/category/arcgis/","title":"arcgis","text":""},{"location":"blog/category/data-science/","title":"data-science","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/page/5/","title":"Blog","text":""},{"location":"blog/page/6/","title":"Blog","text":""},{"location":"blog/page/7/","title":"Blog","text":""}]}